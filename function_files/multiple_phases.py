########################################################################################################################
#
#   MULITPLE_PHASES
#
#   Description: contains classes that are used to perform analysis on several phases (e.g. sleep and exploration)
#
#   Author: Lars Bollmann
#
#   Created: 26/06/2020
#
#   Structure:
#
########################################################################################################################

from collections import OrderedDict
from functools import partial
from matplotlib import colors
from itertools import combinations
from .support_functions import (upper_tri_without_diag, find_hse, compute_sparsity, moving_average, \
    cross_correlate, multi_dim_scaling, perform_TSNE, perform_isomap, perform_PCA, angle_between_col_vectors, \
    evaluate_clustering_fit, down_sample_array_sum, down_sample_array_mean, transition_matrix, bayes_likelihood, \
    nr_goals_coded_per_cell, goal_coding_per_cell, nr_goals_coded_subset_of_cells, distance_peak_firing_to_closest_goal,\
    decode_using_phmm_modes, read_integers, read_arrays, cross_correlate_matrices, compute_correlations_col_fast, \
    partial_correlations, constant_nr_spike_bin_from_mean_firing, decode_using_phmm_modes_fast, decode_using_ising_map,
    decode_using_ising_map_fast, RMM, p_from_cdf, NonLinearNormalize, normalize)
from .single_phase import Sleep, Exploration, Cheeseboard
from .ml_methods import MlMethodsOnePopulation, MlMethodsTwoPopulations, PoissonHMM
from .plotting_functions import plot_3D_scatter, plot_2D_scatter, scatter_animation
from matplotlib.colors import LogNorm, PowerNorm, FuncNorm
from matplotlib.animation import FuncAnimation, PillowWriter
from matplotlib.colors import LogNorm
from matplotlib import animation
from scipy.special import factorial, gammaln
from mpl_toolkits.mplot3d import Axes3D
from scipy.spatial.distance import pdist
from scipy.spatial.distance import squareform
from scipy.cluster.vq import kmeans2
from scipy.ndimage import uniform_filter1d
from sklearn.mixture import GaussianMixture
from scipy.cluster.hierarchy import fclusterdata
import matplotlib.pyplot as plt
import numpy as np
import sys
import time
import psutil
import pickle
import multiprocessing as mp
from matplotlib.ticker import MaxNLocator
import itertools
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import StratifiedShuffleSplit
import copy
import seaborn as sns
import matplotlib.patches as patches
from matplotlib import collections as matcoll
from scipy.stats import ks_2samp
from scipy.spatial.distance import pdist, squareform, cdist
import matplotlib.cm as cm
from statsmodels.tsa import stattools
from scipy.spatial import distance
from scipy import stats
from scipy import optimize
from scipy import fft
from sklearn.manifold import MDS, TSNE
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from scipy.stats import pearsonr, entropy, spearmanr, sem, mannwhitneyu, wilcoxon, ks_2samp, multivariate_normal, \
      zscore, f_oneway, ttest_ind, binom_test, ttest_1samp
from sklearn.metrics import pairwise_distances
from sklearn import svm
from scipy.stats import mode as mode_of_dist
from scipy.stats import kstest
from scipy.signal import correlate
import pandas as pd
import os
import scipy.cluster.hierarchy as sch
from scipy.spatial.distance import squareform
from scipy.signal import correlate2d
from scipy.stats import gaussian_kde
from sklearn import preprocessing
from sklearn.metrics import pairwise_distances
from numpy.linalg import norm
from scipy.ndimage.filters import uniform_filter1d
import matplotlib

matplotlib.rcParams['pdf.fonttype'] = 42
matplotlib.rcParams['ps.fonttype'] = 42

# define default location to save plots
save_path = os.path.dirname(os.path.realpath(__file__)) + "/../plots"


class LongSleep:
    """Class for long sleep"""

    def __init__(self, sleep_data_obj, params, session_params):
        self.params = params
        self.session_params = session_params
        self.cell_type = sleep_data_obj.get_cell_type()
        self.long_sleep = []
        self.session_name = session_params.session_name

        # check if extended data (incl. LFP)  needs to be loaded
        if params.data_to_use == "std":
            all_data_dic = sleep_data_obj.get_standard_data()
        elif params.data_to_use == "ext":
            all_data_dic = sleep_data_obj.get_extended_data()
        elif params.data_to_use == "ext_eegh":
            all_data_dic = sleep_data_obj.get_extended_data(which_file="eegh")

        # initialize each phase
        for phase_id, phase in enumerate(sleep_data_obj.data_description):
            data_dic =all_data_dic[phase_id]
            self.long_sleep.append(Sleep(data_dic=[data_dic], cell_type=sleep_data_obj.get_cell_type(),
                                         params=self.params, session_params=self.session_params,
                                         experiment_phase=phase))

    # <editor-fold desc="Getter methods & saving & plotting">

    def check_sleep(self):
        speed = np.zeros(0)
        for sleep in self.long_sleep:
            sleep_part = sleep.get_speed()
            np.hstack((speed, sleep_part))
        plt.plot(speed)
        plt.ylabel("SPEED")
        plt.xlabel("TIME")
        plt.title("SPEED DURING LONG SLEEP")
        plt.show()

    def sleep_stage_durations(self, plotting=False, save_fig=False):

        speed_threshold = self.session_params.sleep_phase_speed_threshold

        rem = []
        nrem = []
        for sleep in self.long_sleep:
            r_ = sleep.get_sleep_phase(sleep_phase="rem", speed_threshold=speed_threshold)
            nr_ = sleep.get_sleep_phase(sleep_phase="nrem", speed_threshold=speed_threshold)
            rem.append(r_)
            nrem.append(nr_)

        rem = np.vstack(rem)
        nrem = np.vstack(nrem)

        dur_rem = rem[:, 1] - rem[:, 0]
        dur_nrem = nrem[:, 1] - nrem[:, 0]

        if plotting or save_fig:
            if save_fig:
                plt.style.use('default')
            dur_rem_sorted = np.sort(dur_rem)
            p_dur_rem = 1. * np.arange(dur_rem_sorted.shape[0]) / (dur_rem_sorted.shape[0] - 1)
            dur_nrem_sorted = np.sort(dur_nrem)
            p_dur_nrem = 1. * np.arange(dur_nrem_sorted.shape[0]) / (dur_nrem_sorted.shape[0] - 1)
            plt.plot(dur_rem_sorted, p_dur_rem, color="red", label="REM")
            plt.plot(dur_nrem_sorted, p_dur_nrem, color="blue", label="NREM")
            plt.legend()
            plt.ylabel("CDF")
            plt.xlabel("Duration (s)")
            plt.title("Example session")
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "rem_nrem_duration_example.svg"), transparent="True")
            else:
                plt.show()

        return dur_rem, dur_nrem

    def sleep_duration_h(self):
        duration = []
        for sleep in self.long_sleep:
            r = sleep.get_duration_sec()
            duration.append(r)
        duration =np.sum(duration)
        duration_h =duration/(60*60)
        print("Duration in h: "+str(duration_h))
        return duration_h

    def get_sleep_raster(self):
        raster = []
        for sleep in self.long_sleep:
            r = sleep.get_raster()
            raster.append(r)
        raster = np.hstack(raster)
        return raster

    def get_sleep_phase_raster(self, sleep_phase):

        raster_list = []
        for sleep in self.long_sleep:
            r, _, _ = sleep.get_event_time_bin_rasters(sleep_phase=sleep_phase)
            raster_list.append(r)
        # TODO: check why simple np.hstack() does not work!
        raster = []
        for raster_entry in raster_list:
            raster.append(np.hstack(raster_entry))

        raster = np.hstack(raster)
        return raster

    def get_constant_spike_bin_length(self, plotting=False, return_median=False):

        rem_bin_length = []
        for sleep in self.long_sleep:
            _, event_spike_window_lengths = sleep.get_event_spike_rasters(part_to_analyze="rem")
            rem_bin_length.append(np.hstack(event_spike_window_lengths))
        rem_bin_length = np.hstack(rem_bin_length)


        nrem_bin_length = []

        for sleep in self.long_sleep:
            _, event_spike_window_lengths = sleep.get_event_spike_rasters(part_to_analyze="nrem")
            if len(event_spike_window_lengths) > 0:
                nrem_bin_length.append(np.hstack(event_spike_window_lengths))

        nrem_bin_length = np.hstack(nrem_bin_length)

        if plotting:
            p_rem = 1. * np.arange(rem_bin_length.shape[0]) / (rem_bin_length.shape[0] - 1)
            p_nrem = 1. * np.arange(nrem_bin_length.shape[0]) / (nrem_bin_length.shape[0] - 1)
            plt.plot(np.sort(nrem_bin_length), p_nrem, color="blue", label="NREM")
            plt.plot(np.sort(rem_bin_length), p_rem, color="red", label="REM")
            plt.legend()
            plt.xscale("log")
            plt.ylabel("cdf")
            plt.xlabel("12-spike-bin duration (s)")
            plt.show()
        else:
            if return_median:
                return np.median(nrem_bin_length), np.median(rem_bin_length)
            else:
                return nrem_bin_length, rem_bin_length

    def save_spike_rasters(self, part_to_analyze, file_name):
        """
        save constant #spike rasters for either REM, SWR, or NREM

        :param part_to_analyze: which sleep epoch to return ("REM","NREM")
        :type part_to_analyze: str
        :param file_name: where to save
        :type file_name: str
        """
        all_rasters = []
        for l_s in self.long_sleep:
            event_spike_rasters, event_spike_window_lenghts=l_s.get_event_spike_rasters(part_to_analyze=part_to_analyze)
            all_rasters.extend(event_spike_rasters)
        with open(file_name, "wb") as fp:  # Pickling
            pickle.dump(all_rasters, fp)

    def mean_lfp_for_interval(self, intervals_s):
        """
        assesses mean lfp for intervals

        :param intervals_s: start and end of each interval in seconds
        :type intervals_s: np.array

        :return: array with average firing rates in Hz
        """
        # get stable, decreasing, increasing cells

        mean_lfp = []
        start_s = 0
        end_s = 0
        good_intervals = np.zeros(intervals_s.shape[0])
        for l_s in self.long_sleep:
            end_s += l_s.get_duration_sec()
            # only select intervals for the corresponding sleep chunk
            intervals_chunk = intervals_s[np.logical_and(start_s <= intervals_s[:,0], intervals_s[:,1] <= end_s)]
            good_intervals[np.logical_and(start_s <= intervals_s[:,0], intervals_s[:,1] <= end_s)] = 1
            # need to offset by start of the chunk
            intervals_chunk = intervals_chunk - start_s
            mean_lfp_ = l_s.mean_lfp_for_interval(intervals_s=intervals_chunk)
            mean_lfp.append(mean_lfp_)
            start_s += l_s.get_duration_sec()

        mean_lfp = np.hstack(mean_lfp)
        mean_lfp_good = np.ones(intervals_s.shape[0])*np.nan
        mean_lfp_good[good_intervals.astype(bool)] = mean_lfp

        return mean_lfp_good


    def number_swr_for_interval(self, intervals_s):
        """
        assesses mean lfp for intervals

        :param intervals_s: start and end of each interval in seconds
        :type intervals_s: np.array

        :return: array with average firing rates in Hz
        """
        # get stable, decreasing, increasing cells

        nr_swrs = []
        start_s = 0
        end_s = 0
        good_intervals = np.zeros(intervals_s.shape[0])
        for l_s in self.long_sleep:
            end_s += l_s.get_duration_sec()
            # only select intervals for the corresponding sleep chunk
            intervals_chunk = intervals_s[np.logical_and(start_s <= intervals_s[:,0], intervals_s[:,1] <= end_s)]
            good_intervals[np.logical_and(start_s <= intervals_s[:,0], intervals_s[:,1] <= end_s)] = 1
            # need to offset by start of the chunk
            intervals_chunk = intervals_chunk - start_s
            nr_swr_ = l_s.nr_swr_for_interval(intervals_s=intervals_chunk)
            nr_swrs.append(nr_swr_)
            start_s += l_s.get_duration_sec()

        nr_swrs = np.hstack(nr_swrs)
        nr_swrs_good = np.ones(intervals_s.shape[0])*np.nan
        nr_swrs_good[good_intervals.astype(bool)] = nr_swrs

        return nr_swrs_good

    # </editor-fold>

    # <editor-fold desc="Cell classification">

    def classify_cells_firing_rate_polyfit(self):
        raster = []
        for l_s in self.long_sleep:
            raster.append(l_s.get_raster())

        raster = np.hstack(raster)

        m = []

        for cell_id, cell_firing in enumerate(raster):
            smooth_firing = moving_average(a=cell_firing, n=1000)
            smooth_firing /= np.max(smooth_firing)
            # compute regression
            coef = np.polyfit(np.arange(smooth_firing.shape[0])*self.params.time_bin_size, smooth_firing, 1)
            m.append(coef[0])
            # if abs(coef[0]) < 5e-6:
            #     stable_cells_k_means.append(cell_id)
            # # if abs(coef[0]) < 2e-6:
            #     poly1d_fn = np.poly1d(coef)
            #     plt.plot(smooth_firing)
            #     plt.plot(np.arange(smooth_firing.shape[0]), poly1d_fn(np.arange(smooth_firing.shape[0])), '--w')
            #     plt.title(coef)
            #     plt.show()

        m = np.array(m)
        # find stable cells: < median --> not a good measure, but for the time being
        stable_cells = np.squeeze(np.argwhere(np.abs(m) < np.median(np.abs(m))))

        plt.hist(np.abs(np.array(m)), bins=80)
        plt.xlabel("M")
        plt.ylabel("DENSITY")
        print(np.median(np.array(np.abs(m))))
        plt.show()

        np.save(self.params.pre_proc_dir+"stable_cells_k_means/"+self.params.session_name+"_regression", np.array(stable_cells))

    def classifiy_cells_firing_rate_distribution(self, alpha=0.01, test="ks"):

        raster = []
        for l_s in self.long_sleep:
            raster.append(l_s.get_raster())

        raster = np.hstack(raster)

        raster_1 = raster[:, :int(raster.shape[1]/10)]
        raster_2 = raster[:, -int(raster.shape[1]/10):]

        stable_cell_ids = []
        increase_cell_ids = []
        decrease_cell_ids = []
        for cell_id, (cell_fir_bef, cell_fir_aft) in enumerate(zip(raster_1, raster_2)):
            if test == "ks":
                if ks_2samp(data1=cell_fir_aft, data2=cell_fir_bef, alternative="less")[1] < alpha:
                    increase_cell_ids.append(cell_id)
                elif ks_2samp(data1=cell_fir_aft, data2=cell_fir_bef, alternative="greater")[1] < alpha:
                    decrease_cell_ids.append(cell_id)
                else:
                    stable_cell_ids.append(cell_id)
            elif test == "mwu":
                if mannwhitneyu(x=cell_fir_aft, y=cell_fir_bef, alternative="less")[1] < alpha:
                    decrease_cell_ids.append(cell_id)
                elif mannwhitneyu(x=cell_fir_aft, y=cell_fir_bef, alternative="greater")[1] < alpha:
                    increase_cell_ids.append(cell_id)
                else:
                    stable_cell_ids.append(cell_id)

        print("#stable: "+str(len(stable_cell_ids))+" ,#inc: "+
              str(len(increase_cell_ids))+" ,#dec:"+str(len(decrease_cell_ids)))

        cell_class_dic = {
            "stable_cell_ids": np.array(stable_cell_ids),
            "decrease_cell_ids": np.array(decrease_cell_ids),
            "increase_cell_ids": np.array(increase_cell_ids)
        }

        with open(self.params.pre_proc_dir+"cell_classification/"+self.params.session_name+"_"+test+
                  "_sleep.pickle", "wb") as f:
            pickle.dump(cell_class_dic, f, pickle.HIGHEST_PROTOCOL)

    def classify_cells_firing_rate_mean(self):

        raster = []
        for l_s in self.long_sleep:
            raster.append(l_s.get_raster())

        raster = np.hstack(raster)

        raster_1 = raster[:, :int(raster.shape[1]/10)]
        raster_2 = raster[:, -int(raster.shape[1]/10):]

        fir_diff = []
        for cell_id, (cell_fir_bef, cell_fir_aft) in enumerate(zip(raster_1, raster_2)):
            fir_diff.append(np.mean(cell_fir_aft)-np.mean(cell_fir_bef))

        fir_diff = np.abs(np.array(fir_diff))
        stable_cells = np.squeeze(np.argwhere(fir_diff < 0.025))
        plt.hist(fir_diff, bins=80)
        plt.show()

        np.save(self.params.pre_proc_dir+"stable_cells_k_means/"+self.params.session_name+"_sleep_fir_diff", np.array(stable_cells))

    def classify_cells_firing_rate_k_means(self, nr_clusters=3):

        # get rasters
        raster = []
        first = 0
        for l_s in self.long_sleep:
            duration = l_s.get_duration_sec()
            r, t = l_s.get_spike_binned_raster(return_estimated_times=True)
            raster.append(r)
            first += duration
        raster = np.hstack(raster)

        # down sample raster
        raster_ds = down_sample_array_mean(x=raster, chunk_size=5000)
        raster_ds = raster_ds / np.max(raster_ds, axis=1, keepdims=True)

        # smooth down sampled rasters
        raster_ds_smoothed = []

        for cell_arr in raster_ds:
            s = moving_average(a=cell_arr, n=50)
            s = s/np.max(s)
            raster_ds_smoothed.append(s)

        raster_ds_smoothed = np.array(raster_ds_smoothed)

        # k-means clustering
        kmeans = KMeans(n_clusters=nr_clusters).fit(X=raster_ds_smoothed)
        k_labels = kmeans.labels_

        stable = []
        increase = []
        decrease = []
        # find clusters with constant/decreasing/increasing firing rates
        # compare firing during first 20% with firing during last 20%
        for cl_id in np.unique(k_labels):
            cl_data = raster_ds_smoothed[k_labels == cl_id, :]
            diff = np.mean(cl_data[:, -int(0.2 * cl_data.shape[1]):].flatten()) - np.mean(
                cl_data[:, :int(0.2 * cl_data.shape[1])].flatten())
            if diff < -0.09:
                decrease.append(cl_id)
            elif diff > 0.09:
                increase.append(cl_id)
            else:
                stable.append(cl_id)

        decrease = np.array(decrease)
        increase = np.array(increase)
        stable = np.array(stable)

        print("STABLE CLUSTER IDS: "+str(stable))
        print("INCREASING CLUSTER IDS: " + str(increase))
        print("DECREASING CLUSTER IDS: " + str(decrease))

        stable_cell_ids = []
        decrease_cell_ids = []
        increase_cell_ids = []
        for stable_cluster_id in stable:
            stable_cell_ids.append(np.where(k_labels==stable_cluster_id)[0])
        for dec_cluster_id in decrease:
            decrease_cell_ids.append(np.where(k_labels==dec_cluster_id)[0])
        for inc_cluster_id in increase:
            increase_cell_ids.append(np.where(k_labels==inc_cluster_id)[0])

        stable_cell_ids = np.hstack(stable_cell_ids)
        decrease_cell_ids = np.hstack(decrease_cell_ids)
        increase_cell_ids = np.hstack(increase_cell_ids)

        # create dictionary with labels

        cell_class_dic = {
            "stable_cell_ids": stable_cell_ids,
            "decrease_cell_ids": decrease_cell_ids,
            "increase_cell_ids": increase_cell_ids
        }

        with open(self.params.pre_proc_dir+"cell_classification/"+self.params.session_name+"_k_means.pickle", "wb") as f:
            pickle.dump(cell_class_dic, f, pickle.HIGHEST_PROTOCOL)

        k_labels_sorted = k_labels.argsort()

        fig = plt.figure(figsize=(8,6))
        gs = fig.add_gridspec(6, 20)

        ax1 = fig.add_subplot(gs[:, 0])
        ax1.set_title("CLUSTER \n IDs")
        ax2 = fig.add_subplot(gs[:, 3:-2])
        ax3 = fig.add_subplot(gs[:, -1:])

        # plotting

        ax1.imshow(np.expand_dims(k_labels[k_labels_sorted], 1), aspect="auto", cmap="tab10")
        ax1.axis("off")
        rate_map = ax2.imshow(raster_ds_smoothed[k_labels_sorted,:], interpolation='nearest', aspect='auto')
        ax2.set_xlabel("BIN ID")
        ax2.set_ylabel("CELLS SORTED")
        a = plt.colorbar(rate_map, cax=ax3)
        a.set_label("#SPIKES / NORMALIZED")
        plt.show()


        exit()

        # new_ml = MlMethodsOnePopulation()
        # weights = new_ml.ridge_time_bin_progress(x=raster, y=times, new_time_bin_size=str(self.params.spikes_per_bin)+" SPIKE")
        # sort according to weights
        # weights_sorted = weights.argsort()

        # new labels to sort according to increase/decrease
        # decreasing labels: 30+
        # stable labels: 50+
        # increasing labels: 80+

        new_labels_1 = np.zeros(raster_ds_smoothed.shape[0])

        for cl_id in decrease:
            new_labels_1[k_labels == cl_id] = 30+cl_id

        for cl_id in increase:
            new_labels_1[k_labels == cl_id] = 80+cl_id

        for cl_id in stable:
            new_labels_1[k_labels == cl_id] = 50+cl_id

        # have smaller numbers again
        combined_k_labels_sorted = new_labels_1.argsort()

        a = np.unique(new_labels_1)
        a = np.sort(a)
        for c_id, n_id in zip(a, np.arange(a.shape[0])):
            new_labels_1[new_labels_1 == c_id] = n_id

        # new_labels = np.zeros(raster_ds_smoothed.shape[0])
        # for cl_id in decrease:
        #     new_labels[k_labels == cl_id] = 1
        #
        # for cl_id in increase:
        #     new_labels[k_labels == cl_id] = 2

        combined_k_labels_sorted = new_labels_1.argsort()


        fig = plt.figure(figsize=(8,6))
        gs = fig.add_gridspec(6, 20)

        ax1 = fig.add_subplot(gs[:, 0])
        ax1.set_title("CLUSTERS")
        ax2 = fig.add_subplot(gs[:, 3:-2])
        ax3 = fig.add_subplot(gs[:, -1:])

        # plotting

        ax1.imshow(np.expand_dims(new_labels_1[combined_k_labels_sorted], 1), aspect="auto", cmap="tab10")
        ax1.axis("off")
        rate_map = ax2.imshow(raster_ds_smoothed[combined_k_labels_sorted,:], interpolation='nearest', aspect='auto')
        ax2.set_xlabel("BIN ID")
        ax2.set_ylabel("CELLS SORTED")
        a = plt.colorbar(rate_map, cax=ax3)
        a.set_label("#SPIKES / NORMALIZED")
        ax2.set_title("COMBINED CLUSTERS")
        plt.savefig(os.path.join(save_path, "ex1.pdf"))
        plt.show()

    # </editor-fold>

    # <editor-fold desc="Firing rate analysis">

    def firing_rate_changes(self, smoothing=200, plotting=False, pop_vec_threshold_rem=10,
                            pop_vec_threshold_nrem=2, stats_test="mwu", use_only_non_stationary_periods=False,
                            save_fig=False, return_p_value=False, use_firing_prob=True, filter_initial_period=True,
                            return_merged_times=False, return_raw_firing=False):
        """
        assesses firing rate changes of subsets of cells (stable, inc, dec) during different sleep phases (nrem, rem)

        :param use_firing_prob: whether to use firing probability (True) or firing rates
        :type use_firing_prob: bool
        :param return_p_value: whether to return p-values from statistical tests
        :type return_p_value: bool
        :param smoothing: smoothing used to smooth firing rates across epochs
        :type smoothing: int
        :param plotting: whether to plot (True) or return the results (False)
        :type plotting: bool
        :param pop_vec_threshold_rem: min. number of pop. vec. per rem epochs (shorter epochs are discarded)
        :type pop_vec_threshold_rem: int
        :param pop_vec_threshold_nrem: min. number of pop. vec. per nrem epoch (shorter epochs are discarded)
        :type pop_vec_threshold_nrem: int
        :param stats_test: which stats to use for comparing firing rate changes between nrem/rem ("t_test", "mwu", "ks",
                           "mwu_one_sided", "anova", "t_test_one_sided")
        :type stats_test: str
        :param use_only_non_stationary_periods: whether to use only periods where firing rates change (True)
        :type use_only_non_stationary_periods: bool
        :param save_fig: whether to save figure (True) or not
        :type save_fig: bool
        :return: p_dec, p_inc --> p-value for difference between NREM/REM for decreasing cells, p-value for difference
                                  between NREM/REM for increasing cells
        :rtype: float, float
        """
        # if use_only_non_stationary_periods:
        #     raise Exception("Needs to be implemented properly first!!!")
        #     TODO: merged_events_times needs to be split into merged_events_times_dec and merged_events_times_inc
            # because some epochs might be non-stationary for decreasing but not for increasing cells.

        # get stable, decreasing, increasing cells
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_"+self.params.stable_cell_method+".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_ids = class_dic["stable_cell_ids"]
        inc_ids = class_dic["increase_cell_ids"]
        dec_ids = class_dic["decrease_cell_ids"]

        print("#stable: "+str(stable_ids.shape[0])+", #inc: "+str(inc_ids.shape[0])+", #dec: "+str(dec_ids.shape[0]))

        # get REM and NREM rasters & times
        # --------------------------------------------------------------------------------------------------------------
        rem_rasters = []
        nrem_rasters = []
        event_times_rem = []
        event_end_times_rem = []
        event_times_nrem = []
        event_end_times_nrem = []
        duration_rem = []
        duration_nrem = []

        first = 0
        for l_s in self.long_sleep:
            duration = l_s.get_duration_sec()
            # all_event_rasters = all_event_rasters + l_s.get_event_spike_rasters(part_to_analyze=part_to_analyze)[0]
            ls_rem_raster, _, start_times_rem, end_times_rem = l_s.get_event_spike_rasters(part_to_analyze="rem",
                                                                                return_event_times=True,
                                                                                pop_vec_threshold=pop_vec_threshold_rem)

            part_to_analyze = "nrem"

            ls_nrem_raster, _, start_times_nrem, end_times_nrem = \
                l_s.get_event_spike_rasters(part_to_analyze=part_to_analyze, return_event_times=True,
                                            pop_vec_threshold=pop_vec_threshold_nrem)
            rem_rasters.extend(ls_rem_raster)
            nrem_rasters.extend(ls_nrem_raster)
            duration_rem.extend(end_times_rem - start_times_rem)
            duration_nrem.extend(end_times_nrem - start_times_nrem)
            event_times_rem.extend(first+start_times_rem)
            event_times_nrem.extend(first + start_times_nrem)
            event_end_times_rem.extend(first + end_times_rem)
            event_end_times_nrem.extend(first + end_times_nrem)
            first += duration

        # compute duration in hours
        duration_h = first/60/60

        event_times_nrem = np.vstack(event_times_nrem)
        event_times_rem = np.vstack(event_times_rem)
        duration_nrem = np.vstack(duration_nrem)
        duration_rem = np.vstack(duration_rem)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------------------------------
        all_events_raster = rem_rasters + nrem_rasters
        labels_events = np.zeros(len(all_events_raster))
        labels_events[:len(rem_rasters)] = 1
        all_times = np.vstack((event_times_rem, event_times_nrem))
        all_durations = np.vstack((duration_rem, duration_nrem))
        all_end_times = np.hstack((event_end_times_rem, event_end_times_nrem))

        # sort events according to time
        sorted_events_raster = [x for _, x in sorted(zip(all_times, all_events_raster))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]
        sorted_durations = [x for _, x in sorted(zip(all_times, all_durations))]

        if sorted_labels_events[0] == 1:
            first_event_label = "rem"
        else:
            first_event_label = "nrem"

        # merge neighboring epochs if they have the same label (rem events == 1, nrem events == 0)
        merged_event_rasters = []
        merged_event_labels = []
        tmp_merged_event_labels = []
        tmp_merged_event_time = np.zeros(2)
        prev_end_time = None
        tmp_merged_event_raster = np.empty((sorted_events_raster[0].shape[0],0))
        prev = sorted_labels_events[0]
        tmp_merged_event_time[0] = sorted_times[0]

        merged_events_times = []
        for label, raster, start_time, end_time, duration_event in zip(sorted_labels_events, sorted_events_raster,
                                                       sorted_times, sorted_end_times, sorted_durations):
            if label == prev:
                # need to merge the two
                tmp_merged_event_raster = np.hstack((tmp_merged_event_raster, raster))
                tmp_merged_event_labels.append(label)
            else:
                # add last merged event
                merged_event_rasters.append(tmp_merged_event_raster)
                merged_event_labels.append(tmp_merged_event_labels)
                # get end time from prev. event (is the last event belonging to the merged event)
                tmp_merged_event_time[1] = prev_end_time
                merged_events_times.append(tmp_merged_event_time)
                # need to start with new merged event
                tmp_merged_event_raster = np.empty((sorted_events_raster[0].shape[0],0))
                tmp_merged_event_labels = []
                tmp_merged_event_time = np.zeros(2)
                # need start time since it is a new merged event
                tmp_merged_event_time[0] = start_time
                tmp_merged_event_raster = np.hstack((tmp_merged_event_raster, raster))
                tmp_merged_event_labels.append(label)
                # remember current end time, in case this is the last event belonging to the merged event
            prev_end_time = end_time
            prev = label
        len_new_events = [x.shape[1] for x in merged_event_rasters]
        merged_event_labels = [x[0] for x in merged_event_labels]
        one_raster_all_events = np.hstack(merged_event_rasters)

        merged_events_times = np.vstack(merged_events_times)

        # merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        # merged_nrem_event_times = merged_events_times[merged_events_labels == 0]
        merged_events_length_s = merged_events_times[:,1]-merged_events_times[:,0]

        # get nr of all cells
        nr_cells = one_raster_all_events.shape[0]
        # split raster for each subset
        one_raster_all_events_stable = one_raster_all_events[stable_ids, :]
        one_raster_all_events_inc = one_raster_all_events[inc_ids, :]
        one_raster_all_events_dec = one_raster_all_events[dec_ids, :]

        if use_firing_prob:
            # firing probability
            one_raster_all_events_dec_mean = np.sum(one_raster_all_events_dec, axis=0) / \
                                             self.params.spikes_per_bin
            one_raster_all_events_inc_mean = np.sum(one_raster_all_events_inc, axis=0) / \
                                             self.params.spikes_per_bin
            one_raster_all_events_stable_mean = np.sum(one_raster_all_events_stable, axis=0) / \
                                                self.params.spikes_per_bin

        else:
            # compute mean firing for all subsets of cells (stable, inc, dec) and apply smoothing
            one_raster_all_events_dec_mean = np.mean(one_raster_all_events_dec, axis=0)
            one_raster_all_events_inc_mean = np.mean(one_raster_all_events_inc, axis=0)
            one_raster_all_events_stable_mean = np.mean(one_raster_all_events_stable, axis=0)

        one_raster_all_events_dec_mean_smooth = moving_average(one_raster_all_events_dec_mean, n=smoothing)

        one_raster_all_events_inc_mean_smooth = moving_average(one_raster_all_events_inc_mean, n=smoothing)

        one_raster_all_events_stable_mean_smooth = moving_average(one_raster_all_events_stable_mean, n=smoothing)

        pop_vec_per_h = one_raster_all_events_dec_mean_smooth.shape[0]/duration_h

        window_size = int(pop_vec_per_h/2)

        # check if mean firing is actually decreasing
        dec_mean_smooth_decreasing = np.zeros(one_raster_all_events_dec_mean_smooth.shape[0])
        inc_mean_smooth_increasing = np.zeros(one_raster_all_events_inc_mean_smooth.shape[0])

        for window_id in range(int(one_raster_all_events_dec_mean_smooth.shape[0]/window_size)):
            x = np.arange(window_size)
            segment_dec = one_raster_all_events_dec_mean_smooth[window_id*window_size:(window_id+1)*window_size]
            segment_inc = one_raster_all_events_inc_mean_smooth[window_id*window_size:(window_id+1)*window_size]
            coef_dec = np.polyfit(x, segment_dec, 1)
            coef_inc = np.polyfit(x, segment_inc, 1)
            # only use periods where decresing cells decrease their firing
            # check if coefficient of regression is negative --> cells decrease their firing
            if coef_dec[0] < 0:
                dec_mean_smooth_decreasing[window_id*window_size:(window_id+1)*window_size] = 1
            if coef_inc[0] > 0:
                inc_mean_smooth_increasing[window_id*window_size:(window_id+1)*window_size] = 1

        # split raster into single epochs/events again
        dec_smooth_rem = []
        dec_smooth_nrem = []
        dec_smooth = []
        dec_smooth_delta = []
        inc_smooth_rem = []
        inc_smooth_nrem = []
        inc_smooth = []
        inc_smooth_delta = []
        stable_smooth_rem = []
        stable_smooth_nrem = []
        stable_smooth = []
        stable_smooth_delta = []
        valid_dec_event = np.zeros(len(merged_event_labels))
        valid_inc_event = np.zeros(len(merged_event_labels))

        start = 0

        for i, (event_len, label) in enumerate(zip(len_new_events, merged_event_labels)):

            end = min(start + event_len, one_raster_all_events_dec_mean_smooth.shape[0]-1)

            # check if start still lies within raster start --> get shorter through smoothing
            if start >= one_raster_all_events_dec_mean_smooth.shape[0]:
                break
            # check if firing rates were actually decreasing: 70% of event need to lie in decreasing period
            if (end - start) != 0 and np.count_nonzero(dec_mean_smooth_decreasing[start:end]) / (end - start) > 0.7:
                valid_dec_event[i] = 1

            if (end - start) != 0 and np.count_nonzero(inc_mean_smooth_increasing[start:end]) / (end - start) > 0.7:
                valid_inc_event[i] = 1

            # check which label (rem == 1, nrem == 0)
            if label == 1:
                dec_smooth_rem.append(one_raster_all_events_dec_mean_smooth[start:end])
                inc_smooth_rem.append(one_raster_all_events_inc_mean_smooth[start:end])
                stable_smooth_rem.append(one_raster_all_events_stable_mean_smooth[start:end])
            else:
                dec_smooth_nrem.append(one_raster_all_events_dec_mean_smooth[start:end])
                inc_smooth_nrem.append(one_raster_all_events_inc_mean_smooth[start:end])
                stable_smooth_nrem.append(one_raster_all_events_stable_mean_smooth[start:end])

            dec_smooth.append(one_raster_all_events_dec_mean_smooth[start:end])
            dec_smooth_delta.append((one_raster_all_events_dec_mean_smooth[end]-
                                     one_raster_all_events_dec_mean_smooth[start]))
            inc_smooth.append(one_raster_all_events_inc_mean_smooth[start:end])
            inc_smooth_delta.append(one_raster_all_events_inc_mean_smooth[end]-
                                    one_raster_all_events_inc_mean_smooth[start])
            stable_smooth.append(one_raster_all_events_stable_mean_smooth[start:end])
            stable_smooth_delta.append(one_raster_all_events_stable_mean_smooth[end]-
                                       one_raster_all_events_stable_mean_smooth[start])
            start = end

        # might need to delete last label
        merged_event_labels = merged_event_labels[:len(dec_smooth)]
        # correlation of delta average firing rate and epoch length
        # compute duration of event
        merged_times = np.vstack(merged_events_times)
        merged_dur = merged_times[:, 1] - merged_times[:, 0]

        # REM vs. NREM
        merged_event_labels_arr = np.array(merged_event_labels)

        # # REM vs. NREM
        # rem_raster = np.hstack(rem_rasters)
        # rem_raster_dec = rem_raster[dec_ids,:]
        # mean_firing_rem_dec = np.mean(rem_raster_dec, axis=0)
        # mean_firing_rem_dec_smooth = moving_average(a=mean_firing_rem_dec, n=2000)
        #
        # nrem_raster = np.hstack(nrem_rasters)
        # nrem_raster_dec = nrem_raster[dec_ids,:]
        # mean_firing_nrem_dec = np.mean(nrem_raster_dec, axis=0)
        # mean_firing_nrem_dec_smooth = moving_average(a=mean_firing_nrem_dec, n=200)
        #
        # plt.plot(mean_firing_rem_dec_smooth, color="r")
        # plt.plot(mean_firing_nrem_dec_smooth, color="b")
        # plt.show()

        # filter stationary periods if requested
        if use_only_non_stationary_periods:
            rem_dec_smooth = np.array(dec_smooth_delta)[(merged_event_labels_arr == 1) & (valid_dec_event==1)]
            nrem_dec_smooth = np.array(dec_smooth_delta)[(merged_event_labels_arr == 0) & (valid_dec_event==1)]

            rem_inc_smooth = np.array(inc_smooth_delta)[(merged_event_labels_arr == 1) & (valid_inc_event==1)]
            nrem_inc_smooth = np.array(inc_smooth_delta)[(merged_event_labels_arr == 0) & (valid_inc_event==1)]
        else:
            rem_dec_smooth = np.array(dec_smooth_delta)[merged_event_labels_arr == 1]
            nrem_dec_smooth = np.array(dec_smooth_delta)[merged_event_labels_arr == 0]

            rem_inc_smooth = np.array(inc_smooth_delta)[merged_event_labels_arr == 1]
            nrem_inc_smooth = np.array(inc_smooth_delta)[merged_event_labels_arr == 0]

        rem_stable_smooth = np.array(stable_smooth_delta)[merged_event_labels_arr == 1]
        nrem_stable_smooth = np.array(stable_smooth_delta)[merged_event_labels_arr == 0]

        # --------------------------------------------------------------------------------------------------------------
        # check directionality
        # --------------------------------------------------------------------------------------------------------------
        rem_dec_pos = np.count_nonzero(rem_dec_smooth > 0)
        rem_dec_neg = np.count_nonzero(rem_dec_smooth < 0)

        rem_bin_dist = np.hstack((np.zeros(rem_dec_pos),np.ones(rem_dec_neg)))

        nrem_dec_pos = np.count_nonzero(nrem_dec_smooth > 0)
        nrem_dec_neg = np.count_nonzero(nrem_dec_smooth < 0)

        nrem_bin_dist = np.hstack((np.zeros(nrem_dec_pos), np.ones(nrem_dec_neg)))

        # --------------------------------------------------------------------------------------------------------------
        # perform statistical test
        # --------------------------------------------------------------------------------------------------------------

        if stats_test == "mwu":
            p_dec = mannwhitneyu(rem_dec_smooth, nrem_dec_smooth, alternative="greater")[1]
            p_inc = mannwhitneyu(rem_inc_smooth, nrem_inc_smooth, alternative="less")[1]
        elif stats_test == "mwu_two_sided":
            p_dec = mannwhitneyu(rem_dec_smooth, nrem_dec_smooth)[1]
            p_inc = mannwhitneyu(rem_inc_smooth, nrem_inc_smooth)[1]
        elif stats_test == "ks":
            p_dec = ks_2samp(rem_dec_smooth, nrem_dec_smooth, alternative="less")[1]
            p_inc = ks_2samp(rem_inc_smooth, nrem_inc_smooth, alternative="greater")[1]
        elif stats_test == "anova":
            p_dec = f_oneway(rem_dec_smooth, nrem_dec_smooth)[1]
            p_inc = f_oneway(rem_inc_smooth, nrem_inc_smooth)[1]
        elif stats_test == "t_test":
            p_dec = ttest_ind(rem_dec_smooth, nrem_dec_smooth, alternative="greater")[1]
            p_inc = ttest_ind(rem_inc_smooth, nrem_inc_smooth, alternative="less")[1]
        elif stats_test == "t_test_two_sided":
            p_dec = ttest_ind(rem_dec_smooth, nrem_dec_smooth)[1]
            p_inc = ttest_ind(rem_inc_smooth, nrem_inc_smooth)[1]
        else:
            raise Exception("THIS STATS TEST IS NOT IMPLEMENTED!!!")

        print("Stats test for decreasing cells ("+stats_test+"):")
        print("p-value = "+str(p_dec))
        print("Stats test for increasing cells ("+stats_test+"):")
        print("p-value = "+str(p_inc))

        # --------------------------------------------------------------------------------------------------------------
        # compute CDF
        # --------------------------------------------------------------------------------------------------------------

        rem_dec_sorted = np.sort(rem_dec_smooth)
        nrem_dec_sorted = np.sort(nrem_dec_smooth)
        # calculate the proportional values of samples
        p_dec_data = 1. * np.arange(rem_dec_sorted.shape[0]) / (rem_dec_sorted.shape[0] - 1)
        p_dec_shuffle = 1. * np.arange(nrem_dec_sorted.shape[0]) / (nrem_dec_sorted.shape[0] - 1)

        rem_inc_sorted = np.sort(rem_inc_smooth)
        nrem_inc_sorted = np.sort(nrem_inc_smooth)
        # calculate the proportional values of samples
        p_inc_data = 1. * np.arange(rem_inc_sorted.shape[0]) / (rem_inc_sorted.shape[0] - 1)
        p_inc_shuffle = 1. * np.arange(nrem_inc_sorted.shape[0]) / (nrem_inc_sorted.shape[0] - 1)

        # --------------------------------------------------------------------------------------------------------------
        # plot CDF
        # --------------------------------------------------------------------------------------------------------------
        if save_fig:
            plt.style.use('default')

        if save_fig or plotting:
            plt.figure(figsize=(6,2))
            plt.plot(rem_dec_sorted, p_dec_data, label="REM", color="r")
            plt.plot(nrem_dec_sorted, p_dec_shuffle, label="NREM", color="b")
            plt.legend()
            plt.ylabel("CDF")
            plt.xlabel("DELTA FIRING PROBABILITY")
            plt.xlim(np.min(nrem_dec_sorted), np.max(rem_dec_sorted))
            plt.ylim(0,1)
            plt.yticks([0,0.5,1])
            plt.xticks([np.min(nrem_dec_sorted), 0, np.max(rem_dec_sorted)])
            plt.grid(axis="x")
            plt.title("Decreasing cells")
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "dec_firing_changes.svg"), transparent="True")
                plt.close()
            elif plotting:
                plt.show()

        if save_fig or plotting:
            # increasing cells
            plt.figure(figsize=(6,2))
            plt.plot(rem_inc_sorted, p_inc_data, label="REM", color="r")
            plt.plot(nrem_inc_sorted, p_inc_shuffle, label="NREM", color="b")
            plt.legend()
            plt.ylabel("CDF")
            plt.xlabel("DELTA FIRING PROBABILITY")
            plt.ylim(0,1)
            plt.yticks([0,0.5,1])
            # plt.xticks([-0.035, 0, 0.035])
            plt.grid(axis="x")
            plt.title("Increasing cells")
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "inc_firing_changes.svg"), transparent="True")
            elif plotting:
                plt.show()

        if save_fig:
            dur_per_pop_vec = duration_h/np.hstack(inc_smooth).shape[0]
            fig = plt.figure()
            ax = fig.add_subplot()
            first = 0
            for label, res, duration_current_event in zip(merged_event_labels, inc_smooth, merged_events_length_s):
                # rem
                if label == 1:
                    ax.plot(np.linspace(first, first+duration_current_event,
                                                res.shape[0]), res, c="r", label="REM")
                else:
                    ax.plot(np.linspace(first, first+duration_current_event,
                                                res.shape[0]), res, c="b", label="NREM")
                first += duration_current_event
            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())
            plt.xlabel("Time (min)")
            plt.ylabel("Firing probability")
            plt.xlim(46900, 52700)
            # plt.yticks([])
            # plt.xlim([51200,52900])
            plt.xticks([46900, 52700],np.round(np.array([46900, 52700])/60))
            plt.ylim([0.30, 0.43])
            plt.rcParams['svg.fonttype'] = 'none'
            plt.savefig(os.path.join(save_path, "firing_changes_increasing_cell_example.svg"), transparent="True")
            # plt.show()

        # --------------------------------------------------------------------------------------------------------------
        # plot additional figures & controls
        # --------------------------------------------------------------------------------------------------------------
        if plotting:
            # plot CDF for stable cells
            # ----------------------------------------------------------------------------------------------------------
            rem_sorted = np.sort(rem_stable_smooth)
            nrem_sorted = np.sort(nrem_stable_smooth)
            # calculate the proportional values of samples
            p_stable_rem = 1. * np.arange(rem_sorted.shape[0]) / (rem_sorted.shape[0] - 1)
            p_stable_nrem = 1. * np.arange(nrem_sorted.shape[0]) / (nrem_sorted.shape[0] - 1)

            plt.plot(rem_sorted, p_stable_rem, label="REM", color="r")
            plt.plot(nrem_sorted, p_stable_nrem, label="NREM", color="b")
            plt.legend()
            plt.ylabel("CDF")
            plt.xlabel("DELTA MEAN FIRING")
            plt.title("Stable")
            plt.show()

            # plot histograms
            # ----------------------------------------------------------------------------------------------------------
            plt.hist(rem_inc_smooth, color="r", label="REM", orientation='horizontal', density=True)
            plt.hist(nrem_inc_smooth, color="b", label="NREM", alpha=0.5, orientation='horizontal', density=True)
            plt.title("DELTA MEAN FIRING: INCREASING CELLS")
            plt.legend()
            plt.xlabel("DENSITY")
            plt.ylabel("DELTA MEAN FIRING PER EPOCH")
            plt.ylim(-max(abs(np.array(inc_smooth_delta))), max(abs(np.array(inc_smooth_delta))))
            plt.show()

            plt.hist(rem_stable_smooth, color="r", label="REM", orientation='horizontal', density=True)
            plt.hist(nrem_stable_smooth, color="b", label="NREM", alpha=0.5, orientation='horizontal', density=True)
            plt.title("DELTA MEAN FIRING: STABLE CELLS")
            plt.legend()
            plt.xlabel("DENSITY")
            plt.ylabel("DELTA MEAN FIRING PER EPOCH")
            plt.ylim(-max(abs(np.array(stable_smooth_delta))), max(abs(np.array(stable_smooth_delta))))
            plt.show()

            plt.hist(rem_dec_smooth, color="r", label="REM", orientation='horizontal', density=True)
            plt.hist(nrem_dec_smooth, color="b", label="NREM", alpha=0.5, orientation='horizontal', density=True)
            plt.title("DELTA MEAN FIRING: DECREASING CELLS")
            plt.legend()
            plt.xlabel("DENSITY")
            plt.ylabel("DELTA MEAN FIRING PER EPOCH")
            plt.ylim(-max(abs(np.array(dec_smooth_delta))), max(abs(np.array(dec_smooth_delta))))
            plt.show()

            # plot delta mean firing values per epoch
            # ----------------------------------------------------------------------------------------------------------
            fig = plt.figure()
            ax = fig.add_subplot()
            # plot results
            if use_only_non_stationary_periods:
                first = 0
                for label, res, val in zip(merged_event_labels, dec_smooth_delta, valid_dec_event):
                    # rem
                    if label == 1 and val == 1:
                        ax.scatter(first, res, c="r", label="REM")
                    elif label == 0 and val == 1:
                        ax.scatter(first, res, c="b", label="NREM")
                    first += 1
            else:
                first = 0
                for label, res in zip(merged_event_labels, dec_smooth_delta):
                    # rem
                    if label == 1:
                        ax.scatter(first, res, c="r", label="REM")
                    else:
                        ax.scatter(first, res, c="b", label="NREM")
                    first += 1

            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())
            plt.title("DECREASING CELLS")
            plt.ylabel("DELTA IN MEAN FIRING RATE")
            plt.xlabel("EPOCH ID")
            plt.show()

            fig = plt.figure()
            ax = fig.add_subplot()
            if use_only_non_stationary_periods:
                first = 0
                for label, res, val in zip(merged_event_labels, inc_smooth_delta, valid_inc_event):
                    # rem
                    if label == 1 and val == 1:
                        ax.scatter(first, res, c="r", label="REM")
                    elif label == 0 and val == 1:
                        ax.scatter(first, res, c="b", label="NREM")
                    first += 1
            else:
                first = 0
                for label, res in zip(merged_event_labels, inc_smooth_delta):
                    # rem
                    if label == 1:
                        ax.scatter(first, res, c="r", label="REM")
                    else:
                        ax.scatter(first, res, c="b", label="NREM")
                    first += 1

            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())
            plt.title("INCREASING CELLS")
            plt.ylabel("DELTA IN MEAN FIRING RATE")
            plt.xlabel("EPOCH ID")
            plt.show()
            #
            fig = plt.figure()
            ax = fig.add_subplot()
            first = 0
            for label, res in zip(merged_event_labels, stable_smooth_delta):
                # rem
                if label == 1:
                    ax.scatter(first, res, c="r", label="REM")
                else:
                    ax.scatter(first, res, c="b", label="NREM")
                first += 1

            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())
            plt.title("STABLE CELLS")
            plt.ylabel("DELTA IN MEAN FIRING RATE")
            plt.xlabel("EPOCH ID")
            plt.show()

            # plot mean firing over time
            # ----------------------------------------------------------------------------------------------------------

            fig = plt.figure()
            ax = fig.add_subplot()
            first = 0
            for label, res in zip(merged_event_labels, dec_smooth):
                # rem
                if label == 1:
                    ax.plot(first + np.arange(res.shape[0]), res, c="r", label="REM")
                else:
                    ax.plot(first + np.arange(res.shape[0]), res, c="b", label="NREM")
                first += res.shape[0]
            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())
            plt.xlabel("POP. VEC ID")
            plt.ylabel("MEAN FIRING RATE")
            if use_only_non_stationary_periods:
                plt.plot(dec_mean_smooth_decreasing*0.05, c="yellow")
            plt.title("DECREASING CELLS")
            plt.show()

            fig = plt.figure()
            ax = fig.add_subplot()
            first = 0
            for label, res in zip(merged_event_labels, inc_smooth):
                # rem
                if label == 1:
                    ax.plot(first + np.arange(res.shape[0]), res, c="r", label="REM")
                else:
                    ax.plot(first + np.arange(res.shape[0]), res, c="b", label="NREM")
                first += res.shape[0]
            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())
            plt.xlabel("POP. VEC ID")
            plt.ylabel("MEAN FIRING RATE")
            if use_only_non_stationary_periods:
                plt.plot(inc_mean_smooth_increasing*0.05, c="yellow")
            plt.title("INCREASING CELLS")
            plt.show()

            fig = plt.figure()
            ax = fig.add_subplot()
            first = 0
            for label, res in zip(merged_event_labels, stable_smooth):
                # rem
                if label == 1:
                    ax.plot(first + np.arange(res.shape[0]), res, c="r", label="REM")
                else:
                    ax.plot(first + np.arange(res.shape[0]), res, c="b", label="NREM")
                first += res.shape[0]
            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())
            plt.xlabel("POP. VEC ID")
            plt.ylabel("MEAN FIRING RATE")
            plt.title("STABLE CELLS")
            plt.show()

            # plot delta firing for neighbouring epochs (scatter plot)
            # ----------------------------------------------------------------------------------------------------------

            max_len = min(rem_dec_smooth.shape[0], nrem_dec_smooth.shape[0])
            plt.scatter(rem_dec_smooth[:max_len], nrem_dec_smooth[:max_len])
            plt.title("NEIGBOURING PERIODS: DECREASING CELLS\nR=" + str(np.round(pearsonr(
                rem_dec_smooth[:max_len], nrem_dec_smooth[:max_len])[0], 2)))
            plt.xlabel("DELTA MEAN FIRING REM")
            plt.ylabel("DELTA MEAN FIRING NREM")
            plt.show()

            plt.scatter(rem_inc_smooth[:max_len], nrem_inc_smooth[:max_len])
            plt.title("NEIGBOURING PERIODS: INCREASING CELLS\nR=" + str(np.round(pearsonr(
                rem_inc_smooth[:max_len], nrem_inc_smooth[:max_len])[0], 2)))
            plt.xlabel("DELTA MEAN FIRING REM")
            plt.ylabel("DELTA MEAN FIRING NREM")
            plt.show()

            plt.scatter(rem_stable_smooth[:max_len], nrem_stable_smooth[:max_len])
            plt.title("NEIGBOURING PERIODS: STABLE CELLS\nR=" + str(np.round(pearsonr(
                rem_stable_smooth[:max_len], nrem_stable_smooth[:max_len])[0], 2)))
            plt.xlabel("DELTA MEAN FIRING REM")
            plt.ylabel("DELTA MEAN FIRING NREM")
            plt.show()

            plt.scatter(merged_dur, np.array(dec_smooth_delta), label="DEC. CELLS, R = "+str(
                np.round(pearsonr(merged_dur, np.array(dec_smooth_delta))[0],2)), marker="_", color="lightcoral")
            plt.scatter(merged_dur, np.array(inc_smooth_delta), label="INC. CELLS, R = "+str(
                np.round(pearsonr(merged_dur, np.array(inc_smooth_delta))[0],2)), marker="+", color="cornflowerblue")
            plt.scatter(merged_dur, np.array(stable_smooth_delta), label="STABLE. CELLS, R = "+str(
                np.round(pearsonr(merged_dur, np.array(stable_smooth_delta))[0],2)), s=2.5, color="white")
            plt.xlabel("DURATION EPOCH (s)")
            plt.ylabel("DELTA MEAN FIRING")
            plt.legend()
            plt.title("DURATION EPOCH VS. DELTA MEAN FIRING")
            plt.show()

            # control for epoch length
            # ----------------------------------------------------------------------------------------------------------
            thresh_dur = 3000
            plt.scatter(merged_dur[merged_dur < thresh_dur], np.array(dec_smooth_delta)[merged_dur < thresh_dur],
                        label="DEC. CELLS, R = "+str(np.round(pearsonr(merged_dur[merged_dur < thresh_dur],
                        np.array(dec_smooth_delta)[merged_dur < thresh_dur])[0],2)), marker="_", color="lightcoral")
            plt.scatter(merged_dur[merged_dur < thresh_dur],
                        np.array(inc_smooth_delta)[merged_dur < thresh_dur], label="INC. CELLS, R = "+str(
                        np.round(pearsonr(merged_dur[merged_dur < thresh_dur],
                        np.array(inc_smooth_delta)[merged_dur < thresh_dur])[0],2)), marker="+", color="cornflowerblue")
            plt.scatter(merged_dur[merged_dur < thresh_dur], np.array(stable_smooth_delta)[merged_dur < thresh_dur],
                        label="STABLE. CELLS, R = "+str(np.round(pearsonr(merged_dur[merged_dur < thresh_dur],
                                np.array(stable_smooth_delta)[merged_dur < thresh_dur])[0],2)), s=2.5, color="white")
            plt.xlabel("DURATION EPOCH (s)")
            plt.ylabel("DELTA MEAN FIRING")
            plt.legend()
            plt.title("DURATION EPOCH VS. DELTA MEAN FIRING\n DURATION < "+str(thresh_dur)+"s")
            plt.show()


        if return_p_value:
            return p_dec, p_inc
        elif return_merged_times:
            return nrem_dec_smooth, rem_dec_smooth, rem_inc_smooth, nrem_inc_smooth, rem_stable_smooth, \
                   nrem_stable_smooth, first_event_label, merged_events_times
        elif return_raw_firing:
            return inc_smooth, stable_smooth, dec_smooth, first_event_label
        else:
            return nrem_dec_smooth, rem_dec_smooth, rem_inc_smooth, nrem_inc_smooth, rem_stable_smooth, \
                   nrem_stable_smooth, first_event_label

    def firing_rate_changes_neighbouring_epochs(self, first_type="random"):

        nrem_dec_smooth, rem_dec_smooth, rem_inc_smooth, nrem_inc_smooth, _, _, first_event_label = \
            self.firing_rate_changes(return_p_value=False, use_only_non_stationary_periods=False)

        if nrem_dec_smooth.shape[0] < rem_dec_smooth.shape[0] and first_event_label == "rem":
            rem_dec_smooth = rem_dec_smooth[:-1]
            rem_inc_smooth = rem_inc_smooth[:-1]
        elif nrem_dec_smooth.shape[0] > rem_dec_smooth.shape[0] and first_event_label == "nrem": 
            nrem_dec_smooth = nrem_dec_smooth[:-1]
            nrem_inc_smooth = nrem_inc_smooth[:-1]
        #
        if not first_type == "random":
            if first_type == "nrem":
                # check which phase came first
                if first_event_label == "rem":
                    # first event was a rem event
                    rem_dec_smooth = rem_dec_smooth[1:]
                    rem_inc_smooth = rem_inc_smooth[1:]
                    nrem_dec_smooth = nrem_dec_smooth[:-1]
                    nrem_inc_smooth = nrem_inc_smooth[:-1]
                    print("First epoch: REM --> deleted first REM and last NREM epoch")
                elif first_event_label == "nrem":
                    # first event was a nrem event
                    print("First epoch: NREM --> nothing was done")
            elif first_type == "rem":
                # check which phase came first
                if first_event_label == "rem":
                    # first event was a rem event
                    print("First epoch: REM --> nothing was done")
                elif first_event_label == "nrem":
                    # first event was a nrem event
                    nrem_dec_smooth = nrem_dec_smooth[1:]
                    nrem_inc_smooth = nrem_inc_smooth[1:]
                    rem_dec_smooth = rem_dec_smooth[:-1]
                    rem_inc_smooth = rem_inc_smooth[:-1]
                    print("First epoch: NREM --> delete first NREM and last REM epoch")

        return nrem_dec_smooth, rem_dec_smooth, nrem_inc_smooth, rem_inc_smooth

    def firing_rate_distributions(self, cells_to_use="stable", plotting=True, separate_sleep_phases=True,
                                  chunks_in_min=2, measure="mean", z_score=False):
        print("Processing " +self.session_name)

        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle", "rb") as f:
            class_dic = pickle.load(f)

        if cells_to_use == "stable":
            cell_ids = class_dic["stable_cell_ids"].flatten()
        elif cells_to_use == "increasing":
            cell_ids = class_dic["increase_cell_ids"].flatten()
        elif cells_to_use == "decreasing":
            cell_ids = class_dic["decrease_cell_ids"].flatten()
        else:
            raise Exception("Cell subset needs to be one of [stable, increasing, decreasing]")

        chunk_size = int(chunks_in_min/self.params.time_bin_size)

        if separate_sleep_phases:
            raster_nrem = self.get_sleep_phase_raster(sleep_phase="nrem")
            raster_rem = self.get_sleep_phase_raster(sleep_phase="rem")

            # go through REM
            nr_chunks_rem = int(raster_rem.shape[1] / chunk_size)
            firing_rem = np.zeros((raster_rem.shape[0], nr_chunks_rem))
            for chunk in range(nr_chunks_rem):
                if measure == "mean":
                    firing_rem[:, chunk] = np.mean(
                        raster_rem[:, chunk * chunk_size:(chunk + 1) * chunk_size], axis=1) / self.params.time_bin_size
                elif measure == "max":
                    firing_rem[:, chunk] = np.max(
                        raster_rem[:, chunk * chunk_size:(chunk + 1) * chunk_size], axis=1) / self.params.time_bin_size

            # go through NREM
            nr_chunks_nrem = int(raster_nrem.shape[1] / chunk_size)
            firing_nrem = np.zeros((raster_nrem.shape[0], nr_chunks_nrem))
            for chunk in range(nr_chunks_nrem):
                if measure == "mean":
                    firing_nrem[:, chunk] = np.mean(
                        raster_nrem[:, chunk * chunk_size:(chunk + 1) * chunk_size], axis=1) / self.params.time_bin_size
                elif measure == "max":
                    firing_nrem[:, chunk] = np.max(
                        raster_nrem[:, chunk * chunk_size:(chunk + 1) * chunk_size], axis=1) / self.params.time_bin_size

            # combine to z-score for each cell
            all_firing = np.hstack((firing_rem, firing_nrem))
            all_firing_z = zscore(all_firing, axis=1)

            # split again into rem and nrem
            firing_rem_z = all_firing_z[cell_ids, :firing_rem.shape[1]].flatten()
            firing_nrem_z = all_firing_z[cell_ids, firing_rem.shape[1]:].flatten()

            if plotting:
                p_rem = 1. * np.arange(firing_rem_z.shape[0]) / (firing_rem_z.shape[0] - 1)
                p_nrem = 1. * np.arange(firing_nrem_z.shape[0]) / (firing_nrem_z.shape[0] - 1)

                plt.plot(np.sort(firing_rem_z), p_rem, label="PRE")
                plt.plot(np.sort(firing_nrem_z), p_nrem, label="NREM")
                plt.title(cells_to_use)
                if measure == "mean":
                    plt.xlabel("Mean firing rate (z-scored)")
                elif measure == "max":
                    plt.xlabel("Max firing rate (z-scored)")
                plt.ylabel("CDF")
                plt.legend()
                plt.show()
            else:
                return firing_rem_z, firing_nrem_z

        else:
            raster_sleep = self.get_sleep_raster()
            # go through entire sleep and chunk data
            nr_chunks_sleep = int(raster_sleep.shape[1] / chunk_size)
            firing_sleep = np.zeros((raster_sleep.shape[0], nr_chunks_sleep))
            for chunk in range(nr_chunks_sleep):
                if measure == "mean":
                    firing_sleep[:, chunk] = np.mean(
                        raster_sleep[:, chunk * chunk_size:(chunk + 1) * chunk_size], axis=1) / self.params.time_bin_size

                elif measure == "max":
                    firing_sleep[:, chunk] = np.max(
                        raster_sleep[:, chunk * chunk_size:(chunk + 1) * chunk_size], axis=1) / self.params.time_bin_size

            if z_score:
                firing_sleep = zscore(firing_sleep[cell_ids,:], axis=1).flatten()

            if plotting:

                p_sleep = 1. * np.arange(firing_sleep.shape[0]) / (firing_sleep.shape[0] - 1)

                plt.plot(np.sort(firing_sleep), p_sleep, label="sleep")
                plt.title(cells_to_use)
                if measure == "mean":
                    plt.xlabel("Mean firing rate")
                elif measure == "max":
                    plt.xlabel("Max firing rate")
                plt.ylabel("CDF")
                plt.legend()
                plt.show()
            else:
                return firing_sleep

    def mean_firing_rate_for_interval(self, intervals_s):
        """
        assesses firing rate averages for defined intervals

        :param intervals_s: start and end of each interval in seconds
        :type intervals_s: np.array

        :return: array with average firing rates in Hz
        """
        # get stable, decreasing, increasing cells

        raster_list = []
        times_with_offset = []
        offset_at_raster_resolution = 0
        for l_s in self.long_sleep:
            # all_event_rasters = all_event_rasters + l_s.get_event_spike_rasters(part_to_analyze=part_to_analyze)[0]
            raster = l_s.get_raster()
            times_with_offset.append(np.arange(offset_at_raster_resolution,offset_at_raster_resolution+raster.shape[1]))
            raster_list.append(raster)
            duration = l_s.get_duration_sec()
            offset_at_raster_resolution += np.round(duration/self.params.time_bin_size)

        times_with_offset = np.hstack(times_with_offset)
        rasters = np.hstack(raster_list)

        # now go through intervals and compute mean firing using rasters
        firing_rate = np.zeros((rasters.shape[0], intervals_s.shape[0]))

        # convert intervals to raster resolution
        intervals_s_adj = intervals_s / self.params.time_bin_size

        for interval_id, (start, end) in enumerate(intervals_s_adj):
            if np.count_nonzero(np.logical_and(start < times_with_offset, times_with_offset < end)):
                # need to divide #spikes by length of interval in seconds --> yields firing rate in Hz
                firing_rate[:, interval_id] = np.sum(rasters[:,np.logical_and(start < times_with_offset, times_with_offset < end)],
                                                axis=1)/(intervals_s[interval_id, 1]-intervals_s[interval_id, 0])
            else:
                firing_rate[:, interval_id] = np.nan

        return firing_rate

    def rasters_for_intervals(self, intervals_s, time_bin_size=None):
        """
        assesses firing rate averages for defined intervals

        :param intervals_s: start and end of each interval in seconds
        :type intervals_s: np.array

        :return: array with average firing rates in Hz
        """
        # get stable, decreasing, increasing cells

        if time_bin_size is None:
            time_bin_size=self.params.time_bin_size

        raster_list = []
        times_with_offset = []
        offset_at_raster_resolution = 0
        for l_s in self.long_sleep:
            # all_event_rasters = all_event_rasters + l_s.get_event_spike_rasters(part_to_analyze=part_to_analyze)[0]
            raster = l_s.get_raster(time_bin_size=time_bin_size)
            times_with_offset.append(np.arange(offset_at_raster_resolution,offset_at_raster_resolution+raster.shape[1]))
            raster_list.append(raster)
            duration = l_s.get_duration_sec()
            offset_at_raster_resolution += np.round(duration/time_bin_size)

        times_with_offset = np.hstack(times_with_offset)
        rasters = np.hstack(raster_list)

        # convert intervals to raster resolution
        intervals_s_adj = intervals_s / time_bin_size

        rasters_per_interval = []
        for interval_id, (start, end) in enumerate(intervals_s_adj):
            # need to divide #spikes by length of interval in seconds --> yields firing rate in Hz
            rasters_per_interval.append(rasters[:,np.logical_and(start < times_with_offset,
                 times_with_offset < end)]/time_bin_size)

        return rasters_per_interval

    def firing_probability_for_interval_subsets(self, intervals_s):
        """
        assesses firing rate averages for defined intervals

        :param intervals_s: start and end of each interval in seconds
        :type intervals_s: np.array

        :return: array with average firing rates
        """
        # get stable, decreasing, increasing cells

        raise Exception("FUNCTION NOT VALIDATED: INTERVALS in seconds, but probability based on spike bins")

        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_"+self.params.stable_cell_method+".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_ids = class_dic["stable_cell_ids"]
        inc_ids = class_dic["increase_cell_ids"]
        dec_ids = class_dic["decrease_cell_ids"]

        print("#stable: "+str(stable_ids.shape[0])+", #inc: "+str(inc_ids.shape[0])+", #dec: "+str(dec_ids.shape[0]))

        raster_list = []
        times_with_offset = []
        offset_at_raster_resolution = 0
        for l_s in self.long_sleep:
            # all_event_rasters = all_event_rasters + l_s.get_event_spike_rasters(part_to_analyze=part_to_analyze)[0]
            # TODO: need spike rasters and not temporal binning
            raster = l_s.get_raster()
            times_with_offset.append(np.arange(offset_at_raster_resolution,offset_at_raster_resolution+raster.shape[1]))
            raster_list.append(raster)
            duration = l_s.get_duration_sec()
            offset_at_raster_resolution += np.round(duration/self.params.time_bin_size)

        times_with_offset = np.hstack(times_with_offset)
        rasters = np.hstack(raster_list)

        # now go through intervals and compute mean firing using rasters
        delta_prob_stable = np.zeros(intervals_s.shape[0])
        delta_prob_inc = np.zeros(intervals_s.shape[0])
        delta_prob_dec = np.zeros(intervals_s.shape[0])

        # convert intervals to raster resolution
        intervals_s_adj = intervals_s / self.params.time_bin_size

        for interval_id, (start, end) in enumerate(intervals_s_adj):
            if np.count_nonzero(np.logical_and(start < times_with_offset, times_with_offset < end)):
                interval_data = rasters[:,np.logical_and(start < times_with_offset, times_with_offset < end)]
                # compute for stable cells
                interval_fir_prob_stable = np.sum(interval_data[stable_ids,:], axis=0)/self.params.spikes_per_bin
                delta_prob_stable[interval_id] = interval_fir_prob_stable[-1] - interval_fir_prob_stable[0]
                # compute for dec cells
                interval_fir_prob_dec = np.sum(interval_data[dec_ids,:], axis=0)/self.params.spikes_per_bin
                delta_prob_stable[interval_id] = interval_fir_prob_dec[-1] - interval_fir_prob_dec[0]
                # compute for inc cells
                interval_fir_prob_inc = np.sum(interval_data[inc_ids,:], axis=0)/self.params.spikes_per_bin
                delta_prob_inc[interval_id] = interval_fir_prob_inc[-1] - interval_fir_prob_inc[0]

            else:
                delta_prob_stable[interval_id] = np.nan
                delta_prob_dec[interval_id] = np.nan
                delta_prob_inc[interval_id] = np.nan

        return delta_prob_stable, delta_prob_dec, delta_prob_inc

    def Delta_firing_for_interval(self, intervals_s):
        """
        assesses firing rate changes for defined intervals

        :param intervals_s: start and end of each interval in seconds
        :type intervals_s: np.array

        :return: array with average firing rates
        """
        # get stable, decreasing, increasing cells

        raster_list = []
        times_with_offset = []
        offset_at_raster_resolution = 0
        for l_s in self.long_sleep:
            # all_event_rasters = all_event_rasters + l_s.get_event_spike_rasters(part_to_analyze=part_to_analyze)[0]
            raster = l_s.get_raster()
            times_with_offset.append(np.arange(offset_at_raster_resolution,offset_at_raster_resolution+raster.shape[1]))
            raster_list.append(raster)
            duration = l_s.get_duration_sec()
            offset_at_raster_resolution += np.round(duration/self.params.time_bin_size)

        times_with_offset = np.hstack(times_with_offset)
        rasters = np.hstack(raster_list)

        # now go through intervals and compute mean firing using rasters
        delta_firing = np.zeros((rasters.shape[0], intervals_s.shape[0]))

        # convert intervals to raster resolution
        intervals_s_adj = intervals_s / self.params.time_bin_size

        for interval_id, (start, end) in enumerate(intervals_s_adj):
            if np.count_nonzero(np.logical_and(start < times_with_offset, times_with_offset < end)):
                interval_data = rasters[:,np.logical_and(start < times_with_offset, times_with_offset < end)]
                # compute for stable cells
                # interval_fir_prob_stable = np.sum(interval_data[stable_ids,:], axis=0)/self.params.spikes_per_bin

                # check length of interval --> compute difference between first half and second half
                interval_length = interval_data.shape[1]
                delta_firing[:, interval_id] = np.mean(interval_data[:, -int(interval_length/2):], axis=1) - \
                                               np.mean(interval_data[:, :int(interval_length/2)], axis=1)

            else:
                delta_firing[:, interval_id] = np.nan

        return delta_firing

    # </editor-fold>

    # <editor-fold desc="Memory drift support functions">

    @staticmethod
    def compute_values_from_likelihoods(pre_prob_list, post_prob_list, pre_prob_z_list, post_prob_z_list):
        # per event results
        event_pre_post_ratio = []
        event_pre_post_ratio_z = []
        event_pre_prob = []
        event_post_prob = []
        event_len_seq = []

        # per population vector results
        pop_vec_pre_post_ratio = []
        pre_seq_list = []
        pre_seq_list_z = []
        post_seq_list = []
        pre_seq_list_prob = []
        post_seq_list_prob = []
        pop_vec_post_prob = []
        pop_vec_pre_prob = []

        # go trough all events
        for pre_array, post_array, pre_array_z, post_array_z in zip(pre_prob_list, post_prob_list, pre_prob_z_list,
                                                                    post_prob_z_list):
            # make sure that there is any data for the current SWR
            if pre_array.shape[0] > 0:
                pre_sequence = np.argmax(pre_array, axis=1)
                pre_sequence_z = np.argmax(pre_array_z, axis=1)
                pre_sequence_prob = np.max(pre_array, axis=1)
                post_sequence = np.argmax(post_array, axis=1)
                post_sequence_prob = np.max(post_array, axis=1)
                pre_seq_list_z.extend(pre_sequence_z)
                pre_seq_list.extend(pre_sequence)
                post_seq_list.extend(post_sequence)
                pre_seq_list_prob.extend(pre_sequence_prob)
                post_seq_list_prob.extend(post_sequence_prob)

                # check how likely observed sequence is considering transitions from model (awake behavior)
                event_len_seq.append(pre_sequence.shape[0])

                # per SWR computations
                # ----------------------------------------------------------------------------------------------
                # arrays: [nr_pop_vecs_per_SWR, nr_time_spatial_time_bins]
                # get maximum value per population vector and take average across the SWR
                if pre_array.shape[0] > 0:
                    # save pre and post probabilities
                    event_pre_prob.append(np.mean(np.max(pre_array, axis=1)))
                    event_post_prob.append(np.mean(np.max(post_array, axis=1)))
                    # compute ratio by picking "winner" mode by first comparing z scored probabilities
                    # then the probability of the most over expressed mode (highest z-score) is used
                    pre_sequence_z = np.argmax(pre_array_z, axis=1)
                    prob_pre_z = np.mean(pre_array[:, pre_sequence_z])
                    post_sequence_z = np.argmax(post_array_z, axis=1)
                    prob_post_z = np.mean(post_array[:, post_sequence_z])
                    event_pre_post_ratio_z.append((prob_post_z - prob_pre_z) / (prob_post_z + prob_pre_z))

                    # compute ratio using probabilites
                    prob_pre = np.mean(np.max(pre_array, axis=1))
                    prob_post = np.mean(np.max(post_array, axis=1))
                    event_pre_post_ratio.append((prob_post - prob_pre) / (prob_post + prob_pre))
                else:
                    event_pre_prob.append(np.nan)
                    event_post_prob.append(np.nan)
                    event_pre_post_ratio.append(np.nan)

                # per population vector computations
                # ----------------------------------------------------------------------------------------------
                # compute per population vector similarity score
                prob_post = np.max(post_array, axis=1)
                prob_pre = np.max(pre_array, axis=1)
                pop_vec_pre_post_ratio.extend((prob_post - prob_pre) / (prob_post + prob_pre))

                if pre_array.shape[0] > 0:
                    pop_vec_pre_prob.extend(np.max(pre_array, axis=1))
                    pop_vec_post_prob.extend(np.max(post_array, axis=1))
                else:
                    pop_vec_pre_prob.extend([np.nan])
                    pop_vec_post_prob.extend([np.nan])

        pop_vec_pre_prob = np.array(pop_vec_pre_prob)
        pop_vec_post_prob = np.array(pop_vec_post_prob)
        pop_vec_pre_post_ratio = np.array(pop_vec_pre_post_ratio)
        pre_seq_list = np.array(pre_seq_list)

        return event_pre_post_ratio, event_pre_post_ratio_z, event_pre_prob, event_post_prob, event_len_seq,\
               pop_vec_pre_post_ratio, pre_seq_list, pre_seq_list_z, post_seq_list, pre_seq_list_prob, \
               post_seq_list_prob, pop_vec_post_prob, pop_vec_pre_prob

    def memory_drift_compute_results(self, template_type, pre_file_name=None, post_file_name=None, cells_to_use="all",
                                     shuffling=False, sleep_classification_method="std", speed_threshold=None):

        for i, l_s in enumerate(self.long_sleep):
            # compute all REM results
            l_s.decode_activity_using_pre_post(template_type=template_type, pre_file_name=pre_file_name,
                                               post_file_name=post_file_name, part_to_analyze="rem",
                                               cells_to_use=cells_to_use, return_results=False, shuffling=shuffling,
                                               speed_threshold=speed_threshold,
                                               sleep_classification_method=sleep_classification_method)
            # compute all NREM results
            l_s.decode_activity_using_pre_post(template_type=template_type, pre_file_name=pre_file_name,
                                               post_file_name=post_file_name, part_to_analyze="nrem",
                                               cells_to_use=cells_to_use, return_results=False, shuffling=shuffling,
                                               speed_threshold=speed_threshold,
                                               sleep_classification_method=sleep_classification_method)

    def memory_drift_phmm_results_event_times(self, event_times):

        all_pre = []
        all_post = []
        for i, l_s in enumerate(self.long_sleep):
            # compute all REM results
            pre_res, post_res = l_s.decode_phmm_pre_post_using_event_times(event_times=event_times)
            all_pre.append(pre_res)
            all_post.append(post_res)

        return all_pre, all_post

    def memory_drift_long_sleep_get_results(self, template_type, part_to_analyze, pop_vec_threshold,
                                            measure="normalized_ratio", pre_file_name=None, post_file_name=None,
                                            cells_to_use="all", shuffling=False, sleep_classification_method="std",
                                            return_pre_prob_list=False):

        first = 0
        pre_prob_list = []
        post_prob_list = []
        event_times_list = []
        for i, l_s in enumerate(self.long_sleep):
            if not i:
                default_pre_phmm_model, default_post_phmm_model,_ ,_ = l_s.get_pre_post_templates()
            duration = l_s.get_duration_sec()
            pre_prob, post_prob, ev_t, _ = l_s.decode_activity_using_pre_post(template_type=template_type,
                                                                              pre_file_name=pre_file_name,
                                                                              post_file_name=post_file_name,
                                                                              part_to_analyze=part_to_analyze,
                                                                              cells_to_use=cells_to_use,
                                                                              shuffling=shuffling,
                                                                              sleep_classification_method=
                                                                              sleep_classification_method)
            pre_prob_list.extend(pre_prob)
            post_prob_list.extend(post_prob)
            event_times_list.extend(ev_t + first)
            first += duration

        pre_prob_arr = np.vstack(pre_prob_list).astype(np.float32)
        post_prob_arr = np.vstack(post_prob_list).astype(np.float32)
        if measure == "normalized_ratio":
            result_all = (np.max(post_prob_arr, axis=1) - np.max(pre_prob_arr, axis=1)) / \
                        (np.max(pre_prob_arr, axis=1) + np.max(post_prob_arr, axis=1))
        elif measure == "log_like_ratio":
            result_all = np.log(np.max(post_prob_arr, axis=1)/np.max(pre_prob_arr, axis=1))
        elif measure == "model_evidence":
            if template_type == "phmm":
                # load pHMM models
                with open(self.params.pre_proc_dir + "phmm/" + default_pre_phmm_model + '.pkl', 'rb') as f:
                    model_pre_dic = pickle.load(f)
                stationary_dist_pre = model_pre_dic.get_stationary_distribution()
                with open(self.params.pre_proc_dir + "phmm/" + default_post_phmm_model + '.pkl', 'rb') as f:
                    model_post_dic = pickle.load(f)
                stationary_dist_post = model_post_dic.get_stationary_distribution()

                pre_prob = np.sum(pre_prob_arr * stationary_dist_pre, axis=1)
                post_prob = np.sum(post_prob_arr * stationary_dist_post, axis=1)
                result_all = np.log(post_prob/pre_prob)

        # assign array chunks to events again (either SWR or rem phases)
        length_per_event = [x.shape[0] for x in pre_prob_list]
        result_per_event = []
        pre_prob_per_event = []
        post_prob_per_event = []
        first = 0
        for swr_id in range(len(length_per_event)):
            result_per_event.append(result_all[first:first + length_per_event[swr_id]])
            pre_prob_per_event.append(pre_prob_arr[first:first + length_per_event[swr_id]])
            post_prob_per_event.append(post_prob_arr[first:first + length_per_event[swr_id]])
            first += length_per_event[swr_id]

        # delete swr that are too short (< 2 population vectors)
        short_nrem_events_to_delete = np.where(np.array(length_per_event) < pop_vec_threshold)[0]
        for index in sorted(short_nrem_events_to_delete, reverse=True):
            del result_per_event[index]
            del pre_prob_per_event[index]
            del post_prob_per_event[index]
            del length_per_event[index]
            del event_times_list[index]

        event_times = np.vstack(event_times_list)

        # compute length in seconds
        duration_event_in_s = event_times[:, 1] - event_times[:, 0]

        if return_pre_prob_list:
            return result_per_event, event_times, length_per_event, duration_event_in_s, pre_prob_per_event, post_prob_per_event
        else:
            return result_per_event, event_times, length_per_event, duration_event_in_s, pre_prob_arr, post_prob_arr

    def memory_drift_long_sleep_get_results_and_bin_times(self, template_type, part_to_analyze, pop_vec_threshold,
                                            measure="normalized_ratio", pre_file_name=None, post_file_name=None,
                                            cells_to_use="all", shuffling=False, sleep_classification_method="std",
                                            return_pre_prob_list=False):

        first = 0
        pre_prob_list = []
        post_prob_list = []
        event_times_list = []
        spike_bin_timings = []
        for i, l_s in enumerate(self.long_sleep):
            if not i:
                default_pre_phmm_model, default_post_phmm_model,_ ,_ = l_s.get_pre_post_templates()
            duration = l_s.get_duration_sec()
            _, _, bin_times = l_s.get_event_spike_rasters_and_times(part_to_analyze=part_to_analyze)
            spike_bin_timings.extend([x + first for x in bin_times])
            pre_prob, post_prob, ev_t, _ = l_s.decode_activity_using_pre_post(template_type=template_type,
                                                                              pre_file_name=pre_file_name,
                                                                              post_file_name=post_file_name,
                                                                              part_to_analyze=part_to_analyze,
                                                                              cells_to_use=cells_to_use,
                                                                              shuffling=shuffling,
                                                                              sleep_classification_method=
                                                                              sleep_classification_method)
            pre_prob_list.extend(pre_prob)
            post_prob_list.extend(post_prob)
            event_times_list.extend(ev_t + first)
            first += duration

        pre_prob_arr = np.vstack(pre_prob_list).astype(np.float32)
        post_prob_arr = np.vstack(post_prob_list).astype(np.float32)
        if measure == "normalized_ratio":
            result_all = (np.max(post_prob_arr, axis=1) - np.max(pre_prob_arr, axis=1)) / \
                        (np.max(pre_prob_arr, axis=1) + np.max(post_prob_arr, axis=1))
        elif measure == "log_like_ratio":
            result_all = np.log(np.max(post_prob_arr, axis=1)/np.max(pre_prob_arr, axis=1))
        elif measure == "model_evidence":
            if template_type == "phmm":
                # load pHMM models
                with open(self.params.pre_proc_dir + "phmm/" + default_pre_phmm_model + '.pkl', 'rb') as f:
                    model_pre_dic = pickle.load(f)
                stationary_dist_pre = model_pre_dic.get_stationary_distribution()
                with open(self.params.pre_proc_dir + "phmm/" + default_post_phmm_model + '.pkl', 'rb') as f:
                    model_post_dic = pickle.load(f)
                stationary_dist_post = model_post_dic.get_stationary_distribution()

                pre_prob = np.sum(pre_prob_arr * stationary_dist_pre, axis=1)
                post_prob = np.sum(post_prob_arr * stationary_dist_post, axis=1)
                result_all = np.log(post_prob/pre_prob)

        # assign array chunks to events again (either SWR or rem phases)
        length_per_event = [x.shape[0] for x in pre_prob_list]
        result_per_event = []
        pre_prob_per_event = []
        first = 0
        for swr_id in range(len(length_per_event)):
            result_per_event.append(result_all[first:first + length_per_event[swr_id]])
            pre_prob_per_event.append(pre_prob_arr[first:first + length_per_event[swr_id]])
            first += length_per_event[swr_id]

        # delete swr that are too short (< 2 population vectors)
        short_nrem_events_to_delete = np.where(np.array(length_per_event) < pop_vec_threshold)[0]
        for index in sorted(short_nrem_events_to_delete, reverse=True):
            del result_per_event[index]
            del pre_prob_per_event[index]
            del length_per_event[index]
            del event_times_list[index]
            try:
                del spike_bin_timings[index]
            except:
                print("Spike bin timings could not be adjusted")
        event_times = np.vstack(event_times_list)

        # compute length in seconds
        duration_event_in_s = event_times[:, 1] - event_times[:, 0]

        if return_pre_prob_list:
            return result_per_event, event_times, length_per_event, duration_event_in_s, pre_prob_per_event, \
                spike_bin_timings
        else:
            return result_per_event, event_times, length_per_event, duration_event_in_s, pre_prob_arr, post_prob_arr, \
                spike_bin_timings

    def memory_drift_long_sleep_get_merged_interval_times(self, rem_pop_vec_threshold=10, nrem_pop_vec_threshold=2,
                                            pre_file_name=None, post_file_name=None,
                                            cells_to_use="all", shuffling=False, sleep_classification_method="std"):

        # first get results for rem & nrem

        first = 0
        pre_prob_list_rem = []
        event_times_list_rem= []
        pre_prob_list_nrem = []
        event_times_list_nrem= []
        for i, l_s in enumerate(self.long_sleep):
            if not i:
                default_pre_phmm_model, default_post_phmm_model,_ ,_ = l_s.get_pre_post_templates()
            duration = l_s.get_duration_sec()
            pre_prob_rem_, _, ev_t_rem_, _ = l_s.decode_activity_using_pre_post(template_type="phmm",
                                                                              pre_file_name=pre_file_name,
                                                                              post_file_name=post_file_name,
                                                                              part_to_analyze="rem",
                                                                              cells_to_use=cells_to_use,
                                                                              shuffling=shuffling,
                                                                              sleep_classification_method=
                                                                              sleep_classification_method)
            pre_prob_nrem_, _, ev_t_nrem_, _ = l_s.decode_activity_using_pre_post(template_type="phmm",
                                                                              pre_file_name=pre_file_name,
                                                                              post_file_name=post_file_name,
                                                                              part_to_analyze="nrem",
                                                                              cells_to_use=cells_to_use,
                                                                              shuffling=shuffling,
                                                                              sleep_classification_method=
                                                                              sleep_classification_method)


            pre_prob_list_rem.extend(pre_prob_rem_)
            event_times_list_rem.extend(ev_t_rem_ + first)
            pre_prob_list_nrem.extend(pre_prob_nrem_)
            event_times_list_nrem.extend(ev_t_nrem_ + first)
            first += duration

        pre_prob_arr_rem = np.vstack(pre_prob_list_rem).astype(np.float32)
        pre_prob_arr_nrem = np.vstack(pre_prob_list_nrem).astype(np.float32)

        # assign array chunks to events again (either SWR or rem phases)
        length_per_event_rem = [x.shape[0] for x in pre_prob_list_rem]
        length_per_event_nrem = [x.shape[0] for x in pre_prob_list_nrem]

        # delete rem events that are too short (< 10 population vectors)
        short_rem_events_to_delete = np.where(np.array(length_per_event_rem) < rem_pop_vec_threshold)[0]

        for index in sorted(short_rem_events_to_delete, reverse=True):
            del length_per_event_rem[index]
            del event_times_list_rem[index]

        event_times_rem = np.vstack(event_times_list_rem)

        # compute length in seconds
        duration_event_in_s_rem = event_times_rem[:, 1] - event_times_rem[:, 0]

        # delete swr that are too short (< 2 population vectors)
        short_nrem_events_to_delete = np.where(np.array(length_per_event_nrem) < nrem_pop_vec_threshold)[0]

        for index in sorted(short_nrem_events_to_delete, reverse=True):
            del length_per_event_nrem[index]
            del event_times_list_nrem[index]

        event_times_nrem = np.vstack(event_times_list_nrem)

        # compute length in seconds
        duration_event_in_s_nrem = event_times_nrem[:, 1] - event_times_nrem[:, 0]

        all_events_length =  length_per_event_rem +  length_per_event_nrem
        all_events_length_s = np.hstack((duration_event_in_s_rem, duration_event_in_s_nrem))
        labels_events = np.zeros(len(length_per_event_rem)+len(length_per_event_nrem))
        # rem has label = 1
        labels_events[:len(length_per_event_rem)] = 1
        all_start_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort all events according to time they occurred
        sorted_labels_events = np.array([x for _, x in sorted(zip(all_start_times, labels_events))])
        sorted_end_times = np.array([x for _, x in sorted(zip(all_end_times, all_end_times))])
        sorted_start_times = np.array([x for x in sorted(all_start_times)])

        # now merge subsequent intervals with the same label
        first_interval = sorted_labels_events[0]
        # transitions = np.diff(sorted_labels_events)

        transitions = np.abs(np.diff(sorted_labels_events))

        # need to add one element at the beginning
        transitions = np.insert(transitions, 0, 1)
        start = np.nonzero(transitions==1)[0]
        end = start[1:]
        end = np.append(end, transitions.shape[0]-1)

        intervals = np.zeros((end.shape[0],2))
        labels = np.zeros(end.shape[0])
        for i_interval, (start_event, end_event) in enumerate(zip(start, end)):
            intervals[i_interval,0] = sorted_start_times[start_event]
            intervals[i_interval,1] = sorted_end_times[end_event]
            labels[i_interval] = sorted_labels_events[start_event]


        # rem label = 1
        rem_intervals = intervals[labels==1,:]
        # nrem label = 0
        nrem_intervals = intervals[labels==0,:]

        return rem_intervals, nrem_intervals

    def memory_drift_long_sleep_get_raw_results(self, template_type, part_to_analyze, pop_vec_threshold=2,
                                                pre_file_name=None, post_file_name=None, cells_to_use="all"):

        first = 0
        pre_prob_list = []
        post_prob_list = []
        event_times_list = []
        for l_s in self.long_sleep:
            duration = l_s.get_duration_sec()
            pre_prob, post_prob, ev_t, _ = l_s.decode_activity_using_pre_post(template_type=template_type,
                                                                              pre_file_name=pre_file_name,
                                                                              post_file_name=post_file_name,
                                                                              part_to_analyze=part_to_analyze,
                                                                              cells_to_use=cells_to_use)
            pre_prob_list.extend(pre_prob)
            post_prob_list.extend(post_prob)
            event_times_list.extend(ev_t + first)
            first += duration

        length_per_event = [x.shape[0] for x in pre_prob_list]
        # delete events that are too short (< 2 population vectors)
        short_nrem_events_to_delete = np.where(np.array(length_per_event) < pop_vec_threshold)[0]
        for index in sorted(short_nrem_events_to_delete, reverse=True):
            del pre_prob_list[index]
            del post_prob_list[index]
            del event_times_list[index]

        return pre_prob_list, post_prob_list, event_times_list

    def memory_drift_long_sleep_get_raw_results_and_spike_bins(self, template_type, part_to_analyze, pop_vec_threshold=2,
                                                pre_file_name=None, post_file_name=None, cells_to_use="all",
                                                               return_bin_duration=False):

        first = 0
        pre_prob_list = []
        post_prob_list = []
        event_times_list = []
        spike_bins = []
        event_spike_rasters = []
        bin_duration=[]
        for l_s in self.long_sleep:

            duration = l_s.get_duration_sec()
            pre_prob, post_prob, ev_t, _ = l_s.decode_activity_using_pre_post(template_type=template_type,
                                                                              pre_file_name=pre_file_name,
                                                                              post_file_name=post_file_name,
                                                                              part_to_analyze=part_to_analyze,
                                                                              cells_to_use=cells_to_use)
            event_spike_rasters_, event_spike_window_lengths_ = l_s.get_event_spike_rasters(part_to_analyze=part_to_analyze)
            event_spike_rasters.extend(event_spike_rasters_)
            pre_prob_list.extend(pre_prob)
            post_prob_list.extend(post_prob)
            event_times_list.extend(ev_t + first)
            bin_duration.extend(event_spike_window_lengths_)
            first += duration

        length_per_event = [x.shape[0] for x in pre_prob_list]
        # delete events that are too short (< 2 population vectors)
        short_nrem_events_to_delete = np.where(np.array(length_per_event) < pop_vec_threshold)[0]
        for index in sorted(short_nrem_events_to_delete, reverse=True):
            del pre_prob_list[index]
            del post_prob_list[index]
            del event_times_list[index]
            del event_spike_rasters[index]
            del bin_duration[index]

        if return_bin_duration:
            return pre_prob_list, post_prob_list, event_times_list, event_spike_rasters, bin_duration
        else:
            return pre_prob_list, post_prob_list, event_times_list, event_spike_rasters

    def memory_drift_long_sleep_save_raw_data(self, part_to_analyze, pre_or_post, file_name):
        pre_prob_list, post_prob_list, event_times_list = self.memory_drift_long_sleep_get_raw_results(
                                                               template_type="phmm",
                                                               part_to_analyze=part_to_analyze)
        if pre_or_post == "post":
            with open(file_name, 'wb') as f:
                pickle.dump(post_prob_list, f)

    # </editor-fold>

    # <editor-fold desc="Memory drift: decoding analysis">

    def memory_drift(self, template_type, measure="normalized_ratio", pre_file_name=None, post_file_name=None,
                     n_moving_average_pop_vec=200, rem_pop_vec_threshold=10, nrem_pop_vec_threshold=2,
                     plotting=False, cells_to_use="all", sleep_classification_method="std", shuffling=False):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param cells_to_use: which cells to use ("all", "stable", "inc", "dec")
        :type cells_to_use: str
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """
        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, _, _ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use, sleep_classification_method=
                                                     sleep_classification_method, shuffling=shuffling)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, _ ,_ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=nrem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use, sleep_classification_method=
                                                     sleep_classification_method, shuffling=shuffling)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------_decoding_similarity_temporal------------------------

        all_events_ratio = ratio_per_rem_event + ratio_per_nrem_event
        all_events_length = event_lengths_rem + event_lengths_nrem
        all_events_length_s = np.hstack((event_duration_rem_in_s, event_duration_nrem_in_s))
        labels_events = np.zeros(len(ratio_per_rem_event)+len(ratio_per_nrem_event))
        labels_events[:len(ratio_per_rem_event)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_ratio_list = [x for _, x in sorted(zip(all_times, all_events_ratio))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_lengths_s = [x for _, x in sorted(zip(all_times, all_events_length_s))]
        sorted_pop_vec_ratio = np.hstack(sorted_events_ratio_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_length_s = []
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat==1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat==-1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))
        merged_events_length_s = np.array(merged_events_length_s)
        merged_events_times = np.vstack(merged_events_times)

        # merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        # merged_nrem_event_times = merged_events_times[merged_events_labels == 0]
        merged_events_length_s = merged_events_times[:,1]-merged_events_times[:,0]


        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps==1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre_post ratio) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_merged_rem_event = []
        ratio_per_merged_nrem_event = []
        ratio_rem_nrem_events = []
        rem_nrem_events_label = []
        ratio_rem_nrem_pop_vec = []
        rem_nrem_pop_vec_label = []

        for start_event, end_event in zip(start, end):
            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                ratio_per_merged_rem_event.append(sorted_pop_vec_ratio[start_event:end_event])
            # nrem event
            else:
                ratio_per_merged_nrem_event.append(sorted_pop_vec_ratio[start_event:end_event])

            ratio_rem_nrem_events.append(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])

            ratio_rem_nrem_pop_vec.extend(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_pop_vec_label.extend(labels_per_pop_vec[start_event:end_event])


        # smooth across population vectors --> that means also across REM/NREM epochs
        # --------------------------------------------------------------------------------------------------------------
        len_new_events = [x.shape[0] for x in ratio_rem_nrem_events]
        ratio_per_pop_vec_new = np.hstack(ratio_rem_nrem_events)
        ratio_per_pop_vec_new_smooth = moving_average(a=np.array(ratio_per_pop_vec_new), n=n_moving_average_pop_vec)

        ratio_rem_nrem_events_smooth = []
        first = 0
        for event_id in range(len(len_new_events)):
            # check if data is still available (due to smoothing --> data gets shorter)
            # if not --> leave loop
            if first >= ratio_per_pop_vec_new_smooth.shape[0]:
                break
            end_event = min(first + len_new_events[event_id], ratio_per_pop_vec_new_smooth.shape[0])
            ratio_rem_nrem_events_smooth.append(ratio_per_pop_vec_new_smooth[first:end_event])
            first += len_new_events[event_id]

        # need to delete indices of events that happend after the last data point after smoothing
        rem_nrem_events_label = rem_nrem_events_label[:len(ratio_rem_nrem_events_smooth)]

        # get rem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        rem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 1))
        ds_rem = []
        ds_rem_smoothed_within = []
        merged_events_rem_length = []
        ratio_rem_events_smooth = []
        for rem_index in rem_events_indices:
            ds_rem.append(ratio_rem_nrem_events_smooth[rem_index][-1]-ratio_rem_nrem_events_smooth[rem_index][0])
            smooth_event = moving_average(a=ratio_rem_nrem_events[rem_index], n=10)
            ds_rem_smoothed_within.append(smooth_event[-1]-smooth_event[0])
            merged_events_rem_length.append(ratio_rem_nrem_events_smooth[rem_index].shape[0])
            ratio_rem_events_smooth.append(ratio_rem_nrem_events_smooth[rem_index])

        # get nrem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        nrem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 0))
        ds_nrem = []
        ds_nrem_smoothed_within = []
        merged_events_nrem_length = []
        ratio_nrem_events_smooth = []
        for nrem_index in nrem_events_indices:
            ds_nrem.append(ratio_rem_nrem_events_smooth[nrem_index][-1]-ratio_rem_nrem_events_smooth[nrem_index][0])
            merged_events_nrem_length.append(ratio_rem_nrem_events_smooth[nrem_index].shape[0])
            ratio_nrem_events_smooth.append(ratio_rem_nrem_events_smooth[nrem_index])

        # compute delta score sum & cum sum
        # --------------------------------------------------------------------------------------------------------------
        ds_nrem_sum_smoothed_within = np.sum(np.array(ds_nrem_smoothed_within))
        ds_rem_sum_smoothed_within = np.sum(np.array(ds_rem_smoothed_within))
        ds_nrem_sum = np.sum(np.array(ds_nrem))
        ds_rem_sum = np.sum(np.array(ds_rem))
        ds_nrem_cum = np.cumsum(np.array(ds_nrem))
        ds_rem_cum = np.cumsum(np.array(ds_rem))

        # compute cross correlation of delta scores NREM/REM
        # --------------------------------------------------------------------------------------------------------------
        min_len = min(len(ds_rem), len(ds_nrem))
        ds_rem_arr = np.array(ds_rem)[:min_len]
        ds_nrem_arr = np.array(ds_nrem)[:min_len]
        corr_list_pos = []
        corr_list_neg = []
        shift_array_cross_corr = [0, 1, 2, 3, 4, 5, 6]
        for shift in shift_array_cross_corr:
            corr_list_pos.append(
                np.round(pearsonr(ds_rem_arr[shift:], ds_nrem_arr[:ds_nrem_arr.shape[0] - shift])[0], 2))
            corr_list_neg.append(
                np.round(pearsonr(ds_nrem_arr[shift:], ds_rem_arr[:ds_rem_arr.shape[0] - shift])[0], 2))
        corr_list = np.hstack((np.flip(np.array(corr_list_neg)), np.array(corr_list_pos)))

        # get mean score per event
        # --------------------------------------------------------------------------------------------------------------
        mean_nrem = []
        for i, event in enumerate(ratio_per_merged_nrem_event):
            mean_nrem.append(np.mean(event))
        mean_rem = []
        for i, event in enumerate(ratio_per_merged_rem_event):
            mean_rem.append(np.mean(event))

        # get data smoothed only across REM events and only across NREM events
        # --------------------------------------------------------------------------------------------------------------
        ratio_merged_nrem_events_arr = np.hstack(ratio_per_merged_nrem_event)
        ratio_merged_nrem_events_arr_smooth = moving_average(a=ratio_merged_nrem_events_arr, n=n_moving_average_pop_vec)
        ratio_merged_rem_events_arr = np.hstack(ratio_per_merged_rem_event)
        ratio_merged_rem_events_arr_smooth = moving_average(a=ratio_merged_rem_events_arr, n=n_moving_average_pop_vec)

        # compute oscillations of REM/NREM periods
        # --------------------------------------------------------------------------------------------------------------
        # moving window
        window_size = 100
        step_size = 100

        window_start = np.arange(0, ratio_merged_rem_events_arr_smooth.shape[0] - window_size, step_size)
        min_max_rem = []
        std_rem = []
        for w_s in window_start:
            min_max_rem.append(max(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size])
                               - min(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size]))
            std_rem.append(np.std(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size]))
        window_start = np.arange(0, ratio_merged_nrem_events_arr_smooth.shape[0] - window_size, step_size)
        min_max_nrem = []
        std_nrem = []
        for w_s in window_start:
            min_max_nrem.append(max(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size])
                                - min(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size]))
            std_nrem.append(np.std(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size]))

        if plotting:

            # plotting
            fig = plt.figure()
            ax = fig.add_subplot()
            start = 0
            for event, label in zip(ratio_rem_nrem_events_smooth, rem_nrem_events_label):
                event_length = event.shape[0]
                if label:
                    ax.plot(np.arange(start, start + event_length), event, c="r", label="REM", linewidth=0.4)
                else:
                    ax.plot(np.arange(start, start + event_length), event, c="b", label="NREM")
                start += event_length
            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())
            plt.grid()
            plt.title("SMOOTHING: n="+str(n_moving_average_pop_vec))
            plt.xlabel("POPULATION VECTOR ID")
            if measure == "normalized_ratio":
                plt.ylim(-1, 1)
                plt.ylabel("PRE_POST RATIO")
            elif measure == "log_like_ratio":
                plt.ylabel("LOG-LIKELIHOOD RATIO")
                y_min, y_max = ax.get_ylim()
                plt.ylim(y_min, -1*y_min)

            elif measure == "model_evidence":
                plt.ylabel("MODEL EVIDENCE")
                y_min, y_max = ax.get_ylim()
                plt.ylim(y_min, -1*y_min)

            plt.show()
            plt.plot(ratio_merged_rem_events_arr_smooth, c="r", label="REM")
            plt.plot(ratio_merged_nrem_events_arr_smooth, c="b", label="NREM", alpha=0.8)
            plt.legend()
            plt.xlabel("POP.VEC.ID")
            plt.ylabel("PRE_POST_SCORE")
            plt.show()

            plt.hist(min_max_rem, color="r", density=True, label="REM")
            plt.hist(min_max_nrem, color="b", density=True, alpha=0.6, label="NREM")
            plt.legend()
            plt.xlabel("MAX-MIN IN SLIDING WINDOW OF "+str(window_size)+" POP.VEC.")
            plt.ylabel("DENSITY")
            plt.show()

            plt.hist(std_rem, color="r", density=True, label="REM")
            plt.hist(std_nrem, color="b", density=True, alpha=0.6, label="NREM")
            plt.legend()
            plt.xlabel("STD IN SLIDING WINDOW OF "+str(window_size)+" POP.VEC.")
            plt.ylabel("DENSITY")
            plt.show()

            def make_square_axes(ax):
                """Make an axes square in screen units.

                Should be called after plotting.
                """
                ax.set_aspect(1 / ax.get_data_ratio())

            plt.scatter(ds_rem_arr, ds_nrem_arr)
            plt.title("NEIGHBOURING PERIODS, R="+str(np.round(pearsonr(ds_rem_arr, ds_nrem_arr)[0], 2)))
            plt.xlabel("DELTA SCORE REM")
            plt.ylabel("DELTA SCORE NREM")
            make_square_axes(plt.gca())
            plt.show()

            x_axis_cross_corr = -1 * np.flip(np.array(shift_array_cross_corr)+1)
            x_axis_cross_corr = np.hstack((x_axis_cross_corr, np.array(shift_array_cross_corr)+1))
            plt.plot(x_axis_cross_corr, corr_list, marker=".")
            plt.xlabel("REM OFFSET (+X MEANS REM BEHIND NREM BY X)")
            plt.ylabel("CORRELATION DELTA SCORE REM VS. NREM")
            plt.title("CROSS CORRELATION OF DELTA SCORES REM VS. NREM")
            plt.show()
            #
            # plt.plot(ds_rem, color="gray", linewidth=0.8)
            # plt.scatter(range(len(ds_rem)), ds_rem, c=merged_events_length_s[merged_events_labels == 1], cmap=cm.Reds)
            # a = plt.colorbar()
            # a.set_label("DURATION / s")
            # plt.hlines(0, 0, len(ds_rem), color="w", linewidth=0.5, zorder=-1000)
            # plt.ylabel("DELTA SCORE")
            # plt.xlabel("EVENT ID")
            # plt.title("REM: PER EVENT DELTA SCORE & DURATION")
            # plt.show()
            #
            # plt.plot(ds_nrem, color="gray", linewidth=0.8)
            # plt.scatter(range(len(ds_nrem)), ds_nrem, c=merged_events_length_s[merged_events_labels == 0], cmap=cm.Reds)
            # a = plt.colorbar()
            # a.set_label("DURATION / s")
            # plt.hlines(0, 0, len(ds_nrem), color="w", linewidth=0.5, zorder=-1000)
            # plt.ylabel("DELTA SCORE")
            # plt.xlabel("EVENT ID")
            # plt.title("NREM: PER EVENT DELTA SCORE & DURATION")
            # plt.show()

            # plt.plot(merged_events_length_s[merged_events_labels == 1], color="gray", linewidth=0.8)
            # plt.scatter(range(len(merged_events_length_s[merged_events_labels == 1])),
            #             merged_events_length_s[merged_events_labels == 1], c=ds_rem, cmap=cm.coolwarm)
            # a = plt.colorbar()
            # a.set_label("DELTA SCORE")
            # plt.ylabel("DURATION EVENT")
            # plt.xlabel("EVENT ID")
            # plt.title("REM: PER EVENT DELTA SCORE & DURATION")
            # plt.show()
            #
            # plt.plot(merged_events_length_s[merged_events_labels == 0], color="gray", linewidth=0.8)
            # plt.scatter(range(len(merged_events_length_s[merged_events_labels == 0])),
            #             merged_events_length_s[merged_events_labels == 0], c=ds_nrem, cmap=cm.coolwarm)
            # a = plt.colorbar()
            # a.set_label("DELTA SCORE")
            # plt.ylabel("DURATION EVENT")
            # plt.xlabel("EVENT ID")
            # plt.title("NREM: PER EVENT DELTA SCORE & DURATION")
            # plt.show()

            # check which phase came first
            if rem_nrem_events_label[0] == 1:
                # first event was a rem event
                rem_x_axis = np.arange(0,2*len(ds_rem), 2)
                nrem_x_axis = np.arange(1, 2 * len(ds_nrem) + 1, 2)
            elif rem_nrem_events_label[0] == 0:
                # first event was a nrem event
                nrem_x_axis = np.arange(0,2*len(ds_nrem), 2)
                rem_x_axis = np.arange(1, 2 * len(ds_rem) + 1, 2)

            plt.plot(rem_x_axis, ds_rem, marker="." ,label="REM", color="r")
            plt.plot(nrem_x_axis, ds_nrem, marker=".", label="NREM", color="b")
            plt.legend()
            plt.hlines(0, 0, rem_x_axis.shape[0]+nrem_x_axis.shape[0], color="gray")
            plt.ylabel("DELTA SCORE")
            plt.xlabel("EVENT ID")
            plt.title("PER EVENT DELTA SCORE")
            plt.show()

            # plt.figure(figsize=(11, 6))
            # plt.subplot(1, 2, 1)
            # plt.plot(rem_x_axis, merged_events_rem_length, marker="." ,label="REM", color="r")
            # plt.plot(nrem_x_axis, merged_events_nrem_length, marker=".", label="NREM", color="b")
            # plt.legend()
            # plt.ylabel("#POPULATION VECTORS")
            # plt.xlabel("EVENT ID")
            # plt.title("#POPULATION VECTORS PER EVENT")
            # plt.subplot(1, 2, 2)
            # plt.plot(rem_x_axis, merged_events_length_s[merged_events_labels == 1], marker="." ,label="REM", color="r")
            # plt.plot(nrem_x_axis, merged_events_length_s[merged_events_labels == 0], marker=".", label="NREM", color="b")
            # plt.ylabel("DURATION / s")
            # plt.xlabel("EVENT ID")
            # plt.title("DURATION OF EACH EVENT")
            # plt.legend()
            # plt.show()


            plt.hist(merged_events_length_s[merged_events_labels == 1], bins=10, label="REM", density=True, color="r")
            plt.hist(merged_events_length_s[merged_events_labels == 0], bins=10, label="NREM", density=True, color="b")
            plt.title("EVENT DURATION (REM.POP.VEC.THRS.="+str(rem_pop_vec_threshold)+")")
            plt.xlabel("DURATION OF EVENT / s")
            plt.ylabel("DENSITY")
            plt.legend()
            plt.show()


            plt.figure(figsize=(11, 6))
            plt.subplot(1, 2, 1)
            plt.scatter(merged_events_length_s[rem_events_indices], ds_rem, color="r")
            plt.hlines(0, 0, plt.gca().get_xlim()[1], color="gray", linewidth=0.5)
            plt.ylabel("DELTA SCORE")
            plt.xlabel("DURATION / s")
            plt.title("REM PER EVENT (POP.THR.REM.="+str(rem_pop_vec_threshold)+")")
            make_square_axes(plt.gca())
            plt.subplot(1, 2, 2)
            plt.scatter(merged_events_length_s[nrem_events_indices], ds_nrem, color="b")
            plt.xlabel("DURATION / s")
            plt.hlines(0, 0, plt.gca().get_xlim()[1], color="gray", linewidth=0.5)
            plt.title("NREM PER EVENT")
            make_square_axes(plt.gca())
            plt.show()

            plt.scatter(merged_events_length_s[rem_events_indices], ds_rem, color="r", label="REM")
            plt.scatter(merged_events_length_s[nrem_events_indices], ds_nrem, color="b", label="NREM")
            plt.hlines(0, 0, plt.gca().get_xlim()[1], color="gray", linewidth=0.5)
            plt.ylabel("DELTA SCORE")
            plt.xlabel("DURATION / s")
            plt.title("DURATION - DELTA SCORE (POP.THR.REM.=" + str(rem_pop_vec_threshold) + ")")
            make_square_axes(plt.gca())
            plt.legend()
            plt.show()


            plt.figure(figsize=(11, 6))
            plt.subplot(1, 2, 1)
            plt.scatter(merged_events_rem_length, ds_rem, color="r")
            plt.hlines(0, 0, plt.gca().get_xlim()[1], color="gray", linewidth=0.5)
            plt.ylabel("DELTA SCORE")
            plt.xlabel("#POP.VEC.")
            plt.title("REM PER EVENT (POP.THR.REM.="+str(rem_pop_vec_threshold)+")")
            make_square_axes(plt.gca())
            plt.subplot(1, 2, 2)
            plt.scatter(merged_events_nrem_length, ds_nrem, color="b")
            plt.xlabel("#POP.VEC.")
            plt.hlines(0, 0, plt.gca().get_xlim()[1], color="gray", linewidth=0.5)
            plt.title("NREM PER EVENT")
            make_square_axes(plt.gca())
            plt.show()

            plt.title("SMOOTHING n=" + str(n_moving_average_pop_vec) + ", POP.THR.REM=" + str(rem_pop_vec_threshold))
            plt.plot(mean_nrem, marker=".", color="b", label="NREM")
            plt.plot(mean_rem, marker=".", color="r", label="REM")
            plt.xlabel("EVENT ID")
            plt.ylabel("MEAN SCORE PER EVENT")
            plt.legend()
            plt.show()
            common_nr = min(len(mean_nrem), len(mean_rem))
            diff_in_mean = np.abs(np.array(mean_nrem[:common_nr]) - np.array(mean_rem[:common_nr]))
            plt.title("DIFF REM NREM, SMOOTHING n=" + str(n_moving_average_pop_vec) + ", POP.THR.REM=" + str(rem_pop_vec_threshold))
            plt.plot(diff_in_mean, marker=".", color="w")
            plt.xlabel("EVENT ID")
            plt.ylabel("ABS. DIFF. BETWEEN MEAN SCORE PER EVENT")
            plt.show()

            plt.scatter(1, ds_nrem_sum, color="b", label="NREM", zorder=1000)
            # plt.scatter(1, ds_nrem_cum_smoothed_within, color="lightskyblue", label="NREM - WITHIN", zorder=1000)
            plt.scatter(1, ds_rem_sum, color="r", label="REM", zorder=1000)
            # plt.scatter(1, ds_rem_cum_smoothed_within, color="lightcoral", label="REM - WITHIN", zorder=1000)
            plt.legend()
            plt.ylabel("CUMULATIVE DELTA SCORE")
            plt.ylim(-(max(abs(ds_nrem_sum_smoothed_within), abs(ds_rem_sum_smoothed_within), abs(ds_rem_sum), abs(ds_nrem_sum))+1),
                     (max(abs(ds_nrem_sum_smoothed_within), abs(ds_rem_sum_smoothed_within), abs(ds_rem_sum), abs(ds_nrem_sum))+1))
            plt.title("SMOOTHING: n="+str(n_moving_average_pop_vec))
            plt.grid()
            plt.show()

            plt.title("SMOOTHING n=" + str(n_moving_average_pop_vec) + ", POP.THR.REM=" + str(rem_pop_vec_threshold))
            plt.plot(ds_nrem_cum, color="b", label="NREM")
            plt.plot(ds_rem_cum, color="r", label="REM")
            plt.grid()
            plt.xlabel("EVENT ID")
            plt.legend()
            plt.ylabel("CUM SUM")
            plt.show()

            y_rem,_,_ =plt.hist(ds_rem, color="r", label="REM", density=True)
            y_nrem,_,_ = plt.hist(ds_nrem, color="b", alpha=0.5, label="NREM", density=True)
            plt.vlines(np.mean(ds_nrem),0,y_nrem.max(), color="lightskyblue", label="MEAN NREM")
            plt.vlines(np.mean(ds_rem), 0, y_rem.max(), color="lightcoral", label="MEAN REM")
            plt.xlabel("DELTA SCORE")
            plt.ylabel("DENSITY")
            plt.title("SMOOTHING: n=" + str(n_moving_average_pop_vec))
            plt.legend()
            plt.show()

            # event shape

            plt.figure(figsize=(12, 6))
            plt.subplot(1, 2, 1)

            x_len = len(ratio_rem_events_smooth)
            y_len = len(max(ratio_rem_events_smooth, key=lambda x: len(x)))

            b = np.empty((x_len, y_len))
            b[:] = np.nan
            for i, j in enumerate(ratio_rem_events_smooth):
                b[i][0:len(j)] = j

            # subtract means to center
            b = b - np.nanmean(b, axis=1, keepdims=True)
            # scale values to lie between -1 and 1
            b = b / np.nanmax(np.abs(b), axis=1, keepdims= True)

            mean_shape = np.nanmean(b, axis=0)
            for shape in b:
                plt.plot(shape, c="gray")
            plt.plot(mean_shape, c="r")
            plt.ylabel("PRE_POST SCORE - SINGLE EVENTS + MEAN")
            plt.title("REM")
            plt.xlabel("POP.VEC.ID. FROM EVENT ONSET")

            plt.subplot(1, 2, 2)
            x_len = len(ratio_nrem_events_smooth)
            y_len = len(max(ratio_nrem_events_smooth, key=lambda x: len(x)))

            b = np.empty((x_len, y_len))
            b[:] = np.nan
            for i, j in enumerate(ratio_nrem_events_smooth):
                b[i][0:len(j)] = j

            # subtract means to center
            b = b - np.nanmean(b, axis=1, keepdims=True)
            # scale values to lie between -1 and 1
            b = b / np.nanmax(np.abs(b), axis=1, keepdims= True)

            mean_shape = np.nanmean(b, axis=0)
            for shape in b:
                plt.plot(shape, c="gray")
            plt.plot(mean_shape, c="b")
            plt.title("NREM")
            plt.xlabel("POP.VEC.ID. FROM EVENT ONSET")
            plt.show()

        else:
            return ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event

    def memory_drift_entire_sleep(self, random_seed=1, shuffling=False, n_smoothing=400, rem_pop_vec_threshold=10,
                                  nrem_pop_vec_threshold=2, plotting=True, swapping_factor=10):
        """
        instead of getting results from single shorter sleep files, this function computes results for the concatenated
        data --> needed for shuffling
        """

        # get model names
        # --------------------------------------------------------------------------------------------------------------
        pre_file_name = self.session_params.default_pre_phmm_model
        post_file_name = self.session_params.default_post_phmm_model

        # compute constant nr spike bins
        # --------------------------------------------------------------------------------------------------------------
        event_spike_raster_rem = []
        event_spike_window_lengths_rem = []
        start_times_rem = []
        end_times_rem = []
        event_spike_raster_nrem = []
        event_spike_window_lengths_nrem = []
        start_times_nrem = []
        end_times_nrem = []

        start_sleep_chunk = 0
        for i, l_s in enumerate(self.long_sleep):
            duration = l_s.get_duration_sec()
            esr_rem, eswl_rem, st_rem, et_rem = l_s.get_event_spike_rasters(part_to_analyze="rem",
                                                                             return_event_times=True)
            event_spike_raster_rem.extend(esr_rem)
            event_spike_window_lengths_rem.extend(eswl_rem)
            start_times_rem.extend(st_rem+start_sleep_chunk)
            end_times_rem.extend(et_rem+start_sleep_chunk)

            esr_nrem, eswl_nrem, st_nrem, et_nrem = l_s.get_event_spike_rasters(part_to_analyze="nrem",
                                                                             return_event_times=True)
            event_spike_raster_nrem.extend(esr_nrem)
            event_spike_window_lengths_nrem.extend(eswl_nrem)
            start_times_nrem.extend(st_nrem+start_sleep_chunk)
            end_times_nrem.extend(et_nrem+start_sleep_chunk)
            start_sleep_chunk += duration

        duration_sleep_h = start_sleep_chunk/60/60

        # filter nrem/rem events that are too short!
        # --------------------------------------------------------------------------------------------------------------

        # assign array chunks to events again (either SWR or rem phases)
        rem_event_length = [x.shape[0] for x in event_spike_raster_rem]
        nrem_event_length = [x.shape[0] for x in event_spike_raster_nrem]

        # delete nrem events that are too short
        short_nrem_events_to_delete = np.where(np.array(nrem_event_length) < nrem_pop_vec_threshold)[0]
        for index in sorted(short_nrem_events_to_delete, reverse=True):
            del event_spike_raster_nrem[index]
            del start_times_nrem[index]

        # delete rem events that are too short (< 2 population vectors)
        short_rem_events_to_delete = np.where(np.array(rem_event_length) < rem_pop_vec_threshold)[0]
        for index in sorted(short_rem_events_to_delete, reverse=True):
            del event_spike_raster_rem[index]
            del start_times_rem[index]

        # sort according to time
        # --------------------------------------------------------------------------------------------------------------
        all_spike_rasters = event_spike_raster_nrem + event_spike_raster_rem
        all_times = np.array(start_times_nrem + start_times_rem)

        # sort all events according to start time
        # --------------------------------------------------------------------------------------------------------------
        new_order = np.argsort(all_times)
        all_spike_rasters_ordered = []
        for id in new_order:
            all_spike_rasters_ordered.append(all_spike_rasters[id])

        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            model_dic_pre = pickle.load(f)
            # get means of model (lambdas) for decoding
        mode_means_pre = model_dic_pre.means_

        time_bin_size_encoding = model_dic_pre.time_bin_size

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            model_dic_post = pickle.load(f)
            # get means of model (lambdas) for decoding
        mode_means_post = model_dic_post.means_

        # check if const. #spike bins are correct for the loaded compression factor
        if not self.params.spikes_per_bin == 12:
            raise Exception("TRYING TO LOAD COMPRESSION FACTOR FOR 12 SPIKES PER BIN, "
                            "BUT CURRENT #SPIKES PER BIN != 12")

        # load correct compression factor (as defined in parameter file of the session)
        if time_bin_size_encoding == 0.01:
            compression_factor = np.round(self.session_params.sleep_compression_factor_12spikes_100ms * 10, 3)
        elif time_bin_size_encoding == 0.1:
            compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms
        else:
            raise Exception("COMPRESSION FACTOR NEITHER PROVIDED NOR FOUND IN PARAMETER FILE")

        cell_selection = "all"
        cells_ids = np.empty(0)

        # shuffling
        # --------------------------------------------------------------------------------------------------------------
        if shuffling:
            print("\nStarted shuffling procedure (random seed:"+str(random_seed)+") ... ")
            np.random.seed(random_seed)
            conc_data = np.hstack(all_spike_rasters_ordered)
            nr_swaps = conc_data.shape[1]*swapping_factor
            for shuffle_id in range(nr_swaps):
                # select two random time bins
                t1 = 1
                t2 = 1
                while (t1 == t2):
                    t1 = np.random.randint(conc_data.shape[1])
                    t2 = np.random.randint(conc_data.shape[1])
                # check in both time bins which cells are active
                act_cells_t1 = np.argwhere(conc_data[:, t1].flatten() > 0).flatten()
                act_cells_t2 = np.argwhere(conc_data[:, t2].flatten() > 0).flatten()
                # find intersect (same cells need to be firing in t1 and t2 in order to exchange spikes)
                # original code
                # ------------------------------------------------------------------------------------------------------
                # cells_firing_in_both = np.intersect1d(act_cells_t1, act_cells_t2)
                # if cells_firing_in_both.shape[0] > 1:
                #     # select first cell to swap
                #     cell_1 = 1
                #     cell_2 = 1
                #     while (cell_1 == cell_2):
                #         cell_1 = np.random.choice(cells_firing_in_both)
                #         cell_2 = np.random.choice(cells_firing_in_both)
                #     # do the actual swapping
                #     conc_data[cell_1, t1] += 1
                #     conc_data[cell_1, t2] -= 1
                #     conc_data[cell_2, t1] -= 1
                #     conc_data[cell_2, t2] += 1

                # modified code
                # ------------------------------------------------------------------------------------------------------
                if act_cells_t1.shape[0] > 1 and act_cells_t2.shape[0] > 1:
                    # select first cell to swap
                    cell_1 = 1
                    cell_2 = 1
                    while (cell_1 == cell_2):
                        cell_1 = np.random.choice(act_cells_t2)
                        cell_2 = np.random.choice(act_cells_t1)
                    # do the actual swapping
                    conc_data[cell_1, t1] += 1
                    conc_data[cell_1, t2] -= 1
                    conc_data[cell_2, t1] -= 1
                    conc_data[cell_2, t2] += 1

            print(" -- ... done!")
            # split data again into list
            event_lengths = [x.shape[1] for x in all_spike_rasters_ordered]

            event_spike_rasters_shuffled = []
            start = 0
            for el in event_lengths:
                event_spike_rasters_shuffled.append(conc_data[:, start:start + el])
                start = el

            all_spike_rasters_ordered = event_spike_rasters_shuffled

        # do the actual decoding: PRE and POST
        # --------------------------------------------------------------------------------------------------------------
        results_list_pre = decode_using_phmm_modes(mode_means=mode_means_pre,
                                               event_spike_rasters=all_spike_rasters_ordered,
                                               compression_factor=compression_factor,
                                               cell_selection=cell_selection, cells_to_use=cells_ids)

        results_list_post = decode_using_phmm_modes(mode_means=mode_means_post,
                                               event_spike_rasters=all_spike_rasters_ordered,
                                               compression_factor=compression_factor,
                                               cell_selection=cell_selection, cells_to_use=cells_ids)

        pre_likeli = np.vstack(results_list_pre)
        post_likeli = np.vstack(results_list_post)

        pre_max_likeli = np.max(pre_likeli, axis=1)
        post_max_likeli = np.max(post_likeli, axis=1)

        # compute and smooth similarity ratio
        # --------------------------------------------------------------------------------------------------------------
        sim_ratio = (post_max_likeli-pre_max_likeli)/(post_max_likeli+pre_max_likeli)
        sim_ratio_smooth = moving_average(a=sim_ratio, n=n_smoothing)

        if plotting:
            plt.plot(sim_ratio_smooth)
            plt.plot(np.linspace(0,np.round(duration_sleep_h,0),sim_ratio_smooth.shape[0]),sim_ratio_smooth)
            if shuffling:
                plt.title("Shuffled data")
                plt.ylabel("sim_ratio - shuffled")
            else:
                plt.title("Similarity ratio")
                plt.ylabel("sim_ratio")
            plt.ylim(-1,1)
            plt.xlabel("Time (h)")
            plt.show()
        else:
            return sim_ratio, sim_ratio_smooth, duration_sleep_h

    def memory_drift_entire_sleep_spike_shuffle_vs_data(self, save_fig=False, nr_shuffles=2, n_smoothing=400,
                                                        swapping_factor=10):

        # get data
        # --------------------------------------------------------------------------------------------------------------
        print("Computing results for data ...")
        data, data_smooth, duration_sleep_h = self.memory_drift_entire_sleep(shuffling=False, plotting=False,
                                                                             n_smoothing=n_smoothing)
        # compute shuffle
        # --------------------------------------------------------------------------------------------------------------

        if nr_shuffles > 1:
            print("Computing results for shuffled data (#"+str(nr_shuffles)+") ...")
            with mp.Pool(nr_shuffles) as p:
                # use partial to pass several arguments to cross_val_model, only nr_clusters_array is a changing one,
                # the others are constant
                multi_arg = partial(self.memory_drift_entire_sleep, plotting=False,  n_smoothing=n_smoothing, 
                                    swapping_factor=swapping_factor, shuffling=True)

                # provide random seed to each parallel run
                results = p.map(multi_arg, np.random.randint(0,5000,nr_shuffles))
                # result format: sim_ratio, sim_ratio_smooth, duration_sleep_h

                data_shuffle_smooth = []
                data_shuffle = []
                # put results together
                for shuffle_id in range(len(results)):
                    data_shuffle.append(results[shuffle_id][0])
                    data_shuffle_smooth.append(results[shuffle_id][1])

                data_shuffle_smooth = np.vstack(data_shuffle_smooth)
                data_shuffle = np.vstack(data_shuffle)

                data_shuffle_smooth_mean = np.mean(data_shuffle_smooth, axis=0)
                data_shuffle_smooth_std = np.std(data_shuffle_smooth, axis=0)

        else:

            data_shuffled, data_shuffled_smooth, _ = self.memory_drift_entire_sleep(shuffling=True, plotting=False,
                                                                                      n_smoothing=n_smoothing)
            # if we only do one shuffle --> we get a constant mean/std for the entire duration
            data_shuffle_smooth_mean = np.mean(data_shuffled_smooth)
            data_shuffle_smooth_std = np.std(data_shuffled_smooth)
            # if only one shuffle: can only compute mean and std for entire duration
            data_shuffle_smooth_mean = np.repeat(data_shuffle_smooth_mean, data_shuffled_smooth.shape[0])
            data_shuffle_smooth_std = np.repeat(data_shuffle_smooth_std, data_shuffled_smooth.shape[0])

        # save z-scored data to compute p-values
        data_smooth_z_scored_with_shuffle = (data_smooth - data_shuffle_smooth_mean)/data_shuffle_smooth_std
        np.save(self.params.pre_proc_dir+"/drift_vs_spike_shuffle/"+self.session_name, data_smooth_z_scored_with_shuffle)

        if save_fig:
            plt.style.use('default')

        # plotting
        fig = plt.figure()
        ax = fig.add_subplot()
        ax.plot(np.linspace(0, np.round(duration_sleep_h,0),data_shuffle_smooth_mean.shape[0]),
                data_shuffle_smooth_mean + data_shuffle_smooth_std, color="#B4915C",
                linestyle="dashed", linewidth=0.1)
        ax.plot(np.linspace(0,np.round(duration_sleep_h,0),data_shuffle_smooth_mean.shape[0]),
                data_shuffle_smooth_mean - data_shuffle_smooth_std, color="#B4915C",
                linestyle="dashed", linewidth=0.1)
        ax.fill_between(np.linspace(0,np.round(duration_sleep_h,0),data_shuffle_smooth_mean.shape[0]),
                        data_shuffle_smooth_mean-data_shuffle_smooth_std,
                        data_shuffle_smooth_mean+data_shuffle_smooth_std, facecolor="#D7BA8F")
        ax.plot(np.linspace(0,np.round(duration_sleep_h,0),data_shuffle_smooth_mean.shape[0]),data_shuffle_smooth_mean,
                color="#B4915C",
                label="Shuffle", linewidth=1)
        ax.plot(np.linspace(0,np.round(duration_sleep_h,0),data_smooth.shape[0]),data_smooth, c="#3D7865", label="Data")
        plt.legend()
        plt.grid(axis='y')
        plt.xlabel("Duration (h)")
        y_axis = [0, 7, 14, 21]
        plt.xticks(y_axis, y_axis)
        plt.yticks([-1,-0.5,0,0.5,1])
        plt.xlim(0, np.round(duration_sleep_h,0))
        plt.ylim(-1, 1)
        plt.ylabel("sim_ratio")
        if save_fig:
            plt.rcParams['font.family'] = 'arial'
            plt.rcParams['svg.fonttype'] = 'none'
            plt.savefig(os.path.join(save_path, "drift.svg"), transparent="True")
        else:
            plt.show()

    def memory_drift_entire_sleep_spike_shuffle_vs_data_compute_p_values(self):

        z_scored_data = np.load(self.params.pre_proc_dir+"drift_vs_spike_shuffle/"+self.session_name+".npy")

        # p-value for initial part of sleep: true data < shuffle --> use first 100 points
        z_scored_initial = np.mean(z_scored_data[:100])
        # p-value for last part of sleep: true data > shuffle --> use last 100 points
        z_scored_end = np.mean(z_scored_data[-100:])

        return z_scored_initial, z_scored_end

    def memory_drift_plot_epochs_separate(self, template_type, part_to_analyze, pre_file_name=None, post_file_name=None,
                                n_moving_average_swr=15, n_moving_average_pop_vec=60, cells_to_use="all"):
        """
        plot results of memory drift analysis for epochs separately (or in same plot, but not concatenated)

        :param template_type: which template to use ("phmm", "ising")
        :type template_type: str
        :param part_to_analyze: which sleep epoch to analyze ("rem", "nrem", "nrem_rem")
        :type part_to_analyze: str
        :param pre_file_name: name of PRE template, if None --> use default
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> use default
        :type post_file_name: str
        :param n_moving_average_swr: how much smoothing to apply across SWR
        :type n_moving_average_swr: int
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_vec: int
        :param only_stable_cells: whether to only use stable cells
        :type only_stable_cells: bool
        """
        if part_to_analyze == "nrem_rem":
            first = 0
            # plot nrem data first
            # ----------------------------------------------------------------------------------------------------------
            pre_prob_list = []
            post_prob_list = []
            event_times_list = []
            for l_s in self.long_sleep:
                duration = l_s.get_duration_sec()
                pre_prob, post_prob, ev_t, _ = l_s.decode_activity_using_pre_post(template_type=template_type,
                                                                                  pre_file_name=pre_file_name, post_file_name=post_file_name, part_to_analyze="nrem",
                                                                                  cells_to_use=cells_to_use)
                pre_prob_list.extend(pre_prob)
                post_prob_list.extend(post_prob)
                event_times_list.extend(ev_t+first)
                first += duration

            pre_prob_arr = np.vstack(pre_prob_list)
            post_prob_arr = np.vstack(post_prob_list)
            event_times = np.vstack(event_times_list)

            # z-scoring of probabilites
            pre_prob_arr_z = zscore(pre_prob_arr, axis=0)
            post_prob_arr_z = zscore(post_prob_arr, axis=0)

            # assign array chunks to events again (either SWR or rem phases)
            event_lengths = [x.shape[0] for x in pre_prob_list]
            pre_prob_z = []
            post_prob_z = []
            first = 0
            for swr_id in range(len(event_lengths)):
                pre_prob_z.append(pre_prob_arr_z[first:first + event_lengths[swr_id], :])
                post_prob_z.append(post_prob_arr_z[first:first + event_lengths[swr_id], :])
                first += event_lengths[swr_id]

            event_pre_post_ratio, event_pre_post_ratio_z, event_pre_prob, event_post_prob, event_len_seq, \
            pop_vec_pre_post_ratio, pre_seq_list, pre_seq_list_z, post_seq_list, pre_seq_list_prob, \
            post_seq_list_prob, pop_vec_post_prob, pop_vec_pre_prob = self.compute_values_from_likelihoods(
                pre_prob_list=pre_prob_list, post_prob_list=post_prob_list, post_prob_z_list=post_prob_z,
                pre_prob_z_list=pre_prob_z)

            # smoothen
            # ------------------------------------------------------------------------------------------------------
            event_pre_post_ratio_smooth = moving_average(a=np.array(event_pre_post_ratio), n=n_moving_average_swr)
            event_pre_post_ratio_smooth_z = moving_average(a=np.array(event_pre_post_ratio_z), n=n_moving_average_swr)
            pop_vec_pre_post_ratio = np.array(pop_vec_pre_post_ratio)
            # compute moving average to smooth signal
            pop_vec_pre_post_ratio_smooth = moving_average(a=pop_vec_pre_post_ratio, n=n_moving_average_pop_vec)
            event_len_seq_smooth = moving_average(a=np.array(event_len_seq), n=n_moving_average_swr)

            event_times = event_times[:event_pre_post_ratio_smooth.shape[0], :]
            # # plot per nrem phase
            fig = plt.figure()
            ax = fig.add_subplot()
            # for nrem_id in range(swr_to_nrem.shape[0]):
            #     ax.plot(event_times[swr_to_nrem[nrem_id,:]==1,1],
            #             event_pre_post_ratio_smooth[swr_to_nrem[nrem_id,:]==1],c="blue", label="NREM")

            # plot per nrem phase
            start = 0
            for rem_length, rem_time in zip(event_lengths, event_times):
                if start + rem_length > pop_vec_pre_post_ratio_smooth.shape[0]:
                    continue
                ax.plot(np.linspace(rem_time[0], rem_time[1], rem_length),
                        pop_vec_pre_post_ratio_smooth[start:start + rem_length], c="blue", label="NREM")
                start += rem_length


            # plot rem data
            # ----------------------------------------------------------------------------------------------------------
            first = 0

            pre_prob_list = []
            post_prob_list = []
            event_times_list = []
            for l_s in self.long_sleep:
                duration = l_s.get_duration_sec()
                pre_prob, post_prob, ev_t, _ = l_s.decode_activity_using_pre_post(template_type=template_type,
                                                                                  pre_file_name=pre_file_name, post_file_name=post_file_name, part_to_analyze="rem",
                                                                                  cells_to_use=cells_to_use)
                pre_prob_list.extend(pre_prob)
                post_prob_list.extend(post_prob)
                event_times_list.extend(ev_t+first)
                first += duration

            pre_prob_arr = np.vstack(pre_prob_list)
            post_prob_arr = np.vstack(post_prob_list)
            event_times = np.vstack(event_times_list)

            # assign array chunks to events again (either SWR or rem phases)
            event_lengths = [x.shape[0] for x in pre_prob_list]
            pre_prob_z = []
            post_prob_z = []
            first = 0
            for swr_id in range(len(event_lengths)):
                pre_prob_z.append(pre_prob_arr_z[first:first + event_lengths[swr_id], :])
                post_prob_z.append(post_prob_arr_z[first:first + event_lengths[swr_id], :])
                first += event_lengths[swr_id]

            event_pre_post_ratio, event_pre_post_ratio_z, event_pre_prob, event_post_prob, event_len_seq, \
            pop_vec_pre_post_ratio, pre_seq_list, pre_seq_list_z, post_seq_list, pre_seq_list_prob, \
            post_seq_list_prob, pop_vec_post_prob, pop_vec_pre_prob = self.compute_values_from_likelihoods(
                pre_prob_list=pre_prob_list, post_prob_list=post_prob_list, post_prob_z_list=post_prob_z,
            pre_prob_z_list=pre_prob_z)

            # smoothen
            # ------------------------------------------------------------------------------------------------------
            event_pre_post_ratio_smooth = moving_average(a=np.array(event_pre_post_ratio), n=n_moving_average_swr)
            event_pre_post_ratio_smooth_z = moving_average(a=np.array(event_pre_post_ratio_z), n=n_moving_average_swr)
            pop_vec_pre_post_ratio = np.array(pop_vec_pre_post_ratio)
            # compute moving average to smooth signal
            pop_vec_pre_post_ratio_smooth = moving_average(a=pop_vec_pre_post_ratio, n=n_moving_average_pop_vec)
            event_len_seq_smooth = moving_average(a=np.array(event_len_seq), n=n_moving_average_swr)

            # plot per nrem phase
            start = 0
            for rem_length, rem_time in zip(event_lengths, event_times):
                if start + rem_length > pop_vec_pre_post_ratio_smooth.shape[0]:
                    continue
                ax.plot(np.linspace(rem_time[0], rem_time[1], rem_length),
                        pop_vec_pre_post_ratio_smooth[start:start+rem_length],c="red", label="REM", alpha=0.5)
                start += rem_length

            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())
            plt.xlabel("TIME / s")
            plt.ylabel("PRE_POST SIMILARITY")
            plt.ylim(-1,1)
            plt.grid()
            plt.show()

        else:
            first = 0
            pre_prob_list = []
            post_prob_list = []
            event_times_list = []
            for l_s in self.long_sleep:
                duration = l_s.get_duration_sec()
                pre_prob, post_prob, ev_t, _ = l_s.decode_activity_using_pre_post(template_type=template_type,
                                                                                  pre_file_name=pre_file_name, post_file_name=post_file_name, part_to_analyze=part_to_analyze)
                pre_prob_list.extend(pre_prob)
                post_prob_list.extend(post_prob)
                event_times_list.extend(ev_t+first)
                first += duration

            pre_prob_arr = np.vstack(pre_prob_list)
            post_prob_arr = np.vstack(post_prob_list)

            # z-scoring of probabilites
            pre_prob_arr_z = zscore(pre_prob_arr, axis=0)
            post_prob_arr_z = zscore(post_prob_arr, axis=0)

            # assign array chunks to events again (either SWR or rem phases)
            event_lengths = [x.shape[0] for x in pre_prob_list]
            pre_prob_z = []
            post_prob_z = []
            first = 0
            for swr_id in range(len(event_lengths)):
                pre_prob_z.append(pre_prob_arr_z[first:first + event_lengths[swr_id], :])
                post_prob_z.append(post_prob_arr_z[first:first + event_lengths[swr_id], :])
                first += event_lengths[swr_id]

            nr_modes_pre = pre_prob_list[0].shape[1]
            nr_modes_post = post_prob_list[0].shape[1]

            # per event results
            event_pre_post_ratio = []
            event_pre_post_ratio_z = []
            event_pre_prob = []
            event_post_prob = []
            event_len_seq = []

            # per population vector results
            pop_vec_pre_post_ratio = []
            pre_seq_list = []
            pre_seq_list_z = []
            post_seq_list = []
            pre_seq_list_prob = []
            post_seq_list_prob = []
            pop_vec_post_prob = []
            pop_vec_pre_prob = []

            # go trough all events
            for pre_array, post_array, pre_array_z, post_array_z in zip(pre_prob_list, post_prob_list, pre_prob_z,
                                                                        post_prob_z):
                # make sure that there is any data for the current SWR
                if pre_array.shape[0] > 0:
                    pre_sequence = np.argmax(pre_array, axis=1)
                    pre_sequence_z = np.argmax(pre_array_z, axis=1)
                    pre_sequence_prob = np.max(pre_array, axis=1)
                    post_sequence = np.argmax(post_array, axis=1)
                    post_sequence_prob = np.max(post_array, axis=1)
                    pre_seq_list_z.extend(pre_sequence_z)
                    pre_seq_list.extend(pre_sequence)
                    post_seq_list.extend(post_sequence)
                    pre_seq_list_prob.extend(pre_sequence_prob)
                    post_seq_list_prob.extend(post_sequence_prob)

                    # check how likely observed sequence is considering transitions from model (awake behavior)
                    mode_before = pre_sequence[:-1]
                    mode_after = pre_sequence[1:]
                    event_len_seq.append(pre_sequence.shape[0])

                    # per SWR computations
                    # ----------------------------------------------------------------------------------------------
                    # arrays: [nr_pop_vecs_per_SWR, nr_time_spatial_time_bins]
                    # get maximum value per population vector and take average across the SWR
                    if pre_array.shape[0] > 0:
                        # save pre and post probabilities
                        event_pre_prob.append(np.mean(np.max(pre_array, axis=1)))
                        event_post_prob.append(np.mean(np.max(post_array, axis=1)))
                        # compute ratio by picking "winner" mode by first comparing z scored probabilities
                        # then the probability of the most over expressed mode (highest z-score) is used
                        pre_sequence_z = np.argmax(pre_array_z, axis=1)
                        prob_pre_z = np.mean(pre_array[:, pre_sequence_z])
                        post_sequence_z = np.argmax(post_array_z, axis=1)
                        prob_post_z = np.mean(post_array[:, post_sequence_z])
                        event_pre_post_ratio_z.append((prob_post_z - prob_pre_z) / (prob_post_z + prob_pre_z))

                        # compute ratio using probabilites
                        prob_pre = np.mean(np.max(pre_array, axis=1))
                        prob_post = np.mean(np.max(post_array, axis=1))
                        event_pre_post_ratio.append((prob_post - prob_pre) / (prob_post + prob_pre))
                    else:
                        event_pre_prob.append(np.nan)
                        event_post_prob.append(np.nan)
                        event_pre_post_ratio.append(np.nan)

                    # per population vector computations
                    # ----------------------------------------------------------------------------------------------
                    # compute per population vector similarity score
                    prob_post = np.max(post_array, axis=1)
                    prob_pre = np.max(pre_array, axis=1)
                    pop_vec_pre_post_ratio.extend((prob_post - prob_pre) / (prob_post + prob_pre))

                    if pre_array.shape[0] > 0:
                        pop_vec_pre_prob.extend(np.max(pre_array, axis=1))
                        pop_vec_post_prob.extend(np.max(post_array, axis=1))
                    else:
                        pop_vec_pre_prob.extend([np.nan])
                        pop_vec_post_prob.extend([np.nan])

            pop_vec_pre_prob = np.array(pop_vec_pre_prob)
            pop_vec_post_prob = np.array(pop_vec_post_prob)
            pop_vec_pre_post_ratio = np.array(pop_vec_pre_post_ratio)
            pre_seq_list = np.array(pre_seq_list)

            r_to_plot = range(0, 200)
            plt.figure(figsize=(10, 15))
            plt.subplot(3, 1, 1)
            plt.plot(pop_vec_pre_prob[r_to_plot], label="MAX. PROB. PRE")
            plt.plot(pop_vec_post_prob[r_to_plot], label="MAX. PROB. POST")
            # plt.plot(pre_SWR_prob_arr[r_to_plot, 10], c="r", label="PROB. MODE 60")
            plt.legend()
            plt.ylabel("PROB")
            plt.grid()
            plt.yscale("log")
            plt.subplot(3, 1, 2)
            plt.scatter(r_to_plot, pop_vec_pre_post_ratio[r_to_plot], c="magenta")
            plt.ylabel("PRE_POST RATIO")
            plt.grid()
            plt.subplot(3, 1, 3)
            plt.scatter(r_to_plot, pre_seq_list[r_to_plot], c="y")
            plt.ylabel("PRE MODE ID")
            plt.grid()
            plt.xlabel("POP. VEC. ID")
            plt.show()

            # smoothen
            # ------------------------------------------------------------------------------------------------------
            event_pre_post_ratio_smooth = moving_average(a=np.array(event_pre_post_ratio), n=n_moving_average_swr)
            event_pre_post_ratio_smooth_z = moving_average(a=np.array(event_pre_post_ratio_z), n=n_moving_average_swr)
            pop_vec_pre_post_ratio = np.array(pop_vec_pre_post_ratio)
            # compute moving average to smooth signal
            pop_vec_pre_post_ratio_smooth = moving_average(a=pop_vec_pre_post_ratio, n=n_moving_average_pop_vec)
            event_len_seq_smooth = moving_average(a=np.array(event_len_seq), n=n_moving_average_swr)

            # compute per mode info
            # ------------------------------------------------------------------------------------------------------
            pre_seq = np.array(pre_seq_list)
            post_seq = np.array(post_seq_list)
            mode_score_mean_pre = np.zeros(pre_prob_list[0].shape[1])
            mode_score_std_pre = np.zeros(pre_prob_list[0].shape[1])
            mode_score_mean_post = np.zeros(post_prob_list[0].shape[1])
            mode_score_std_post = np.zeros(post_prob_list[0].shape[1])

            # go through all pre modes and check the average score
            for i in range(pre_prob_list[0].shape[1]):
                ind_sel = np.where(pre_seq == i)[0]
                if ind_sel.size == 0 or ind_sel.size == 1:
                    mode_score_mean_pre[i] = np.nan
                    mode_score_std_pre[i] = np.nan
                else:
                    # delete all indices that are too large (becaue of moving average)
                    ind_sel = ind_sel[ind_sel < pop_vec_pre_post_ratio.shape[0]]
                    mode_score_mean_pre[i] = np.mean(pop_vec_pre_post_ratio[ind_sel])
                    mode_score_std_pre[i] = np.std(pop_vec_pre_post_ratio[ind_sel])

            # go through all post modes and check the average score
            for i in range(post_prob_list[0].shape[1]):
                ind_sel = np.where(post_seq == i)[0]
                if ind_sel.size == 0 or ind_sel.size == 1:
                    mode_score_mean_post[i] = np.nan
                    mode_score_std_post[i] = np.nan
                else:
                    # delete all indices that are too large (becaue of moving average)
                    ind_sel = ind_sel[ind_sel < pop_vec_pre_post_ratio.shape[0]]
                    mode_score_mean_post[i] = np.mean(pop_vec_pre_post_ratio[ind_sel])
                    mode_score_std_post[i] = np.std(pop_vec_pre_post_ratio[ind_sel])

            low_score_modes = np.argsort(mode_score_mean_pre)
            # need to skip nans
            nr_nans = np.count_nonzero(np.isnan(mode_score_mean_pre))
            high_score_modes = np.flip(low_score_modes)[nr_nans:]

            # check if modes get more often/less often reactivated over time
            pre_seq_list = np.array(pre_seq_list)
            nr_pop_vec = 20
            nr_windows = int(pre_seq_list.shape[0] / nr_pop_vec)
            occurence_modes_pre = np.zeros((nr_modes_pre, nr_windows))
            for i in range(nr_windows):
                seq = pre_seq_list[i * nr_pop_vec:(i + 1) * nr_pop_vec]
                mode, counts = np.unique(seq, return_counts=True)
                occurence_modes_pre[mode, i] = counts

            # check if modes get more often/less often reactivated over time
            post_seq_list = np.array(post_seq_list)
            nr_pop_vec = 20
            nr_windows = int(post_seq_list.shape[0] / nr_pop_vec)
            occurence_modes_post = np.zeros((nr_modes_pre, nr_windows))
            for i in range(nr_windows):
                seq = post_seq_list[i * nr_pop_vec:(i + 1) * nr_pop_vec]
                mode, counts = np.unique(seq, return_counts=True)
                occurence_modes_post[mode, i] = counts

            # plot similarity scores for SWR & pop vec
            # ------------------------------------------------------------------------------------------------------
            plt.plot(event_pre_post_ratio_smooth, c="r", label="n_mov_avg = " + str(n_moving_average_swr))
            plt.title("PRE-POST RATIO FOR EACH EVENT: PHMM")
            if part_to_analyze == "nrem":
                plt.xlabel("SWR ID")
            elif part_to_analyze == "rem":
                plt.xlabel("REM PHASE ID")
            plt.ylabel("PRE-POST SIMILARITY")
            plt.ylim(-1, 1)
            plt.grid()
            plt.legend()
            plt.show()

            plt.plot(event_pre_post_ratio_smooth_z, c="r", label="n_mov_avg = " + str(n_moving_average_swr))
            plt.title("PRE-POST RATIO FOR EACH EVENT: PHMM\n Z-SCORED TO SELECT WINNER")
            if part_to_analyze == "nrem":
                plt.xlabel("SWR ID")
            elif part_to_analyze == "rem":
                plt.xlabel("REM PHASE ID")
            plt.ylabel("PRE-POST SIMILARITY")
            plt.ylim(-1, 1)
            plt.grid()
            plt.legend()
            plt.show()

            plt.plot(pop_vec_pre_post_ratio_smooth, label="n_mov_avg = " + str(n_moving_average_pop_vec))
            plt.title("PRE-POST RATIO FOR EACH POP. VECTOR: PHMM")
            plt.xlabel("POP.VEC. ID")
            plt.ylabel("PRE-POST SIMILARITY")
            plt.ylim(-1, 1)
            plt.grid()
            plt.legend()
            plt.show()

            plt.plot(event_len_seq_smooth, label="n_mov_avg = " + str(n_moving_average_swr))
            plt.title("EVENT LENGTH")
            if part_to_analyze == "nrem":
                plt.xlabel("SWR ID")
            elif part_to_analyze == "rem":
                plt.xlabel("REM PHASE ID")
            plt.ylabel("#POP.VEC. PER SWR")
            plt.grid()
            plt.legend()
            plt.show()

            plt.imshow(occurence_modes_pre, interpolation='nearest', aspect='auto')
            plt.ylabel("MODE ID")
            plt.xlabel("WINDOW ID")
            a = plt.colorbar()
            a.set_label("#WINS/" + str(nr_pop_vec) + " POP. VEC. WINDOW")
            plt.title("PRE: OCCURENCE (#WINS) OF MODES IN WINDOWS OF FIXED LENGTH")
            plt.show()

            plt.imshow(occurence_modes_post, interpolation='nearest', aspect='auto')
            plt.ylabel("MODE ID")
            plt.xlabel("WINDOW ID")
            a = plt.colorbar()
            a.set_label("#WINS/" + str(nr_pop_vec) + " POP. VEC. WINDOW")
            plt.title("POST: OCCURENCE (#WINS) OF MODES IN WINDOWS OF FIXED LENGTH")
            plt.show()

            plt.errorbar(range(pre_prob[0].shape[1]), mode_score_mean_pre, yerr=mode_score_std_pre,
                         linestyle="")
            plt.scatter(range(pre_prob[0].shape[1]), mode_score_mean_pre)
            plt.title("PRE-POST SCORE PER MODE: PRE")
            plt.xlabel("MODE ID")
            plt.ylabel("PRE-POST SCORE: MEAN AND STD")
            plt.show()

            plt.errorbar(range(post_prob[0].shape[1]), mode_score_mean_post, yerr=mode_score_std_post,
                         linestyle="")
            plt.scatter(range(post_prob[0].shape[1]), mode_score_mean_post)
            plt.title("PRE-POST SCORE PER MODE: POST")
            plt.xlabel("MODE ID")
            plt.ylabel("PRE-POST SCORE: MEAN AND STD")
            plt.show()

    def memory_drift_time_course(self, template_type, n_moving_average_pop_vec=20000, rem_pop_vec_threshold=10,
                                        cells_to_use="all"):

        r_smooth = self.memory_drift_plot_temporal_trend(template_type=template_type, measure="normalized_ratio",
                                                         n_moving_average_pop_vec=n_moving_average_pop_vec,
                                                         rem_pop_vec_threshold=rem_pop_vec_threshold,
                                                         plotting=False, cells_to_use=cells_to_use, save_fig=False)

        r_first_half = r_smooth[:int(0.5*r_smooth.shape[0])]
        r_second_half = r_smooth[int(0.5 * r_smooth.shape[0]):]
        r_delta_first_half = np.mean(r_first_half[-50:]) - np.mean(r_first_half[:50])
        r_delta_second_half = np.mean(r_second_half[-50:]) - np.mean(r_second_half[:50])

        return r_delta_first_half, r_delta_second_half

    def memory_drift_plot_temporal_trend(self, template_type, measure="normalized_ratio", pre_file_name=None,
                                         post_file_name=None, n_moving_average_pop_vec=200, rem_pop_vec_threshold=10,
                                         plotting=False, cells_to_use="all", save_fig=False):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_ - np.mean(dec, axis=0)[0]vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param only_stable_cells: whether to only use stable cells
        :type only_stable_cells: bool
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """
        # get length of sleep in seconds

        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, _, _ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, _ ,_ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, measure=measure,
                                                     cells_to_use=cells_to_use)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------_decoding_similarity_temporal------------------------

        all_events_ratio = ratio_per_rem_event + ratio_per_nrem_event
        all_events_length = event_lengths_rem + event_lengths_nrem
        all_events_length_s = np.hstack((event_duration_rem_in_s, event_duration_nrem_in_s))
        labels_events = np.zeros(len(ratio_per_rem_event)+len(ratio_per_nrem_event))
        labels_events[:len(ratio_per_rem_event)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_ratio_list = [x for _, x in sorted(zip(all_times, all_events_ratio))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_lengths_s = [x for _, x in sorted(zip(all_times, all_events_length_s))]
        sorted_pop_vec_ratio = np.hstack(sorted_events_ratio_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_length_s = []
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat==1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat==-1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))
        merged_events_length_s = np.array(merged_events_length_s)
        merged_events_times = np.vstack(merged_events_times)

        # merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        # merged_nrem_event_times = merged_events_times[merged_events_labels == 0]
        merged_events_length_s = merged_events_times[:,1]-merged_events_times[:,0]

        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps==1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre_post ratio) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_merged_rem_event = []
        ratio_per_merged_nrem_event = []
        ratio_rem_nrem_events = []
        rem_nrem_events_label = []
        ratio_rem_nrem_pop_vec = []
        rem_nrem_pop_vec_label = []

        for start_event, end_event in zip(start, end):
            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                ratio_per_merged_rem_event.append(sorted_pop_vec_ratio[start_event:end_event])
            # nrem event
            else:
                ratio_per_merged_nrem_event.append(sorted_pop_vec_ratio[start_event:end_event])

            ratio_rem_nrem_events.append(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])

            ratio_rem_nrem_pop_vec.extend(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_pop_vec_label.extend(labels_per_pop_vec[start_event:end_event])


        # smooth across population vectors --> that means also across REM/NREM epochs
        # --------------------------------------------------------------------------------------------------------------
        len_new_events = [x.shape[0] for x in ratio_rem_nrem_events]
        ratio_per_pop_vec_new = np.hstack(ratio_rem_nrem_events)
        if n_moving_average_pop_vec > 0:

            ratio_per_pop_vec_new_smooth = moving_average(a=np.array(ratio_per_pop_vec_new), n=n_moving_average_pop_vec)

        else:
            ratio_per_pop_vec_new_smooth = np.array(ratio_per_pop_vec_new)

        ratio_rem_nrem_events_smooth = []
        first = 0
        for event_id in range(len(len_new_events)):
            # check if data is still available (due to smoothing --> data gets shorter)
            # if not --> leave loop
            if first >= ratio_per_pop_vec_new_smooth.shape[0]:
                break
            end_event = min(first + len_new_events[event_id], ratio_per_pop_vec_new_smooth.shape[0])
            ratio_rem_nrem_events_smooth.append(ratio_per_pop_vec_new_smooth[first:end_event])
            first += len_new_events[event_id]

        # need to delete indices of events that happend after the last data point after smoothing
        rem_nrem_events_label = rem_nrem_events_label[:len(ratio_rem_nrem_events_smooth)]

        r_smooth = np.hstack(ratio_rem_nrem_events_smooth)

        # control = []
        # slope_control = []
        # s = np.copy(ratio_per_pop_vec_new)
        # for i in range(500):
        #     np.random.shuffle(s)
        #     s_smooth = moving_average(a=np.array(s), n=n_moving_average_pop_vec)
        #     control.append(s_smooth)
        #     Y = np.expand_dims(s_smooth,0)
        #     X = np.expand_dims(np.linspace(0,np.round(len_sleep_h,0),s_smooth.shape[0]),0)
        #     slope = ((X*Y).mean(axis=1) - X.mean()*Y.mean(axis=1)) / ((X**2).mean() - (X.mean())**2)
        #     slope_control.append(slope)
        #
        # slope_control = np.array(slope_control)
        # Y = np.expand_dims(r_smooth, 0)
        # X = np.expand_dims(np.linspace(0, np.round(len_sleep_h, 0), r_smooth.shape[0]),0)
        # slope_data = ((X * Y).mean(axis=1) - X.mean() * Y.mean(axis=1)) / ((X ** 2).mean() - (X.mean()) ** 2)
        #
        # slope_control_99 = np.percentile(slope_control, 99)

        # if slope_data > slope_control_99:
        #     print("STATS OK: > 99%ile")
        # else:
        #     print("STATS NOT OK")

        # control = np.array(control)
        # con_mean = np.mean(control, axis=0)
        # con_std = np.std(control, axis=0)

        if plotting or save_fig:

            if save_fig:
                plt.style.use('default')

            # plotting
            fig = plt.figure()
            ax = fig.add_subplot()
            ax.plot(np.linspace(0,np.round(len_sleep_h,0),con_mean.shape[0]),con_mean + con_std, color="#B4915C", linestyle="dashed", linewidth=0.1)
            ax.plot(np.linspace(0,np.round(len_sleep_h,0),con_mean.shape[0]),con_mean - con_std, color="#B4915C", linestyle="dashed", linewidth=0.1)
            ax.fill_between(np.linspace(0,np.round(len_sleep_h,0),con_mean.shape[0]), con_mean-con_std, con_mean+con_std, facecolor="#D7BA8F")
            ax.plot(np.linspace(0,np.round(len_sleep_h,0),con_mean.shape[0]),con_mean, color="#B4915C", label="Shuffle", linewidth=1)
            ax.plot(np.linspace(0,np.round(len_sleep_h,0),r_smooth.shape[0]),r_smooth, c="#3D7865", label="Data")
            plt.legend()
            plt.grid(axis='y')
            plt.xlabel("Duration (h)")
            y_axis = [0, int(len_sleep_h * 0.25), int(len_sleep_h * 0.5), int(len_sleep_h * 0.75), int(len_sleep_h)]
            plt.xticks(y_axis, y_axis)
            plt.yticks([-1,-0.5,0,0.5,1])
            plt.xlim(0, np.round(len_sleep_h,0))
            plt.ylim(-1, 1)
            plt.ylabel("sim_ratio")
            plt.tight_layout()
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "drift.svg"), transparent="True")
            else:
                plt.show()

        return r_smooth

    def memory_drift_plot_likelihoods_temporal(self, template_type, pre_file_name=None,
                                         post_file_name=None, n_moving_average_pop_vec=1000, rem_pop_vec_threshold=10,
                                         plotting=False, cells_to_use="all", save_fig=False, nr_chunks=4):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_ - np.mean(dec, axis=0)[0]vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param only_stable_cells: whether to only use stable cells
        :type only_stable_cells: bool
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """
        # get length of sleep in seconds

        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_rem, post_prob_rem, event_times_rem = \
            self.memory_drift_long_sleep_get_raw_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_nrem, post_prob_nrem, event_times_nrem  = \
            self.memory_drift_long_sleep_get_raw_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, cells_to_use=cells_to_use)

        event_times_rem = np.vstack(event_times_rem)
        event_times_nrem = np.vstack(event_times_nrem)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------
        all_events_pre_prob = pre_prob_rem + pre_prob_nrem
        all_events_post_prob = post_prob_rem + post_prob_nrem
        event_lengths_rem = [x.shape[0] for x in pre_prob_rem]
        event_lengths_nrem = [x.shape[0] for x in pre_prob_nrem]
        all_events_length = event_lengths_rem + event_lengths_nrem
        labels_events = np.zeros(len(pre_prob_rem)+len(pre_prob_nrem))
        labels_events[:len(pre_prob_rem)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_pre_prob_list = [x for _, x in sorted(zip(all_times, all_events_pre_prob))]
        sorted_events_post_prob_list = [x for _, x in sorted(zip(all_times, all_events_post_prob))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_pre_prob = np.vstack(sorted_events_pre_prob_list)
        sorted_post_prob = np.vstack(sorted_events_post_prob_list)

        sorted_pre_prob_max = np.max(sorted_pre_prob, axis=1)
        sorted_post_prob_max = np.max(sorted_post_prob, axis=1)

        # filter large values

        z_pre = zscore(sorted_pre_prob_max)
        z_post = zscore(sorted_post_prob_max)

        sorted_pre_prob_max = sorted_pre_prob_max[z_pre < 2]
        sorted_post_prob_max = sorted_post_prob_max[z_post < 2]

        if plotting or save_fig:

            # seperate sleep into 4 chunks and compute boxplots
            chunk_len = int(sorted_pre_prob_max.shape[0]/nr_chunks)
            res_pre = []
            res_post = []
            for i_chunk in range(nr_chunks):
                res_pre.append(sorted_pre_prob_max[i_chunk*chunk_len:(i_chunk+1)*chunk_len])
                res_post.append(sorted_post_prob_max[i_chunk*chunk_len:(i_chunk+1)*chunk_len])

            c = "white"
            res = []
            for pre,post in zip(res_pre, res_post):
                res.append(pre)
                res.append(post)

            bplot = plt.boxplot(res, positions=[1, 2, 3, 4, 5, 6, 7, 8], patch_artist=True,
                                labels=["1st PRE", "1st POST", "2nd PRE", "2nd POST", "3rd PRE", "3rd POST",
                                        "4th PRE", "4th POST"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            colors = ["grey", 'lightgrey', "grey", 'lightgrey',"grey", 'lightgrey',"grey", 'lightgrey']
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            plt.title("Remapping using single cells")
            plt.ylabel("Pearson R: PRE map - POST map")
            plt.grid(color="grey", axis="y")
            plt.yscale("log")
            plt.show()

            smooth_pre_prob = moving_average(a=sorted_pre_prob_max, n=n_moving_average_pop_vec)
            smooth_post_prob = moving_average(a=sorted_post_prob_max, n=n_moving_average_pop_vec)
            if save_fig:
                plt.style.use('default')
            plt.plot(smooth_pre_prob, label="PRE")
            plt.plot(smooth_post_prob, label="POST")
            plt.yscale("log")
            plt.ylabel("Likelihood")
            plt.xlabel("Duration (h)")
            y_axis = [0, int(len_sleep_h*0.25), int(len_sleep_h*0.5), int(len_sleep_h*0.75), int(len_sleep_h)]
            y_ticks_pos = [0, int(smooth_pre_prob.shape[0]*0.25), int(smooth_pre_prob.shape[0]*0.5),
                           int(smooth_pre_prob.shape[0]*0.75), int(smooth_pre_prob.shape[0])]
            plt.xticks(y_ticks_pos, y_axis)
            plt.legend()
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "pre_post_likelihoods"+template_type+".svg"), transparent="True")
            else:
                plt.show()

        return sorted_pre_prob_max, sorted_post_prob_max, sorted_pre_prob, sorted_post_prob

    def memory_drift_decoding_quality_temporal(self, template_type, pre_file_name=None,
                                         post_file_name=None, n_moving_average_pop_vec=4000, rem_pop_vec_threshold=10,
                                         plotting=False, cells_to_use="all", save_fig=False, quality_thresh=0.8,
                                               window_size=1000):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_ - np.mean(dec, axis=0)[0]vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param only_stable_cells: whether to only use stable cells
        :type only_stable_cells: bool
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """
        # get length of sleep in seconds

        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        pre_control = []
        post_control = []
        # get control data
        for l_s in self.long_sleep:
            pre_control.append(l_s.decode_activity_control(pre_or_post="pre"))
            post_control.append(l_s.decode_activity_control(pre_or_post="post"))

        pre_control = np.hstack(pre_control)
        post_control = np.hstack(post_control)

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_rem, post_prob_rem, event_times_rem = \
            self.memory_drift_long_sleep_get_raw_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_nrem, post_prob_nrem, event_times_nrem  = \
            self.memory_drift_long_sleep_get_raw_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, cells_to_use=cells_to_use)

        event_times_rem = np.vstack(event_times_rem)
        event_times_nrem = np.vstack(event_times_nrem)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------
        all_events_pre_prob = pre_prob_rem + pre_prob_nrem
        all_events_post_prob = post_prob_rem + post_prob_nrem
        event_lengths_rem = [x.shape[0] for x in pre_prob_rem]
        event_lengths_nrem = [x.shape[0] for x in pre_prob_nrem]
        all_events_length = event_lengths_rem + event_lengths_nrem
        labels_events = np.zeros(len(pre_prob_rem)+len(pre_prob_nrem))
        labels_events[:len(pre_prob_rem)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_pre_prob_list = [x for _, x in sorted(zip(all_times, all_events_pre_prob))]
        sorted_events_post_prob_list = [x for _, x in sorted(zip(all_times, all_events_post_prob))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_pre_prob = np.vstack(sorted_events_pre_prob_list)
        sorted_post_prob = np.vstack(sorted_events_post_prob_list)

        dec_pre_prob = np.max(sorted_pre_prob, axis=1)
        dec_post_prob = np.max(sorted_post_prob, axis=1)

        pre_control_mean = np.mean(pre_control)
        pre_control_std = np.std(pre_control)
        post_control_mean = np.mean(post_control)
        post_control_std = np.std(post_control)

        dec_pre_prob_z = (dec_pre_prob - pre_control_mean)/pre_control_std
        dec_post_prob_z = (dec_post_prob - post_control_mean) / post_control_std

        pre_control_sorted = np.sort(pre_control)
        p_pre_control = 1. * np.arange(pre_control.shape[0]) / (pre_control.shape[0] - 1)
        plt.plot(pre_control_sorted, p_pre_control)
        # plt.yscale("log")
        plt.xlabel("likelihood control")
        plt.ylabel("CDF")
        plt.show()


        plt.plot(moving_average(dec_pre_prob_z, 10000))
        plt.yscale("log")
        plt.title("PRE")
        plt.xlabel("Time")
        plt.ylabel("Likelihood z-scored using control")
        plt.xlabel("Time")
        plt.show()

        plt.plot(moving_average(dec_post_prob_z, 10000))
        plt.yscale("log")
        plt.title("POST")
        plt.ylabel("Likelihood z-scored using control")
        plt.xlabel("Time")
        plt.show()

        sorted_pre_prob_z = zscore(sorted_pre_prob, axis=0)
        plt.plot(sorted_pre_prob_z)
        plt.yscale("log")
        plt.show()

        norm_pre_prob = sorted_pre_prob / np.sum(sorted_pre_prob, axis=1, keepdims=True)
        norm_pre_prob_decoded = np.max(norm_pre_prob, axis=1)
        norm_post_prob = sorted_post_prob / np.sum(sorted_post_prob, axis=1, keepdims=True)
        norm_post_prob_decoded = np.max(norm_post_prob, axis=1)

        sorted_pre_prob = np.max(sorted_pre_prob, axis=1)
        sorted_post_prob = np.max(sorted_post_prob, axis=1)

        smooth_pre_prob = moving_average(a=sorted_pre_prob, n=n_moving_average_pop_vec)
        smooth_post_prob = moving_average(a=sorted_post_prob, n=n_moving_average_pop_vec)

        plt.plot(smooth_pre_prob, color="blue")
        plt.xlabel("Time")
        plt.ylabel("Max PRE likeli")
        plt.show()

        plt.plot(smooth_post_prob, color="yellow")
        plt.xlabel("Time")
        plt.ylabel("Max POST likeli")
        plt.show()

        # check what percentage has been decoded with high quality
        nr_windows = np.round(norm_pre_prob_decoded.shape[0]/window_size).astype(int)

        fraction_high_quality_pre_decoding = np.zeros(nr_windows)
        fraction_high_quality_post_decoding = np.zeros(nr_windows)

        for i_window in range(nr_windows):
            fraction_high_quality_pre_decoding[i_window] = \
                np.count_nonzero(norm_pre_prob_decoded[i_window*window_size:(i_window+1)*window_size]>quality_thresh)/window_size
            fraction_high_quality_post_decoding[i_window] = \
                np.count_nonzero(norm_post_prob_decoded[i_window*window_size:(i_window+1)*window_size]>quality_thresh)/window_size


        plt.plot(moving_average(fraction_high_quality_pre_decoding,n=20))
        plt.title("PRE mode decoding quality")
        plt.ylabel("fraction events with high decoding quality")
        plt.xlabel("Moving window ID (time)")
        plt.ylim(0.3,0.5)
        plt.show()

        plt.plot(moving_average(fraction_high_quality_post_decoding,n=20))
        plt.title("POST mode decoding quality")
        plt.ylabel("fraction events with high decoding quality")
        plt.xlabel("Moving window ID (time)")
        plt.ylim(0.3,0.5)
        plt.show()

        plt.plot(moving_average(fraction_high_quality_pre_decoding,n=20), label="PRE", color="blue")
        plt.plot(moving_average(fraction_high_quality_post_decoding,n=20), label="POST", color="yellow")
        plt.title("Mode decoding quality")
        plt.ylabel("fraction events with high decoding quality")
        plt.xlabel("Moving window ID (time)")
        plt.legend()
        # plt.ylim(0.3,0.5)
        plt.show()

    def memory_drift_pre_post_mode_probability(self, template_type="phmm", pre_file_name=None,
                                         post_file_name=None, n_moving_average_pop_vec=100, rem_pop_vec_threshold=10,
                                         plotting=False, cells_to_use="all", save_fig=False):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_ - np.mean(dec, axis=0)[0]vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param only_stable_cells: whether to only use stable cells
        :type only_stable_cells: bool
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """
        # get length of sleep in seconds

        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_rem, post_prob_rem, event_times_rem = \
            self.memory_drift_long_sleep_get_raw_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_nrem, post_prob_nrem, event_times_nrem  = \
            self.memory_drift_long_sleep_get_raw_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, cells_to_use=cells_to_use)

        event_times_rem = np.vstack(event_times_rem)
        event_times_nrem = np.vstack(event_times_nrem)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------
        all_events_pre_prob = pre_prob_rem + pre_prob_nrem
        all_events_post_prob = post_prob_rem + post_prob_nrem
        event_lengths_rem = [x.shape[0] for x in pre_prob_rem]
        rem_bin_labels = [np.ones(x.shape[0]) for x in pre_prob_rem]
        nrem_bin_labels = [np.zeros(x.shape[0]) for x in pre_prob_nrem]
        event_lengths_nrem = [x.shape[0] for x in pre_prob_nrem]
        all_bin_labels = rem_bin_labels + nrem_bin_labels
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_pre_prob_list = [x for _, x in sorted(zip(all_times, all_events_pre_prob))]
        sorted_events_post_prob_list = [x for _, x in sorted(zip(all_times, all_events_post_prob))]
        sorted_bin_labels = [x for _, x in sorted(zip(all_times, all_bin_labels))]
        sorted_pre_prob = np.vstack(sorted_events_pre_prob_list)
        sorted_post_prob = np.vstack(sorted_events_post_prob_list)

        sorted_pre_prob = np.max(sorted_pre_prob, axis=1)
        sorted_post_prob = np.max(sorted_post_prob, axis=1)
        sorted_bin_labels = np.hstack(sorted_bin_labels)

        bins_per_window = 100
        n_windows = np.round(sorted_pre_prob.shape[0]/bins_per_window).astype(int)
        pre_proportion_per_window = np.zeros(n_windows)
        window_labels = np.zeros(n_windows)

        for window_id in range(n_windows):
            pre_subset = sorted_pre_prob[window_id*bins_per_window:(window_id+1)*bins_per_window]
            post_subset = sorted_post_prob[window_id * bins_per_window:(window_id + 1) * bins_per_window]
            pre_proportion_per_window[window_id] = np.count_nonzero(np.greater(pre_subset, post_subset))/bins_per_window
            if np.count_nonzero(sorted_bin_labels[
                                window_id * bins_per_window:(window_id + 1) * bins_per_window]) > bins_per_window/2:
                window_labels[window_id] = 1
            else:
                window_labels[window_id] = 0

        plt.plot(pre_proportion_per_window)
        plt.ylabel("Prob. of decoding PRE mode")
        plt.xlim(0, pre_proportion_per_window.shape[0])
        plt.xlabel("Window ID")
        plt.show()

        plt.figure()
        from matplotlib.collections import LineCollection
        c = []
        labels = []
        for time_point in range(window_labels.shape[0]):
            if window_labels[time_point] == 0:
                c.append("blue")
                labels.append("NREM")
            elif window_labels[time_point] == 1:
                c.append("red")
                labels.append("REM")

        x_val = np.arange(pre_proportion_per_window.shape[0])
        lines = [((x0,y0), (x1,y1)) for x0, y0, x1, y1 in zip(x_val[:-1], pre_proportion_per_window[:-1], x_val[1:],
                                                              pre_proportion_per_window[1:])]
        colored_lines = LineCollection(lines, colors=c, linewidths=(2,), label=labels)
        plt.gca().add_collection(colored_lines)
        # ax3.plot(neigh_dist_1, color="white")
        # handles, labels = plt.gca().get_legend_handles_labels()
        # by_label = dict(zip(labels, handles))
        # plt.legend(by_label.values(), by_label.keys())
        # plt.legend()
        plt.ylabel("Prob. of decoding PRE mode")
        plt.xlim(0, pre_proportion_per_window.shape[0])
        plt.xlabel("Window ID")
        plt.show()

        pre_proportion_per_window_smooth = moving_average(a=pre_proportion_per_window, n=n_moving_average_pop_vec)
        plt.plot(pre_proportion_per_window_smooth)
        plt.ylabel("Prob. of decoding PRE mode (smooth)")
        plt.xlim(0, pre_proportion_per_window_smooth.shape[0])
        plt.xlabel("Window ID")
        plt.show()

    def memory_drift_pre_post_mode_probability_raster(self, template_type="phmm", pre_file_name=None,
                                         post_file_name=None, n_moving_average=30, rem_pop_vec_threshold=10,
                                         plotting=False, cells_to_use="all", duration_window_s = 100, save_fig=False):

        # get length of sleep in seconds
        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_rem, post_prob_rem, event_times_rem, _, bin_duration_rem= \
            self.memory_drift_long_sleep_get_raw_results_and_spike_bins(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold,
                                                    cells_to_use=cells_to_use, return_bin_duration=True)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_nrem, post_prob_nrem, event_times_nrem, _, bin_duration_nrem  = \
            self.memory_drift_long_sleep_get_raw_results_and_spike_bins(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, cells_to_use=cells_to_use, return_bin_duration=True)

        event_times_rem = np.vstack(event_times_rem)
        event_times_nrem = np.vstack(event_times_nrem)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------
        all_events_pre_prob = pre_prob_rem + pre_prob_nrem
        all_events_post_prob = post_prob_rem + post_prob_nrem
        event_lengths_rem = [x.shape[0] for x in pre_prob_rem]
        rem_bin_labels = [np.ones(x.shape[0]) for x in pre_prob_rem]
        nrem_bin_labels = [np.zeros(x.shape[0]) for x in pre_prob_nrem]
        event_lengths_nrem = [x.shape[0] for x in pre_prob_nrem]
        all_bin_labels = rem_bin_labels + nrem_bin_labels
        all_bin_durations = bin_duration_rem + bin_duration_nrem
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_pre_prob_list = [x for _, x in sorted(zip(all_times, all_events_pre_prob))]
        sorted_events_post_prob_list = [x for _, x in sorted(zip(all_times, all_events_post_prob))]
        sorted_bin_labels = [x for _, x in sorted(zip(all_times, all_bin_labels))]
        sorted_bin_durations = [x for _, x in sorted(zip(all_times, all_bin_durations))]
        sorted_pre_prob = np.vstack(sorted_events_pre_prob_list)
        sorted_post_prob = np.vstack(sorted_events_post_prob_list)

        # need to combine pre and post probabilities when we look at decoding
        decoded_mode = np.argmax(np.hstack((sorted_pre_prob, sorted_post_prob)), axis=1)
        sorted_bin_labels = np.hstack(sorted_bin_labels)
        sorted_bin_durations = np.hstack(sorted_bin_durations)

        bins_per_window=600
        n_windows = np.round(sorted_pre_prob.shape[0]/bins_per_window).astype(int)
        pre_proportion_per_window = np.zeros(n_windows)
        window_labels = np.zeros(n_windows)
        # decoded_pre_per_window = np.zeros((pre_prob_nrem[0].shape[1], n_windows))
        # decoded_post_per_window = np.zeros((post_prob_nrem[0].shape[1], n_windows))

        # matching the duration of each window
        # duration = np.cumsum(sorted_bin_durations)
        # bin_cutoffs = np.argwhere(np.diff(np.mod(duration, duration_window_s))<0).flatten()
        # n_windows=bin_cutoffs.shape[0]
        decoded_per_window = np.zeros((sorted_pre_prob.shape[1]+sorted_post_prob.shape[1], n_windows))

        # start_window=0
        for window_id in range(n_windows):
            # pre_mode_ids, counts_pre = np.unique(decoded_mode[start_window:bin_cutoffs[window_id]],
            #                               return_counts=True)
            mode_ids, counts = np.unique(decoded_mode[window_id*bins_per_window:(window_id+1)*bins_per_window],
                                          return_counts=True)
            # duration of window
            # dur_window = np.sum(sorted_bin_durations[window_id*bins_per_window:(window_id+1)*bins_per_window])

            decoded_per_window[mode_ids, window_id] = counts/bins_per_window
            # start_window = bin_cutoffs[window_id]

        decoded_pre_per_window = decoded_per_window[:sorted_pre_prob.shape[1],:]
        decoded_pre_per_window_sorted = decoded_pre_per_window[np.flip(np.argsort(np.mean(decoded_pre_per_window[:,
                                                     :int(decoded_pre_per_window.shape[1]/2)], axis=1))), :]
        
        # clustering
        # _, a = kmeans2(data=decoded_pre_per_window, k=8)
        #
        # clustered_pre_per_window = np.zeros((decoded_pre_per_window.shape[0], decoded_pre_per_window.shape[1]))
        # # for each cluster compute mean rate
        # cluster_ids = np.unique(a)
        # cluster_means = np.zeros(cluster_ids.shape[0])
        # for i, cluster_id in enumerate(cluster_ids):
        #     cluster_means[i] = np.mean(np.mean(decoded_pre_per_window[np.argwhere(a==cluster_id).flatten(),:],
        #                                                 axis=1))
        # cluster_ids_sorted = cluster_ids[np.flip(np.argsort(cluster_means))]
        # i=0
        # for cluster_id in cluster_ids_sorted:
        #     for cluster_modes in np.argwhere(a==cluster_id).flatten():
        #         clustered_pre_per_window[i,:] = decoded_pre_per_window[cluster_modes,:]
        #         i += 1
        

        decoded_post_per_window = decoded_per_window[sorted_pre_prob.shape[1]:,:]
        decoded_post_per_window_sorted = decoded_post_per_window[np.flip(np.argsort(np.mean(decoded_post_per_window[:,
                                                                                            -int(decoded_post_per_window.shape[1]/2):], axis=1))), :]

        # clustering
        # n_smoothing=100
        # decoded_post_per_window_smooth = np.zeros((decoded_post_per_window.shape[0], decoded_post_per_window.shape[1]-n_smoothing+1))
        # for i in range(decoded_post_per_window.shape[0]):
        #     decoded_post_per_window_smooth[i,:] = moving_average(decoded_post_per_window[i,:],n_smoothing)
        #
        # _, a = kmeans2(data=decoded_post_per_window_smooth, k=8)
        #
        # clustered_post_per_window = np.zeros((decoded_post_per_window.shape[0], decoded_post_per_window.shape[1]))
        # # for each cluster compute mean rate
        # cluster_ids = np.unique(a)
        # cluster_means = np.zeros(cluster_ids.shape[0])
        # for i, cluster_id in enumerate(cluster_ids):
        #     cluster_means[i] = np.mean(np.mean(decoded_post_per_window[np.argwhere(a==cluster_id).flatten(),:],
        #                                                 axis=1))
        # cluster_ids_sorted = cluster_ids[np.flip(np.argsort(cluster_means))]
        # i=0
        # for cluster_id in cluster_ids_sorted:
        #     for cluster_modes in np.argwhere(a==cluster_id).flatten():
        #         clustered_post_per_window[i,:] = decoded_post_per_window[cluster_modes,:]
        #         i += 1
        #

        summed_decoded_post = moving_average(np.sum(decoded_post_per_window_sorted, axis=0), n=n_moving_average)
        summed_decoded_pre = moving_average(np.sum(decoded_pre_per_window_sorted, axis=0), n=n_moving_average)

        if plotting or save_fig:
            if save_fig:
                plt.style.use('default')

            plt.plot(summed_decoded_post, label="Recall modes")
            plt.plot(summed_decoded_pre, label="Acquisition modes")
            plt.xlabel("Time")
            plt.ylabel("Reactivation probability")
            plt.legend()
            plt.show()

            y_min = np.min(np.min(decoded_per_window[decoded_per_window>0], axis=0))
            y_max = np.max(np.max(decoded_per_window, axis=0))

            fig = plt.figure(figsize=(7, 6))
            gs = fig.add_gridspec(20, 40)
            ax1 = fig.add_subplot(gs[:10, :-2])
            m = ax1.imshow(decoded_pre_per_window_sorted, norm=LogNorm(y_min, y_max),
                           interpolation='none', aspect='auto', cmap="BuPu")
            # ax1.set_yticks(list(np.round(np.linspace(0, decoded_pre_per_window.shape[0]-1, 10))),
            #                list(np.round(np.linspace(0, decoded_pre_per_window.shape[0], 10)).astype(int)))
            ax1.set_xticks([])
            ax1.set_ylabel("Acquisition modes (sorted)")
            ax2 = fig.add_subplot(gs[11:, :-2])
            m = ax2.imshow(decoded_post_per_window_sorted, norm=LogNorm(y_min, y_max),
                           interpolation='none', aspect='auto', cmap="BuPu")
            ax2.set_ylabel("Recall modes (sorted)")
            ax2.set_xlabel("Time")
            ax2.set_xticks(np.linspace(0, decoded_pre_per_window_sorted.shape[1], 4), np.round(np.linspace(0, len_sleep_h, 4)).astype(str))
            ax2.yaxis.tick_right()
            ax2.yaxis.set_label_position("right")
            ax3 = fig.add_subplot(gs[:9, -1])
            c = plt.colorbar(mappable=m, cax=ax3, ax=ax3)
            c.set_label("Reactivation probability")
            plt.tight_layout()
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "pre_post_modes_decoding_probability.svg"), transparent="True")
            else:
                plt.show()

        return summed_decoded_pre, summed_decoded_post

    def memory_drift_pre_post_mode_probability_stability_vs_similarity(self, template_type="phmm", pre_file_name=None,
                                                      post_file_name=None, distance_metric="correlation",
                                                      rem_pop_vec_threshold=10,
                                                      plotting=False, cells_to_use="all", thresh_stab=2,
                                                      save_fig=False, bins_per_window = 600, return_mode_ids=False):

        # get length of sleep in seconds
        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep / 60 / 60

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_rem, post_prob_rem, event_times_rem, _, bin_duration_rem = \
            self.memory_drift_long_sleep_get_raw_results_and_spike_bins(template_type=template_type,
                                                                        pre_file_name=pre_file_name,
                                                                        post_file_name=post_file_name,
                                                                        part_to_analyze="rem",
                                                                        pop_vec_threshold=rem_pop_vec_threshold,
                                                                        cells_to_use=cells_to_use,
                                                                        return_bin_duration=True)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_nrem, post_prob_nrem, event_times_nrem, _, bin_duration_nrem = \
            self.memory_drift_long_sleep_get_raw_results_and_spike_bins(template_type=template_type,
                                                                        pre_file_name=pre_file_name,
                                                                        post_file_name=post_file_name,
                                                                        part_to_analyze="nrem",
                                                                        pop_vec_threshold=2, cells_to_use=cells_to_use,
                                                                        return_bin_duration=True)

        event_times_rem = np.vstack(event_times_rem)
        event_times_nrem = np.vstack(event_times_nrem)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------
        all_events_pre_prob = pre_prob_rem + pre_prob_nrem
        all_events_post_prob = post_prob_rem + post_prob_nrem
        event_lengths_rem = [x.shape[0] for x in pre_prob_rem]
        rem_bin_labels = [np.ones(x.shape[0]) for x in pre_prob_rem]
        nrem_bin_labels = [np.zeros(x.shape[0]) for x in pre_prob_nrem]
        event_lengths_nrem = [x.shape[0] for x in pre_prob_nrem]
        all_bin_labels = rem_bin_labels + nrem_bin_labels
        all_bin_durations = bin_duration_rem + bin_duration_nrem
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:, 0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_pre_prob_list = [x for _, x in sorted(zip(all_times, all_events_pre_prob))]
        sorted_events_post_prob_list = [x for _, x in sorted(zip(all_times, all_events_post_prob))]
        sorted_bin_labels = [x for _, x in sorted(zip(all_times, all_bin_labels))]
        sorted_bin_durations = [x for _, x in sorted(zip(all_times, all_bin_durations))]
        sorted_pre_prob = np.vstack(sorted_events_pre_prob_list)
        sorted_post_prob = np.vstack(sorted_events_post_prob_list)

        # need to combine pre and post probabilities when we look at decoding
        decoded_mode = np.argmax(np.hstack((sorted_pre_prob, sorted_post_prob)), axis=1)
        sorted_bin_labels = np.hstack(sorted_bin_labels)
        sorted_bin_durations = np.hstack(sorted_bin_durations)

        n_windows = np.round(sorted_pre_prob.shape[0] / bins_per_window).astype(int)
        pre_proportion_per_window = np.zeros(n_windows)
        window_labels = np.zeros(n_windows)

        decoded_per_window = np.zeros((sorted_pre_prob.shape[1] + sorted_post_prob.shape[1], n_windows))

        # start_window=0
        for window_id in range(n_windows):
            mode_ids, counts = np.unique(decoded_mode[window_id * bins_per_window:(window_id + 1) * bins_per_window],
                                         return_counts=True)
            decoded_per_window[mode_ids, window_id] = counts / bins_per_window

        # fit line to get stability
        coeff_mode = np.zeros(decoded_per_window.shape[0])
        for mode_id, mode_data in enumerate(decoded_per_window):
            if np.sum(mode_data) < 0.05:
                coeff_mode[mode_id] = np.nan
            else:
                coeff = np.polyfit(np.linspace(0,1, mode_data.shape[0]), mode_data, 1)
                # plt.plot(np.linspace(0,1, mode_data.shape[0]), mode_data)
                # plt.plot(np.linspace(0,1, mode_data.shape[0]), coeff[1]+np.linspace(0,1, mode_data.shape[0])*coeff[0])
                # plt.show()
                coeff_mode[mode_id] = coeff[0]

        coeff_mode_pre = coeff_mode[:sorted_pre_prob.shape[1]]
        coeff_mode_post = coeff_mode[sorted_pre_prob.shape[1]:]

        # get mode lambdas and compute distance PRE to POST modes
        with open(self.params.pre_proc_dir + "phmm/" + self.session_params.default_pre_phmm_model + '.pkl', 'rb') as f:
            pre_model = pickle.load(f)
        # get means of model (lambdas) for decoding
        modes_lambdas_pre = pre_model.means_

        with open(self.params.pre_proc_dir + "phmm/" + self.session_params.default_post_phmm_model + '.pkl', 'rb') as f:
            post_model = pickle.load(f)
        # get means of model (lambdas) for decoding
        modes_lambdas_post = post_model.means_

        dist_mat = cdist(modes_lambdas_pre, modes_lambdas_post, metric=distance_metric)

        min_distance_per_pre_mode = np.min(dist_mat, axis=1)
        min_distance_per_post_mode = np.min(dist_mat, axis=0)

        # select only valid modes (at least n_min reactivations)
        mode_ids_pre = np.arange(min_distance_per_pre_mode.shape[0])
        min_distance_per_pre_mode = min_distance_per_pre_mode[~np.isnan(coeff_mode_pre)]
        mode_ids_pre = mode_ids_pre[~np.isnan(coeff_mode_pre)]
        coeff_mode_pre=coeff_mode_pre[~np.isnan(coeff_mode_pre)]

        min_distance_per_pre_mode = min_distance_per_pre_mode[zscore(coeff_mode_pre) < -1* thresh_stab]
        mode_ids_pre = mode_ids_pre[zscore(coeff_mode_pre) < -1* thresh_stab]
        coeff_mode_pre=coeff_mode_pre[zscore(coeff_mode_pre) < -1* thresh_stab]

        # select only valid modes (at least n_min reactivations)
        mode_ids_post = np.arange(min_distance_per_post_mode.shape[0])
        min_distance_per_post_mode = min_distance_per_post_mode[~np.isnan(coeff_mode_post)]
        mode_ids_post = mode_ids_post[~np.isnan(coeff_mode_post)]
        coeff_mode_post=coeff_mode_post[~np.isnan(coeff_mode_post)]

        min_distance_per_post_mode = min_distance_per_post_mode[zscore(coeff_mode_post) > thresh_stab]
        mode_ids_post = mode_ids_post[zscore(coeff_mode_post) > thresh_stab]
        coeff_mode_post=coeff_mode_post[zscore(coeff_mode_post) > thresh_stab]

        if plotting:

            plt.scatter(min_distance_per_post_mode, coeff_mode_post)
            plt.xlabel("Min. distance to any PRE mode "+distance_metric)
            plt.ylabel("Non-stationarity of reactivation probability")
            plt.title("POST states, "+str(pearsonr(min_distance_per_post_mode, coeff_mode_post)))
            plt.show()

            plt.scatter(min_distance_per_pre_mode, coeff_mode_pre)
            plt.xlabel("Min. distance to any POST mode "+distance_metric)
            plt.ylabel("Non-stationarity of reactivation probability")
            plt.title("PRE states, "+str(pearsonr(min_distance_per_pre_mode, coeff_mode_pre)))
            plt.show()

        if return_mode_ids:
            return min_distance_per_pre_mode, coeff_mode_pre, min_distance_per_post_mode, \
                coeff_mode_post, mode_ids_pre, mode_ids_post
        else:
            return min_distance_per_pre_mode, coeff_mode_pre, min_distance_per_post_mode, \
                coeff_mode_post

    def memory_drift_sparsity_and_distance_of_modes(self, pre_file_name=None, post_file_name=None, rem_pop_vec_threshold=10,
                                         plotting=False, save_fig=False, th=3, n_lim=30, q_lim=0.7, n_segments=5,
                                       pre_or_post="pre", metric="cosine"):
        """

        Parameters
        ----------
        pre_file_name phmm model pre (str)
        post_file_name phmm model post (str)
        rem_pop_vec_threshold min number pop vecs per REM event
        plotting whether to plot or not (bool)
        save_fig saving or not (bool)
        th Activity threshold to compute sparsity
        n_lim Minimum Number of activations for one mode to be used
        q_lim Minimum reactivation strength for taking a mode as a winner
        n_segments number of segments to divide sleep
        """

        if pre_or_post == "pre":
            file_name = self.session_params.default_pre_phmm_model
        elif pre_or_post == "post":
            file_name = self.session_params.default_post_phmm_model
        # need PRE and POST modes
        with open(self.params.pre_proc_dir + "phmm/" + file_name + '.pkl', 'rb') as f:
            model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        modes_lambdas = model_dic.means_.T

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_rem, post_prob_rem, _, rasters_rem = \
            self.memory_drift_long_sleep_get_raw_results_and_spike_bins(template_type="phmm", pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, cells_to_use="all")

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_nrem, post_prob_nrem, _, rasters_nrem  = \
            self.memory_drift_long_sleep_get_raw_results_and_spike_bins(template_type="phmm", pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, cells_to_use="all")

        raster_rem = np.hstack(rasters_rem)
        raster_nrem = np.hstack(rasters_nrem)
        if pre_or_post == "pre":
            prob_rem = np.vstack(pre_prob_rem)
            prob_nrem = np.vstack(pre_prob_nrem)
        elif pre_or_post == "post":
            prob_rem = np.vstack(post_prob_rem)
            prob_nrem = np.vstack(post_prob_nrem)

        # COMPUTE POISSON 12-Vector Spike VECTOR PROBABILITES FROM MODES
        # (TO HAVE DATA IN THE SAME FORMAT OF REACTIVATIONS)
        # go trough all the modes
        for mo in range(modes_lambdas.shape[1]):
            l_i = modes_lambdas[:, mo]
            sp_t_all = np.zeros((0,))
            sp_i_all = np.zeros((0,))
            sp_m = np.zeros(l_i.shape[0])
            # cycle through each neuron
            for nn in range(l_i.shape[0]):
                # generate 10000 inter-spike intervals --> cumulative sum to get timing of the spike
                sp_t = np.cumsum(np.random.exponential(1 / l_i[nn], (10000, 1)))
                # use the maximum to define minimum window that contains spikes from all the cells
                sp_m[nn] = np.max(sp_t)
                # combine for all the cells
                sp_t_all = np.concatenate((sp_t_all, sp_t))
                # cell identities to see which spike from all
                sp_i = np.ones((10000,)) * nn
                sp_i_all = np.concatenate((sp_i_all, sp_i))
                # Take the earlier last spike from any cell
            # get minimum window that contains spikes from all cells
            thr = np.min(sp_m)
            sp_i_all = sp_i_all[sp_t_all < thr]
            sp_t_all = sp_t_all[sp_t_all < thr]

            # Rearrange spike in time --> to get spiking in sequential order with corresponding cell ID
            aa = np.argsort(sp_t_all)
            sp_i_all = sp_i_all[aa]

            # Build average spike occurrence
            n_samp = int(np.floor(sp_t_all.shape[0] / 12))
            raster_mod = np.zeros((l_i.shape[0], n_samp))
            for ss in range(n_samp):
                take_sp = sp_i_all[ss * 12:ss * 12 + 12].astype(int)
                for sp in range(len(take_sp)):
                    raster_mod[take_sp[sp], ss] += 1
            modes_lambdas[:, mo] = np.mean(raster_mod, axis=1)

        # NREM
        # --------------------------------------------------------------------------------------------------------------
        # Temporal interval to consider (in number of population vectors)
        t_tot = raster_nrem.shape[1]
        t_int = np.floor(t_tot / 6).astype(int)

        sparsity_reactivations_nrem = np.zeros((n_segments, modes_lambdas.shape[1]))
        Spars_Ratio = np.zeros((n_segments, modes_lambdas.shape[1]))  # Sparsity Comparison

        distance_modes_nrem = np.zeros((n_segments, int(modes_lambdas.shape[1]*(modes_lambdas.shape[1]-1)/2)))  # Correlation in modes and reactivations
        distance_reactivations_nrem = np.zeros((n_segments, int(modes_lambdas.shape[1]*(modes_lambdas.shape[1]-1)/2)))

        # Cycle over sleep segments
        for ii in range(n_segments):

            th_fr = th * 0.09

            Raster_subset = raster_nrem[:, 0 + t_int * ii:0 + t_int * (ii + 1)]
            # Raster = zscore(Raster,axis=1)

            prob_nrem_subset = prob_nrem[0 + t_int * ii:0 + t_int * (ii + 1)]

            # normalize likelihoods
            Nor = np.sum(prob_nrem_subset, axis=1)
            prob_nrem_subset = prob_nrem_subset / np.reshape(Nor, (t_int, 1))

            # find decoded mode
            BT = np.argmax(prob_nrem_subset, axis=1)
            # get maximum normalized likelihood
            VT = np.max(prob_nrem_subset, axis=1)
            # set modes that were decoded with a worse quality than q_lim to 1000 --> indicates that mode bin is not
            # considered for following computations
            BT[VT < q_lim] = 1000
            Best_Template = BT  # np.ones((BT.shape))*1000
            # Best_Template[0+7000*ii:0+7000*(ii+1)]=BT[0+7000*ii:0+7000*(ii+1)]

            React = np.zeros((Raster_subset.shape[0], prob_nrem_subset.shape[1]))
            Mode_Prob = np.zeros(prob_nrem_subset.shape[1])

            # go through all modes
            for te in range(modes_lambdas.shape[1]):
                # check how often each mode was reactivated --> compute probability of reactivation
                Mode_Prob[te] = len(np.where(Best_Template == te)[0]) / len(Best_Template)
                # if mode was reactivated more than n_lim times --> compute mean across all reactivations of this mode
                if (Raster_subset[:, Best_Template == te].shape[1] > n_lim):
                    React[:, te] = np.mean(Raster_subset[:, Best_Template == te], axis=1)  # -React_m

            React[np.isnan(React)] = 0
            React = React - th_fr
            React[React < 0] = 0

            Modes = modes_lambdas - th_fr
            Modes[Modes < 0] = 0
            Modes[np.isnan(Modes)] = 0

            # Compute correlation between mode and reactivation average
            # CC = np.corrcoef(React.T, Modes.T)
            # Simil_Mo[th, :] = np.diag(CC[:modes_lambdas.shape[1], modes_lambdas.shape[1]:])

            distance_reactivations_nrem[ii, :] = pdist(React.T, metric=metric)
            distance_modes_nrem[ii, :] = pdist(Modes.T, metric=metric
                                               )

            # Compute sparsity of vectors
            React_Sp = (np.nansum(React, axis=0) ** 2) / (np.nansum(React ** 2, axis=0)) / modes_lambdas.shape[0]
            Mode_Sp = (np.nansum(Modes, axis=0) ** 2) / (np.nansum(Modes ** 2, axis=0)) / modes_lambdas.shape[0]
            React_Sp[np.isnan(React_Sp)] = 0

            p1 = React_Sp
            p2 = Mode_Sp  # modes_lambdas

            Spars_Ratio[ii, :] = p1 / p2
            sparsity_reactivations_nrem[ii, :] = React_Sp# Ratio of sparsity React/Modes

        Spars_Ratio_NREM = Spars_Ratio

        # DO THE REM
        # --------------------------------------------------------------------------------------------------------------

        # Temporal interval to consider (in number of population vectors)
        t_tot = raster_rem.shape[1]
        t_int = np.floor(t_tot / 6).astype(int)

        # distance_modes_nrem = np.zeros((n_segments, int(modes_lambdas.shape[1]*(modes_lambdas.shape[1]-1)/2)))
        sparsity_reactivations_rem = np.zeros((n_segments, modes_lambdas.shape[1]))
        Spars_Ratio = np.zeros((n_segments, modes_lambdas.shape[1]))  # Sparsity Comparison

        distance_modes_rem = np.zeros((n_segments, int(modes_lambdas.shape[1]*(modes_lambdas.shape[1]-1)/2)))  # Correlation in modes and reactivations
        distance_reactivations_rem = np.zeros((n_segments, int(modes_lambdas.shape[1]*(modes_lambdas.shape[1]-1)/2)))

        # Cycle over different activity thresholds

        for ii in range(n_segments):
            th_fr = th * 0.09

            raster_rem_subset = raster_rem[:, 0 + t_int * ii:0 + t_int * (ii + 1)]
            # Raster = zscore(Raster,axis=1)

            prob_rem_subset = prob_rem[0 + t_int * ii:0 + t_int * (ii + 1)]

            Nor = np.sum(prob_rem_subset, axis=1)
            prob_rem_subset = prob_rem_subset / np.reshape(Nor, (t_int, 1))

            BT = np.argmax(prob_rem_subset, axis=1)
            VT = np.max(prob_rem_subset, axis=1)
            BT[VT < q_lim] = 1000
            Best_Template = BT  # np.ones((BT.shape))*1000
            # Best_Template[0+7000*ii:0+7000*(ii+1)]=BT[0+7000*ii:0+7000*(ii+1)]

            React = np.zeros((raster_rem_subset.shape[0], prob_rem_subset.shape[1]))
            Mode_Prob = np.zeros(prob_rem_subset.shape[1])

            for te in range(prob_rem_subset.shape[1]):
                Mode_Prob[te] = len(np.where(Best_Template == te)[0]) / len(Best_Template);
                if (raster_rem_subset[:, Best_Template == te].shape[1] > n_lim):
                    React[:, te] = np.mean(raster_rem_subset[:, Best_Template == te], axis=1)  # -React_m

            React[np.isnan(React)] = 0

            React = React - th_fr
            React[React < 0] = 0

            Modes = modes_lambdas - th_fr
            Modes[Modes < 0] = 0
            Modes[np.isnan(Modes)] = 0

            # Compute correlation between mode and reactivation average
            CC = np.corrcoef(React.T, Modes.T)
            # distance_modes_nrem[th, :] = np.diag(CC[:modes_lambdas.shape[1], modes_lambdas.shape[1]:])

            # distance_modes_nrem[ii, :] = pdist(modes_lambdas.T, metric=metric)


            distance_reactivations_rem[ii, :] = pdist(React.T, metric=metric)
            distance_modes_rem[ii, :] = pdist(Modes.T, metric=metric)

            # Compute sparsity of vectors
            React_Sp = (np.nansum(React, axis=0) ** 2) / (np.nansum(React ** 2, axis=0)) / modes_lambdas.shape[0]
            Mode_Sp = (np.nansum(Modes, axis=0) ** 2) / (np.nansum(Modes ** 2, axis=0)) / modes_lambdas.shape[0]
            React_Sp[np.isnan(React_Sp)] = 0

            p1 = React_Sp
            p2 = Mode_Sp  # modes_lambdas

            Spars_Ratio[ii, :] = p1 / p2  # Ratio of sparsity React/Modes
            sparsity_reactivations_rem[ii, :] = React_Sp

        Spars_Ratio_REM = Spars_Ratio

        if plotting:

            s_mod = np.argwhere(distance_modes_rem[0, :] > 0.5)  # Use only mode pairs with high correlations ?
            # Compare difference in correlation between REM and NREM in time
            fig = plt.figure(figsize=(25, 20))
            for th in range(5):
                ax = fig.add_subplot(3, 2, th + 1)
                ax.scatter((distance_modes_rem[th, s_mod] - distance_reactivations_rem[th, s_mod]) / distance_modes_nrem[th, s_mod],
                           (distance_modes_nrem[th, s_mod] - distance_reactivations_nrem[th, s_mod]) / distance_modes_nrem[th, s_mod])
                ax.plot([0, 1], [0, 1])
            plt.show()

            s_mod = np.argwhere(distance_modes_rem[0, :] > 0.5)  # Use only mode pairs with high correlations ?

            # Compare difference in correlation between modes and reactivations in REM
            fig = plt.figure(figsize=(25, 20))
            for th in range(5):
                ax = fig.add_subplot(3, 2, th + 1)
                ax.scatter(distance_modes_rem[th, s_mod], distance_reactivations_rem[th, s_mod])
                print(np.nanmean(distance_modes_rem[th, s_mod] - distance_reactivations_rem[th, s_mod]))
                ax.plot([0, 1], [0, 1])
                ax.set_xlabel('Similarity Between Original Modes')
                ax.set_ylabel('Similarity Between Reactivated Modes')
            plt.show()

            # Plot pair correlation difference in time
            # FOR NREM

            s_mod = np.argwhere(distance_modes_rem[0, :] > 0.1)

            To_Plot = Modes_C_NREM[:, s_mod.flatten()] - React_C_NREM[:, s_mod.flatten()]
            tak = np.where(np.prod(To_Plot, axis=0) > 0)
            Corr_Diff_plt = np.squeeze(To_Plot[:, np.asarray(tak)])
            plt.plot(Corr_Diff_plt, alpha=0.5)
            plt.plot(np.nanmean(Corr_Diff_plt, axis=1), c='white', linewidth=4, linestyle='--')
            plt.xlabel("Part of sleep")
            plt.ylabel("Distance between reactivated activity \n and corresponding mode (1- Pearson R)")
            plt.title("NREM ("+pre_or_post+")")
            x_min,x_max = plt.gca().get_xlim()
            y_min,y_max = plt.gca().get_ylim()
            plt.text(x_min+0.1, y_min+0.1, "beginning vs. end, p="+str(stats.ttest_ind(Corr_Diff_plt[0, :],
                                                                                       Corr_Diff_plt[-1, :])[1]))
            plt.show()

            stats.ttest_ind(Corr_Diff_plt[0, :], Corr_Diff_plt[-1, :])

            # Plot pair correlation difference in time
            # FOR REM

            s_mod = np.argwhere(distance_modes_rem[0, :] > 0.1)

            To_Plot = distance_modes_rem[:, s_mod.flatten()] - distance_reactivations_rem[:, s_mod.flatten()]
            tak = np.where(np.prod(To_Plot, axis=0) > 0)
            Corr_Diff_plt = np.squeeze(To_Plot[:, np.asarray(tak)])
            plt.plot(Corr_Diff_plt, alpha=0.5)
            plt.plot(np.nanmean(Corr_Diff_plt, axis=1), c='white', linewidth=4, linestyle='--')
            plt.xlabel("Part of sleep")
            plt.ylabel("Distance between reactivated activity \n and corresponding mode (1- Pearson R)")
            plt.title("REM ("+pre_or_post+")")
            x_min,x_max = plt.gca().get_xlim()
            y_min,y_max = plt.gca().get_ylim()
            plt.text(x_min+0.1, y_min+0.1, "beginning vs. end, p="+str(stats.ttest_ind(Corr_Diff_plt[0, :],
                                                                                       Corr_Diff_plt[-1, :])[1]))
            plt.show()
            stats.ttest_ind(Corr_Diff_plt[0, :], Corr_Diff_plt[-1, :])

            # Plot sparsity ratio in time
            # FOR NREM

            tak = np.where(np.prod(Spars_Ratio_NREM, axis=0) > 0)
            Spars_Ratio_plt = np.squeeze(Spars_Ratio_NREM[:, np.asarray(tak)])
            plt.plot(Spars_Ratio_plt, alpha=0.5)
            plt.plot(np.nanmean(Spars_Ratio_plt, axis=1), c='w', linewidth=4, linestyle='--')
            plt.xlabel("Sleep part")
            plt.ylabel("Sparsity ratio")
            plt.title("NREM ("+pre_or_post+")")
            x_min,x_max = plt.gca().get_xlim()
            y_min,y_max = plt.gca().get_ylim()
            plt.text(x_min+0.1, y_min+0.1, "beginning vs. end, p="+str(stats.ttest_ind(Spars_Ratio_plt[0, :],
                                                                                       Spars_Ratio_plt[-1, :])[1]))
            plt.show()


            # Plot sparsity ratio in time
            # FOR REM

            tak = np.where(np.prod(Spars_Ratio_REM, axis=0) > 0)
            Spars_Ratio_plt = np.squeeze(Spars_Ratio_REM[:, np.asarray(tak)])
            plt.plot(Spars_Ratio_plt, alpha=0.5)
            plt.plot(np.nanmean(Spars_Ratio_plt, axis=1), c='w', linewidth=4, linestyle='--')
            plt.xlabel("Sleep part")
            plt.ylabel("Sparsity ratio")
            plt.title("REM ("+pre_or_post+")")
            x_min,x_max = plt.gca().get_xlim()
            y_min,y_max = plt.gca().get_ylim()
            plt.text(x_min+0.1, y_min+0.1, "beginning vs. end, p="+str(stats.ttest_ind(Spars_Ratio_plt[0, :],
                                                                                       Spars_Ratio_plt[-1, :])[1]))
            plt.show()

            tak = np.where(np.prod(Spars_Ratio_NREM - Spars_Ratio_REM, axis=0) > 0)
            Spars_Ratio_plt = np.squeeze(Spars_Ratio_NREM[:, np.asarray(tak)] - Spars_Ratio_REM[:, np.asarray(tak)])
            plt.plot(Spars_Ratio_plt, alpha=0.5)
            plt.plot(np.nanmean(Spars_Ratio_plt, axis=1), c='w', linewidth=4, linestyle='--')
            plt.xlabel("Sleep part")
            plt.ylabel("Sparsity ratio difference (NREM-REM) ("+pre_or_post+")")
            x_min,x_max = plt.gca().get_xlim()
            y_min,y_max = plt.gca().get_ylim()
            plt.text(x_min+0.1, y_min+0.1, "beginning vs. end, p="+str(stats.ttest_ind(Spars_Ratio_plt[0, :],
                                                                                       Spars_Ratio_plt[-1, :])[1]))
            plt.show()


        distance_reactivations_rem = distance_reactivations_rem[~np.isnan(distance_reactivations_rem)]
        distance_reactivations_nrem = distance_reactivations_nrem[~np.isnan(distance_reactivations_nrem)]
        sparsity_reactivations_rem = sparsity_reactivations_rem[sparsity_reactivations_rem>0]
        sparsity_reactivations_nrem = sparsity_reactivations_nrem[sparsity_reactivations_nrem>0]

        return sparsity_reactivations_rem, sparsity_reactivations_nrem, distance_reactivations_rem, \
            distance_reactivations_nrem

    def memory_drift_distance_between_modes(self, pre_file_name=None, post_file_name=None, rem_pop_vec_threshold=10,
                                         plotting=False, save_fig=False, th=3, n_lim=30, q_lim=0.7, n_segments=5,
                                       pre_or_post="pre"):
        """

        Parameters
        ----------
        pre_file_name phmm model pre (str)
        post_file_name phmm model post (str)
        rem_pop_vec_threshold min number pop vecs per REM event
        plotting whether to plot or not (bool)
        save_fig saving or not (bool)
        th Activity threshold to compute sparsity
        n_lim Minimum Number of activations for one mode to be used
        q_lim Minimum reactivation strength for taking a mode as a winner
        n_segments number of segments to divide sleep
        """

        if pre_or_post == "pre":
            file_name = self.session_params.default_pre_phmm_model
        elif pre_or_post == "post":
            file_name = self.session_params.default_post_phmm_model
        # need PRE and POST modes
        with open(self.params.pre_proc_dir + "phmm/" + file_name + '.pkl', 'rb') as f:
            model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        modes_lambdas = model_dic.means_.T

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_rem, post_prob_rem, _, rasters_rem = \
            self.memory_drift_long_sleep_get_raw_results_and_spike_bins(template_type="phmm", pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, cells_to_use="all")

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_nrem, post_prob_nrem, _, rasters_nrem  = \
            self.memory_drift_long_sleep_get_raw_results_and_spike_bins(template_type="phmm", pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, cells_to_use="all")

        raster_rem = np.hstack(rasters_rem)
        raster_nrem = np.hstack(rasters_nrem)
        if pre_or_post == "pre":
            prob_rem = np.vstack(pre_prob_rem)
            prob_nrem = np.vstack(pre_prob_nrem)
        elif pre_or_post == "post":
            prob_rem = np.vstack(post_prob_rem)
            prob_nrem = np.vstack(post_prob_nrem)

        # COMPUTE POISSON 12-Vector Spike VECTOR PROBABILITES FROM MODES
        # (TO HAVE DATA IN THE SAME FORMAT OF REACTIVATIONS)
        for mo in range(modes_lambdas.shape[1]):

            l_i = modes_lambdas[:, mo]
            sp_t_all = np.zeros((0,))
            sp_i_all = np.zeros((0,))
            sp_m = np.zeros(l_i.shape[0])
            for nn in range(l_i.shape[0]):
                sp_t = np.cumsum(np.random.exponential(1 / l_i[nn], (10000, 1)))
                sp_m[nn] = np.max(sp_t)
                sp_t_all = np.concatenate((sp_t_all, sp_t))
                sp_i = np.ones((10000,)) * nn
                sp_i_all = np.concatenate((sp_i_all, sp_i))
                # Take the earlier last spike from any cell
            thr = np.min(sp_m)
            sp_i_all = sp_i_all[sp_t_all < thr]
            sp_t_all = sp_t_all[sp_t_all < thr]

            # Rearrange spike in time
            aa = np.argsort(sp_t_all)
            sp_i_all = sp_i_all[aa]

            # Build avarage spike occurrence
            n_samp = int(np.floor(sp_t_all.shape[0] / 12))
            raster_mod = np.zeros((l_i.shape[0], n_samp))
            for ss in range(n_samp):
                take_sp = sp_i_all[ss * 12:ss * 12 + 12].astype(int)
                for sp in range(len(take_sp)):
                    raster_mod[take_sp[sp], ss] += 1
            modes_lambdas[:, mo] = np.mean(raster_mod, axis=1)

        # NREM
        # --------------------------------------------------------------------------------------------------------------
        # Temporal interval to consider (in number of population vectors)
        t_tot = raster_nrem.shape[1]
        t_int = np.floor(t_tot / 6).astype(int)

        Simil_Mo = np.zeros((n_segments, modes_lambdas.shape[1]))
        Spars_Ratio = np.zeros((n_segments, modes_lambdas.shape[1]))  # Sparsity Comparison

        Modes_C = np.zeros((n_segments, int(modes_lambdas.shape[1]*(modes_lambdas.shape[1]-1)/2)))  # Correlation in modes and reactivations
        React_C = np.zeros((n_segments, int(modes_lambdas.shape[1]*(modes_lambdas.shape[1]-1)/2)))

        # Cycle over sleep segments
        for ii in range(n_segments):

            th_fr = th * 0.09

            Raster_subset = raster_nrem[:, 0 + t_int * ii:0 + t_int * (ii + 1)]
            # Raster = zscore(Raster,axis=1)

            prob_nrem_subset = prob_nrem[0 + t_int * ii:0 + t_int * (ii + 1)]

            # normalize likelihoods
            Nor = np.sum(prob_nrem_subset, axis=1)
            prob_nrem_subset = prob_nrem_subset / np.reshape(Nor, (t_int, 1))

            # find decoded mode
            BT = np.argmax(prob_nrem_subset, axis=1)
            # get maximum normalized likelihood
            VT = np.max(prob_nrem_subset, axis=1)
            # set modes that were decoded with a worse quality than q_lim to 1000 --> indicates that mode bin is not
            # considered for following computations
            BT[VT < q_lim] = 1000
            Best_Template = BT  # np.ones((BT.shape))*1000
            # Best_Template[0+7000*ii:0+7000*(ii+1)]=BT[0+7000*ii:0+7000*(ii+1)]

            React = np.zeros((Raster_subset.shape[0], prob_nrem_subset.shape[1]))
            Mode_Prob = np.zeros(prob_nrem_subset.shape[1])

            # go through all modes
            for te in range(modes_lambdas.shape[1]):
                # check how often each mode was reactivated --> compute probability of reactivation
                Mode_Prob[te] = len(np.where(Best_Template == te)[0]) / len(Best_Template)
                # if mode was reactivated more than n_lim times --> compute mean across all reactivations of this mode
                if (Raster_subset[:, Best_Template == te].shape[1] > n_lim):
                    React[:, te] = np.mean(Raster_subset[:, Best_Template == te], axis=1)  # -React_m

            React[np.isnan(React)] = 0
            React = React - th_fr
            React[React < 0] = 0

            Modes = modes_lambdas - th_fr
            Modes[Modes < 0] = 0
            Modes[np.isnan(Modes)] = 0

            # Compute correlation between mode and reactivation average
            CC = np.corrcoef(React.T, Modes.T)
            Simil_Mo[th, :] = np.diag(CC[:modes_lambdas.shape[1], modes_lambdas.shape[1]:])

            React_C[ii, :] = 1 - pdist(React.T, metric='correlation')
            Modes_C[ii, :] = 1 - pdist(Modes.T, metric='correlation')

            # Compute sparsity of vectors
            React_Sp = (np.nansum(React, axis=0) ** 2) / (np.nansum(React ** 2, axis=0)) / modes_lambdas.shape[0]
            Mode_Sp = (np.nansum(Modes, axis=0) ** 2) / (np.nansum(Modes ** 2, axis=0)) / modes_lambdas.shape[0]
            React_Sp[np.isnan(React_Sp)] = 0

            p1 = React_Sp
            p2 = Mode_Sp  # modes_lambdas

            Spars_Ratio[ii, :] = p1 / p2  # Ratio of sparsity React/Modes

        Modes_C_NREM = Modes_C
        React_C_NREM = React_C
        Spars_Ratio_NREM = Spars_Ratio

        # DO THE REM
        # --------------------------------------------------------------------------------------------------------------

        # Temporal interval to consider (in number of population vectors)
        t_tot = raster_rem.shape[1]
        t_int = np.floor(t_tot / 6).astype(int)

        Simil_Mo = np.zeros((n_segments, modes_lambdas.shape[1]))
        Spars_Ratio = np.zeros((n_segments, modes_lambdas.shape[1]))  # Sparsity Comparison

        Modes_C = np.zeros((n_segments, int(modes_lambdas.shape[1]*(modes_lambdas.shape[1]-1)/2)))  # Correlation in modes and reactivations
        React_C = np.zeros((n_segments, int(modes_lambdas.shape[1]*(modes_lambdas.shape[1]-1)/2)))

        # Cycle over different activity thresholds

        for ii in range(n_segments):
            th_fr = th * 0.09

            raster_rem_subset = raster_rem[:, 0 + t_int * ii:0 + t_int * (ii + 1)]
            # Raster = zscore(Raster,axis=1)

            prob_rem_subset = prob_rem[0 + t_int * ii:0 + t_int * (ii + 1)]

            Nor = np.sum(prob_rem_subset, axis=1)
            prob_rem_subset = prob_rem_subset / np.reshape(Nor, (t_int, 1))

            BT = np.argmax(prob_rem_subset, axis=1)
            VT = np.max(prob_rem_subset, axis=1)
            BT[VT < q_lim] = 1000
            Best_Template = BT  # np.ones((BT.shape))*1000
            # Best_Template[0+7000*ii:0+7000*(ii+1)]=BT[0+7000*ii:0+7000*(ii+1)]

            React = np.zeros((raster_rem_subset.shape[0], prob_rem_subset.shape[1]))
            Mode_Prob = np.zeros(prob_rem_subset.shape[1])

            for te in range(prob_rem_subset.shape[1]):
                Mode_Prob[te] = len(np.where(Best_Template == te)[0]) / len(Best_Template);
                if (raster_rem_subset[:, Best_Template == te].shape[1] > n_lim):
                    React[:, te] = np.mean(raster_rem_subset[:, Best_Template == te], axis=1)  # -React_m

            React[np.isnan(React)] = 0

            React = React - th_fr
            React[React < 0] = 0

            Modes = modes_lambdas - th_fr
            Modes[Modes < 0] = 0
            Modes[np.isnan(Modes)] = 0

            # Compute correlation between mode and reactivation average
            CC = np.corrcoef(React.T, Modes.T)
            Simil_Mo[th, :] = np.diag(CC[:modes_lambdas.shape[1], modes_lambdas.shape[1]:])

            React_C[ii, :] = 1 - pdist(React.T, metric='correlation')
            Modes_C[ii, :] = 1 - pdist(Modes.T, metric='correlation')

            # Compute sparsity of vectors
            React_Sp = (np.nansum(React, axis=0) ** 2) / (np.nansum(React ** 2, axis=0)) / modes_lambdas.shape[0]
            Mode_Sp = (np.nansum(Modes, axis=0) ** 2) / (np.nansum(Modes ** 2, axis=0)) / modes_lambdas.shape[0]
            React_Sp[np.isnan(React_Sp)] = 0

            p1 = React_Sp
            p2 = Mode_Sp  # modes_lambdas

            Spars_Ratio[ii, :] = p1 / p2  # Ratio of sparsity React/Modes

        Modes_C_REM = Modes_C
        React_C_REM = React_C
        Spars_Ratio_REM = Spars_Ratio

        s_mod = np.argwhere(Modes_C_REM[0, :] > 0.5)  # Use only mode pairs with high correlations ?

        # Compare difference in correlation between REM and NREM in time
        fig = plt.figure(figsize=(25, 20))
        for th in range(5):
            ax = fig.add_subplot(3, 2, th + 1)
            ax.scatter((Modes_C_REM[th, s_mod] - React_C_REM[th, s_mod]) / Modes_C_NREM[th, s_mod],
                       (Modes_C_NREM[th, s_mod] - React_C_NREM[th, s_mod]) / Modes_C_NREM[th, s_mod])
            ax.plot([0, 1], [0, 1])
        plt.show()

        s_mod = np.argwhere(Modes_C_REM[0, :] > 0.5)  # Use only mode pairs with high correlations ?

        # Compare difference in correlation between modes and reactivations in REM
        fig = plt.figure(figsize=(25, 20))
        for th in range(5):
            ax = fig.add_subplot(3, 2, th + 1)
            ax.scatter(Modes_C_REM[th, s_mod], React_C_REM[th, s_mod])
            print(np.nanmean(Modes_C_REM[th, s_mod] - React_C_REM[th, s_mod]))
            ax.plot([0, 1], [0, 1])
            ax.set_xlabel('Similarity Between Original Modes')
            ax.set_ylabel('Similarity Between Reactivated Modes')
        plt.show()

        # Plot pair correlation difference in time
        # FOR NREM

        s_mod = np.argwhere(Modes_C_REM[0, :] > 0.1)

        To_Plot = Modes_C_NREM[:, s_mod.flatten()] - React_C_NREM[:, s_mod.flatten()]
        tak = np.where(np.prod(To_Plot, axis=0) > 0)
        Corr_Diff_plt = np.squeeze(To_Plot[:, np.asarray(tak)])
        plt.plot(Corr_Diff_plt, alpha=0.5)
        plt.plot(np.nanmean(Corr_Diff_plt, axis=1), c='white', linewidth=4, linestyle='--')
        plt.xlabel("Part of sleep")
        plt.ylabel("Distance between reactivated activity \n and corresponding mode (1- Pearson R)")
        plt.title("NREM ("+pre_or_post+")")
        x_min,x_max = plt.gca().get_xlim()
        y_min,y_max = plt.gca().get_ylim()
        plt.text(x_min+0.1, y_min+0.1, "beginning vs. end, p="+str(stats.ttest_ind(Corr_Diff_plt[0, :],
                                                                                   Corr_Diff_plt[-1, :])[1]))
        plt.show()

        stats.ttest_ind(Corr_Diff_plt[0, :], Corr_Diff_plt[-1, :])

        # Plot pair correlation difference in time
        # FOR REM

        s_mod = np.argwhere(Modes_C_REM[0, :] > 0.1)

        To_Plot = Modes_C_REM[:, s_mod.flatten()] - React_C_REM[:, s_mod.flatten()]
        tak = np.where(np.prod(To_Plot, axis=0) > 0)
        Corr_Diff_plt = np.squeeze(To_Plot[:, np.asarray(tak)])
        plt.plot(Corr_Diff_plt, alpha=0.5)
        plt.plot(np.nanmean(Corr_Diff_plt, axis=1), c='white', linewidth=4, linestyle='--')
        plt.xlabel("Part of sleep")
        plt.ylabel("Distance between reactivated activity \n and corresponding mode (1- Pearson R)")
        plt.title("REM ("+pre_or_post+")")
        x_min,x_max = plt.gca().get_xlim()
        y_min,y_max = plt.gca().get_ylim()
        plt.text(x_min+0.1, y_min+0.1, "beginning vs. end, p="+str(stats.ttest_ind(Corr_Diff_plt[0, :],
                                                                                   Corr_Diff_plt[-1, :])[1]))
        plt.show()
        stats.ttest_ind(Corr_Diff_plt[0, :], Corr_Diff_plt[-1, :])

        # Plot sparsity ratio in time
        # FOR NREM

        tak = np.where(np.prod(Spars_Ratio_NREM, axis=0) > 0)
        Spars_Ratio_plt = np.squeeze(Spars_Ratio_NREM[:, np.asarray(tak)])
        plt.plot(Spars_Ratio_plt, alpha=0.5)
        plt.plot(np.nanmean(Spars_Ratio_plt, axis=1), c='w', linewidth=4, linestyle='--')
        plt.xlabel("Sleep part")
        plt.ylabel("Sparsity ratio")
        plt.title("NREM ("+pre_or_post+")")
        x_min,x_max = plt.gca().get_xlim()
        y_min,y_max = plt.gca().get_ylim()
        plt.text(x_min+0.1, y_min+0.1, "beginning vs. end, p="+str(stats.ttest_ind(Spars_Ratio_plt[0, :],
                                                                                   Spars_Ratio_plt[-1, :])[1]))
        plt.show()


        # Plot sparsity ratio in time
        # FOR REM

        tak = np.where(np.prod(Spars_Ratio_REM, axis=0) > 0)
        Spars_Ratio_plt = np.squeeze(Spars_Ratio_REM[:, np.asarray(tak)])
        plt.plot(Spars_Ratio_plt, alpha=0.5)
        plt.plot(np.nanmean(Spars_Ratio_plt, axis=1), c='w', linewidth=4, linestyle='--')
        plt.xlabel("Sleep part")
        plt.ylabel("Sparsity ratio")
        plt.title("REM ("+pre_or_post+")")
        x_min,x_max = plt.gca().get_xlim()
        y_min,y_max = plt.gca().get_ylim()
        plt.text(x_min+0.1, y_min+0.1, "beginning vs. end, p="+str(stats.ttest_ind(Spars_Ratio_plt[0, :],
                                                                                   Spars_Ratio_plt[-1, :])[1]))
        plt.show()

        tak = np.where(np.prod(Spars_Ratio_NREM - Spars_Ratio_REM, axis=0) > 0)
        Spars_Ratio_plt = np.squeeze(Spars_Ratio_NREM[:, np.asarray(tak)] - Spars_Ratio_REM[:, np.asarray(tak)])
        plt.plot(Spars_Ratio_plt, alpha=0.5)
        plt.plot(np.nanmean(Spars_Ratio_plt, axis=1), c='w', linewidth=4, linestyle='--')
        plt.xlabel("Sleep part")
        plt.ylabel("Sparsity ratio difference (NREM-REM) ("+pre_or_post+")")
        x_min,x_max = plt.gca().get_xlim()
        y_min,y_max = plt.gca().get_ylim()
        plt.text(x_min+0.1, y_min+0.1, "beginning vs. end, p="+str(stats.ttest_ind(Spars_Ratio_plt[0, :],
                                                                                   Spars_Ratio_plt[-1, :])[1]))
        plt.show()

    def memory_drift_plot_temporal_trend_stable_cells(self, n_moving_average_pop_vec=400, rem_pop_vec_threshold=10,
                                                      plotting=True, nr_parts_to_split_data=4, save_fig=False):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_ - np.mean(dec, axis=0)[0]vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param only_stable_cells: whether to only use stable cells
        :type only_stable_cells: bool
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """

        # get length of sleep in seconds
        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        r_smooth_stable = self.memory_drift_plot_temporal_trend(template_type="phmm", measure="normalized_ratio",
                                                                pre_file_name=None, post_file_name=None,
                                                                n_moving_average_pop_vec=n_moving_average_pop_vec,
                                                                rem_pop_vec_threshold=rem_pop_vec_threshold,
                                                                plotting=False, cells_to_use="stable")

        r_smooth_all = self.memory_drift_plot_temporal_trend(template_type="phmm", measure="normalized_ratio",
                                                                pre_file_name=None, post_file_name=None,
                                                                n_moving_average_pop_vec=n_moving_average_pop_vec,
                                                                rem_pop_vec_threshold=rem_pop_vec_threshold,
                                                                plotting=False, cells_to_use="all")

        r_smooth_dec_inc = self.memory_drift_plot_temporal_trend(template_type="phmm", measure="normalized_ratio",
                                                                pre_file_name=None, post_file_name=None,
                                                                n_moving_average_pop_vec=n_moving_average_pop_vec,
                                                                rem_pop_vec_threshold=rem_pop_vec_threshold,
                                                                plotting=False, cells_to_use="dec_inc")

        coef_all_list = []
        coef_stable_list = []
        coef_dec_inc_list = []
        # pop_vec_per_h  = r_smooth_all.shape[0] / len_sleep_h
        window_size=int(r_smooth_all.shape[0]/nr_parts_to_split_data)
        for window_id in range(int(r_smooth_all.shape[0]/window_size)):
            x = np.linspace(0,1/nr_parts_to_split_data, window_size)
            segment_all = r_smooth_all[window_id*window_size:(window_id+1)*window_size]
            segment_stable = r_smooth_stable[window_id*window_size:(window_id+1)*window_size]
            segment_dec_inc = r_smooth_dec_inc[window_id*window_size:(window_id+1)*window_size]
            coef_all = np.polyfit(x, segment_all, 1)
            coef_all_list.append(coef_all[0])
            coef_stable = np.polyfit(x, segment_stable, 1)
            coef_stable_list.append(coef_stable[0])
            coef_dec_inc = np.polyfit(x, segment_dec_inc, 1)
            coef_dec_inc_list.append(coef_dec_inc[0])

        slope_all = np.mean(np.array(coef_all_list))
        slope_stable = np.mean(np.array(coef_stable_list))
        slope_dec_inc = np.mean(np.array(coef_dec_inc_list))

        perc_slope_stable = slope_stable/slope_all

        if plotting or save_fig:
            if save_fig:
                plt.style.use('default')
            # plotting
            fig = plt.figure(figsize=(5,3))
            ax = fig.add_subplot()
            ax.plot(np.linspace(0,np.round(len_sleep_h,0),r_smooth_all.shape[0]),r_smooth_all, c="grey", label="all cells")
            ax.plot(np.linspace(0,np.round(len_sleep_h,0),r_smooth_stable.shape[0]),r_smooth_stable, c="#6B345C",
                    label="stable cells", alpha=1, zorder=100000)
            ax.plot(np.linspace(0, np.round(len_sleep_h, 0), r_smooth_dec_inc.shape[0]), r_smooth_dec_inc, c="orange",
                    alpha=1, linestyle="-", linewidth=1)
            ax.plot(np.linspace(0, np.round(len_sleep_h, 0), r_smooth_dec_inc.shape[0]), r_smooth_dec_inc,
                    c="turquoise",
                    label="dec_inc cells", alpha=1, linestyle=":", linewidth=0.5)
            plt.legend()
            plt.grid(axis='y')
            plt.xlabel("Sleep duration (h)")
            plt.xticks([0,5,15,20])
            plt.yticks([-1, -0.5, 0])
            plt.xlim(0, np.round(len_sleep_h,0))
            # plt.ylim(-0.35, 0.35)
            plt.ylabel("sim_ratio")
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "stable_drift.svg"), transparent="True")
            else:
                plt.show()

            # coef_inc = np.polyfit(x, segment_inc, 1)
            # poly1d_fn = np.poly1d(coef_all)
            # x_plotting = np.arange(window_id*window_size, (window_id+1)*window_size)

        else:
            return slope_all, slope_stable, slope_dec_inc

    def memory_drift_temporal_subsets(self, n_moving_average_pop_vec=10000, rem_pop_vec_threshold=10,
                                                      plotting=True, nr_parts_to_split_data=1, save_fig=False):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_ - np.mean(dec, axis=0)[0]vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param only_stable_cells: whether to only use stable cells
        :type only_stable_cells: bool
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """

        # get length of sleep in seconds
        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        r_smooth_stable = self.memory_drift_plot_temporal_trend(template_type="phmm", measure="normalized_ratio",
                                                                pre_file_name=None, post_file_name=None,
                                                                n_moving_average_pop_vec=n_moving_average_pop_vec,
                                                                rem_pop_vec_threshold=rem_pop_vec_threshold,
                                                                plotting=False, cells_to_use="stable")

        r_smooth_inc = self.memory_drift_plot_temporal_trend(template_type="phmm", measure="normalized_ratio",
                                                                pre_file_name=None, post_file_name=None,
                                                                n_moving_average_pop_vec=n_moving_average_pop_vec,
                                                                rem_pop_vec_threshold=rem_pop_vec_threshold,
                                                                plotting=False, cells_to_use="increasing")

        r_smooth_dec = self.memory_drift_plot_temporal_trend(template_type="phmm", measure="normalized_ratio",
                                                                pre_file_name=None, post_file_name=None,
                                                                n_moving_average_pop_vec=n_moving_average_pop_vec,
                                                                rem_pop_vec_threshold=rem_pop_vec_threshold,
                                                                plotting=False, cells_to_use="decreasing")

        r_smooth_all = self.memory_drift_plot_temporal_trend(template_type="phmm", measure="normalized_ratio",
                                                                pre_file_name=None, post_file_name=None,
                                                                n_moving_average_pop_vec=n_moving_average_pop_vec,
                                                                rem_pop_vec_threshold=rem_pop_vec_threshold,
                                                                plotting=False, cells_to_use="all")


        if nr_parts_to_split_data > 1:
            coef_inc_list = []
            coef_stable_list = []
            coef_dec_list = []
            coef_all_list = []
            # pop_vec_per_h  = r_smooth_all.shape[0] / len_sleep_h
            window_size=int(r_smooth_dec.shape[0]/nr_parts_to_split_data)
            for window_id in range(int(r_smooth_dec.shape[0]/window_size)):
                x = np.linspace(0,1/nr_parts_to_split_data, window_size)
                segment_inc = r_smooth_inc[window_id*window_size:(window_id+1)*window_size]
                segment_stable = r_smooth_stable[window_id*window_size:(window_id+1)*window_size]
                segment_dec = r_smooth_dec[window_id*window_size:(window_id+1)*window_size]
                segment_all = r_smooth_all[window_id*window_size:(window_id+1)*window_size]
                coef_inc = np.polyfit(x, segment_inc, 1)
                coef_inc_list.append(coef_inc[0])
                coef_stable = np.polyfit(x, segment_stable, 1)
                coef_stable_list.append(coef_stable[0])
                coef_dec = np.polyfit(x, segment_dec, 1)
                coef_dec_list.append(coef_dec[0])
                coef_all = np.polyfit(x, segment_all, 1)
                coef_all_list.append(coef_all[0])

            slope_inc = np.mean(np.array(coef_inc_list))
            slope_stable = np.mean(np.array(coef_stable_list))
            slope_dec = np.mean(np.array(coef_dec_list))
            slope_all = np.mean(np.array(coef_all_list))
        else:
            slope_inc  = np.polyfit(np.linspace(0,1,r_smooth_inc.shape[0]), r_smooth_inc, 1)[0]
            slope_dec  = np.polyfit(np.linspace(0,1,r_smooth_dec.shape[0]), r_smooth_dec, 1)[0]
            slope_stable  = np.polyfit(np.linspace(0,1,r_smooth_stable.shape[0]), r_smooth_stable, 1)[0]
            slope_all  = np.polyfit(np.linspace(0,1,r_smooth_all.shape[0]), r_smooth_all, 1)[0]

        if plotting or save_fig:
            if save_fig:
                plt.style.use('default')
            # plotting
            fig = plt.figure(figsize=(5, 4))
            ax = fig.add_subplot()
            ax.plot(np.linspace(0, np.round(len_sleep_h, 0), r_smooth_inc.shape[0]), r_smooth_inc, c="orange",
                    label="increasing", linewidth=0.8)
            ax.plot(np.linspace(0, np.round(len_sleep_h, 0), r_smooth_stable.shape[0]), r_smooth_stable, c="#6B345C",
                    label="persistent", alpha=1, zorder=100000, linewidth=0.8)
            ax.plot(np.linspace(0, np.round(len_sleep_h, 0), r_smooth_dec.shape[0]), r_smooth_dec, c="turquoise",
                    alpha=1, linestyle="-", label="decreasing", linewidth=0.8)
            ax.plot(np.linspace(0, np.round(len_sleep_h, 0), r_smooth_all.shape[0]), r_smooth_all, c="grey",
                    alpha=1, linestyle="-", label="all", zorder=-1000, linewidth=0.8)
            plt.legend()
            plt.grid(axis='y')
            plt.xlabel("Rest duration (h)")
            plt.xticks([0, 5, 10, 15, 20])
            plt.yticks([-1, 0, 1])
            plt.ylim(-1,1)
            plt.xlim(0, np.round(len_sleep_h, 0))
            # plt.ylim(-0.35, 0.35)
            plt.ylabel("sim_ratio")
            plt.tight_layout()
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "drift_subsets.svg"), transparent="True")
            else:
                plt.show()

            plt.scatter([0,1,2,3], [slope_all, slope_dec, slope_inc, slope_stable])
            plt.xticks([0,1,2,3], ["all", "dec", "inc", "stable"], rotation=45)
            plt.ylabel("Slope")
            plt.tight_layout()
            plt.show()

        else:
            return slope_stable, slope_dec, slope_inc, slope_all

    def memory_drift_and_power_spectrum(self, template_type, measure="normalized_ratio", pre_file_name=None,
                                        post_file_name=None, plot_for_control=False,
                                        n_moving_average_pop_vec=200, rem_pop_vec_threshold=10, nrem_pop_vec_threshold=2,
                                        cells_to_use="all", sleep_classification_method="std", shuffling=False, debug=False,
                                        all_tetrodes=False, scales_first_half=False, scales_second_half=False):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param cells_to_use: which cells to use ("all", "stable", "inc", "dec")
        :type cells_to_use: str
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """
        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, _, _ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use, sleep_classification_method=
                                                     sleep_classification_method, shuffling=shuffling)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, _ ,_ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=nrem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use, sleep_classification_method=
                                                     sleep_classification_method, shuffling=shuffling)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------_decoding_similarity_temporal------------------------

        all_events_ratio = ratio_per_rem_event + ratio_per_nrem_event
        all_events_length = event_lengths_rem + event_lengths_nrem
        all_events_length_s = np.hstack((event_duration_rem_in_s, event_duration_nrem_in_s))
        labels_events = np.zeros(len(ratio_per_rem_event)+len(ratio_per_nrem_event))
        labels_events[:len(ratio_per_rem_event)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_ratio_list = [x for _, x in sorted(zip(all_times, all_events_ratio))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_lengths_s = [x for _, x in sorted(zip(all_times, all_events_length_s))]
        sorted_pop_vec_ratio = np.hstack(sorted_events_ratio_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_length_s = []
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat==1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat==-1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))
        merged_events_length_s = np.array(merged_events_length_s)
        merged_events_times = np.vstack(merged_events_times)

        # merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        # merged_nrem_event_times = merged_events_times[merged_events_labels == 0]
        merged_events_length_s = merged_events_times[:,1]-merged_events_times[:,0]


        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps==1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre_post ratio) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_merged_rem_event = []
        ratio_per_merged_nrem_event = []
        ratio_rem_nrem_events = []
        rem_nrem_events_label = []
        ratio_rem_nrem_pop_vec = []
        rem_nrem_pop_vec_label = []

        for event_id, (start_event, end_event) in enumerate(zip(start, end)):
            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                ratio_per_merged_rem_event.append(sorted_pop_vec_ratio[start_event:end_event])
            # nrem event
            else:
                ratio_per_merged_nrem_event.append(sorted_pop_vec_ratio[start_event:end_event])

            ratio_rem_nrem_events.append(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])

            ratio_rem_nrem_pop_vec.extend(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_pop_vec_label.extend(labels_per_pop_vec[start_event:end_event])


        # smooth across population vectors --> that means also across REM/NREM epochs
        # --------------------------------------------------------------------------------------------------------------
        len_new_events = [x.shape[0] for x in ratio_rem_nrem_events]
        ratio_per_pop_vec_new = np.hstack(ratio_rem_nrem_events)
        ratio_per_pop_vec_new_smooth = moving_average(a=np.array(ratio_per_pop_vec_new), n=n_moving_average_pop_vec)

        ratio_rem_nrem_events_smooth = []
        first = 0
        for event_id in range(len(len_new_events)):
            # check if data is still available (due to smoothing --> data gets shorter)
            # if not --> leave loop
            if first >= ratio_per_pop_vec_new_smooth.shape[0]:
                break
            end_event = min(first + len_new_events[event_id], ratio_per_pop_vec_new_smooth.shape[0])
            ratio_rem_nrem_events_smooth.append(ratio_per_pop_vec_new_smooth[first:end_event])
            first += len_new_events[event_id]

        # need to delete indices of events that happend after the last data point after smoothing
        rem_nrem_events_label = rem_nrem_events_label[:len(ratio_rem_nrem_events_smooth)]

        # get rem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        rem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 1))
        ds_rem = []
        ds_rem_smoothed_within = []
        merged_events_rem_length = []
        ratio_rem_events_smooth = []
        merged_rem_events_time_stamps = []
        for rem_index in rem_events_indices:
            ds_rem.append(ratio_rem_nrem_events_smooth[rem_index][-1]-ratio_rem_nrem_events_smooth[rem_index][0])
            smooth_event = moving_average(a=ratio_rem_nrem_events[rem_index], n=10)
            ds_rem_smoothed_within.append(smooth_event[-1]-smooth_event[0])
            merged_events_rem_length.append(ratio_rem_nrem_events_smooth[rem_index].shape[0])
            ratio_rem_events_smooth.append(ratio_rem_nrem_events_smooth[rem_index])
            merged_rem_events_time_stamps.append(merged_events_times[rem_index])

        ds_rem = np.array(ds_rem)
        # get nrem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        nrem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 0))
        ds_nrem = []
        ds_nrem_smoothed_within = []
        merged_events_nrem_length = []
        merged_nrem_events_time_stamps = []
        ratio_nrem_events_smooth = []
        for nrem_index in nrem_events_indices:
            ds_nrem.append(ratio_rem_nrem_events_smooth[nrem_index][-1]-ratio_rem_nrem_events_smooth[nrem_index][0])
            merged_events_nrem_length.append(ratio_rem_nrem_events_smooth[nrem_index].shape[0])
            ratio_nrem_events_smooth.append(ratio_rem_nrem_events_smooth[nrem_index])
            merged_nrem_events_time_stamps.append(merged_events_times[nrem_index])

        ds_nrem = np.array(ds_nrem)
        # compute power spectrum for nrem periods
        # --------------------------------------------------------------------------------------------------------------
        print("Starting now with NREM ...")
        print("--> Memory usage:" + str(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2))

        end_prev_sleep = 0
        mean_pow = []
        max_pow = []
        file_transitions_nrem = []

        # do wavelet analysis for each nrem epoch
        if plot_for_control:
            n = 5
            count=0
            colors = plt.cm.jet(np.linspace(0, 1, n))
        for i_sleep, l_s in enumerate(self.long_sleep):
            print("processing sleep file" + str(i_sleep))
            # need to check which epochs happend during which sleep
            duration = l_s.get_duration_sec()
            if i_sleep == 4:
                merged_nrem_events_time_stamps_current_sleep = [x for x in merged_nrem_events_time_stamps if
                                                                end_prev_sleep < x[0]]
            else:
                merged_nrem_events_time_stamps_current_sleep = [x for x in merged_nrem_events_time_stamps if
                                                                (end_prev_sleep) < x[0] and x[1]< (end_prev_sleep+duration)]
            # need to subtract the duration of the previous sleep sessions
            merged_nrem_events_time_stamps_current_sleep_offset = [x-end_prev_sleep for x in merged_nrem_events_time_stamps_current_sleep]
            if plot_for_control:
                for i_event, (start, end) in enumerate(merged_nrem_events_time_stamps_current_sleep_offset):
                    plt.hlines(count, end_prev_sleep+start, end_prev_sleep+end, color=colors[i_sleep])
                    count +=1
                plt.vlines(end_prev_sleep, 0, len(merged_nrem_events_time_stamps), color="white" ,linewidth=1)

            # go through all epochs
            if all_tetrodes:
                nr_tetrodes = self.long_sleep[0].eegh.shape[1]
                # compute spectrum for each tetrode, z-score each frequency, once there are at least 2 tetrodes,
                # pick maximum across both tetrodes per time point and move to next tetrode
                max_power_z = None
                for tetrode_id in range(nr_tetrodes):
                    print(" - processing tet" + str(tetrode_id))
                    power_list = []
                    for start, end in merged_nrem_events_time_stamps_current_sleep_offset:
                        f, t, p = l_s.power_spectrum_analysis(time_interval_s=[start, end], debug=debug,
                                                              tetrode=tetrode_id, scales_first_half=scales_first_half,
                                                              scales_second_half=scales_second_half)
                        power_list.append(p)
                    print("--> Memory usage:"+str(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2))  # in bytes
                    # join all intervals and z-score
                    power = np.hstack(power_list)
                    power_z = zscore(power, axis=1)
                    del power
                    # check if there is already a previous tetrode
                    if max_power_z is None:
                        max_power_z = np.copy(power_z)
                        del power_z
                    else:
                        power_combined = np.dstack((power_z, max_power_z))
                        max_power_z = np.nanmax(power_combined, axis=2)
                        del power_combined
                        del power_z

                # compute length of each interval to separate events again
                len_per_event = [x.shape[1] for x in power_list]
                # need to compute now mean/max per event
                start = 0
                for len_event in len_per_event:
                    mean_pow.append(np.nanmean(max_power_z[:,start:start+len_event], axis=1))
                    max_pow.append(np.nanmax(max_power_z[:, start:start + len_event], axis=1))
                    start += len_event

                # power_list = []
                # for start, end in merged_nrem_events_time_stamps_current_sleep_offset:
                #     f, t, p = l_s.power_spectrum_analysis_all_tetrodes(time_interval_s=[start, end], debug=debug)
                #     power_list.append(p)
                #
                # # each list entry: nr_electrodes x frequencies x time
                # len_per_event = [x.shape[2] for x in power_list]
                # power = np.dstack(power_list)
                # power_z = zscore(power, axis=2)
                # max_power_across_tetrodes = np.max(power_z, axis=0)
                # # need to compute now mean/max per event
                # start = 0
                # for len_event in len_per_event:
                #     mean_pow.append(np.nanmean(max_power_across_tetrodes[:,start:start+len_event], axis=1))
                #     max_pow.append(np.nanmax(max_power_across_tetrodes[:, start:start + len_event], axis=1))
                #     start += len_event

            else:
                for start, end in merged_nrem_events_time_stamps_current_sleep_offset:
                    f, t, p = l_s.power_spectrum_analysis(time_interval_s=[start, end], debug=debug)
                    mean_pow.append(np.nanmean(p, axis=1))
                    max_pow.append(np.nanmax(p, axis=1))

            end_prev_sleep += duration
            file_transitions_nrem.append(end_prev_sleep)

        if plot_for_control:
            for i_event, (start, end) in enumerate(merged_nrem_events_time_stamps):
                plt.hlines(i_event+1, start, end, color="white")
            plt.show()


        # need to find events that span across sleep files

        bad_events_nrem = []
        for i_event, (start, end) in enumerate(merged_nrem_events_time_stamps):
            for file_t in file_transitions_nrem:
                if start < file_t and file_t < end:
                    bad_events_nrem.append(i_event)
        if len(bad_events_nrem) > 0:
            bad_events_nrem = np.array(bad_events_nrem)
            ds_nrem = np.delete(ds_nrem, bad_events_nrem)


        mean_power = np.vstack(mean_pow).T
        ds_and_mean_power = np.vstack((ds_nrem, mean_power))
        is_nan = np.argwhere(np.isnan(np.sum(ds_and_mean_power, axis=0)))
        ds_and_mean_power = np.delete(ds_and_mean_power, is_nan.flatten(), axis=1)
        corr_ds_power = np.corrcoef(ds_and_mean_power)[0,1:]

        plt.scatter(f, corr_ds_power, s=1)
        plt.xlabel("Freq [Hz]")
        plt.ylabel("Corr. with Delta score")
        plt.title("NREM Correlation drift vs. MEAN power")
        # plt.xlim(0, 260)
        if scales_first_half:
            plt.savefig(os.path.join(save_path, self.session_name+"first_half_nrem_mean.png"))
        elif scales_second_half:
            plt.savefig(os.path.join(save_path, self.session_name + "second_half_nrem_mean.png"))
        else:
            plt.savefig(os.path.join(save_path, self.session_name + "_nrem_mean.png"))
        plt.close()

        max_power = np.vstack(max_pow).T
        ds_nrem = np.array(ds_nrem)
        ds_and_max_power = np.vstack((ds_nrem, max_power))
        is_nan = np.argwhere(np.isnan(np.sum(ds_and_mean_power, axis=0)))
        ds_and_max_power = np.delete(ds_and_max_power, is_nan.flatten(), axis=1)
        corr_ds_power = np.corrcoef(ds_and_max_power)[0,1:]

        plt.scatter(f, corr_ds_power, s=1)
        plt.xlabel("Freq [Hz]")
        plt.ylabel("Corr. with Delta score")
        plt.title("NREM Correlation drift vs. MAX power")
        # plt.xlim(0, 260)
        if scales_first_half:
            plt.savefig(os.path.join(save_path, self.session_name+"first_half_nrem_max.png"))
        elif scales_second_half:
            plt.savefig(os.path.join(save_path, self.session_name + "second_half_nrem_max.png"))
        else:
            plt.savefig(os.path.join(save_path, self.session_name + "_nrem_max.png"))
        plt.close()
        # compute power spectrum for rem periods
        # --------------------------------------------------------------------------------------------------------------
        print("Starting now with REM ...")
        end_prev_sleep = 0
        mean_pow = []
        max_pow = []
        event_count=0
        file_transitions_rem = []
        # do wavelet analysis for each nrem epoch
        if plot_for_control:
            n = 5
            count = 0
            colors = plt.cm.jet(np.linspace(0, 1, n))
        for i_sleep, l_s in enumerate(self.long_sleep):
            print("processing sleep file" + str(i_sleep))
            # need to check which epochs happend during which sleep
            duration = l_s.get_duration_sec()
            if i_sleep == 4:
                merged_rem_events_time_stamps_current_sleep = [x for x in merged_rem_events_time_stamps if
                                                                end_prev_sleep < x[0]]
            else:
                merged_rem_events_time_stamps_current_sleep = [x for x in merged_rem_events_time_stamps if
                                                               end_prev_sleep< x[0] and x[1] < (end_prev_sleep + duration)]
            # need to subtract the duration of the previous sleep sessions
            merged_rem_events_time_stamps_current_sleep_offset = [x - end_prev_sleep for x in
                                                                   merged_rem_events_time_stamps_current_sleep]
            if plot_for_control:
                for i_event, (start, end) in enumerate(merged_rem_events_time_stamps_current_sleep_offset):
                    plt.hlines(count, end_prev_sleep+start, end_prev_sleep+end, color=colors[i_sleep])
                    count +=1
                plt.vlines(end_prev_sleep, 0, len(merged_rem_events_time_stamps), color="white" ,linewidth=1)

            # go through all epochs
            if all_tetrodes:
                nr_tetrodes = self.long_sleep[0].eegh.shape[1]
                # compute spectrum for each tetrode, z-score each frequency, once there are at least 2 tetrodes,
                # pick maximum across both tetrodes per time point and move to next tetrode
                max_power_z = None
                for tetrode_id in range(nr_tetrodes):
                    print(" - processing tet" + str(tetrode_id))
                    power_list = []
                    for start, end in merged_rem_events_time_stamps_current_sleep_offset:
                        f, t, p = l_s.power_spectrum_analysis(time_interval_s=[start, end], debug=debug,
                                                              tetrode=tetrode_id, scales_first_half=scales_first_half,
                                                              scales_second_half=scales_second_half)
                        power_list.append(p)
                    print("--> Memory usage:" + str(
                        psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2))  # in bytes
                    # join all intervals and z-score
                    power = np.hstack(power_list)
                    power_z = zscore(power, axis=1)
                    del power
                    # check if there is already a previous tetrode
                    if max_power_z is None:
                        max_power_z = np.copy(power_z)
                        del power_z
                    else:
                        power_combined = np.dstack((power_z, max_power_z))
                        max_power_z = np.nanmax(power_combined, axis=2)
                        del power_combined
                        del power_z

                # compute length of each interval to separate events again
                len_per_event = [x.shape[1] for x in power_list]
                # need to compute now mean/max per event
                start = 0
                for len_event in len_per_event:
                    mean_pow.append(np.nanmean(max_power_z[:,start:start+len_event], axis=1))
                    max_pow.append(np.nanmax(max_power_z[:, start:start + len_event], axis=1))
                    start += len_event

                #
                # power_list = []
                # for start, end in merged_rem_events_time_stamps_current_sleep_offset:
                #     f, t, p = l_s.power_spectrum_analysis_all_tetrodes(time_interval_s=[start, end], debug=debug)
                #     power_list.append(p)
                #
                # # each list entry: nr_electrodes x frequencies x time
                # len_per_event = [x.shape[2] for x in power_list]
                # power = np.dstack(power_list)
                # power_z = zscore(power, axis=2)
                # max_power_across_tetrodes = np.max(power_z, axis=0)
                # # need to compute now mean/max per event
                # start = 0
                # for len_event in len_per_event:
                #     mean_pow.append(np.nanmean(max_power_across_tetrodes[:,start:start+len_event], axis=1))
                #     max_pow.append(np.nanmax(max_power_across_tetrodes[:, start:start + len_event], axis=1))
                #     start += len_event

            else:
                for start, end in merged_rem_events_time_stamps_current_sleep_offset:
                    f, t, p = l_s.power_spectrum_analysis(time_interval_s=[start, end], debug=debug)
                    mean_pow.append(np.nanmean(p, axis=1))
                    max_pow.append(np.nanmax(p, axis=1))

            end_prev_sleep += duration
            file_transitions_rem.append(end_prev_sleep)

        if plot_for_control:
            for i_event, (start, end) in enumerate(merged_rem_events_time_stamps):
                plt.hlines(i_event+1, start, end, color="white")
            plt.show()

        # detect events that span across sleep files --> need to delete these

        bad_events_rem = []
        for i_event, (start, end) in enumerate(merged_rem_events_time_stamps):
            for file_t in file_transitions_rem:
                if start < file_t and file_t < end:
                    bad_events_rem.append(i_event)
        if len(bad_events_rem) > 0:
            bad_events_rem = np.array(bad_events_rem)
            ds_rem = np.delete(ds_rem, bad_events_rem)


        mean_power = np.vstack(mean_pow).T
        ds_and_mean_power = np.vstack((ds_rem, mean_power))
        is_nan = np.argwhere(np.isnan(np.sum(ds_and_mean_power, axis=0)))
        ds_and_mean_power = np.delete(ds_and_mean_power, is_nan.flatten(), axis=1)
        corr_ds_power = np.corrcoef(ds_and_mean_power)[0, 1:]

        plt.scatter(f, corr_ds_power, s=1)
        plt.xlabel("Freq [Hz]")
        plt.ylabel("Corr. with Delta score")
        plt.title("REM Correlation drift vs. MEAN power")
        # plt.xlim(0, 260)
        if scales_first_half:
            plt.savefig(os.path.join(save_path, self.session_name+"first_half_rem_mean.png"))
        elif scales_second_half:
            plt.savefig(os.path.join(save_path, self.session_name + "second_half_rem_mean.png"))
        else:
            plt.savefig(os.path.join(save_path, self.session_name + "_rem_mean.png"))
        plt.close()

        max_power = np.vstack(max_pow).T
        ds_and_max_power = np.vstack((ds_rem, max_power))
        is_nan = np.argwhere(np.isnan(np.sum(ds_and_mean_power, axis=0)))
        ds_and_max_power = np.delete(ds_and_max_power, is_nan.flatten(), axis=1)
        corr_ds_power = np.corrcoef(ds_and_max_power)[0, 1:]

        plt.scatter(f, corr_ds_power, s=1)
        plt.xlabel("Freq [Hz]")
        plt.ylabel("Corr. with Delta score")
        plt.title("REM Correlation drift vs. MAX power")
        # plt.xlim(0, 260)
        if scales_first_half:
            plt.savefig(os.path.join(save_path, self.session_name+"first_half_rem_max.png"))
        elif scales_second_half:
            plt.savefig(os.path.join(save_path, self.session_name + "second_half_rem_max.png"))
        else:
            plt.savefig(os.path.join(save_path, self.session_name + "_rem_max.png"))
        plt.close()
        exit()
        print(event_count)
        print(len(merged_nrem_events_time_stamps))
        # correlate with delta
        # for i, (start, end) in enumerate(merged_nrem_events_time_stamps):
        #     plt.hlines(i+1, start, end, color="white")
        # plt.show()

        # get spectrum


        first = 0
        freq_list = []
        time = []
        ssx = []

        for i, l_s in enumerate(self.long_sleep):
            duration = l_s.get_duration_sec()
            freq, time_, ssx_ = l_s.power_spectrum_analysis()
            freq_list.append(freq)
            time.extend(time_ + first)
            ssx.append(ssx_)
            first += duration

        time = np.array(time)
        ssx = np.hstack(ssx)

        mean_power = np.zeros((ssx.shape[0], len(merged_nrem_events_time_stamps)))

        # go through all epochs and compute mean for all frequencies
        for event_id, (start, end) in enumerate(merged_nrem_events_time_stamps):
            event_power = ssx[:, np.logical_and(start < time, time < end)]
            if event_power.shape[1] > 0:
                mean_power[:,event_id] = np.nanmean(event_power, axis=1)
            else:
                mean_power[:,event_id] = np.nan

        ds_and_mean_power = np.vstack((ds_nrem, mean_power))
        is_nan = np.argwhere(np.isnan(np.sum(ds_and_mean_power, axis=0)))
        ds_and_mean_power = np.delete(ds_and_mean_power, is_nan.flatten(), axis=1)
        corr_ds_power = np.corrcoef(ds_and_mean_power)[0,1:]

        plt.scatter(freq, corr_ds_power)
        plt.xlabel("Freq [Hz]")
        plt.ylabel("Corr. with Delta score")
        plt.title("NREM Correlation drift vs. MEAN power")
        plt.xlim(0, 500)
        plt.show()


        mean_power = np.zeros((ssx.shape[0], len(merged_rem_events_time_stamps)))

        # go through all epochs and compute mean for all frequencies
        for event_id, (start, end) in enumerate(merged_rem_events_time_stamps):
            event_power = ssx[:, np.logical_and(start < time, time < end)]
            # event_time = time[np.logical_and(start < time, time < end)]
            # plt.pcolormesh(event_time, freq, event_power, shading='gouraud')
            # plt.ylim(0, 250)
            # plt.ylabel('Frequency [Hz]')
            # plt.xlabel('Time [sec]')
            # plt.show()

            if event_power.shape[1] > 0:
                mean_power[:,event_id] = np.nanmean(event_power, axis=1)
            else:
                mean_power[:,event_id] = np.nan

        ds_rem = np.array(ds_rem)
        ds_and_mean_power = np.vstack((ds_rem, mean_power))
        is_nan = np.argwhere(np.isnan(np.sum(ds_and_mean_power, axis=0)))
        ds_and_mean_power = np.delete(ds_and_mean_power, is_nan.flatten(), axis=1)
        corr_ds_power = np.corrcoef(ds_and_mean_power)[0,1:]

        plt.scatter(freq, corr_ds_power)
        plt.xlabel("Freq [Hz]")
        plt.ylabel("Corr. with Delta score")
        plt.title("REM Correlation drift vs. MEAN power")
        plt.xlim(0, 500)
        plt.show()

    def memory_drift_and_power_spectrum_high_low_delta_score(self, template_type, measure="normalized_ratio", pre_file_name=None,
                                        post_file_name=None, plot_for_control=False,
                                        n_moving_average_pop_vec=200, rem_pop_vec_threshold=10, nrem_pop_vec_threshold=2,
                                        cells_to_use="all", sleep_classification_method="std", shuffling=False, debug=False,
                                        all_tetrodes=False, scales_first_half=False, scales_second_half=False, use_lower_spectrum=False):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param cells_to_use: which cells to use ("all", "stable", "inc", "dec")
        :type cells_to_use: str
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """
        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, _, _ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use, sleep_classification_method=
                                                     sleep_classification_method, shuffling=shuffling)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, _ ,_ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=nrem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use, sleep_classification_method=
                                                     sleep_classification_method, shuffling=shuffling)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------_decoding_similarity_temporal------------------------

        all_events_ratio = ratio_per_rem_event + ratio_per_nrem_event
        all_events_length = event_lengths_rem + event_lengths_nrem
        all_events_length_s = np.hstack((event_duration_rem_in_s, event_duration_nrem_in_s))
        labels_events = np.zeros(len(ratio_per_rem_event)+len(ratio_per_nrem_event))
        labels_events[:len(ratio_per_rem_event)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_ratio_list = [x for _, x in sorted(zip(all_times, all_events_ratio))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_lengths_s = [x for _, x in sorted(zip(all_times, all_events_length_s))]
        sorted_pop_vec_ratio = np.hstack(sorted_events_ratio_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_length_s = []
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat==1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat==-1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))
        merged_events_length_s = np.array(merged_events_length_s)
        merged_events_times = np.vstack(merged_events_times)

        # merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        # merged_nrem_event_times = merged_events_times[merged_events_labels == 0]
        merged_events_length_s = merged_events_times[:,1]-merged_events_times[:,0]


        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps==1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre_post ratio) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_merged_rem_event = []
        ratio_per_merged_nrem_event = []
        ratio_rem_nrem_events = []
        rem_nrem_events_label = []
        ratio_rem_nrem_pop_vec = []
        rem_nrem_pop_vec_label = []

        for event_id, (start_event, end_event) in enumerate(zip(start, end)):
            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                ratio_per_merged_rem_event.append(sorted_pop_vec_ratio[start_event:end_event])
            # nrem event
            else:
                ratio_per_merged_nrem_event.append(sorted_pop_vec_ratio[start_event:end_event])

            ratio_rem_nrem_events.append(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])

            ratio_rem_nrem_pop_vec.extend(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_pop_vec_label.extend(labels_per_pop_vec[start_event:end_event])


        # smooth across population vectors --> that means also across REM/NREM epochs
        # --------------------------------------------------------------------------------------------------------------
        len_new_events = [x.shape[0] for x in ratio_rem_nrem_events]
        ratio_per_pop_vec_new = np.hstack(ratio_rem_nrem_events)
        ratio_per_pop_vec_new_smooth = moving_average(a=np.array(ratio_per_pop_vec_new), n=n_moving_average_pop_vec)

        ratio_rem_nrem_events_smooth = []
        first = 0
        for event_id in range(len(len_new_events)):
            # check if data is still available (due to smoothing --> data gets shorter)
            # if not --> leave loop
            if first >= ratio_per_pop_vec_new_smooth.shape[0]:
                break
            end_event = min(first + len_new_events[event_id], ratio_per_pop_vec_new_smooth.shape[0])
            ratio_rem_nrem_events_smooth.append(ratio_per_pop_vec_new_smooth[first:end_event])
            first += len_new_events[event_id]

        # need to delete indices of events that happend after the last data point after smoothing
        rem_nrem_events_label = rem_nrem_events_label[:len(ratio_rem_nrem_events_smooth)]

        # get rem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        rem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 1))
        ds_rem = []
        ds_rem_smoothed_within = []
        merged_events_rem_length = []
        ratio_rem_events_smooth = []
        merged_rem_events_time_stamps = []
        for rem_index in rem_events_indices:
            ds_rem.append(ratio_rem_nrem_events_smooth[rem_index][-1]-ratio_rem_nrem_events_smooth[rem_index][0])
            smooth_event = moving_average(a=ratio_rem_nrem_events[rem_index], n=10)
            ds_rem_smoothed_within.append(smooth_event[-1]-smooth_event[0])
            merged_events_rem_length.append(ratio_rem_nrem_events_smooth[rem_index].shape[0])
            ratio_rem_events_smooth.append(ratio_rem_nrem_events_smooth[rem_index])
            merged_rem_events_time_stamps.append(merged_events_times[rem_index])

        ds_rem = np.array(ds_rem)
        # get nrem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        nrem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 0))
        ds_nrem = []
        ds_nrem_smoothed_within = []
        merged_events_nrem_length = []
        merged_nrem_events_time_stamps = []
        ratio_nrem_events_smooth = []
        for nrem_index in nrem_events_indices:
            ds_nrem.append(ratio_rem_nrem_events_smooth[nrem_index][-1]-ratio_rem_nrem_events_smooth[nrem_index][0])
            merged_events_nrem_length.append(ratio_rem_nrem_events_smooth[nrem_index].shape[0])
            ratio_nrem_events_smooth.append(ratio_rem_nrem_events_smooth[nrem_index])
            merged_nrem_events_time_stamps.append(merged_events_times[nrem_index])

        ds_nrem = np.array(ds_nrem)

        # REM
        # --------------------------------------------------------------------------------------------------------------

        # first need to identify high and low delta epochs
        lower_quant_rem = np.quantile(ds_rem, 0.25)
        higher_quant_rem = np.quantile(ds_rem, 0.75)

        merged_rem_events_time_stamps = np.vstack(merged_rem_events_time_stamps)

        merged_rem_events_time_stamps_low = merged_rem_events_time_stamps[ds_rem < lower_quant_rem, :]
        merged_rem_events_time_stamps_high = merged_rem_events_time_stamps[ds_rem > higher_quant_rem, :]

        merged_rem_events_time_stamps_low = [x for x in merged_rem_events_time_stamps_low]
        merged_rem_events_time_stamps_high = [x for x in merged_rem_events_time_stamps_high]

        print("Starting now with REM ...")
        end_prev_sleep = 0
        mean_pow_low = []
        mean_pow_high = []
        max_pow_high = []
        max_pow_low = []

        event_count=0
        file_transitions_rem = []
        # do wavelet analysis for each nrem epoch
        if plot_for_control:
            n = 5
            count = 0
            colors = plt.cm.jet(np.linspace(0, 1, n))
        for i_sleep, l_s in enumerate(self.long_sleep):
            print("processing sleep file" + str(i_sleep))
            # need to check which epochs happend during which sleep
            duration = l_s.get_duration_sec()
            if i_sleep == 4:
                merged_rem_events_time_stamps_current_sleep_low = [x for x in merged_rem_events_time_stamps_low if
                                                                end_prev_sleep < x[0]]
                merged_rem_events_time_stamps_current_sleep_high = [x for x in merged_rem_events_time_stamps_high if
                                                                end_prev_sleep < x[0]]
            else:
                merged_rem_events_time_stamps_current_sleep_low = [x for x in merged_rem_events_time_stamps_low if
                                                                (end_prev_sleep) < x[0] and x[1]< (end_prev_sleep+duration)]
                merged_rem_events_time_stamps_current_sleep_high = [x for x in merged_rem_events_time_stamps_high if
                                                                (end_prev_sleep) < x[0] and x[1]< (end_prev_sleep+duration)]
            # need to subtract the duration of the previous sleep sessions
            merged_rem_events_time_stamps_current_sleep_offset_low = \
                [x-end_prev_sleep for x in merged_rem_events_time_stamps_current_sleep_low]


            merged_rem_events_time_stamps_current_sleep_offset_high = \
                [x - end_prev_sleep for x in merged_rem_events_time_stamps_current_sleep_high]
            if plot_for_control:
                for i_event, (start, end) in enumerate(merged_rem_events_time_stamps_current_sleep_offset):
                    plt.hlines(count, end_prev_sleep+start, end_prev_sleep+end, color=colors[i_sleep])
                    count +=1
                plt.vlines(end_prev_sleep, 0, len(merged_rem_events_time_stamps), color="white" ,linewidth=1)

            # go through all epochs
            if all_tetrodes:
                nr_tetrodes = self.long_sleep[0].eegh.shape[1]
                # compute spectrum for each tetrode, z-score each frequency, once there are at least 2 tetrodes,
                # pick maximum across both tetrodes per time point and move to next tetrode
                # first for low delta score
                # ------------------------------------------------------------------------------------------------------
                max_power_z_low = None
                for tetrode_id in range(nr_tetrodes):
                    print(" - processing tet" + str(tetrode_id))
                    power_list_low = []
                    for start, end in merged_rem_events_time_stamps_current_sleep_offset_low:
                        f, t, p = l_s.power_spectrum_analysis(time_interval_s=[start, end], debug=debug,
                                                              tetrode=tetrode_id, scales_first_half=scales_first_half,
                                                              scales_second_half=scales_second_half)
                        power_list.append(p)
                    print("--> Memory usage:" + str(
                        psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2))  # in bytes
                    # join all intervals and z-score
                    power_low = np.hstack(power_list_low)
                    power_z_low = zscore(power_low, axis=1)
                    del power_low
                    # check if there is already a previous tetrode
                    if max_power_z_low is None:
                        max_power_z_low = np.copy(power_z_low)
                        del power_z_low
                    else:
                        power_combined_low = np.dstack((power_z_low, max_power_z_low))
                        max_power_z_low = np.nanmax(power_combined_low, axis=2)
                        del power_combined_low
                        del power_z_low

                # compute length of each interval to separate events again
                len_per_event = [x.shape[1] for x in power_list_low]
                # need to compute now mean/max per event
                start = 0
                for len_event in len_per_event:
                    mean_pow_low.append(np.nanmean(max_power_z_low[:,start:start+len_event], axis=1))
                    max_pow_low.append(np.nanmax(max_power_z_low[:, start:start + len_event], axis=1))
                    start += len_event

                # next for high delta score
                # ------------------------------------------------------------------------------------------------------
                max_power_z_high = None
                for tetrode_id in range(nr_tetrodes):
                    print(" - processing tet for low_delta_score" + str(tetrode_id))
                    power_list_high = []
                    for start, end in merged_rem_events_time_stamps_current_sleep_offset_high:
                        f, t, p = l_s.power_spectrum_analysis(time_interval_s=[start, end], debug=debug,
                                                              tetrode=tetrode_id, scales_first_half=scales_first_half,
                                                              scales_second_half=scales_second_half, use_lower_spectrum=use_lower_spectrum)
                        power_list_high.append(p)
                    print("--> Memory usage:"+str(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2))  # in bytes
                    # join all intervals and z-score
                    power_high = np.hstack(power_list_high)
                    power_z_high = zscore(power_high, axis=1)
                    del power_high
                    # check if there is already a previous tetrode
                    if max_power_z_high is None:
                        max_power_z_high = np.copy(power_z_high)
                        del power_z_high
                    else:
                        power_combined_high = np.dstack((power_z_high, max_power_z_high))
                        max_power_z_high = np.nanmax(power_combined_high, axis=2)
                        del power_combined_high
                        del power_z_high

                # compute length of each interval to separate events again
                len_per_event = [x.shape[1] for x in power_list_high]
                # need to compute now mean/max per event
                start = 0
                for len_event in len_per_event:
                    mean_pow_high.append(np.nanmean(max_power_z_high[:,start:start+len_event], axis=1))
                    max_pow_high.append(np.nanmax(max_power_z_high[:, start:start + len_event], axis=1))
                    start += len_event

                #
                # power_list = []
                # for start, end in merged_rem_events_time_stamps_current_sleep_offset:
                #     f, t, p = l_s.power_spectrum_analysis_all_tetrodes(time_interval_s=[start, end], debug=debug)
                #     power_list.append(p)
                #
                # # each list entry: nr_electrodes x frequencies x time
                # len_per_event = [x.shape[2] for x in power_list]
                # power = np.dstack(power_list)
                # power_z = zscore(power, axis=2)
                # max_power_across_tetrodes = np.max(power_z, axis=0)
                # # need to compute now mean/max per event
                # start = 0
                # for len_event in len_per_event:
                #     mean_pow.append(np.nanmean(max_power_across_tetrodes[:,start:start+len_event], axis=1))
                #     max_pow.append(np.nanmax(max_power_across_tetrodes[:, start:start + len_event], axis=1))
                #     start += len_event

            else:
                # first for low_delta_score
                for start, end in merged_rem_events_time_stamps_current_sleep_offset_low:
                    f, t, p = l_s.power_spectrum_analysis(time_interval_s=[start, end], debug=debug,
                                                          use_lower_spectrum=use_lower_spectrum)
                    mean_pow_low.append(np.nanmean(p, axis=1))
                    max_pow_low.append(np.nanmax(p, axis=1))
                # next for high delta_score
                for start, end in merged_rem_events_time_stamps_current_sleep_offset_high:
                    f, t, p = l_s.power_spectrum_analysis(time_interval_s=[start, end], debug=debug,
                                                          use_lower_spectrum=use_lower_spectrum)
                    mean_pow_high.append(np.nanmean(p, axis=1))
                    max_pow_high.append(np.nanmax(p, axis=1))
            end_prev_sleep += duration
            file_transitions_rem.append(end_prev_sleep)

        if plot_for_control:
            for i_event, (start, end) in enumerate(merged_rem_events_time_stamps):
                plt.hlines(i_event+1, start, end, color="white")
            plt.show()

        mean_pow_high = np.vstack(mean_pow_high)
        mean_pow_low = np.vstack(mean_pow_low)
        # plot results for low and high
        plt.errorbar(f, np.mean(mean_pow_high, axis=0), yerr=np.std(mean_pow_high, axis=0), color="blue", label="REM: high delta_score")
        plt.errorbar(f+0.5, np.mean(mean_pow_low, axis=0), yerr=np.std(mean_pow_low, axis=0), color="lightblue", label="REM: low delta_score")

        max_y = plt.gca().get_ylim()[1]
        min_y = plt.gca().get_ylim()[0]
        # compute significance
        for f_,h,l in zip(f, mean_pow_high.T, mean_pow_low.T):
            if mannwhitneyu(h,l)[1] < 0.001:
                plt.text(f_, max_y+5, "***")
            elif mannwhitneyu(h, l)[1] < 0.01:
                plt.text(f_, max_y+5, "**")
            elif mannwhitneyu(h, l)[1] < 0.05:
                plt.text(f_, max_y+5, "*")
        plt.ylim(min_y, max_y+10)
        plt.xlabel("Freq (Hz)")
        plt.ylabel("Mean power (mean+-std)")
        plt.legend(loc=3)
        plt.show()


        plt.errorbar(f, np.mean(mean_pow_high, axis=0), yerr=np.std(mean_pow_high, axis=0), color="blue", label="REM: high delta_score")
        plt.errorbar(f+0.5, np.mean(mean_pow_low, axis=0), yerr=np.std(mean_pow_low, axis=0), color="lightblue", label="REM: low delta_score")

        max_y = plt.gca().get_ylim()[1]
        min_y = plt.gca().get_ylim()[0]
        # compute significance
        for f_,h,l in zip(f, mean_pow_high.T, mean_pow_low.T):
            if f_ < 20:
                if mannwhitneyu(h,l)[1] < 0.001:
                    plt.text(f_, max_y+5, "***")
                elif mannwhitneyu(h, l)[1] < 0.01:
                    plt.text(f_, max_y+5, "**")
                elif mannwhitneyu(h, l)[1] < 0.05:
                    plt.text(f_, max_y+5, "*")
        plt.ylim(min_y, max_y+10)
        plt.xlim(0,20)
        plt.xlabel("Freq (Hz)")
        plt.ylabel("Mean power (mean+-std)")
        plt.legend(loc=3)
        plt.show()

        # compute power spectrum for nrem periods
        # --------------------------------------------------------------------------------------------------------------

        # first need to identify high and low delta epochs
        lower_quant = np.quantile(ds_nrem, 0.25)
        higher_quant = np.quantile(ds_nrem, 0.75)

        merged_nrem_events_time_stamps = np.vstack(merged_nrem_events_time_stamps)

        merged_nrem_events_time_stamps_low = merged_nrem_events_time_stamps[ds_nrem < lower_quant, :]
        merged_nrem_events_time_stamps_high = merged_nrem_events_time_stamps[ds_nrem > higher_quant, :]

        merged_nrem_events_time_stamps_low = [x for x in merged_nrem_events_time_stamps_low]
        merged_nrem_events_time_stamps_high = [x for x in merged_nrem_events_time_stamps_high]

        print("Starting now with NREM ...")
        print("--> Memory usage:" + str(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2))

        end_prev_sleep = 0
        mean_pow_low = []
        max_pow_low = []
        mean_pow_high = []
        max_pow_high = []
        file_transitions_nrem = []

        # do wavelet analysis for each nrem epoch
        if plot_for_control:
            n = 5
            count=0
            colors = plt.cm.jet(np.linspace(0, 1, n))
        for i_sleep, l_s in enumerate(self.long_sleep):
            print("processing sleep file" + str(i_sleep))
            # need to check which epochs happend during which sleep
            duration = l_s.get_duration_sec()
            if i_sleep == 4:
                merged_nrem_events_time_stamps_current_sleep_low = [x for x in merged_nrem_events_time_stamps_low if
                                                                end_prev_sleep < x[0]]
                merged_nrem_events_time_stamps_current_sleep_high = [x for x in merged_nrem_events_time_stamps_high if
                                                                end_prev_sleep < x[0]]
            else:
                merged_nrem_events_time_stamps_current_sleep_low = [x for x in merged_nrem_events_time_stamps_low if
                                                                (end_prev_sleep) < x[0] and x[1]< (end_prev_sleep+duration)]
                merged_nrem_events_time_stamps_current_sleep_high = [x for x in merged_nrem_events_time_stamps_high if
                                                                (end_prev_sleep) < x[0] and x[1]< (end_prev_sleep+duration)]

            # need to subtract the duration of the previous sleep sessions
            merged_nrem_events_time_stamps_current_sleep_offset_low = \
                [x-end_prev_sleep for x in merged_nrem_events_time_stamps_current_sleep_low]


            merged_nrem_events_time_stamps_current_sleep_offset_high = \
                [x - end_prev_sleep for x in merged_nrem_events_time_stamps_current_sleep_high]

            if plot_for_control:
                for i_event, (start, end) in enumerate(merged_nrem_events_time_stamps_current_sleep_offset):
                    plt.hlines(count, end_prev_sleep+start, end_prev_sleep+end, color=colors[i_sleep])
                    count +=1
                plt.vlines(end_prev_sleep, 0, len(merged_nrem_events_time_stamps), color="white" ,linewidth=1)

            # go through all epochs
            if all_tetrodes:
                nr_tetrodes = self.long_sleep[0].eegh.shape[1]
                # compute spectrum for each tetrode, z-score each frequency, once there are at least 2 tetrodes,
                # pick maximum across both tetrodes per time point and move to next tetrode

                # first for low delta score
                # ------------------------------------------------------------------------------------------------------
                max_power_z_low = None
                for tetrode_id in range(nr_tetrodes):
                    print(" - processing tet for low_delta_score" + str(tetrode_id))
                    power_list_low = []
                    for start, end in merged_nrem_events_time_stamps_current_sleep_offset_low:
                        f, t, p = l_s.power_spectrum_analysis(time_interval_s=[start, end], debug=debug,
                                                              tetrode=tetrode_id, scales_first_half=scales_first_half,
                                                              scales_second_half=scales_second_half,
                                                              use_lower_spectrum=use_lower_spectrum)
                        power_list_low.append(p)
                    print("--> Memory usage:"+str(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2))  # in bytes
                    # join all intervals and z-score
                    power_low = np.hstack(power_list_low)
                    power_z_low = zscore(power_low, axis=1)
                    del power_low
                    # check if there is already a previous tetrode
                    if max_power_z_low is None:
                        max_power_z_low = np.copy(power_z_low)
                        del power_z_low
                    else:
                        power_combined_low = np.dstack((power_z_low, max_power_z_low))
                        max_power_z_low = np.nanmax(power_combined_low, axis=2)
                        del power_combined_low
                        del power_z_low

                # compute length of each interval to separate events again
                len_per_event = [x.shape[1] for x in power_list_low]
                # need to compute now mean/max per event
                start = 0
                for len_event in len_per_event:
                    mean_pow_low.append(np.nanmean(max_power_z_low[:,start:start+len_event], axis=1))
                    max_pow_low.append(np.nanmax(max_power_z_low[:, start:start + len_event], axis=1))
                    start += len_event

                # next for high delta score
                # ------------------------------------------------------------------------------------------------------
                max_power_z_high = None
                for tetrode_id in range(nr_tetrodes):
                    print(" - processing tet for low_delta_score" + str(tetrode_id))
                    power_list_high = []
                    for start, end in merged_nrem_events_time_stamps_current_sleep_offset_high:
                        f, t, p = l_s.power_spectrum_analysis(time_interval_s=[start, end], debug=debug,
                                                              tetrode=tetrode_id, scales_first_half=scales_first_half,
                                                              scales_second_half=scales_second_half)
                        power_list_high.append(p)
                    print("--> Memory usage:"+str(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2))  # in bytes
                    # join all intervals and z-score
                    power_high = np.hstack(power_list_high)
                    power_z_high = zscore(power_high, axis=1)
                    del power_high
                    # check if there is already a previous tetrode
                    if max_power_z_high is None:
                        max_power_z_high = np.copy(power_z_high)
                        del power_z_high
                    else:
                        power_combined_high = np.dstack((power_z_high, max_power_z_high))
                        max_power_z_high = np.nanmax(power_combined_high, axis=2)
                        del power_combined_high
                        del power_z_high

                # compute length of each interval to separate events again
                len_per_event = [x.shape[1] for x in power_list_high]
                # need to compute now mean/max per event
                start = 0
                for len_event in len_per_event:
                    mean_pow_high.append(np.nanmean(max_power_z_high[:,start:start+len_event], axis=1))
                    max_pow_high.append(np.nanmax(max_power_z_high[:, start:start + len_event], axis=1))
                    start += len_event



            else:
                # first for low_delta_score
                for start, end in merged_nrem_events_time_stamps_current_sleep_offset_low:
                    f, t, p = l_s.power_spectrum_analysis(time_interval_s=[start, end], debug=debug,
                                                          use_lower_spectrum=use_lower_spectrum)
                    mean_pow_low.append(np.nanmean(p, axis=1))
                    max_pow_low.append(np.nanmax(p, axis=1))
                # next for high delta_score
                for start, end in merged_nrem_events_time_stamps_current_sleep_offset_high:
                    f, t, p = l_s.power_spectrum_analysis(time_interval_s=[start, end], debug=debug,
                                                          use_lower_spectrum=use_lower_spectrum)
                    mean_pow_high.append(np.nanmean(p, axis=1))
                    max_pow_high.append(np.nanmax(p, axis=1))

            end_prev_sleep += duration
            file_transitions_nrem.append(end_prev_sleep)

        if plot_for_control:
            for i_event, (start, end) in enumerate(merged_nrem_events_time_stamps):
                plt.hlines(i_event+1, start, end, color="white")
            plt.show()

        mean_pow_high = np.vstack(mean_pow_high)
        mean_pow_low = np.vstack(mean_pow_low)
        # plot results for low and high
        plt.errorbar(f, np.mean(mean_pow_high, axis=0), yerr=np.std(mean_pow_high, axis=0), color="red", label="NREM: high delta_score")
        plt.errorbar(f+0.5, np.mean(mean_pow_low, axis=0), yerr=np.std(mean_pow_low, axis=0), color="salmon", label="NREM: low delta_score")
        max_y = plt.gca().get_ylim()[1]
        min_y = plt.gca().get_ylim()[0]
        # compute significance
        for f_,h,l in zip(f, mean_pow_high.T, mean_pow_low.T):
            if mannwhitneyu(h,l)[1] < 0.001:
                plt.text(f_, max_y+5, "***")
            elif mannwhitneyu(h, l)[1] < 0.01:
                plt.text(f_, max_y+5, "**")
            elif mannwhitneyu(h, l)[1] < 0.05:
                plt.text(f_, max_y+5, "*")
        plt.ylim(min_y, max_y+10)
        plt.xlabel("Freq (Hz)")
        plt.ylabel("Mean power (mean+-std)")
        plt.legend(loc=3)
        plt.show()

        # plot results for low and high
        plt.errorbar(f, np.mean(mean_pow_high, axis=0), yerr=np.std(mean_pow_high, axis=0), color="red", label="NREM: high delta_score")
        plt.errorbar(f+0.5, np.mean(mean_pow_low, axis=0), yerr=np.std(mean_pow_low, axis=0), color="salmon", label="NREM: low delta_score")
        max_y = plt.gca().get_ylim()[1]
        min_y = plt.gca().get_ylim()[0]
        # compute significance
        for f_,h,l in zip(f, mean_pow_high.T, mean_pow_low.T):
            if mannwhitneyu(h,l)[1] < 0.001:
                plt.text(f_, max_y+5, "***")
            elif mannwhitneyu(h, l)[1] < 0.01:
                plt.text(f_, max_y+5, "**")
            elif mannwhitneyu(h, l)[1] < 0.05:
                plt.text(f_, max_y+5, "*")
        plt.ylim(min_y, max_y+10)
        plt.xlim(0, 10)
        plt.xlabel("Freq (Hz)")
        plt.ylabel("Mean power (mean+-std)")
        plt.legend(loc=3)
        plt.show()

    def memory_drift_and_firing_prob(self, template_type, measure="normalized_ratio", pre_file_name=None,
                                        post_file_name=None, use_only_non_stationary_periods = False,
                                        n_moving_average_pop_vec=200, rem_pop_vec_threshold=10, nrem_pop_vec_threshold=2,
                                        plotting=True, cells_to_use="all", sleep_classification_method="std",
                                        shuffling=False, n_smoothing_firing_rates = 200):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param cells_to_use: which cells to use ("all", "stable", "inc", "dec")
        :type cells_to_use: str
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """
        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, _, _ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use, sleep_classification_method=
                                                     sleep_classification_method, shuffling=shuffling)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, _ ,_ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=nrem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use, sleep_classification_method=
                                                     sleep_classification_method, shuffling=shuffling)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------_decoding_similarity_temporal------------------------

        all_events_ratio = ratio_per_rem_event + ratio_per_nrem_event
        all_events_length = event_lengths_rem + event_lengths_nrem
        all_events_length_s = np.hstack((event_duration_rem_in_s, event_duration_nrem_in_s))
        labels_events = np.zeros(len(ratio_per_rem_event)+len(ratio_per_nrem_event))
        labels_events[:len(ratio_per_rem_event)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_ratio_list = [x for _, x in sorted(zip(all_times, all_events_ratio))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_lengths_s = [x for _, x in sorted(zip(all_times, all_events_length_s))]
        sorted_pop_vec_ratio = np.hstack(sorted_events_ratio_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_length_s = []
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat==1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat==-1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))
        merged_events_length_s = np.array(merged_events_length_s)
        merged_events_times = np.vstack(merged_events_times)

        # merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        # merged_nrem_event_times = merged_events_times[merged_events_labels == 0]
        merged_events_length_s = merged_events_times[:,1]-merged_events_times[:,0]


        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps==1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre_post ratio) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_merged_rem_event = []
        ratio_per_merged_nrem_event = []
        ratio_rem_nrem_events = []
        rem_nrem_events_label = []
        ratio_rem_nrem_pop_vec = []
        rem_nrem_pop_vec_label = []

        for event_id, (start_event, end_event) in enumerate(zip(start, end)):
            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                ratio_per_merged_rem_event.append(sorted_pop_vec_ratio[start_event:end_event])
            # nrem event
            else:
                ratio_per_merged_nrem_event.append(sorted_pop_vec_ratio[start_event:end_event])

            ratio_rem_nrem_events.append(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])

            ratio_rem_nrem_pop_vec.extend(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_pop_vec_label.extend(labels_per_pop_vec[start_event:end_event])


        # smooth across population vectors --> that means also across REM/NREM epochs
        # --------------------------------------------------------------------------------------------------------------
        len_new_events = [x.shape[0] for x in ratio_rem_nrem_events]
        ratio_per_pop_vec_new = np.hstack(ratio_rem_nrem_events)
        ratio_per_pop_vec_new_smooth = moving_average(a=np.array(ratio_per_pop_vec_new), n=n_moving_average_pop_vec)

        ratio_rem_nrem_events_smooth = []
        first = 0
        for event_id in range(len(len_new_events)):
            # check if data is still available (due to smoothing --> data gets shorter)
            # if not --> leave loop
            if first >= ratio_per_pop_vec_new_smooth.shape[0]:
                break
            end_event = min(first + len_new_events[event_id], ratio_per_pop_vec_new_smooth.shape[0])
            ratio_rem_nrem_events_smooth.append(ratio_per_pop_vec_new_smooth[first:end_event])
            first += len_new_events[event_id]

        # need to delete indices of events that happend after the last data point after smoothing
        rem_nrem_events_label = rem_nrem_events_label[:len(ratio_rem_nrem_events_smooth)]

        # get rem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        rem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 1))
        ds_rem = []
        ds_rem_smoothed_within = []
        merged_events_rem_length = []
        ratio_rem_events_smooth = []
        merged_rem_events_time_stamps = []
        for rem_index in rem_events_indices:
            ds_rem.append(ratio_rem_nrem_events_smooth[rem_index][-1]-ratio_rem_nrem_events_smooth[rem_index][0])
            smooth_event = moving_average(a=ratio_rem_nrem_events[rem_index], n=10)
            ds_rem_smoothed_within.append(smooth_event[-1]-smooth_event[0])
            merged_events_rem_length.append(ratio_rem_nrem_events_smooth[rem_index].shape[0])
            ratio_rem_events_smooth.append(ratio_rem_nrem_events_smooth[rem_index])
            merged_rem_events_time_stamps.append(merged_events_times[rem_index])

        # get nrem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        nrem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 0))
        ds_nrem = []
        ds_nrem_smoothed_within = []
        merged_events_nrem_length = []
        merged_nrem_events_time_stamps = []
        ratio_nrem_events_smooth = []
        for nrem_index in nrem_events_indices:
            ds_nrem.append(ratio_rem_nrem_events_smooth[nrem_index][-1]-ratio_rem_nrem_events_smooth[nrem_index][0])
            merged_events_nrem_length.append(ratio_rem_nrem_events_smooth[nrem_index].shape[0])
            ratio_nrem_events_smooth.append(ratio_rem_nrem_events_smooth[nrem_index])
            merged_nrem_events_time_stamps.append(merged_events_times[nrem_index])

        ds_nrem = np.array(ds_nrem)
        ds_rem = np.array(ds_rem)

        nrem_dec_smooth, rem_dec_smooth, rem_inc_smooth, \
        nrem_inc_smooth, rem_stable_smooth, nrem_stable_smooth, first_event_label, merged_event_times = \
            self.firing_rate_changes(return_p_value=False, use_only_non_stationary_periods=use_only_non_stationary_periods,
                                     pop_vec_threshold_rem=rem_pop_vec_threshold, smoothing=n_smoothing_firing_rates,
                                     pop_vec_threshold_nrem=nrem_pop_vec_threshold, filter_initial_period=False,
                                     return_merged_times=True)

        # make sure firing prob. and drift align
        # start with NREM
        if first_event_label == "nrem":
            merged_event_times_nrem = merged_event_times[::2]
            merged_event_times_rem = merged_event_times[1::2]
        else:
            merged_event_times_nrem = merged_event_times[1::2]
            merged_event_times_rem = merged_event_times[::2]

        ds_nrem_clean = []
        nrem_dec_clean = []
        nrem_inc_clean = []
        nrem_stable_clean = []
        # only use periods where time stamps of firing probability and delta score match
        for event_id, (drift_times, firing_prob_times) in enumerate(zip(merged_nrem_events_time_stamps,
                                                                        merged_event_times_nrem)):
            if drift_times[0] == firing_prob_times[0]:
                ds_nrem_clean.append(ds_nrem[event_id])
                nrem_dec_clean.append(nrem_dec_smooth[event_id])
                nrem_inc_clean.append(nrem_inc_smooth[event_id])
                nrem_stable_clean.append(nrem_stable_smooth[event_id])

        ds_rem_clean = []
        rem_dec_clean = []
        rem_inc_clean = []
        rem_stable_clean = []
        # only use periods where time stamps of firing probability and delta score match
        for event_id, (drift_times, firing_prob_times) in enumerate(zip(merged_rem_events_time_stamps,
                                                                        merged_event_times_rem)):
            if drift_times[0] == firing_prob_times[0]:
                ds_rem_clean.append(ds_rem[event_id])
                rem_dec_clean.append(rem_dec_smooth[event_id])
                rem_inc_clean.append(rem_inc_smooth[event_id])
                rem_stable_clean.append(rem_stable_smooth[event_id])

        if plotting:
            plt.scatter(ds_nrem_clean, nrem_dec_clean)
            plt.xlabel("Delta_score NREM period")
            plt.ylabel("Change in firing prob")
            plt.title("Decreasing cells")
            plt.text(0.75, 0.5, "R="+str(np.round(pearsonr(ds_nrem_clean, nrem_dec_clean)[0],2)),
                 horizontalalignment='center',
                 verticalalignment='center',
                 transform=plt.gca().transAxes)
            plt.show()

            plt.scatter(ds_nrem_clean, nrem_inc_clean)
            plt.xlabel("Delta_score NREM period")
            plt.ylabel("Change in firing prob")
            plt.title("Increasing cells")
            plt.text(0.75, 0.5, "R="+str(np.round(pearsonr(ds_nrem_clean, nrem_inc_clean)[0],2)),
                 horizontalalignment='center',
                 verticalalignment='center',
                 transform=plt.gca().transAxes)
            plt.show()

            plt.scatter(ds_rem_clean, rem_dec_clean)
            plt.xlabel("Delta_score REM period")
            plt.ylabel("Change in firing prob")
            plt.title("Decreasing cells")
            plt.text(0.75, 0.5, "R="+str(np.round(pearsonr(ds_rem_clean, rem_dec_clean)[0],2)),
                 horizontalalignment='center',
                 verticalalignment='center',
                 transform=plt.gca().transAxes)
            plt.show()

            plt.scatter(ds_rem_clean, rem_inc_clean)
            plt.xlabel("Delta_score REM period")
            plt.ylabel("Change in firing prob")
            plt.title("Increasing cells")
            plt.text(0.75, 0.5, "R="+str(np.round(pearsonr(ds_rem, rem_inc_clean)[0],2)),
                 horizontalalignment='center',
                 verticalalignment='center',
                 transform=plt.gca().transAxes)
            plt.show()

            plt.scatter(ds_nrem_clean, nrem_stable_clean)
            plt.xlabel("Delta_score NREM period")
            plt.ylabel("Change in firing prob")
            plt.title("Stable cells")
            plt.text(0.75, 0.5, "R="+str(np.round(pearsonr(ds_nrem_clean, nrem_stable_clean)[0],2)),
                 horizontalalignment='center',
                 verticalalignment='center',
                 transform=plt.gca().transAxes)
            plt.show()

            plt.scatter(ds_rem_clean, rem_stable_clean)
            plt.xlabel("Delta_score REM period")
            plt.ylabel("Change in firing prob")
            plt.title("Stable cells")
            plt.text(0.75, 0.5, "R="+str(np.round(pearsonr(ds_rem_clean, rem_stable_clean)[0],2)),
                 horizontalalignment='center',
                 verticalalignment='center',
                 transform=plt.gca().transAxes)
            plt.show()

        return ds_rem_clean, ds_nrem_clean, rem_stable_clean, nrem_stable_clean, rem_dec_clean, nrem_dec_clean, \
               rem_inc_clean, nrem_inc_clean

    def memory_drift_delta_log_likelihood_and_firing_prob(self, template_type="phmm", measure="normalized_ratio", pre_file_name=None,
                                     post_file_name=None, use_only_non_stationary_periods = False,
                                     n_moving_average_pop_vec=200, rem_pop_vec_threshold=10, nrem_pop_vec_threshold=2,
                                     plotting=True, cells_to_use="all", sleep_classification_method="std",
                                     shuffling=False, n_smoothing_firing_rates = 200):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param cells_to_use: which cells to use ("all", "stable", "inc", "dec")
        :type cells_to_use: str
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """
        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        _, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_rem, post_prob_rem = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use, sleep_classification_method=
                                                     sleep_classification_method, shuffling=shuffling, return_pre_prob_list=True)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        _, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, post_prob_nrem = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=nrem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use, sleep_classification_method=
                                                     sleep_classification_method, shuffling=shuffling, return_pre_prob_list=True)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------_decoding_similarity_temporal------------------------

        # per event --> pick maximum likelihood and log
        pre_prob_rem = [np.log(np.max(x, axis=1)) for x in pre_prob_rem]
        post_prob_rem = [np.log(np.max(x, axis=1)) for x in post_prob_rem]
        pre_prob_nrem = [np.log(np.max(x, axis=1)) for x in pre_prob_nrem]
        post_prob_nrem = [np.log(np.max(x, axis=1)) for x in post_prob_nrem]

        all_events_pre = pre_prob_rem + pre_prob_nrem
        all_events_post = post_prob_rem + post_prob_nrem
        all_events_length = event_lengths_rem + event_lengths_nrem
        all_events_length_s = np.hstack((event_duration_rem_in_s, event_duration_nrem_in_s))
        labels_events = np.zeros(len(pre_prob_rem)+len(pre_prob_nrem))
        labels_events[:len(pre_prob_rem)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_pre_list = [x for _, x in sorted(zip(all_times, all_events_pre))]
        sorted_events_post_list = [x for _, x in sorted(zip(all_times, all_events_post))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_lengths_s = [x for _, x in sorted(zip(all_times, all_events_length_s))]
        sorted_pop_vec_pre = np.hstack(sorted_events_pre_list)
        sorted_pop_vec_post = np.hstack(sorted_events_post_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_length_s = []
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat==1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat==-1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))
        merged_events_length_s = np.array(merged_events_length_s)
        merged_events_times = np.vstack(merged_events_times)

        # merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        # merged_nrem_event_times = merged_events_times[merged_events_labels == 0]
        merged_events_length_s = merged_events_times[:,1]-merged_events_times[:,0]

        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps==1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre/post log likeli) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        pre_per_merged_rem_event = []
        post_per_merged_rem_event = []
        pre_per_merged_nrem_event = []
        post_per_merged_nrem_event = []
        pre_rem_nrem_events = []
        post_rem_nrem_events = []
        rem_nrem_events_label = []
        pre_rem_nrem_pop_vec = []
        post_rem_nrem_pop_vec = []
        rem_nrem_pop_vec_label = []

        for event_id, (start_event, end_event) in enumerate(zip(start, end)):
            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                pre_per_merged_rem_event.append(sorted_pop_vec_pre[start_event:end_event])
                post_per_merged_rem_event.append(sorted_pop_vec_post[start_event:end_event])
            # nrem event
            else:
                pre_per_merged_nrem_event.append(sorted_pop_vec_pre[start_event:end_event])
                post_per_merged_nrem_event.append(sorted_pop_vec_post[start_event:end_event])

            pre_rem_nrem_events.append(sorted_pop_vec_pre[start_event:end_event])
            post_rem_nrem_events.append(sorted_pop_vec_post[start_event:end_event])

            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])

            pre_rem_nrem_pop_vec.extend(sorted_pop_vec_pre[start_event:end_event])
            post_rem_nrem_pop_vec.extend(sorted_pop_vec_post[start_event:end_event])
            rem_nrem_pop_vec_label.extend(labels_per_pop_vec[start_event:end_event])

        # smooth across population vectors --> that means also across REM/NREM epochs
        # --------------------------------------------------------------------------------------------------------------
        len_new_events = [x.shape[0] for x in pre_rem_nrem_events]
        pre_per_pop_vec_new = np.hstack(pre_rem_nrem_events)
        post_per_pop_vec_new = np.hstack(post_rem_nrem_events)
        pre_per_pop_vec_new_smooth = moving_average(a=np.array(pre_per_pop_vec_new), n=n_moving_average_pop_vec)
        post_per_pop_vec_new_smooth = moving_average(a=np.array(post_per_pop_vec_new), n=n_moving_average_pop_vec)

        pre_rem_nrem_events_smooth = []
        post_rem_nrem_events_smooth = []
        first = 0
        for event_id in range(len(len_new_events)):
            # check if data is still available (due to smoothing --> data gets shorter)
            # if not --> leave loop
            if first >= pre_per_pop_vec_new_smooth.shape[0]:
                break
            end_event = min(first + len_new_events[event_id], pre_per_pop_vec_new_smooth.shape[0])
            pre_rem_nrem_events_smooth.append(pre_per_pop_vec_new_smooth[first:end_event])
            post_rem_nrem_events_smooth.append(post_per_pop_vec_new_smooth[first:end_event])
            first += len_new_events[event_id]

        # need to delete indices of events that happend after the last data point after smoothing
        rem_nrem_events_label = rem_nrem_events_label[:len(pre_rem_nrem_events_smooth)]

        # get rem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        rem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 1))
        ds_pre_rem = []
        ds_post_rem = []
        ds_pre_rem_smoothed_within = []
        ds_post_rem_smoothed_within = []
        merged_events_rem_length = []
        pre_rem_events_smooth = []
        post_rem_events_smooth = []
        merged_rem_events_time_stamps = []
        for rem_index in rem_events_indices:
            ds_pre_rem.append(pre_rem_nrem_events_smooth[rem_index][-1]-pre_rem_nrem_events_smooth[rem_index][0])
            ds_post_rem.append(post_rem_nrem_events_smooth[rem_index][-1]-post_rem_nrem_events_smooth[rem_index][0])
            smooth_event_pre = moving_average(a=pre_rem_nrem_events[rem_index], n=10)
            ds_pre_rem_smoothed_within.append(smooth_event_pre[-1]-smooth_event_pre[0])
            smooth_event_post = moving_average(a=post_rem_nrem_events[rem_index], n=10)
            ds_post_rem_smoothed_within.append(smooth_event_post[-1]-smooth_event_post[0])
            merged_events_rem_length.append(pre_rem_nrem_events_smooth[rem_index].shape[0])
            pre_rem_events_smooth.append(pre_rem_nrem_events_smooth[rem_index])
            post_rem_events_smooth.append(post_rem_nrem_events_smooth[rem_index])
            merged_rem_events_time_stamps.append(merged_events_times[rem_index])

        # get nrem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        nrem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 0))
        ds_pre_nrem = []
        ds_post_nrem = []
        merged_events_nrem_length = []
        merged_nrem_events_time_stamps = []
        pre_nrem_events_smooth = []
        post_nrem_events_smooth = []
        for nrem_index in nrem_events_indices:
            ds_pre_nrem.append(pre_rem_nrem_events_smooth[nrem_index][-1]-pre_rem_nrem_events_smooth[nrem_index][0])
            ds_post_nrem.append(post_rem_nrem_events_smooth[nrem_index][-1]-post_rem_nrem_events_smooth[nrem_index][0])
            merged_events_nrem_length.append(pre_rem_nrem_events_smooth[nrem_index].shape[0])
            pre_nrem_events_smooth.append(pre_rem_nrem_events_smooth[nrem_index])
            post_nrem_events_smooth.append(post_rem_nrem_events_smooth[nrem_index])
            merged_nrem_events_time_stamps.append(merged_events_times[nrem_index])

        ds_pre_nrem = np.array(ds_pre_nrem)
        ds_pre_rem = np.array(ds_pre_rem)
        ds_post_nrem = np.array(ds_post_nrem)
        ds_post_rem = np.array(ds_post_rem)

        nrem_dec_smooth, rem_dec_smooth, rem_inc_smooth, \
            nrem_inc_smooth, rem_stable_smooth, nrem_stable_smooth, first_event_label, merged_event_times = \
            self.firing_rate_changes(return_p_value=False, use_only_non_stationary_periods=use_only_non_stationary_periods,
                                     pop_vec_threshold_rem=rem_pop_vec_threshold, smoothing=n_smoothing_firing_rates,
                                     pop_vec_threshold_nrem=nrem_pop_vec_threshold, filter_initial_period=False,
                                     return_merged_times=True)

        # plot firing times
        # for ev in merged_event_times:
        #     plt.hlines(1.1, ev[0], ev[1])
        # for ev in merged_nrem_events_time_stamps:
        #     plt.hlines(1.2, ev[0], ev[1], color="blue")
        # for ev in merged_rem_events_time_stamps:
        #     plt.hlines(1.3, ev[0], ev[1], color="red")
        # plt.ylim(1,2)
        # plt.show()

        # make sure firing prob. and drift align
        # start with NREM
        if first_event_label == "nrem":
            merged_event_times_nrem = merged_event_times[::2]
            merged_event_times_rem = merged_event_times[1::2]
        else:
            merged_event_times_nrem = merged_event_times[1::2]
            merged_event_times_rem = merged_event_times[::2]

        ds_pre_nrem_clean = []
        ds_post_nrem_clean = []
        nrem_dec_clean = []
        nrem_inc_clean = []
        nrem_stable_clean = []
        # only use periods where time stamps of firing probability and delta score match
        for event_id, (drift_times, firing_prob_times) in enumerate(zip(merged_nrem_events_time_stamps,
                                                                        merged_event_times_nrem)):
            if drift_times[0] == firing_prob_times[0]:
                ds_pre_nrem_clean.append(ds_pre_nrem[event_id])
                ds_post_nrem_clean.append(ds_post_nrem[event_id])
                nrem_dec_clean.append(nrem_dec_smooth[event_id])
                nrem_inc_clean.append(nrem_inc_smooth[event_id])
                nrem_stable_clean.append(nrem_stable_smooth[event_id])

        ds_pre_rem_clean = []
        ds_post_rem_clean = []
        rem_dec_clean = []
        rem_inc_clean = []
        rem_stable_clean = []
        # only use periods where time stamps of firing probability and delta score match
        for event_id, (drift_times, firing_prob_times) in enumerate(zip(merged_rem_events_time_stamps,
                                                                        merged_event_times_rem)):
            if drift_times[0] == firing_prob_times[0]:
                ds_pre_rem_clean.append(ds_pre_rem[event_id])
                ds_post_rem_clean.append(ds_post_rem[event_id])
                rem_dec_clean.append(rem_dec_smooth[event_id])
                rem_inc_clean.append(rem_inc_smooth[event_id])
                rem_stable_clean.append(rem_stable_smooth[event_id])

        if plotting:

            plt.style.use('default')
            # decreasing cells
            # ----------------------------------------------------------------------------------------------------------
            plt.figure(figsize=(8,8))
            plt.subplot(2,2,1)
            plt.scatter(ds_pre_nrem_clean, nrem_dec_clean, c = "turquoise")
            plt.xlabel("Delta Acquisition log-likeli")
            plt.ylabel("Delta firing prob")
            plt.title("Acquisition NREM")
            plt.text(0.75, 0.5, "R="+str(np.round(pearsonr(ds_pre_nrem_clean, nrem_dec_clean)[0],2)),
                     horizontalalignment='center',
                     verticalalignment='center',
                     transform=plt.gca().transAxes)
            plt.subplot(2,2,2)
            plt.scatter(ds_post_nrem_clean, nrem_dec_clean, c = "turquoise")
            plt.xlabel("Delta Recall log-likeli")
            plt.ylabel("Delta firing prob")
            plt.title("Recall NREM")
            plt.text(0.75, 0.5, "R="+str(np.round(pearsonr(ds_post_nrem_clean, nrem_dec_clean)[0],2)),
                     horizontalalignment='center',
                     verticalalignment='center',
                     transform=plt.gca().transAxes)
            plt.subplot(2,2,3)
            plt.scatter(ds_pre_rem_clean, rem_dec_clean, c = "turquoise")
            plt.xlabel("Delta Acquisition log-likeli")
            plt.ylabel("Delta firing prob")
            plt.title("Acquisition REM")
            plt.text(0.75, 0.5, "R="+str(np.round(pearsonr(ds_pre_rem_clean, rem_dec_clean)[0],2)),
                     horizontalalignment='center',
                     verticalalignment='center',
                     transform=plt.gca().transAxes)
            plt.subplot(2,2,4)
            plt.scatter(ds_post_rem_clean, rem_dec_clean, c = "turquoise")
            plt.xlabel("Delta Recall log-likeli")
            plt.ylabel("Delta firing prob")
            plt.title("Recall REM")
            plt.text(0.75, 0.5, "R="+str(np.round(pearsonr(ds_post_rem_clean, rem_dec_clean)[0],2)),
                     horizontalalignment='center',
                     verticalalignment='center',
                     transform=plt.gca().transAxes)
            plt.tight_layout()
            plt.show()


            # increasing cells
            # ----------------------------------------------------------------------------------------------------------
            plt.figure(figsize=(8,8))
            plt.subplot(2,2,1)
            plt.scatter(ds_pre_nrem_clean, nrem_inc_clean, c = "orange")
            plt.xlabel("Delta Acquisition log-likeli")
            plt.ylabel("Delta firing prob")
            plt.title("Acquisition NREM")
            plt.text(0.75, 0.5, "R="+str(np.round(pearsonr(ds_pre_nrem_clean, nrem_inc_clean)[0],2)),
                     horizontalalignment='center',
                     verticalalignment='center',
                     transform=plt.gca().transAxes)
            plt.subplot(2,2,2)
            plt.scatter(ds_post_nrem_clean, nrem_inc_clean, c = "orange")
            plt.xlabel("Delta Recall log-likeli")
            plt.ylabel("Delta firing prob")
            plt.title("Recall NREM")
            plt.text(0.75, 0.5, "R="+str(np.round(pearsonr(ds_post_nrem_clean, nrem_inc_clean)[0],2)),
                     horizontalalignment='center',
                     verticalalignment='center',
                     transform=plt.gca().transAxes)
            plt.subplot(2,2,3)
            plt.scatter(ds_pre_rem_clean, rem_inc_clean, c = "orange")
            plt.xlabel("Delta Acquisition log-likeli")
            plt.ylabel("Delta firing prob")
            plt.title("Acquisition REM")
            plt.text(0.75, 0.5, "R="+str(np.round(pearsonr(ds_pre_rem_clean, rem_inc_clean)[0],2)),
                     horizontalalignment='center',
                     verticalalignment='center',
                     transform=plt.gca().transAxes)
            plt.subplot(2,2,4)
            plt.scatter(ds_post_rem_clean, rem_inc_clean, c = "orange")
            plt.xlabel("Delta Recall log-likeli")
            plt.ylabel("Delta firing prob")
            plt.title("Recall REM")
            plt.text(0.75, 0.5, "R="+str(np.round(pearsonr(ds_post_rem_clean, rem_inc_clean)[0],2)),
                     horizontalalignment='center',
                     verticalalignment='center',
                     transform=plt.gca().transAxes)
            plt.tight_layout()
            plt.show()


        return ds_pre_rem_clean, ds_post_rem_clean, ds_pre_nrem_clean, ds_post_nrem_clean, rem_stable_clean, \
            nrem_stable_clean, rem_dec_clean, nrem_dec_clean, rem_inc_clean, nrem_inc_clean

    def memory_drift_and_firing_prob_firing_rate(self, template_type, measure="normalized_ratio", pre_file_name=None,
                                        post_file_name=None,
                                        n_moving_average_pop_vec=200, rem_pop_vec_threshold=10, nrem_pop_vec_threshold=2,
                                        plotting=True, cells_to_use="all", sleep_classification_method="std", shuffling=False):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param cells_to_use: which cells to use ("all", "stable", "inc", "dec")
        :type cells_to_use: str
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """
        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, _, _ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use, sleep_classification_method=
                                                     sleep_classification_method, shuffling=shuffling)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, _ ,_ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=nrem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use, sleep_classification_method=
                                                     sleep_classification_method, shuffling=shuffling)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------_decoding_similarity_temporal------------------------

        all_events_ratio = ratio_per_rem_event + ratio_per_nrem_event
        all_events_length = event_lengths_rem + event_lengths_nrem
        all_events_length_s = np.hstack((event_duration_rem_in_s, event_duration_nrem_in_s))
        labels_events = np.zeros(len(ratio_per_rem_event)+len(ratio_per_nrem_event))
        labels_events[:len(ratio_per_rem_event)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_ratio_list = [x for _, x in sorted(zip(all_times, all_events_ratio))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_lengths_s = [x for _, x in sorted(zip(all_times, all_events_length_s))]
        sorted_pop_vec_ratio = np.hstack(sorted_events_ratio_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_length_s = []
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat==1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat==-1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))
        merged_events_length_s = np.array(merged_events_length_s)
        merged_events_times = np.vstack(merged_events_times)

        # merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        # merged_nrem_event_times = merged_events_times[merged_events_labels == 0]
        merged_events_length_s = merged_events_times[:,1]-merged_events_times[:,0]


        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps==1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre_post ratio) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_merged_rem_event = []
        ratio_per_merged_nrem_event = []
        ratio_rem_nrem_events = []
        rem_nrem_events_label = []
        ratio_rem_nrem_pop_vec = []
        rem_nrem_pop_vec_label = []

        for event_id, (start_event, end_event) in enumerate(zip(start, end)):
            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                ratio_per_merged_rem_event.append(sorted_pop_vec_ratio[start_event:end_event])
            # nrem event
            else:
                ratio_per_merged_nrem_event.append(sorted_pop_vec_ratio[start_event:end_event])

            ratio_rem_nrem_events.append(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])

            ratio_rem_nrem_pop_vec.extend(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_pop_vec_label.extend(labels_per_pop_vec[start_event:end_event])


        # smooth across population vectors --> that means also across REM/NREM epochs
        # --------------------------------------------------------------------------------------------------------------
        len_new_events = [x.shape[0] for x in ratio_rem_nrem_events]
        ratio_per_pop_vec_new = np.hstack(ratio_rem_nrem_events)
        ratio_per_pop_vec_new_smooth = moving_average(a=np.array(ratio_per_pop_vec_new), n=n_moving_average_pop_vec)

        ratio_rem_nrem_events_smooth = []
        first = 0
        for event_id in range(len(len_new_events)):
            # check if data is still available (due to smoothing --> data gets shorter)
            # if not --> leave loop
            if first >= ratio_per_pop_vec_new_smooth.shape[0]:
                break
            end_event = min(first + len_new_events[event_id], ratio_per_pop_vec_new_smooth.shape[0])
            ratio_rem_nrem_events_smooth.append(ratio_per_pop_vec_new_smooth[first:end_event])
            first += len_new_events[event_id]

        # need to delete indices of events that happend after the last data point after smoothing
        rem_nrem_events_label = rem_nrem_events_label[:len(ratio_rem_nrem_events_smooth)]

        # get rem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        rem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 1))
        ds_rem = []
        ds_rem_smoothed_within = []
        merged_events_rem_length = []
        ratio_rem_events_smooth = []
        merged_rem_events_time_stamps = []
        for rem_index in rem_events_indices:
            ds_rem.append(ratio_rem_nrem_events_smooth[rem_index][-1]-ratio_rem_nrem_events_smooth[rem_index][0])
            smooth_event = moving_average(a=ratio_rem_nrem_events[rem_index], n=10)
            ds_rem_smoothed_within.append(smooth_event[-1]-smooth_event[0])
            merged_events_rem_length.append(ratio_rem_nrem_events_smooth[rem_index].shape[0])
            ratio_rem_events_smooth.append(ratio_rem_nrem_events_smooth[rem_index])
            merged_rem_events_time_stamps.append(merged_events_times[rem_index])

        # get nrem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        nrem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 0))
        ds_nrem = []
        ds_nrem_smoothed_within = []
        merged_events_nrem_length = []
        merged_nrem_events_time_stamps = []
        ratio_nrem_events_smooth = []
        for nrem_index in nrem_events_indices:
            ds_nrem.append(ratio_rem_nrem_events_smooth[nrem_index][-1]-ratio_rem_nrem_events_smooth[nrem_index][0])
            merged_events_nrem_length.append(ratio_rem_nrem_events_smooth[nrem_index].shape[0])
            ratio_nrem_events_smooth.append(ratio_rem_nrem_events_smooth[nrem_index])
            merged_nrem_events_time_stamps.append(merged_events_times[nrem_index])

        ds_nrem = np.array(ds_nrem)
        ds_rem = np.array(ds_rem)

        nrem_dec_smooth, rem_dec_smooth, rem_inc_smooth, \
        nrem_inc_smooth, rem_stable_smooth, nrem_stable_smooth, first_event_label, merged_event_times = \
            self.firing_rate_changes(return_p_value=False, use_only_non_stationary_periods=False,
                                     pop_vec_threshold_rem=rem_pop_vec_threshold,
                                     pop_vec_threshold_nrem=nrem_pop_vec_threshold, filter_initial_period=False,
                                     return_merged_times=True)

        # make sure firing prob. and drift align
        # start with NREM
        if first_event_label == "nrem":
            merged_event_times_nrem = merged_event_times[::2]
            merged_event_times_rem = merged_event_times[1::2]
        else:
            merged_event_times_nrem = merged_event_times[1::2]
            merged_event_times_rem = merged_event_times[::2]

        ds_nrem_clean = []
        nrem_dec_clean = []
        nrem_inc_clean = []
        nrem_stable_clean = []
        nrem_event_times_clean = []
        # only use periods where time stamps of firing probability and delta score match
        for event_id, (drift_times, firing_prob_times) in enumerate(zip(merged_nrem_events_time_stamps,
                                                                        merged_event_times_nrem)):
            if drift_times[0] == firing_prob_times[0]:
                ds_nrem_clean.append(ds_nrem[event_id])
                nrem_dec_clean.append(nrem_dec_smooth[event_id])
                nrem_inc_clean.append(nrem_inc_smooth[event_id])
                nrem_stable_clean.append(nrem_stable_smooth[event_id])
                nrem_event_times_clean.append(firing_prob_times)

        ds_rem_clean = []
        rem_dec_clean = []
        rem_inc_clean = []
        rem_stable_clean = []
        rem_event_times_clean = []
        # only use periods where time stamps of firing probability and delta score match
        for event_id, (drift_times, firing_prob_times) in enumerate(zip(merged_rem_events_time_stamps,
                                                                        merged_event_times_rem)):
            if drift_times[0] == firing_prob_times[0]:
                ds_rem_clean.append(ds_rem[event_id])
                rem_dec_clean.append(rem_dec_smooth[event_id])
                rem_inc_clean.append(rem_inc_smooth[event_id])
                rem_stable_clean.append(rem_stable_smooth[event_id])
                rem_event_times_clean.append(firing_prob_times)

        # convert list to numpy array
        rem_event_times_clean = np.vstack(rem_event_times_clean)
        nrem_event_times_clean = np.vstack(nrem_event_times_clean)

        # get average firing rates for rem and nrem periods
        firing_rates_rem = self.firing_rate_for_interval(intervals_s=rem_event_times_clean)
        firing_rates_nrem = self.firing_rate_for_interval(intervals_s=nrem_event_times_clean)

        # need to separate into subsets
        # get stable, decreasing, increasing cells
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_"+self.params.stable_cell_method+".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_ids = class_dic["stable_cell_ids"]
        inc_ids = class_dic["increase_cell_ids"]
        dec_ids = class_dic["decrease_cell_ids"]

        firing_rates_rem_dec = firing_rates_rem[dec_ids, :]
        firing_rates_nrem_dec = firing_rates_nrem[dec_ids, :]

        # need to exclude very short intervals (estimation of firing rate)
        nrem_dec_clean = np.array(nrem_dec_clean)
        dur_nrem = nrem_event_times_clean[:, 1] - nrem_event_times_clean[:, 0]
        nrem_dec_clean = nrem_dec_clean[dur_nrem > 5]
        firing_rates_nrem_dec = firing_rates_nrem_dec[:, dur_nrem > 5]

        mean_firing_rate_rem_dec = np.mean(firing_rates_rem_dec, axis=0)
        mean_firing_rate_nrem_dec = np.mean(firing_rates_nrem_dec, axis=0)

        if plotting:
            plt.scatter(mean_firing_rate_rem_dec, rem_dec_clean)
            plt.xlabel("Mean firing rate REM")
            plt.ylabel("Change in firing prob REM")
            plt.title("Decreasing cells")
            plt.text(0.75, 0.5, "R="+str(np.round(pearsonr(mean_firing_rate_rem_dec, rem_dec_clean)[0],2)),
                 horizontalalignment='center',
                 verticalalignment='center',
                 transform=plt.gca().transAxes)
            plt.show()

            plt.scatter(mean_firing_rate_nrem_dec, nrem_dec_clean)
            plt.xlabel("Mean firing rate NREM")
            plt.ylabel("Change in firing prob NREM")
            plt.title("Decreasing cells")
            plt.text(0.45, 0.75, "R="+str(np.round(pearsonr(mean_firing_rate_nrem_dec, nrem_dec_clean)[0],2)),
                 horizontalalignment='center',
                 verticalalignment='center',
                 transform=plt.gca().transAxes)
            plt.show()

        else:
            return zscore(mean_firing_rate_rem_dec), zscore(rem_dec_clean), \
                   zscore(mean_firing_rate_nrem_dec), zscore(nrem_dec_clean)

    def memory_drift_and_mean_firing(self, template_type, measure="normalized_ratio", pre_file_name=None,
                                     post_file_name=None, n_moving_average_pop_vec=200, rem_pop_vec_threshold=10,
                                     nrem_pop_vec_threshold=2, plotting=True, cells_to_use="all",
                                     sleep_classification_method="std", shuffling=False):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param cells_to_use: which cells to use ("all", "stable", "inc", "dec")
        :type cells_to_use: str
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """
        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, _, _ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use, sleep_classification_method=
                                                     sleep_classification_method, shuffling=shuffling)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, _ ,_ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=nrem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use, sleep_classification_method=
                                                     sleep_classification_method, shuffling=shuffling)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------_decoding_similarity_temporal------------------------

        all_events_ratio = ratio_per_rem_event + ratio_per_nrem_event
        all_events_length = event_lengths_rem + event_lengths_nrem
        all_events_length_s = np.hstack((event_duration_rem_in_s, event_duration_nrem_in_s))
        labels_events = np.zeros(len(ratio_per_rem_event)+len(ratio_per_nrem_event))
        labels_events[:len(ratio_per_rem_event)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_ratio_list = [x for _, x in sorted(zip(all_times, all_events_ratio))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_lengths_s = [x for _, x in sorted(zip(all_times, all_events_length_s))]
        sorted_pop_vec_ratio = np.hstack(sorted_events_ratio_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_length_s = []
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat==1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat==-1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))
        merged_events_length_s = np.array(merged_events_length_s)
        merged_events_times = np.vstack(merged_events_times)

        # merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        # merged_nrem_event_times = merged_events_times[merged_events_labels == 0]
        merged_events_length_s = merged_events_times[:,1]-merged_events_times[:,0]


        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps==1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre_post ratio) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_merged_rem_event = []
        ratio_per_merged_nrem_event = []
        ratio_rem_nrem_events = []
        rem_nrem_events_label = []
        ratio_rem_nrem_pop_vec = []
        rem_nrem_pop_vec_label = []

        for event_id, (start_event, end_event) in enumerate(zip(start, end)):
            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                ratio_per_merged_rem_event.append(sorted_pop_vec_ratio[start_event:end_event])
            # nrem event
            else:
                ratio_per_merged_nrem_event.append(sorted_pop_vec_ratio[start_event:end_event])

            ratio_rem_nrem_events.append(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])

            ratio_rem_nrem_pop_vec.extend(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_pop_vec_label.extend(labels_per_pop_vec[start_event:end_event])


        # smooth across population vectors --> that means also across REM/NREM epochs
        # --------------------------------------------------------------------------------------------------------------
        len_new_events = [x.shape[0] for x in ratio_rem_nrem_events]
        ratio_per_pop_vec_new = np.hstack(ratio_rem_nrem_events)
        ratio_per_pop_vec_new_smooth = moving_average(a=np.array(ratio_per_pop_vec_new), n=n_moving_average_pop_vec)

        ratio_rem_nrem_events_smooth = []
        first = 0
        for event_id in range(len(len_new_events)):
            # check if data is still available (due to smoothing --> data gets shorter)
            # if not --> leave loop
            if first >= ratio_per_pop_vec_new_smooth.shape[0]:
                break
            end_event = min(first + len_new_events[event_id], ratio_per_pop_vec_new_smooth.shape[0])
            ratio_rem_nrem_events_smooth.append(ratio_per_pop_vec_new_smooth[first:end_event])
            first += len_new_events[event_id]

        # need to delete indices of events that happend after the last data point after smoothing
        rem_nrem_events_label = rem_nrem_events_label[:len(ratio_rem_nrem_events_smooth)]

        # get rem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        rem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 1))
        ds_rem = []
        ds_rem_smoothed_within = []
        merged_events_rem_length = []
        ratio_rem_events_smooth = []
        merged_rem_events_time_stamps = []
        for rem_index in rem_events_indices:
            ds_rem.append(ratio_rem_nrem_events_smooth[rem_index][-1]-ratio_rem_nrem_events_smooth[rem_index][0])
            smooth_event = moving_average(a=ratio_rem_nrem_events[rem_index], n=10)
            ds_rem_smoothed_within.append(smooth_event[-1]-smooth_event[0])
            merged_events_rem_length.append(ratio_rem_nrem_events_smooth[rem_index].shape[0])
            ratio_rem_events_smooth.append(ratio_rem_nrem_events_smooth[rem_index])
            merged_rem_events_time_stamps.append(merged_events_times[rem_index])

        # get nrem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        nrem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 0))
        ds_nrem = []
        ds_nrem_smoothed_within = []
        merged_events_nrem_length = []
        merged_nrem_events_time_stamps = []
        ratio_nrem_events_smooth = []
        for nrem_index in nrem_events_indices:
            ds_nrem.append(ratio_rem_nrem_events_smooth[nrem_index][-1]-ratio_rem_nrem_events_smooth[nrem_index][0])
            merged_events_nrem_length.append(ratio_rem_nrem_events_smooth[nrem_index].shape[0])
            ratio_nrem_events_smooth.append(ratio_rem_nrem_events_smooth[nrem_index])
            merged_nrem_events_time_stamps.append(merged_events_times[nrem_index])

        ds_nrem = np.array(ds_nrem)
        ds_rem = np.array(ds_rem)

        # need to get mean firing rate for time intervals
        mean_firing_rem = self.mean_firing_rate_for_interval(intervals_s=np.vstack(merged_rem_events_time_stamps))
        mean_firing_nrem = self.mean_firing_rate_for_interval(intervals_s=np.vstack(merged_nrem_events_time_stamps))

        if plotting:
            plt.scatter(ds_nrem, np.mean(mean_firing_nrem, axis=0))
            plt.show()

            plt.scatter(ds_rem, np.mean(mean_firing_rem, axis=0))
            plt.show()
        return ds_nrem, np.mean(mean_firing_nrem, axis=0), ds_rem, np.mean(mean_firing_rem, axis=0)

    def memory_drift_and_lfp(self, template_type, measure="normalized_ratio", pre_file_name=None,
                                     post_file_name=None, n_moving_average_pop_vec=200, rem_pop_vec_threshold=10,
                                     nrem_pop_vec_threshold=2, plotting=True, cells_to_use="all",
                                     sleep_classification_method="std", shuffling=False):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param cells_to_use: which cells to use ("all", "stable", "inc", "dec")
        :type cells_to_use: str
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """
        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, _, _ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use, sleep_classification_method=
                                                     sleep_classification_method, shuffling=shuffling)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, _ ,_ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=nrem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use, sleep_classification_method=
                                                     sleep_classification_method, shuffling=shuffling)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------_decoding_similarity_temporal------------------------

        all_events_ratio = ratio_per_rem_event + ratio_per_nrem_event
        all_events_length = event_lengths_rem + event_lengths_nrem
        all_events_length_s = np.hstack((event_duration_rem_in_s, event_duration_nrem_in_s))
        labels_events = np.zeros(len(ratio_per_rem_event)+len(ratio_per_nrem_event))
        labels_events[:len(ratio_per_rem_event)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_ratio_list = [x for _, x in sorted(zip(all_times, all_events_ratio))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_lengths_s = [x for _, x in sorted(zip(all_times, all_events_length_s))]
        sorted_pop_vec_ratio = np.hstack(sorted_events_ratio_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_length_s = []
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat==1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat==-1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))
        merged_events_length_s = np.array(merged_events_length_s)
        merged_events_times = np.vstack(merged_events_times)

        # merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        # merged_nrem_event_times = merged_events_times[merged_events_labels == 0]
        merged_events_length_s = merged_events_times[:,1]-merged_events_times[:,0]


        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps==1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre_post ratio) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_merged_rem_event = []
        ratio_per_merged_nrem_event = []
        ratio_rem_nrem_events = []
        rem_nrem_events_label = []
        ratio_rem_nrem_pop_vec = []
        rem_nrem_pop_vec_label = []

        for event_id, (start_event, end_event) in enumerate(zip(start, end)):
            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                ratio_per_merged_rem_event.append(sorted_pop_vec_ratio[start_event:end_event])
            # nrem event
            else:
                ratio_per_merged_nrem_event.append(sorted_pop_vec_ratio[start_event:end_event])

            ratio_rem_nrem_events.append(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])

            ratio_rem_nrem_pop_vec.extend(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_pop_vec_label.extend(labels_per_pop_vec[start_event:end_event])


        # smooth across population vectors --> that means also across REM/NREM epochs
        # --------------------------------------------------------------------------------------------------------------
        len_new_events = [x.shape[0] for x in ratio_rem_nrem_events]
        ratio_per_pop_vec_new = np.hstack(ratio_rem_nrem_events)
        ratio_per_pop_vec_new_smooth = moving_average(a=np.array(ratio_per_pop_vec_new), n=n_moving_average_pop_vec)

        ratio_rem_nrem_events_smooth = []
        first = 0
        for event_id in range(len(len_new_events)):
            # check if data is still available (due to smoothing --> data gets shorter)
            # if not --> leave loop
            if first >= ratio_per_pop_vec_new_smooth.shape[0]:
                break
            end_event = min(first + len_new_events[event_id], ratio_per_pop_vec_new_smooth.shape[0])
            ratio_rem_nrem_events_smooth.append(ratio_per_pop_vec_new_smooth[first:end_event])
            first += len_new_events[event_id]

        # need to delete indices of events that happend after the last data point after smoothing
        rem_nrem_events_label = rem_nrem_events_label[:len(ratio_rem_nrem_events_smooth)]

        # get rem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        rem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 1))
        ds_rem = []
        ds_rem_smoothed_within = []
        merged_events_rem_length = []
        ratio_rem_events_smooth = []
        merged_rem_events_time_stamps = []
        for rem_index in rem_events_indices:
            ds_rem.append(ratio_rem_nrem_events_smooth[rem_index][-1]-ratio_rem_nrem_events_smooth[rem_index][0])
            smooth_event = moving_average(a=ratio_rem_nrem_events[rem_index], n=10)
            ds_rem_smoothed_within.append(smooth_event[-1]-smooth_event[0])
            merged_events_rem_length.append(ratio_rem_nrem_events_smooth[rem_index].shape[0])
            ratio_rem_events_smooth.append(ratio_rem_nrem_events_smooth[rem_index])
            merged_rem_events_time_stamps.append(merged_events_times[rem_index])

        # get nrem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        nrem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 0))
        ds_nrem = []
        ds_nrem_smoothed_within = []
        merged_events_nrem_length = []
        merged_nrem_events_time_stamps = []
        ratio_nrem_events_smooth = []
        for nrem_index in nrem_events_indices:
            ds_nrem.append(ratio_rem_nrem_events_smooth[nrem_index][-1]-ratio_rem_nrem_events_smooth[nrem_index][0])
            merged_events_nrem_length.append(ratio_rem_nrem_events_smooth[nrem_index].shape[0])
            ratio_nrem_events_smooth.append(ratio_rem_nrem_events_smooth[nrem_index])
            merged_nrem_events_time_stamps.append(merged_events_times[nrem_index])

        ds_nrem = np.array(ds_nrem)
        ds_rem = np.array(ds_rem)

        # need to get lfp for interval
        mean_lfp_rem = self.mean_lfp_for_interval(intervals_s=np.vstack(merged_rem_events_time_stamps))
        mean_lfp_nrem = self.mean_lfp_for_interval(intervals_s=np.vstack(merged_nrem_events_time_stamps))

        # only use good intervals
        ds_nrem = ds_nrem[~np.isnan(mean_lfp_nrem)]
        ds_rem = ds_rem[~np.isnan(mean_lfp_rem)]
        mean_lfp_nrem = mean_lfp_nrem[~np.isnan(mean_lfp_nrem)]
        mean_lfp_rem = mean_lfp_rem[~np.isnan(mean_lfp_rem)]

        if plotting:
            plt.scatter(ds_nrem, mean_lfp_nrem)
            plt.show()

            plt.scatter(ds_rem, mean_lfp_rem)
            plt.show()
        return ds_nrem, mean_lfp_nrem, ds_rem, mean_lfp_rem

    def memory_drift_and_ripples(self, template_type, measure="normalized_ratio", pre_file_name=None,
                                 post_file_name=None, n_moving_average_pop_vec=200, rem_pop_vec_threshold=10,
                                 nrem_pop_vec_threshold=2, plotting=True, cells_to_use="all",
                                 sleep_classification_method="std", shuffling=False):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param cells_to_use: which cells to use ("all", "stable", "inc", "dec")
        :type cells_to_use: str
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """
        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, _, _ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use, sleep_classification_method=
                                                     sleep_classification_method, shuffling=shuffling)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, _ ,_ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=nrem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use, sleep_classification_method=
                                                     sleep_classification_method, shuffling=shuffling)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------_decoding_similarity_temporal------------------------

        all_events_ratio = ratio_per_rem_event + ratio_per_nrem_event
        all_events_length = event_lengths_rem + event_lengths_nrem
        all_events_length_s = np.hstack((event_duration_rem_in_s, event_duration_nrem_in_s))
        labels_events = np.zeros(len(ratio_per_rem_event)+len(ratio_per_nrem_event))
        labels_events[:len(ratio_per_rem_event)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_ratio_list = [x for _, x in sorted(zip(all_times, all_events_ratio))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_lengths_s = [x for _, x in sorted(zip(all_times, all_events_length_s))]
        sorted_pop_vec_ratio = np.hstack(sorted_events_ratio_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_length_s = []
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat==1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat==-1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))
        merged_events_length_s = np.array(merged_events_length_s)
        merged_events_times = np.vstack(merged_events_times)

        # merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        # merged_nrem_event_times = merged_events_times[merged_events_labels == 0]
        merged_events_length_s = merged_events_times[:,1]-merged_events_times[:,0]


        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps==1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre_post ratio) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_merged_rem_event = []
        ratio_per_merged_nrem_event = []
        ratio_rem_nrem_events = []
        rem_nrem_events_label = []
        ratio_rem_nrem_pop_vec = []
        rem_nrem_pop_vec_label = []

        for event_id, (start_event, end_event) in enumerate(zip(start, end)):
            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                ratio_per_merged_rem_event.append(sorted_pop_vec_ratio[start_event:end_event])
            # nrem event
            else:
                ratio_per_merged_nrem_event.append(sorted_pop_vec_ratio[start_event:end_event])

            ratio_rem_nrem_events.append(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])

            ratio_rem_nrem_pop_vec.extend(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_pop_vec_label.extend(labels_per_pop_vec[start_event:end_event])


        # smooth across population vectors --> that means also across REM/NREM epochs
        # --------------------------------------------------------------------------------------------------------------
        len_new_events = [x.shape[0] for x in ratio_rem_nrem_events]
        ratio_per_pop_vec_new = np.hstack(ratio_rem_nrem_events)
        ratio_per_pop_vec_new_smooth = moving_average(a=np.array(ratio_per_pop_vec_new), n=n_moving_average_pop_vec)

        ratio_rem_nrem_events_smooth = []
        first = 0
        for event_id in range(len(len_new_events)):
            # check if data is still available (due to smoothing --> data gets shorter)
            # if not --> leave loop
            if first >= ratio_per_pop_vec_new_smooth.shape[0]:
                break
            end_event = min(first + len_new_events[event_id], ratio_per_pop_vec_new_smooth.shape[0])
            ratio_rem_nrem_events_smooth.append(ratio_per_pop_vec_new_smooth[first:end_event])
            first += len_new_events[event_id]

        # need to delete indices of events that happend after the last data point after smoothing
        rem_nrem_events_label = rem_nrem_events_label[:len(ratio_rem_nrem_events_smooth)]

        # get rem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        rem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 1))
        ds_rem = []
        ds_rem_smoothed_within = []
        merged_events_rem_length = []
        ratio_rem_events_smooth = []
        merged_rem_events_time_stamps = []
        for rem_index in rem_events_indices:
            ds_rem.append(ratio_rem_nrem_events_smooth[rem_index][-1]-ratio_rem_nrem_events_smooth[rem_index][0])
            smooth_event = moving_average(a=ratio_rem_nrem_events[rem_index], n=10)
            ds_rem_smoothed_within.append(smooth_event[-1]-smooth_event[0])
            merged_events_rem_length.append(ratio_rem_nrem_events_smooth[rem_index].shape[0])
            ratio_rem_events_smooth.append(ratio_rem_nrem_events_smooth[rem_index])
            merged_rem_events_time_stamps.append(merged_events_times[rem_index])

        # get nrem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        nrem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 0))
        ds_nrem = []
        ds_nrem_smoothed_within = []
        merged_events_nrem_length = []
        merged_nrem_events_time_stamps = []
        ratio_nrem_events_smooth = []
        for nrem_index in nrem_events_indices:
            ds_nrem.append(ratio_rem_nrem_events_smooth[nrem_index][-1]-ratio_rem_nrem_events_smooth[nrem_index][0])
            merged_events_nrem_length.append(ratio_rem_nrem_events_smooth[nrem_index].shape[0])
            ratio_nrem_events_smooth.append(ratio_rem_nrem_events_smooth[nrem_index])
            merged_nrem_events_time_stamps.append(merged_events_times[nrem_index])

        ds_nrem = np.array(ds_nrem)
        ds_rem = np.array(ds_rem)

        # need to get number of ripples in interval
        nr_swr_nrem = self.number_swr_for_interval(intervals_s=np.vstack(merged_nrem_events_time_stamps))

        # only use good intervals
        ds_nrem = ds_nrem[~np.isnan(nr_swr_nrem)]
        nr_swr_nrem = nr_swr_nrem[~np.isnan(nr_swr_nrem)]

        if plotting:
            plt.scatter(ds_nrem, nr_swr_nrem)
            plt.show()

        return ds_nrem, nr_swr_nrem

    def memory_drift_and_nr_swrs(self, template_type, measure="normalized_ratio", pre_file_name=None,
                                        post_file_name=None,
                                        n_moving_average_pop_vec=200, rem_pop_vec_threshold=10, nrem_pop_vec_threshold=2,
                                        plotting=False, cells_to_use="all", sleep_classification_method="std", shuffling=False):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param cells_to_use: which cells to use ("all", "stable", "inc", "dec")
        :type cells_to_use: str
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """
        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, _, _ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use, sleep_classification_method=
                                                     sleep_classification_method, shuffling=shuffling)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, _ ,_ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=nrem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use, sleep_classification_method=
                                                     sleep_classification_method, shuffling=shuffling)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------_decoding_similarity_temporal------------------------

        all_events_ratio = ratio_per_rem_event + ratio_per_nrem_event
        all_events_length = event_lengths_rem + event_lengths_nrem
        all_events_length_s = np.hstack((event_duration_rem_in_s, event_duration_nrem_in_s))
        labels_events = np.zeros(len(ratio_per_rem_event)+len(ratio_per_nrem_event))
        labels_events[:len(ratio_per_rem_event)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_ratio_list = [x for _, x in sorted(zip(all_times, all_events_ratio))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_lengths_s = [x for _, x in sorted(zip(all_times, all_events_length_s))]
        sorted_pop_vec_ratio = np.hstack(sorted_events_ratio_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_length_s = []
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat==1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat==-1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))
        merged_events_length_s = np.array(merged_events_length_s)
        merged_events_times = np.vstack(merged_events_times)

        # merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        # merged_nrem_event_times = merged_events_times[merged_events_labels == 0]
        merged_events_length_s = merged_events_times[:,1]-merged_events_times[:,0]

        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps==1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre_post ratio) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_merged_rem_event = []
        ratio_per_merged_nrem_event = []
        ratio_rem_nrem_events = []
        rem_nrem_events_label = []
        ratio_rem_nrem_pop_vec = []
        rem_nrem_pop_vec_label = []

        nr_swrs = []
        nr_events = []

        for event_id, (start_event, end_event) in enumerate(zip(start, end)):
            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                ratio_per_merged_rem_event.append(sorted_pop_vec_ratio[start_event:end_event])
            # nrem event
            else:
                ratio_per_merged_nrem_event.append(sorted_pop_vec_ratio[start_event:end_event])
                nr_swrs.append(np.count_nonzero(np.logical_and(merged_events_times[event_id,0]<event_times_nrem[:,1],
                                                               event_times_nrem[:,1]< merged_events_times[event_id,1])))

            ratio_rem_nrem_events.append(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])
            nr_events.append(np.count_nonzero(np.logical_and(merged_events_times[event_id,0]<event_times_nrem[:,1],
                                                               event_times_nrem[:,1]< merged_events_times[event_id,1])))

            ratio_rem_nrem_pop_vec.extend(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_pop_vec_label.extend(labels_per_pop_vec[start_event:end_event])

        # smooth across population vectors --> that means also across REM/NREM epochs
        # --------------------------------------------------------------------------------------------------------------
        len_new_events = [x.shape[0] for x in ratio_rem_nrem_events]
        ratio_per_pop_vec_new = np.hstack(ratio_rem_nrem_events)
        ratio_per_pop_vec_new_smooth = moving_average(a=np.array(ratio_per_pop_vec_new), n=n_moving_average_pop_vec)

        ratio_rem_nrem_events_smooth = []
        first = 0
        for event_id in range(len(len_new_events)):
            # check if data is still available (due to smoothing --> data gets shorter)
            # if not --> leave loop
            if first >= ratio_per_pop_vec_new_smooth.shape[0]:
                break
            end_event = min(first + len_new_events[event_id], ratio_per_pop_vec_new_smooth.shape[0])
            ratio_rem_nrem_events_smooth.append(ratio_per_pop_vec_new_smooth[first:end_event])
            first += len_new_events[event_id]

        # need to delete indices of events that happend after the last data point after smoothing
        rem_nrem_events_label = rem_nrem_events_label[:len(ratio_rem_nrem_events_smooth)]

        # get nrem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        nrem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 0))
        ds_nrem = []
        ds_nrem_smoothed_within = []
        merged_events_nrem_length = []
        merged_events_nrem_length_s = []
        merged_nrem_events_time_stamps = []
        ratio_nrem_events_smooth = []
        nr_swrs_smoothing = []
        for nrem_index in nrem_events_indices:
            ds_nrem.append(ratio_rem_nrem_events_smooth[nrem_index][-1]-ratio_rem_nrem_events_smooth[nrem_index][0])
            merged_events_nrem_length.append(ratio_rem_nrem_events_smooth[nrem_index].shape[0])
            ratio_nrem_events_smooth.append(ratio_rem_nrem_events_smooth[nrem_index])
            merged_nrem_events_time_stamps.append(merged_events_times[nrem_index])
            nr_swrs_smoothing.append(nr_events[nrem_index])
            merged_events_nrem_length_s.append(merged_events_length_s[nrem_index])

        ds_nrem = np.array(ds_nrem)
        merged_events_nrem_length_s = np.array(merged_events_nrem_length_s)
        nr_swrs_smoothing = np.array(nr_swrs_smoothing)
        nr_swrs_smoothing_per_sec = nr_swrs_smoothing/merged_events_nrem_length_s
        # ds_nrem_wo_outlier = ds_nrem
        ds_nrem_wo_outlier = ds_nrem[nr_swrs_smoothing_per_sec<0.5]
        nr_swrs_smoothing_per_sec_wo_outlier = nr_swrs_smoothing_per_sec[nr_swrs_smoothing_per_sec<0.5]

        if plotting:
            plt.scatter(ds_nrem, nr_swrs_smoothing, c="gray")
            plt.xlabel("delta_score")
            plt.ylabel("#SWRs")
            plt.text(0,150, "R="+str(np.round(pearsonr(ds_nrem, nr_swrs_smoothing)[0],2)), color="red")
            print(pearsonr(ds_nrem, nr_swrs_smoothing))
            plt.title("NREM: "+self.session_name)
            plt.show()



            plt.scatter(ds_nrem_wo_outlier, nr_swrs_smoothing_per_sec, c="gray")
            plt.xlabel("delta_score")
            plt.ylabel("#SWRs/s")
            plt.text(0,0.1, "R="+str(np.round(pearsonr(ds_nrem_wo_outlier, nr_swrs_smoothing_per_sec)[0],2)), color="red")
            print(pearsonr(ds_nrem_wo_outlier, nr_swrs_smoothing_per_sec))
            plt.title("NREM: "+self.session_name)
            plt.show()
        else:
            return ds_nrem, nr_swrs_smoothing, nr_swrs_smoothing_per_sec, merged_events_nrem_length_s, \
                   ds_nrem_wo_outlier, nr_swrs_smoothing_per_sec_wo_outlier

    def memory_drift_similarity_decoded_modes(self, cells_to_use="all", dist_metric="cosine", n_shuffled_modes=100,
                                              use_state_spike_bins=True, rem_pop_vec_threshold=10, window_size = 500,
                                              plotting=True, save_fig=False):
        """
        Computes several measures on spike bins and decoded PRE/POST modes:
            - decoding quality (z-scored distance between spike bin and decoded mode using control distribution)
            - projection of spike bin activity onto the PRE-POST vector

        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :return:
            - sign_pre_only_dec: significant reactivations per window when PRE was decoded
            - sign_post_only_dec: significant reactivations per window when POST was decoded
            - sign_pre_post_only_dec: significant reactivations per window when POST or PRE was decoded
            - fraction_pre_on_mode_vec_only_decoded: relative distance to PRE state when PRE was decoded
            - fraction_post_on_mode_vec_only_decoded: relative distance to POST state when POST was decoded
            - proj_new_pre_only_decoded: Fede's projection measure (similar to above one) for decoded PRE states
            - proj_new_post_only_decoded: Fede's projection measure (similar to above one) for decoded POST states
            - distance_activity_decoded_mode_pre_only_decoded: distance between spike bin and decoded PRE state
            - distance_activity_decoded_mode_post_only_decoded: distance between spike bin and decoded POST state
        :rtype:
        """

        # need PRE and POST modes
        # --------------------------------------------------------------------------------------------------------------
        pre_file_name = self.session_params.default_pre_phmm_model
        post_file_name = self.session_params.default_post_phmm_model

        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        # get mode means in 12 spike format (to be able to compare it to sleep spike bins)
        if use_state_spike_bins:
            pre_mode_means = constant_nr_spike_bin_from_mean_firing(pre_mode_means.T, return_mean_vector=True)
            pre_mode_means = pre_mode_means.T

            post_mode_means = constant_nr_spike_bin_from_mean_firing(post_mode_means.T, return_mean_vector=True)
            post_mode_means = post_mode_means.T

        # create shuffled modes for PRE
        # --------------------------------------------------------------------------------------------------------------
        shuffled_pre_mode_means = np.zeros((pre_mode_means.shape[1], n_shuffled_modes))
        # generate artificial data set by sampling from different modes
        for art_mode_id in np.arange(shuffled_pre_mode_means.shape[1]):
            modes_per_cell = np.random.randint(0, pre_mode_means.shape[0], pre_mode_means.shape[1])
            shuffled_pre_mode_means[:, art_mode_id] = pre_mode_means[modes_per_cell, np.arange(pre_mode_means.shape[1])]
            # shuffled_pre_mode_means[:, art_mode_id] = rng.poisson(lam=mode_means[mode_per_cell,
            # np.arange(mode_means.shape[1])],
            #                                     size=mode_means.shape[1])

        # create shuffled modes for POST
        # --------------------------------------------------------------------------------------------------------------
        shuffled_post_mode_means = np.zeros((post_mode_means.shape[1], n_shuffled_modes))
        # generate artificial data set by sampling from different modes
        for art_mode_id in np.arange(shuffled_post_mode_means.shape[1]):
            modes_per_cell = np.random.randint(0, post_mode_means.shape[0], post_mode_means.shape[1])
            shuffled_post_mode_means[:, art_mode_id] = \
                post_mode_means[modes_per_cell, np.arange(post_mode_means.shape[1])]

        # get mode means in 12 spike format (to be able to compare it to sleep spike bins)
        if use_state_spike_bins:
            shuffled_pre_mode_means = constant_nr_spike_bin_from_mean_firing(shuffled_pre_mode_means.T,
                                                                             return_mean_vector=True)
            shuffled_pre_mode_means = shuffled_pre_mode_means.T

            shuffled_post_mode_means = constant_nr_spike_bin_from_mean_firing(shuffled_post_mode_means.T,
                                                                              return_mean_vector=True)
            shuffled_post_mode_means = shuffled_post_mode_means.T

        # get rem decoding results and spike bins first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_rem, post_prob_rem, event_times_rem, spike_bins_rem = \
            self.memory_drift_long_sleep_get_raw_results_and_spike_bins(template_type="phmm", pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, cells_to_use=cells_to_use)

        if not (np.vstack(pre_prob_rem).shape[0] == np.hstack(spike_bins_rem).shape[1]):
            raise Exception("#decoding results and #spike bins do not match")

        # get nrem decoding results and spike bins first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_nrem, post_prob_nrem, event_times_nrem, spike_bins_nrem  = \
            self.memory_drift_long_sleep_get_raw_results_and_spike_bins(template_type="phmm", pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, cells_to_use=cells_to_use)

        if not (np.vstack(pre_prob_nrem).shape[0] == np.hstack(spike_bins_nrem).shape[1]):
            raise Exception("#decoding results and #spike bins do not match")

        event_times_rem = np.vstack(event_times_rem)
        event_times_nrem = np.vstack(event_times_nrem)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------------------------------
        all_events_pre_prob = pre_prob_rem + pre_prob_nrem
        all_events_post_prob = post_prob_rem + post_prob_nrem
        all_spike_bins = spike_bins_rem + spike_bins_nrem
        event_lengths_rem = [x.shape[0] for x in pre_prob_rem]
        event_lengths_nrem = [x.shape[0] for x in pre_prob_nrem]
        all_events_length = event_lengths_rem + event_lengths_nrem
        labels_events = np.zeros(len(pre_prob_rem)+len(pre_prob_nrem))
        labels_events[:len(pre_prob_rem)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_pre_prob_list = [x for _, x in sorted(zip(all_times, all_events_pre_prob))]
        sorted_events_post_prob_list = [x for _, x in sorted(zip(all_times, all_events_post_prob))]
        sorted_spike_bins = [x for _, x in sorted(zip(all_times, all_spike_bins))]
        sorted_pre_prob = np.vstack(sorted_events_pre_prob_list)
        sorted_post_prob = np.vstack(sorted_events_post_prob_list)
        sorted_spike_bins = np.hstack(sorted_spike_bins).T

        # check which PRE / POST mode was decoded
        sorted_pre_prob_decoded_mode = np.argmax(sorted_pre_prob, axis=1)
        sorted_post_prob_decoded_mode = np.argmax(sorted_post_prob, axis=1)

        # go through all spike bins and compute distance, projection etc.
        # --------------------------------------------------------------------------------------------------------------
        distance_between_modes = np.zeros(sorted_pre_prob_decoded_mode.shape[0])
        distance_activity_decoded_mode_pre = np.zeros(sorted_pre_prob_decoded_mode.shape[0])
        distance_z_scored_pre = np.zeros(sorted_pre_prob_decoded_mode.shape[0])
        distance_z_scored_post = np.zeros(sorted_post_prob_decoded_mode.shape[0])
        distance_activity_decoded_mode_post = np.zeros(sorted_pre_prob_decoded_mode.shape[0])
        fraction_pre_on_mode_vec = np.zeros(sorted_pre_prob_decoded_mode.shape[0])
        fraction_post_on_mode_vec = np.zeros(sorted_pre_prob_decoded_mode.shape[0])
        distance_to_pre_post_mode_vec = np.zeros(sorted_pre_prob_decoded_mode.shape[0])
        proj_new = np.zeros(sorted_pre_prob_decoded_mode.shape[0])
        for i_bin, (pre_mode, post_mode, spike_bin) in enumerate(zip(sorted_pre_prob_decoded_mode,
                                                                     sorted_post_prob_decoded_mode, sorted_spike_bins)):
            distance_between_modes[i_bin] = distance.cdist(np.expand_dims(pre_mode_means[pre_mode, :],0),
                                                           np.expand_dims(post_mode_means[post_mode, :], 0),
                                                           metric=dist_metric)

            pre_post_mode_vec = post_mode_means[post_mode, :] - pre_mode_means[pre_mode, :]
            pre_post_mode_vec_unit = pre_post_mode_vec/np.linalg.norm(pre_post_mode_vec, 2)
            pre_spike_vec = spike_bin - pre_mode_means[pre_mode, :]
            post_spike_vec = spike_bin - post_mode_means[post_mode, :]

            pre_spike_vec_len = np.linalg.norm(pre_spike_vec)
            pre_spike_proj = \
                np.dot(pre_spike_vec, pre_post_mode_vec)/np.linalg.norm(pre_post_mode_vec)* \
                pre_post_mode_vec/np.linalg.norm(pre_post_mode_vec)

            post_spike_proj = \
                np.dot(post_spike_vec, pre_post_mode_vec)/np.linalg.norm(pre_post_mode_vec)* \
                pre_post_mode_vec/np.linalg.norm(pre_post_mode_vec)

            proj_pre_on_mode_vec_len = np.linalg.norm(pre_spike_proj)
            proj_post_on_mode_vec_len = np.linalg.norm(post_spike_proj)
            pre_post_mode_vec_len = np.linalg.norm(pre_post_mode_vec)

            pre_proj = np.dot(pre_post_mode_vec_unit, pre_mode_means[pre_mode, :])
            post_proj = np.dot(pre_post_mode_vec_unit, post_mode_means[post_mode, :])
            spike_proj = np.dot(pre_post_mode_vec_unit, spike_bin)
            proj_new[i_bin] = (spike_proj-
                               pre_proj)/(post_proj - pre_proj)

            fraction_pre_on_mode_vec[i_bin] = proj_pre_on_mode_vec_len / pre_post_mode_vec_len
            fraction_post_on_mode_vec[i_bin] = proj_post_on_mode_vec_len / pre_post_mode_vec_len

            distance_to_pre_post_mode_vec[i_bin] = np.sqrt(pre_spike_vec_len**2 - proj_pre_on_mode_vec_len**2)
            distances_activity_shuffled_pre = distance.cdist(shuffled_pre_mode_means.T,
                                                             np.expand_dims(spike_bin, 0), metric=dist_metric)
            distance_activity_decoded_mode_pre[i_bin] = \
                distance.cdist(np.expand_dims(pre_mode_means[pre_mode, :], 0),
                                                                 np.expand_dims(spike_bin, 0), metric=dist_metric)
            distance_z_scored_pre[i_bin] = (distance_activity_decoded_mode_pre[i_bin] -
                                            np.mean(distances_activity_shuffled_pre.flatten()))/\
                                           np.std(distances_activity_shuffled_pre.flatten())


            distances_activity_shuffled_post = distance.cdist(shuffled_post_mode_means.T,
                                                             np.expand_dims(spike_bin, 0), metric=dist_metric)

            distance_activity_decoded_mode_post[i_bin] = \
                distance.cdist(np.expand_dims(post_mode_means[post_mode, :], 0),
                                                                 np.expand_dims(spike_bin, 0), metric=dist_metric)

            distance_z_scored_post[i_bin] = (distance_activity_decoded_mode_post[i_bin] -
                                            np.mean(distances_activity_shuffled_post.flatten()))/\
                                           np.std(distances_activity_shuffled_post.flatten())

        # find when PRE/POST was decoded anb separate results
        # --------------------------------------------------------------------------------------------------------------
        pre_decoded = np.max(sorted_pre_prob, axis=1) > np.max(sorted_post_prob, axis=1)
        post_decoded = np.invert(pre_decoded)

        if plotting:
            plt.hist(fraction_pre_on_mode_vec, bins=200, color="gray", label="all bins")
            plt.hist(fraction_pre_on_mode_vec[pre_decoded], bins=200, color="blue", alpha=0.8, label="only PRE decoded")
            plt.hist(fraction_pre_on_mode_vec[post_decoded], bins=200, color="red", alpha=0.6, label="only POST decoded")
            _, y_max = plt.gca().get_ylim()
            plt.vlines(0.5, 0 , y_max)
            plt.ylabel("#spike bins")
            plt.xlabel("Relative projected distance to PRE")
            plt.legend()
            plt.xlim(0,3)
            plt.tight_layout()
            plt.show()

            plt.figure(figsize=(8,8))
            plt.hist(proj_new, bins=200, color="gray", label="all bins")
            plt.hist(proj_new[pre_decoded], bins=200, label="PRE decoded", color="blue", alpha=0.8)
            plt.hist(proj_new[post_decoded], bins=200, label="POST decoded", color="red", alpha=0.6)
            _, y_max = plt.gca().get_ylim()
            plt.vlines(0.5, 0 , y_max)
            plt.legend()
            plt.ylabel("#spike bins")
            plt.xlabel("Proj_spike_bin - Proj_PRE /\n(Proj_POST - Proj_PRE)")
            plt.tight_layout()
            plt.show()

            plt.plot(moving_average(proj_new, 1000), label="all bins", color="grey")
            plt.plot(moving_average(proj_new[pre_decoded], 1000), label="only PRE", color="blue")
            plt.plot(moving_average(proj_new[post_decoded], 1000), label="only POST", color="red")
            plt.xlabel("Spike bin ID")
            plt.ylabel("Proj_spike_bin - Proj_PRE /\n(Proj_POST - Proj_PRE)")
            x_min, x_max = plt.gca().get_xlim()
            plt.hlines(0.5, x_min, x_max, zorder=-1000)
            plt.legend()
            plt.ylim(0, 1)
            plt.tight_layout()
            plt.show()

            plt.plot(moving_average(fraction_pre_on_mode_vec, 1000), label="all bins", color="grey")
            plt.plot(moving_average(fraction_pre_on_mode_vec[pre_decoded], 1000), label="only PRE", color="blue")
            plt.plot(moving_average(fraction_pre_on_mode_vec[post_decoded], 1000), label="only POST", color="red")
            plt.xlabel("Spike bin ID")
            plt.ylabel("Relative projected distance to PRE")
            x_min, x_max = plt.gca().get_xlim()
            plt.hlines(0.5, x_min, x_max, zorder=-1000)
            plt.legend()
            plt.ylim(0, 1)
            plt.tight_layout()
            plt.show()

        # pre_decoded_fil = pre_decoded[np.logical_and(0.99999<sum_frac, sum_frac<1.00111)]
        # post_decoded_fil = post_decoded[np.logical_and(0.99999 < sum_frac, sum_frac < 1.00111)]

        # fraction_pre_on_mode_vec_fil = fraction_pre_on_mode_vec[np.logical_and(0.99999<sum_frac, sum_frac<1.00111)]
        # fraction_post_on_mode_vec_fil = fraction_post_on_mode_vec[np.logical_and(0.99999<sum_frac, sum_frac<1.00111)]

        distance_z_scored_pre_only_decoded = distance_z_scored_pre[pre_decoded]
        distance_z_scored_post_only_decoded = distance_z_scored_post[post_decoded]

        # distance_zscored_for_decoded mode --> either pre or post
        distance_z_scored_pre_and_post_only_decoded = np.zeros(distance_z_scored_pre.shape[0])
        distance_z_scored_pre_and_post_only_decoded[pre_decoded] = distance_z_scored_pre_only_decoded
        distance_z_scored_pre_and_post_only_decoded[post_decoded] = distance_z_scored_post_only_decoded

        distance_activity_decoded_mode_post_only_decoded = distance_activity_decoded_mode_post[post_decoded]
        distance_activity_decoded_mode_pre_only_decoded = distance_activity_decoded_mode_pre[pre_decoded]

        # fraction_pre_on_mode_vec_fil_only_decoded = fraction_pre_on_mode_vec_fil[pre_decoded_fil]
        # fraction_post_on_mode_vec_fil_only_decoded = fraction_post_on_mode_vec_fil[post_decoded_fil]

        fraction_pre_on_mode_vec_only_decoded = fraction_pre_on_mode_vec[pre_decoded]
        fraction_post_on_mode_vec_only_decoded = fraction_post_on_mode_vec[post_decoded]
        proj_new_pre_only_decoded = proj_new[pre_decoded]
        proj_new_post_only_decoded = proj_new[post_decoded]

        # check if decoding quality changes over time
        # --------------------------------------------------------------------------------------------------------------
        n_windows = np.round(distance_z_scored_pre.shape[0]/window_size).astype(int)
        sign_pre = np.zeros(n_windows)
        sign_post = np.zeros(n_windows)
        for i_w in range(n_windows):
            sign_pre[i_w] = np.count_nonzero(distance_z_scored_pre[i_w*window_size:(i_w+1)*window_size] < -1.96)/window_size
            sign_post[i_w] = np.count_nonzero(distance_z_scored_post[i_w*window_size:(i_w+1)*window_size] < -1.96)/window_size

        # check if decoding quality changes over time PRE modes
        # --------------------------------------------------------------------------------------------------------------
        n_windows = np.round(distance_z_scored_pre_only_decoded.shape[0]/window_size).astype(int)
        sign_pre_only_dec = np.zeros(n_windows)

        for i_w in range(n_windows):
            sign_pre_only_dec[i_w] = \
                np.count_nonzero(
                    distance_z_scored_pre_only_decoded[i_w*window_size:(i_w+1)*window_size] < -1.96)/window_size

        # check if decoding quality changes over time POST modes
        # --------------------------------------------------------------------------------------------------------------
        n_windows = np.round(distance_z_scored_post_only_decoded.shape[0]/window_size).astype(int)
        sign_post_only_dec = np.zeros(n_windows)
        for i_w in range(n_windows):
            sign_post_only_dec[i_w] = \
                np.count_nonzero(
                    distance_z_scored_post_only_decoded[i_w*window_size:(i_w+1)*window_size] < -1.96)/window_size

        # check if decoding quality changes over time PRE and POST modes
        # --------------------------------------------------------------------------------------------------------------
        n_windows = np.round(distance_z_scored_pre_and_post_only_decoded.shape[0]/window_size).astype(int)
        sign_pre_post_only_dec = np.zeros(n_windows)
        for i_w in range(n_windows):
            sign_pre_post_only_dec[i_w] = \
                np.count_nonzero(
                 distance_z_scored_pre_and_post_only_decoded[i_w*window_size:(i_w+1)*window_size] < -1.96)/window_size

        if plotting:
            # plt.plot(moving_average(sign_pre, 100))
            plt.plot(sign_pre)
            plt.ylabel("Fraction significant")
            plt.xlabel("Window ID (time)")
            plt.title("PRE")
            plt.ylim(0, 1)
            plt.tight_layout()
            plt.show()

            plt.plot(sign_pre_only_dec)
            plt.ylabel("Fraction significant")
            plt.xlabel("Window ID (time)")
            plt.title("PRE: only when PRE decoded")
            plt.ylim(0, 1)
            plt.tight_layout()
            plt.show()

            plt.plot(sign_post)
            plt.ylabel("Fraction significant")
            plt.ylim(0, 1)
            plt.xlabel("Window ID (time)")
            plt.title("POST")
            plt.tight_layout()
            plt.show()

            plt.plot(sign_post_only_dec)
            plt.ylabel("Fraction significant")
            plt.xlabel("Window ID (time)")
            plt.ylim(0, 1)
            plt.title("POST: only when POST decoded")
            plt.tight_layout()
            plt.show()

            plt.plot(sign_pre_post_only_dec)
            plt.ylabel("Fraction significant")
            plt.xlabel("Window ID (time)")
            plt.ylim(0, 1)
            plt.title("PRE & POST: only \n when PRE/POST decoded")
            plt.tight_layout()
            plt.show()

            max_post_prob = np.max(sorted_post_prob, axis=1)
            max_pre_prob = np.max(sorted_pre_prob, axis=1)

            max_decoded_likeli = np.max(np.vstack((max_post_prob, max_pre_prob)), axis=0)

            plt.plot(moving_average(max_decoded_likeli, 10000))
            plt.yscale("log")
            plt.xlabel("Time")
            plt.ylabel("Max. likelihood (from either PRE or POST)")
            plt.tight_layout()
            plt.show()

            sim_ratio = (max_post_prob - max_pre_prob)/(max_post_prob + max_pre_prob)

            parts_to_split = 4
            split_interval = 2 / 4

            res_per_part = []
            range_used = []
            for part in np.arange(parts_to_split):
                range_to_use = [-1+(part*split_interval), -1+(part+1)*split_interval]
                res_per_part.append(distance_between_modes[np.logical_and(sim_ratio > range_to_use[0],
                                                                          sim_ratio < range_to_use[1])])
                range_used.append(range_to_use)

            for res, range_to_use in zip(res_per_part, range_used):
                res_sorted = np.sort(res)
                p_res = 1. * np.arange(res_sorted.shape[0]) / (res_sorted.shape[0] - 1)

                plt.plot(res_sorted, p_res, label=str(range_to_use))
            plt.legend()
            plt.xlabel("Distance between decoded \n PRE / POST mode")
            plt.ylabel("CDF")
            plt.tight_layout()
            plt.show()


            plt.scatter(np.abs(sim_ratio), distance_between_modes, s=0.1)
            plt.xlabel("Abs. sim_ratio")
            plt.ylabel("Distance between \n decoded modes")
            plt.tight_layout()
            plt.show()

            plt.scatter(sim_ratio, distance_between_modes, s=0.1)
            plt.xlabel("Sim_ratio")
            plt.ylabel("Distance between \n decoded modes")
            plt.tight_layout()
            plt.show()

            plt.plot(moving_average(distance_between_modes, n=5000))
            plt.xlabel("Const. #spike bin ID")
            plt.ylabel("Distance between decoded \n PRE/POST mode means")
            # plt.ylim(0,1)
            plt.tight_layout()
            plt.show()

            plt.plot(moving_average(distance_activity_decoded_mode_pre, n=1000))
            plt.xlabel("Const. #spike bin ID")
            plt.ylabel("Dist between decoded \n PRE mode means and spike bin")
            plt.ylim(0, 1)
            plt.tight_layout()
            plt.show()

            plt.plot(moving_average(distance_activity_decoded_mode_pre[pre_decoded], n=1000))
            plt.xlabel("Const. #spike bin ID")
            plt.ylabel("Dist between decoded \n PRE mode means and spike bin")
            plt.ylim(0,1)
            plt.title("Only when PRE reactivated")
            plt.tight_layout()
            plt.show()

            plt.plot(moving_average(distance_activity_decoded_mode_post, n=1000))
            plt.xlabel("Const. #spike bin ID")
            plt.ylabel("Dist between decoded \n POST mode means and spike bin")
            plt.ylim(0,1)
            plt.tight_layout()
            plt.show()

            plt.plot(moving_average(distance_activity_decoded_mode_post[post_decoded], n=1000))
            plt.xlabel("Const. #spike bin ID")
            plt.ylabel("Dist between decoded \n POST mode means and spike bin")
            plt.title("Only when POST was pre-activated")
            plt.ylim(0,1)
            plt.tight_layout()
            plt.show()

        return sign_pre_only_dec, sign_post_only_dec, sign_pre_post_only_dec, \
            fraction_pre_on_mode_vec_only_decoded, fraction_post_on_mode_vec_only_decoded, proj_new_pre_only_decoded, \
            proj_new_post_only_decoded, distance_activity_decoded_mode_pre_only_decoded, \
            distance_activity_decoded_mode_post_only_decoded

    def memory_drift_decoding_quality_across_time_old(self, cells_to_use="all", dist_metric="cosine", n_shuffled_modes=100,
                                              use_state_spike_bins=True, rem_pop_vec_threshold=10, window_size = 500,
                                              plotting=True, n_smoothing=2000):
        """
        Computes several measures on spike bins and decoded PRE/POST modes:
            - decoding quality (z-scored distance between spike bin and decoded mode using control distribution)
            - projection of spike bin activity onto the PRE-POST vector

        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :return:
            - sign_pre_only_dec: significant reactivations per window when PRE was decoded
            - sign_post_only_dec: significant reactivations per window when POST was decoded
            - sign_pre_post_only_dec: significant reactivations per window when POST or PRE was decoded
            - fraction_pre_on_mode_vec_only_decoded: relative distance to PRE state when PRE was decoded
            - fraction_post_on_mode_vec_only_decoded: relative distance to POST state when POST was decoded
            - proj_new_pre_only_decoded: Fede's projection measure (similar to above one) for decoded PRE states
            - proj_new_post_only_decoded: Fede's projection measure (similar to above one) for decoded POST states
            - distance_activity_decoded_mode_pre_only_decoded: distance between spike bin and decoded PRE state
            - distance_activity_decoded_mode_post_only_decoded: distance between spike bin and decoded POST state
        :rtype:
        """

        # need PRE and POST modes
        # --------------------------------------------------------------------------------------------------------------
        pre_file_name = self.session_params.default_pre_phmm_model
        post_file_name = self.session_params.default_post_phmm_model

        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        # get mode means in 12 spike format (to be able to compare it to sleep spike bins)
        if use_state_spike_bins:
            pre_mode_means = constant_nr_spike_bin_from_mean_firing(pre_mode_means.T, return_mean_vector=True)
            pre_mode_means = pre_mode_means.T

            post_mode_means = constant_nr_spike_bin_from_mean_firing(post_mode_means.T, return_mean_vector=True)
            post_mode_means = post_mode_means.T

        # create shuffled modes for PRE
        # --------------------------------------------------------------------------------------------------------------
        shuffled_pre_mode_means = np.zeros((pre_mode_means.shape[1], n_shuffled_modes))
        # generate artificial data set by sampling from different modes
        for art_mode_id in np.arange(shuffled_pre_mode_means.shape[1]):
            modes_per_cell = np.random.randint(0, pre_mode_means.shape[0], pre_mode_means.shape[1])
            shuffled_pre_mode_means[:, art_mode_id] = pre_mode_means[modes_per_cell, np.arange(pre_mode_means.shape[1])]
            # shuffled_pre_mode_means[:, art_mode_id] = rng.poisson(lam=mode_means[mode_per_cell,
            # np.arange(mode_means.shape[1])],
            #                                     size=mode_means.shape[1])

        # create shuffled modes for POST
        # --------------------------------------------------------------------------------------------------------------
        shuffled_post_mode_means = np.zeros((post_mode_means.shape[1], n_shuffled_modes))
        # generate artificial data set by sampling from different modes
        for art_mode_id in np.arange(shuffled_post_mode_means.shape[1]):
            modes_per_cell = np.random.randint(0, post_mode_means.shape[0], post_mode_means.shape[1])
            shuffled_post_mode_means[:, art_mode_id] = \
                post_mode_means[modes_per_cell, np.arange(post_mode_means.shape[1])]

        # get mode means in 12 spike format (to be able to compare it to sleep spike bins)
        if use_state_spike_bins:
            shuffled_pre_mode_means = constant_nr_spike_bin_from_mean_firing(shuffled_pre_mode_means.T,
                                                                             return_mean_vector=True)
            shuffled_pre_mode_means = shuffled_pre_mode_means.T

            shuffled_post_mode_means = constant_nr_spike_bin_from_mean_firing(shuffled_post_mode_means.T,
                                                                              return_mean_vector=True)
            shuffled_post_mode_means = shuffled_post_mode_means.T

        # get rem decoding results and spike bins first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_rem, post_prob_rem, event_times_rem, spike_bins_rem = \
            self.memory_drift_long_sleep_get_raw_results_and_spike_bins(template_type="phmm", pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, cells_to_use=cells_to_use)

        if not (np.vstack(pre_prob_rem).shape[0] == np.hstack(spike_bins_rem).shape[1]):
            raise Exception("#decoding results and #spike bins do not match")

        # get nrem decoding results and spike bins first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_nrem, post_prob_nrem, event_times_nrem, spike_bins_nrem  = \
            self.memory_drift_long_sleep_get_raw_results_and_spike_bins(template_type="phmm", pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, cells_to_use=cells_to_use)

        if not (np.vstack(pre_prob_nrem).shape[0] == np.hstack(spike_bins_nrem).shape[1]):
            raise Exception("#decoding results and #spike bins do not match")

        event_times_rem = np.vstack(event_times_rem)
        event_times_nrem = np.vstack(event_times_nrem)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------------------------------
        all_events_pre_prob = pre_prob_rem + pre_prob_nrem
        all_events_post_prob = post_prob_rem + post_prob_nrem
        all_spike_bins = spike_bins_rem + spike_bins_nrem
        event_lengths_rem = [x.shape[0] for x in pre_prob_rem]
        event_lengths_nrem = [x.shape[0] for x in pre_prob_nrem]
        all_events_length = event_lengths_rem + event_lengths_nrem
        labels_events = np.zeros(len(pre_prob_rem)+len(pre_prob_nrem))
        labels_events[:len(pre_prob_rem)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_pre_prob_list = [x for _, x in sorted(zip(all_times, all_events_pre_prob))]
        sorted_events_post_prob_list = [x for _, x in sorted(zip(all_times, all_events_post_prob))]
        sorted_spike_bins = [x for _, x in sorted(zip(all_times, all_spike_bins))]
        sorted_pre_prob = np.vstack(sorted_events_pre_prob_list)
        sorted_post_prob = np.vstack(sorted_events_post_prob_list)
        sorted_spike_bins = np.hstack(sorted_spike_bins).T

        # check which PRE / POST mode was decoded
        sorted_pre_prob_decoded_mode = np.argmax(sorted_pre_prob, axis=1)
        sorted_post_prob_decoded_mode = np.argmax(sorted_post_prob, axis=1)

        # go through all spike bins and compute distance, projection etc.
        # --------------------------------------------------------------------------------------------------------------
        sim_activity_decoded_mode_pre = np.zeros(sorted_pre_prob_decoded_mode.shape[0])
        sim_z_scored_pre = np.zeros(sorted_pre_prob_decoded_mode.shape[0])
        sim_z_scored_post = np.zeros(sorted_post_prob_decoded_mode.shape[0])
        sim_activity_decoded_mode_post = np.zeros(sorted_pre_prob_decoded_mode.shape[0])
        for i_bin, (pre_mode, post_mode, spike_bin) in enumerate(zip(sorted_pre_prob_decoded_mode,
                                                                     sorted_post_prob_decoded_mode, sorted_spike_bins)):

            sim_activity_shuffled_pre = 1-distance.cdist(shuffled_pre_mode_means.T,
                                                             np.expand_dims(spike_bin, 0), metric=dist_metric)
            sim_activity_decoded_mode_pre[i_bin] = \
                1- distance.cdist(np.expand_dims(pre_mode_means[pre_mode, :], 0),
                                                                 np.expand_dims(spike_bin, 0), metric=dist_metric)
            sim_z_scored_pre[i_bin] = (sim_activity_decoded_mode_pre[i_bin] -
                                            np.mean(sim_activity_shuffled_pre.flatten()))/\
                                           np.std(sim_activity_shuffled_pre.flatten())

            sim_activity_shuffled_post = 1-distance.cdist(shuffled_post_mode_means.T,
                                                             np.expand_dims(spike_bin, 0), metric=dist_metric)

            sim_activity_decoded_mode_post[i_bin] = \
                1 - distance.cdist(np.expand_dims(post_mode_means[post_mode, :], 0),
                                                                 np.expand_dims(spike_bin, 0), metric=dist_metric)

            sim_z_scored_post[i_bin] = (sim_activity_decoded_mode_post[i_bin] -
                                            np.mean(sim_activity_shuffled_post.flatten()))/\
                                           np.std(sim_activity_shuffled_post.flatten())

        # find when PRE/POST was decoded anb separate results
        # --------------------------------------------------------------------------------------------------------------
        pre_decoded = np.max(sorted_pre_prob, axis=1) > np.max(sorted_post_prob, axis=1)
        post_decoded = np.invert(pre_decoded)

        sim_z_scored_pre_only_decoded = sim_z_scored_pre[pre_decoded]
        sim_z_scored_post_only_decoded = sim_z_scored_post[post_decoded]

        # distance_zscored_for_decoded mode --> either pre or post
        sim_z_scored_pre_and_post_only_decoded = np.zeros(sim_z_scored_pre.shape[0])
        sim_z_scored_pre_and_post_only_decoded[pre_decoded] = sim_z_scored_pre_only_decoded
        sim_z_scored_pre_and_post_only_decoded[post_decoded] = sim_z_scored_post_only_decoded

        sim_activity_decoded_mode_pre_pre_decoded = sim_activity_decoded_mode_pre[pre_decoded]
        sim_activity_decoded_mode_post_pre_decoded = sim_activity_decoded_mode_post[pre_decoded]
        sim_activity_decoded_mode_pre_post_decoded = sim_activity_decoded_mode_pre[post_decoded]
        sim_activity_decoded_mode_post_post_decoded = sim_activity_decoded_mode_post[post_decoded]

        # compute difference in cosine distance
        diff_pre_decoded = sim_activity_decoded_mode_pre_pre_decoded - sim_activity_decoded_mode_post_pre_decoded
        diff_post_decoded = sim_activity_decoded_mode_post_post_decoded - sim_activity_decoded_mode_pre_post_decoded

        if plotting:
            plt.plot(moving_average(diff_pre_decoded, 2000))
            plt.title("Pre decoded")
            plt.legend()
            plt.show()

            plt.plot(moving_average(diff_post_decoded, 2000))
            plt.title("Post decoded")
            plt.legend()
            plt.show()

            sim_z_scored_pre_only_decoded_s = moving_average(sim_z_scored_pre_only_decoded, n_smoothing)
            sim_z_scored_post_only_decoded_s = moving_average(sim_z_scored_post_only_decoded, n_smoothing)
            sim_z_scored_pre_and_post_only_decoded_s = moving_average(sim_z_scored_pre_and_post_only_decoded, n_smoothing)

            max_val = np.max(np.hstack((sim_z_scored_pre_only_decoded_s, sim_z_scored_post_only_decoded_s)))

            fig = plt.figure(figsize=(10, 9))
            gs = fig.add_gridspec(10, 10)
            ax1 = fig.add_subplot(gs[:4, :5])
            ax1.plot(np.linspace(0,1,sim_z_scored_pre_only_decoded_s.shape[0]), sim_z_scored_pre_only_decoded_s,
                     label="acquisition states", color="grey")
            ax1.set_xlabel("Normalized duration")
            ax1.set_ylabel("cosine similarity \n with decoded state\n (z-scored using shuffle)")
            ax1.set_ylim(0, max_val+0.2)
            ax1.legend()
            ax2 = fig.add_subplot(gs[:4, 5:])
            ax2.plot(np.linspace(0,1,sim_z_scored_post_only_decoded_s.shape[0]), sim_z_scored_post_only_decoded_s,
                     label="recall states", color="lightgrey")
            ax2.set_ylim(0, max_val+0.2)
            ax2.set_yticks([])
            ax2.set_xlabel("Normalized duration")
            ax2.legend()
            ax3 = fig.add_subplot(gs[5:, :])
            ax3.plot(np.linspace(0,1,sim_z_scored_pre_and_post_only_decoded_s.shape[0]), sim_z_scored_pre_and_post_only_decoded_s,
                     label="recall and acquisition states", color="white")
            ax3.set_ylim(0, max_val+0.2)
            ax3.legend(loc=3)
            ax3.set_xlabel("Normalized duration")
            ax3.set_ylabel("cosine similarity \n with decoded state\n (z-scored using shuffle)")
            plt.tight_layout()
            plt.show()

        return sim_z_scored_pre_only_decoded, sim_z_scored_post_only_decoded, sim_z_scored_pre_and_post_only_decoded, \
            diff_pre_decoded, diff_post_decoded

    def memory_drift_decoding_quality_across_time(self, dist_metric="cosine", n_shuffled_modes=100,
                                                      use_state_spike_bins=True,
                                                      plotting=True, n_smoothing=2000):

        event_spike_rasters = []

        for l_s in self.long_sleep:

            # get original bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_ = (
                l_s.get_spike_binned_raster_combined_sleep_phases())

            event_spike_rasters.append(np.hstack(event_spike_rasters_))

        all_spike_rasters = np.hstack(event_spike_rasters)


        # need PRE and POST modes
        # --------------------------------------------------------------------------------------------------------------
        pre_file_name = self.session_params.default_pre_phmm_model
        post_file_name = self.session_params.default_post_phmm_model

        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        # get mode means in 12 spike format (to be able to compare it to sleep spike bins)
        if use_state_spike_bins:
            pre_mode_means_constant_nr_spike = constant_nr_spike_bin_from_mean_firing(pre_mode_means.T, return_mean_vector=True)
            pre_mode_means_constant_nr_spike = pre_mode_means_constant_nr_spike.T

            post_mode_means_constant_nr_spike = constant_nr_spike_bin_from_mean_firing(post_mode_means.T, return_mean_vector=True)
            post_mode_means_constant_nr_spike = post_mode_means_constant_nr_spike.T

        # create shuffled modes for PRE
        # --------------------------------------------------------------------------------------------------------------
        shuffled_pre_mode_means = np.zeros((pre_mode_means.shape[1], n_shuffled_modes))
        # generate artificial data set by sampling from different modes
        for art_mode_id in np.arange(shuffled_pre_mode_means.shape[1]):
            modes_per_cell = np.random.randint(0, pre_mode_means.shape[0], pre_mode_means.shape[1])
            shuffled_pre_mode_means[:, art_mode_id] = pre_mode_means[modes_per_cell, np.arange(pre_mode_means.shape[1])]
            # shuffled_pre_mode_means[:, art_mode_id] = rng.poisson(lam=mode_means[mode_per_cell,
            # np.arange(mode_means.shape[1])],
            #                                     size=mode_means.shape[1])

        # create shuffled modes for POST
        # --------------------------------------------------------------------------------------------------------------
        shuffled_post_mode_means = np.zeros((post_mode_means.shape[1], n_shuffled_modes))
        # generate artificial data set by sampling from different modes
        for art_mode_id in np.arange(shuffled_post_mode_means.shape[1]):
            modes_per_cell = np.random.randint(0, post_mode_means.shape[0], post_mode_means.shape[1])
            shuffled_post_mode_means[:, art_mode_id] = \
                post_mode_means[modes_per_cell, np.arange(post_mode_means.shape[1])]

        # get mode means in 12 spike format (to be able to compare it to sleep spike bins)
        if use_state_spike_bins:
            shuffled_pre_mode_means = constant_nr_spike_bin_from_mean_firing(shuffled_pre_mode_means.T,
                                                                             return_mean_vector=True)
            shuffled_pre_mode_means = shuffled_pre_mode_means.T

            shuffled_post_mode_means = constant_nr_spike_bin_from_mean_firing(shuffled_post_mode_means.T,
                                                                              return_mean_vector=True)
            shuffled_post_mode_means = shuffled_post_mode_means.T

        # get compression factor
        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

        # do decoding
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                  event_spike_rasters=all_spike_rasters,
                                                  compression_factor=compression_factor)

        post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                   event_spike_rasters=all_spike_rasters,
                                                   compression_factor=compression_factor)

        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        pre_decoded = np.zeros(max_pre_likeli.shape[0])
        pre_decoded[max_pre_likeli > max_post_likeli] = 1
        pre_decoded = pre_decoded.astype(bool)

        # take the log
        max_pre_log_likeli = np.log(max_pre_likeli)
        max_post_log_likeli = np.log(max_post_likeli)


        # check which PRE / POST mode was decoded
        sorted_pre_prob_decoded_mode = np.argmax(pre_likeli, axis=1)
        sorted_post_prob_decoded_mode = np.argmax(post_likeli, axis=1)

        # go through all spike bins and compute distance, projection etc.
        # --------------------------------------------------------------------------------------------------------------
        sim_activity_decoded_mode_pre = np.zeros(sorted_pre_prob_decoded_mode.shape[0])
        sim_z_scored_pre = np.zeros(sorted_pre_prob_decoded_mode.shape[0])
        sim_z_scored_post = np.zeros(sorted_post_prob_decoded_mode.shape[0])
        sim_activity_decoded_mode_post = np.zeros(sorted_pre_prob_decoded_mode.shape[0])
        for i_bin, (pre_mode, post_mode, spike_bin) in enumerate(zip(sorted_pre_prob_decoded_mode,
                                                                     sorted_post_prob_decoded_mode, all_spike_rasters.T)):

            sim_activity_shuffled_pre = 1-distance.cdist(shuffled_pre_mode_means.T,
                                                         np.expand_dims(spike_bin, 0), metric=dist_metric)
            sim_activity_decoded_mode_pre[i_bin] = \
                1- distance.cdist(np.expand_dims(pre_mode_means_constant_nr_spike[pre_mode, :], 0),
                                  np.expand_dims(spike_bin, 0), metric=dist_metric)
            sim_z_scored_pre[i_bin] = (sim_activity_decoded_mode_pre[i_bin] -
                                       np.mean(sim_activity_shuffled_pre.flatten()))/ \
                                      np.std(sim_activity_shuffled_pre.flatten())

            sim_activity_shuffled_post = 1-distance.cdist(shuffled_post_mode_means.T,
                                                          np.expand_dims(spike_bin, 0), metric=dist_metric)

            sim_activity_decoded_mode_post[i_bin] = \
                1 - distance.cdist(np.expand_dims(post_mode_means_constant_nr_spike[post_mode, :], 0),
                                   np.expand_dims(spike_bin, 0), metric=dist_metric)

            sim_z_scored_post[i_bin] = (sim_activity_decoded_mode_post[i_bin] -
                                        np.mean(sim_activity_shuffled_post.flatten()))/ \
                                       np.std(sim_activity_shuffled_post.flatten())

        # find when PRE/POST was decoded anb separate results
        # --------------------------------------------------------------------------------------------------------------
        pre_decoded = max_pre_log_likeli > max_post_log_likeli
        post_decoded = np.invert(pre_decoded)

        sim_z_scored_pre_only_decoded = sim_z_scored_pre[pre_decoded]
        sim_z_scored_post_only_decoded = sim_z_scored_post[post_decoded]

        # distance_zscored_for_decoded mode --> either pre or post
        sim_z_scored_pre_and_post_only_decoded = np.zeros(sim_z_scored_pre.shape[0])
        sim_z_scored_pre_and_post_only_decoded[pre_decoded] = sim_z_scored_pre_only_decoded
        sim_z_scored_pre_and_post_only_decoded[post_decoded] = sim_z_scored_post_only_decoded

        sim_activity_decoded_mode_pre_pre_decoded = sim_activity_decoded_mode_pre[pre_decoded]
        sim_activity_decoded_mode_post_pre_decoded = sim_activity_decoded_mode_post[pre_decoded]
        sim_activity_decoded_mode_pre_post_decoded = sim_activity_decoded_mode_pre[post_decoded]
        sim_activity_decoded_mode_post_post_decoded = sim_activity_decoded_mode_post[post_decoded]

        # compute difference in cosine distance
        diff_pre_decoded = sim_activity_decoded_mode_pre_pre_decoded - sim_activity_decoded_mode_post_pre_decoded
        diff_post_decoded = sim_activity_decoded_mode_post_post_decoded - sim_activity_decoded_mode_pre_post_decoded

        if plotting:
            plt.plot(moving_average(diff_pre_decoded, 2000))
            plt.title("Pre decoded")
            plt.legend()
            plt.show()

            plt.plot(moving_average(diff_post_decoded, 2000))
            plt.title("Post decoded")
            plt.legend()
            plt.show()

            sim_z_scored_pre_only_decoded_s = moving_average(sim_z_scored_pre_only_decoded, n_smoothing)
            sim_z_scored_post_only_decoded_s = moving_average(sim_z_scored_post_only_decoded, n_smoothing)
            sim_z_scored_pre_and_post_only_decoded_s = moving_average(sim_z_scored_pre_and_post_only_decoded, n_smoothing)

            max_val = np.max(np.hstack((sim_z_scored_pre_only_decoded_s, sim_z_scored_post_only_decoded_s)))

            fig = plt.figure(figsize=(10, 9))
            gs = fig.add_gridspec(10, 10)
            ax1 = fig.add_subplot(gs[:4, :5])
            ax1.plot(np.linspace(0,1,sim_z_scored_pre_only_decoded_s.shape[0]), sim_z_scored_pre_only_decoded_s,
                     label="acquisition states", color="grey")
            ax1.set_xlabel("Normalized duration")
            ax1.set_ylabel("cosine similarity \n with decoded state\n (z-scored using shuffle)")
            ax1.set_ylim(0, max_val+0.2)
            ax1.legend()
            ax2 = fig.add_subplot(gs[:4, 5:])
            ax2.plot(np.linspace(0,1,sim_z_scored_post_only_decoded_s.shape[0]), sim_z_scored_post_only_decoded_s,
                     label="recall states", color="lightgrey")
            ax2.set_ylim(0, max_val+0.2)
            ax2.set_yticks([])
            ax2.set_xlabel("Normalized duration")
            ax2.legend()
            ax3 = fig.add_subplot(gs[5:, :])
            ax3.plot(np.linspace(0,1,sim_z_scored_pre_and_post_only_decoded_s.shape[0]), sim_z_scored_pre_and_post_only_decoded_s,
                     label="recall and acquisition states", color="white")
            ax3.set_ylim(0, max_val+0.2)
            ax3.legend(loc=3)
            ax3.set_xlabel("Normalized duration")
            ax3.set_ylabel("cosine similarity \n with decoded state\n (z-scored using shuffle)")
            plt.tight_layout()
            plt.show()

        return sim_z_scored_pre_only_decoded, sim_z_scored_post_only_decoded, sim_z_scored_pre_and_post_only_decoded, \
            diff_pre_decoded, diff_post_decoded

    def memory_drift_decoding_quality_across_time_likelihoods(self, pre_model="pre", post_model="post", n_shuffled_modes=100,
                                                                                      save_result=True):

        event_spike_rasters = []

        for l_s in self.long_sleep:

            # get original bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_ = (
                l_s.get_spike_binned_raster_combined_sleep_phases())

            event_spike_rasters.append(np.hstack(event_spike_rasters_))

        all_spike_rasters = np.hstack(event_spike_rasters)

        # get means of pre and post modes + compression factor
        if pre_model == "pre":
            pre_file_name = self.session_params.default_pre_phmm_model
        if post_model ==  "post":
            post_file_name = self.session_params.default_post_phmm_model

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

        # create shuffled modes for PRE
        # --------------------------------------------------------------------------------------------------------------
        shuffled_pre_mode_means = np.zeros((pre_mode_means.shape[1], n_shuffled_modes))
        # generate artificial data set by sampling from different modes
        for art_mode_id in np.arange(shuffled_pre_mode_means.shape[1]):
            modes_per_cell = np.random.randint(0, pre_mode_means.shape[0], pre_mode_means.shape[1])
            shuffled_pre_mode_means[:, art_mode_id] = pre_mode_means[modes_per_cell, np.arange(pre_mode_means.shape[1])]

        # create shuffled modes for POST
        # --------------------------------------------------------------------------------------------------------------
        shuffled_post_mode_means = np.zeros((post_mode_means.shape[1], n_shuffled_modes))
        # generate artificial data set by sampling from different modes
        for art_mode_id in np.arange(shuffled_post_mode_means.shape[1]):
            modes_per_cell = np.random.randint(0, post_mode_means.shape[0], post_mode_means.shape[1])
            shuffled_post_mode_means[:, art_mode_id] = \
                post_mode_means[modes_per_cell, np.arange(post_mode_means.shape[1])]

        # do decoding for original data
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                  event_spike_rasters=all_spike_rasters,
                                                  compression_factor=compression_factor)

        post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                   event_spike_rasters=all_spike_rasters,
                                                   compression_factor=compression_factor)

        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        pre_decoded = np.zeros(max_pre_likeli.shape[0])
        pre_decoded[max_pre_likeli > max_post_likeli] = 1
        pre_decoded = pre_decoded.astype(bool)

        # take the log
        max_pre_log_likeli = np.log(max_pre_likeli)
        max_post_log_likeli = np.log(max_post_likeli)

        # compute z-scored likelihoods using shuffle (shuffled modes for pre and post)
        # --------------------------------------------------------------------------------------------------------------
        pre_decoded_z_scored = []
        post_decoded_z_scored = []

        for pop_vec in np.arange(pre_likeli.shape[0]):
            if pre_decoded[pop_vec]:
                # decode artificial mode
                pre_likeli_control = decode_using_phmm_modes_fast(mode_means=shuffled_pre_mode_means.T,
                                                                  event_spike_rasters=
                                                                  np.expand_dims(all_spike_rasters[:, pop_vec],1),
                                                                  compression_factor=compression_factor)
                pre_likeli_control_log = np.log(pre_likeli_control)
                pre_decoded_z_scored.append((max_pre_log_likeli[pop_vec]-np.mean(pre_likeli_control_log))/np.std(pre_likeli_control_log))
            else:
                post_likeli_control = decode_using_phmm_modes_fast(mode_means=shuffled_post_mode_means.T,
                                                                   event_spike_rasters=
                                                                   np.expand_dims(all_spike_rasters[:, pop_vec],1),
                                                                   compression_factor=compression_factor)

                post_likeli_control_log = np.log(post_likeli_control)
                post_decoded_z_scored.append((max_post_log_likeli[pop_vec]-np.mean(post_likeli_control_log))/np.std(post_likeli_control_log))

        if save_result:
            res_dic = {}
            res_dic["pre_decoded_z_scored"] = pre_decoded_z_scored
            res_dic["post_decoded_z_scored"] = post_decoded_z_scored

            save_path = os.path.dirname(os.path.realpath(__file__)) + "/../temp_data/decoding_quality_likelihoods"
            filename = save_path+ "/" + self.session_name

            outfile = open(filename, 'wb')
            pickle.dump(res_dic, outfile)
            outfile.close()

        return pre_decoded_z_scored, post_decoded_z_scored

    def memory_drift_nrem_significant_reactivations(self, cells_to_use="all", dist_metric="cosine", n_shuffled_modes=100,
                                              use_state_spike_bins=True, window_size_min = 10,
                                              use_mean_spike_vector=True, smoothing=10, plotting=False):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param plotting: whether to plot
        :type plotting: bool
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """
        # get length of sleep in seconds

        # used for decoding and should be used for the distances as well!

        pre_file_name = self.session_params.default_pre_phmm_model
        post_file_name = self.session_params.default_post_phmm_model
        # need PRE and POST modes
        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        if use_state_spike_bins:
            pre_mode_means = constant_nr_spike_bin_from_mean_firing(pre_mode_means.T, return_mean_vector=True)
            pre_mode_means = pre_mode_means.T

        # create shuffled modes for PRE
        shuffled_pre_mode_means = np.zeros((pre_mode_means.shape[1], n_shuffled_modes))
        rng = np.random.default_rng()
        # generate artificial data set by sampling from different modes
        for art_mode_id in np.arange(shuffled_pre_mode_means.shape[1]):
            modes_per_cell = np.random.randint(0, pre_mode_means.shape[0], pre_mode_means.shape[1])
            shuffled_pre_mode_means[:, art_mode_id] = pre_mode_means[modes_per_cell, np.arange(pre_mode_means.shape[1])]
            # shuffled_pre_mode_means[:, art_mode_id] = rng.poisson(lam=mode_means[mode_per_cell,
            # np.arange(mode_means.shape[1])],
            #                                     size=mode_means.shape[1])
        if use_state_spike_bins:
            shuffled_pre_mode_means = constant_nr_spike_bin_from_mean_firing(shuffled_pre_mode_means.T,
                                                                             return_mean_vector=True)
            shuffled_pre_mode_means = shuffled_pre_mode_means.T


        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        if use_state_spike_bins:
            if use_mean_spike_vector:
                post_mode_means = constant_nr_spike_bin_from_mean_firing(post_mode_means.T, return_mean_vector=True)
                post_mode_means = post_mode_means.T
            else:
                post_mode_samples = constant_nr_spike_bin_from_mean_firing(post_mode_means.T, return_mean_vector=False)

        # create shuffled modes for POST
        shuffled_post_mode_means = np.zeros((post_mode_means.shape[1], n_shuffled_modes))
        rng = np.random.default_rng()
        # generate artificial data set by sampling from different modes
        for art_mode_id in np.arange(shuffled_post_mode_means.shape[1]):
            modes_per_cell = np.random.randint(0, post_mode_means.shape[0], post_mode_means.shape[1])
            shuffled_post_mode_means[:, art_mode_id] = post_mode_means[modes_per_cell, np.arange(post_mode_means.shape[1])]

        if use_state_spike_bins:
            if use_mean_spike_vector:
                shuffled_post_mode_means = constant_nr_spike_bin_from_mean_firing(shuffled_post_mode_means.T,
                                                                                  return_mean_vector=True)
                shuffled_post_mode_means = shuffled_post_mode_means.T
            else:
                shuffled_post_mode_samples = constant_nr_spike_bin_from_mean_firing(shuffled_post_mode_means.T,
                                                                                    return_mean_vector=False)

        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        sleep_dur = len_sleep/60/60

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_nrem, post_prob_nrem, _, spike_bins_nrem  = \
            self.memory_drift_long_sleep_get_raw_results_and_spike_bins(template_type="phmm", pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, cells_to_use=cells_to_use)

        pre_prob_nrem = np.vstack(pre_prob_nrem)
        post_prob_nrem = np.vstack(post_prob_nrem)
        spike_bins_nrem = np.hstack(spike_bins_nrem)


        pre_prob_decoded_mode = np.argmax(pre_prob_nrem, axis=1)
        post_prob_decoded_mode = np.argmax(post_prob_nrem, axis=1)

        # go through all bins and compute distance
        distance_z_scored_pre = np.zeros(pre_prob_decoded_mode.shape[0])
        distance_z_scored_post = np.zeros(post_prob_decoded_mode.shape[0])
        for i_bin, (pre_mode, post_mode, spike_bin) in enumerate(zip(pre_prob_decoded_mode,
                                                                     post_prob_decoded_mode, spike_bins_nrem.T)):

            distances_activity_shuffled_pre = distance.cdist(shuffled_pre_mode_means.T,
                                                             np.expand_dims(spike_bin, 0), metric=dist_metric)
            distance_activity_decoded_mode_pre = \
                distance.cdist(np.expand_dims(pre_mode_means[pre_mode, :], 0),
                                                                 np.expand_dims(spike_bin, 0), metric=dist_metric)
            distance_z_scored_pre[i_bin] = (distance_activity_decoded_mode_pre -
                                            np.mean(distances_activity_shuffled_pre.flatten()))/\
                                           np.std(distances_activity_shuffled_pre.flatten())


            distances_activity_shuffled_post = distance.cdist(shuffled_post_mode_means.T,
                                                             np.expand_dims(spike_bin, 0), metric=dist_metric)

            distance_activity_decoded_mode_post = \
                distance.cdist(np.expand_dims(post_mode_means[post_mode, :], 0),
                                                                 np.expand_dims(spike_bin, 0), metric=dist_metric)

            distance_z_scored_post[i_bin] = (distance_activity_decoded_mode_post -
                                            np.mean(distances_activity_shuffled_post.flatten()))/\
                                           np.std(distances_activity_shuffled_post.flatten())


        # find when PRE was decoded
        pre_decoded = np.max(pre_prob_nrem, axis=1) > np.max(post_prob_nrem, axis=1)
        post_decoded = np.invert(pre_decoded)

        distance_z_scored_pre_only_decoded = distance_z_scored_pre[pre_decoded]
        distance_z_scored_post_only_decoded = distance_z_scored_post[post_decoded]

        # distance_zscored_for_decoded mode --> either pre or post
        distance_z_scored_pre_and_post_only_decoded = np.zeros(distance_z_scored_pre.shape[0])
        distance_z_scored_pre_and_post_only_decoded[pre_decoded] = distance_z_scored_pre_only_decoded
        distance_z_scored_pre_and_post_only_decoded[post_decoded] = distance_z_scored_post_only_decoded

        # compute mean fraction of significant events
        window_size = 500
        n_windows = np.round(distance_z_scored_pre_and_post_only_decoded.shape[0]/window_size).astype(int)
        sign_pre_post_only_dec = np.zeros(n_windows)
        for i_w in range(n_windows):
            sign_pre_post_only_dec[i_w] = \
                np.count_nonzero(
                    distance_z_scored_pre_and_post_only_decoded[i_w*window_size:(i_w+1)*window_size] < -1.96)/window_size

        mean_sign = np.mean(sign_pre_post_only_dec)

        nr_swr_per_window = self.swr_frequency(window_size_min=window_size_min)

        nr_sign_swr_per_window = nr_swr_per_window * mean_sign
        nr_sign_swr_per_window_smooth = moving_average(nr_sign_swr_per_window, smoothing)

        if plotting:
            plt.plot(nr_sign_swr_per_window_smooth)
            plt.xlabel("#sign. SWR per Window")
            plt.ylabel("Window ID")
            plt.show()

        return nr_sign_swr_per_window_smooth

    def bayesian_decoding_rem(self, pre_file_name=None, post_file_name=None, max_jump=15,min_nr_points=2,
                          nrem_pop_vec_threshold=10, plotting=False, cells_to_use="all", save_fig=False,
                          cheeseboard_radius=27, plot_for_control=False, nr_event_to_save=None):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_ - np.mean(dec, axis=0)[0]vec: int
        :param nrem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type nrem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param only_stable_cells: whether to only use stable cells
        :type only_stable_cells: bool
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """
        # get length of sleep in seconds

        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob, _, event_times = \
            self.memory_drift_long_sleep_get_raw_results(template_type="ising", pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=nrem_pop_vec_threshold, cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        # pre_prob_nrem, post_prob_nrem, event_times_nrem  = \
        #     self.memory_drift_long_sleep_get_raw_results(template_type=template_type, pre_file_name=pre_file_name,
        #                                              post_file_name=post_file_name, part_to_analyze="nrem",
        #                                              pop_vec_threshold=2, cells_to_use=cells_to_use)

        # need to get model to convert array to 2D matrix for locations

        template_file_name = self.long_sleep[0].session_params.default_pre_ising_model

        with open(self.params.pre_proc_dir + 'awake_ising_maps/' + template_file_name + '.pkl', 'rb') as f:
            model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        res_2d = model_dic["occ_map"].shape

        # factor a.u. to cm
        # scaling_fac = self.session_params.data_params_dictionary["spatial_factor"]

        # ising maps are decoded using 5 a.u.
        # spatial_bin_size = 5 * scaling_fac

        # goal locations
        gl = self.long_sleep[0].session_params.goal_locations

        # offset for plotting:
        # offset for plotting:
        if self.session_name == "mjc163R4R_0114":
            x_off = 30
            y_off = 35
            cheeseboard_center = np.array([35, 22])
        elif self.session_name == "mjc163R2R_0114":
            x_off = 0
            y_off = 0
        elif self.session_name == "mjc169R1R_0114":
            x_off = 25
            y_off = 15
        elif self.session_name == "mjc169R4R_0114":
            x_off = 25
            y_off = 15
        elif self.session_name == "mjc163R1L_0114":
            x_off = 38
            y_off = 5
        elif self.session_name == "mjc148R4R_0113":
            x_off = 32
            y_off = 5
        elif self.session_name == "mjc163R3L_0114":
            x_off = 25
            y_off = 20

        if plot_for_control:
            plt.imshow(model_dic["occ_map"].T)
            for g_l in gl:
                plt.scatter((g_l[0] - x_off) / 5,
                            (g_l[1] - y_off) / 5,
                            label="goal locations", color="white", s=30)
            handles, labels = plt.gca().get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            plt.gca().legend(by_label.values(), by_label.keys())
            circle1 = plt.Circle((cheeseboard_center[0], cheeseboard_center[1]), cheeseboard_radius,
                                 facecolor="None", edgecolor='grey', lw=1)
            plt.gca().add_patch(circle1)
            plt.xlim(0,80)
            plt.ylim(-20,60)
            plt.show()

        if nr_event_to_save is not None:
            event = pre_prob[nr_event_to_save]
            plt.figure(figsize=(5, 5))
            tr_count = 0
            tr_list = []
            for nr, bin in enumerate(event):
                b = np.reshape(bin, (res_2d[0], res_2d[1]))
                # find maximum
                pos = np.asarray(np.unravel_index(b.argmax(), b.shape))
                plt.scatter(pos[0], pos[1], color="red", s=10, label="decoded position")
                if tr_count == 0:
                    plt.scatter(pos[0], pos[1], color="red", label="start", s=30, marker="x")

                if tr_count > 0:
                    plt.plot([pos[0], pos_prev[0]], [pos[1], pos_prev[1]], color="red", alpha=0.5, linestyle="--")
                pos_prev = pos
                tr_count += 1
            circle1 = plt.Circle((cheeseboard_center[0], cheeseboard_center[1]), cheeseboard_radius,
                                 facecolor="None", edgecolor='grey', lw=1)
            for g_l in gl:
                plt.scatter((g_l[0] - x_off) / 5,
                            (g_l[1] - y_off) / 5,
                            label="goal locations", color="white", s=30, marker="+")
            handles, labels = plt.gca().get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            plt.gca().legend(by_label.values(), by_label.keys())
            handles, labels = plt.gca().get_legend_handles_labels()
            plt.gca().add_patch(circle1)
            plt.xlim(cheeseboard_center[0] - cheeseboard_radius - 5, cheeseboard_center[0] + cheeseboard_radius + 5)
            plt.xlim(cheeseboard_center[1] - cheeseboard_radius - 5, cheeseboard_center[1] + cheeseboard_radius + 5)
            plt.xticks(np.arange(0, 80, 10), np.round(np.arange(0, 80, 10) * 2.25).astype(int))
            plt.yticks(np.arange(-10, 70, 10), np.round(np.arange(0, 80, 10) * 2.25).astype(int))
            plt.xlabel("X (cm)")
            plt.ylabel("Y (cm)")
            plt.axis("equal")
            plt.show()

        else:
            for event_id, event in enumerate(pre_prob):
                plt.figure(figsize=(5, 5))
                # plt.imshow(model_dic["occ_map"].T)
                tr_count = 0
                tr_list = []
                for nr, bin in enumerate(event):
                    b = np.reshape(bin,(res_2d[0], res_2d[1]))
                    # find maximum
                    pos = np.asarray(np.unravel_index(b.argmax(), b.shape))
                    plt.scatter(pos[0], pos[1], color="red", s=10, label="decoded position")
                    if tr_count == 0:
                        plt.scatter(pos[0], pos[1], color="red", label="start", s=30, marker="x")

                    if tr_count > 0:
                        plt.plot([pos[0], pos_prev[0]], [pos[1], pos_prev[1]], color="red", alpha=0.5, linestyle="--")
                        # if np.linalg.norm(pos - pos_prev) < max_jump:
                        #     plt.plot([pos[0], pos_prev[0]], [pos[1], pos_prev[1]], color="red", alpha=0.5, zorder=1000, label="Small jumps")

                    if tr_count > 0:
                        # check if current position is close to previous position
                        if np.linalg.norm(pos - pos_prev) < max_jump:
                            tr_list.append(pos)
                            tr_count += 1
                        else:

                            # check if at least two points in trajectory list
                            if len(tr_list) > min_nr_points:
                                # plot trajectory
                                traj = np.vstack(tr_list)
                                # colors = plt.cm.Reds(np.linspace(0, 1, traj.shape[0]-1))
                                for i in range(traj.shape[0]-1):
                                    plt.plot(traj[i:(i+2),0], traj[i:(i+2),1], color="blue", zorder=2000, label="Trajectories")
                                # handles, labels = plt.gca().get_legend_handles_labels()
                                # by_label = OrderedDict(zip(labels, handles))
                                # plt.gca().legend(by_label.values(), by_label.keys())
                                # plt.xlim(0, res_2d[0])
                                # plt.ylim(0, res_2d[1])
                                # plt.gca().invert_yaxis()
                                # plt.show()
                            tr_count = 0
                            # empty trajectory list
                            tr_list = []

                    if nr==0:
                        tr_count += 1
                        tr_list.append(pos)
                    pos_prev = pos
                    tr_count += 1
                circle1 = plt.Circle((cheeseboard_center[0], cheeseboard_center[1]), cheeseboard_radius,
                                     facecolor="None", edgecolor='grey',lw=1)

                for g_l in gl:
                    plt.scatter((g_l[0] - x_off) / 5,
                                (g_l[1] - y_off) / 5,
                                label="goal locations", color="white", s=30, marker="+")
                handles, labels = plt.gca().get_legend_handles_labels()
                by_label = OrderedDict(zip(labels, handles))
                plt.gca().legend(by_label.values(), by_label.keys())
                handles, labels = plt.gca().get_legend_handles_labels()
                plt.gca().add_patch(circle1)
                plt.title("NREM epoch "+str(event_id))
                plt.xlim(cheeseboard_center[0]-cheeseboard_radius-5, cheeseboard_center[0]+cheeseboard_radius+5)
                plt.xlim(cheeseboard_center[1]-cheeseboard_radius-5, cheeseboard_center[1]+cheeseboard_radius+5)
                plt.xticks(np.arange(0,80,10), np.round(np.arange(0,80,10)*2.25).astype(int))
                plt.yticks(np.arange(-10, 70, 10), np.round(np.arange(0, 80, 10) * 2.25).astype(int))
                plt.xlabel("X (cm)")
                plt.ylabel("Y (cm)")
                plt.axis("equal")
                plt.show()

    def bayesian_decoding_rem_fragments(self, pre_file_name=None, post_file_name=None, max_jump=15,min_nr_points=2,
                          nrem_pop_vec_threshold=10, plotting=False, cells_to_use="all", save_fig=False,
                          cheeseboard_radius=27, plot_for_control=False, nr_event_to_save=None, bins_per_fragment=10,
                          fragment_id_to_save=1):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_ - np.mean(dec, axis=0)[0]vec: int
        :param nrem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type nrem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param only_stable_cells: whether to only use stable cells
        :type only_stable_cells: bool
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """
        # get length of sleep in seconds

        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob, _, event_times = \
            self.memory_drift_long_sleep_get_raw_results(template_type="ising", pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=nrem_pop_vec_threshold, cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        # pre_prob_nrem, post_prob_nrem, event_times_nrem  = \
        #     self.memory_drift_long_sleep_get_raw_results(template_type=template_type, pre_file_name=pre_file_name,
        #                                              post_file_name=post_file_name, part_to_analyze="nrem",
        #                                              pop_vec_threshold=2, cells_to_use=cells_to_use)

        # need to get model to convert array to 2D matrix for locations

        template_file_name = self.long_sleep[0].session_params.default_pre_ising_model

        with open(self.params.pre_proc_dir + 'awake_ising_maps/' + template_file_name + '.pkl', 'rb') as f:
            model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        res_2d = model_dic["occ_map"].shape

        # factor a.u. to cm
        # scaling_fac = self.session_params.data_params_dictionary["spatial_factor"]

        # ising maps are decoded using 5 a.u.
        # spatial_bin_size = 5 * scaling_fac

        # goal locations
        gl = self.long_sleep[0].session_params.goal_locations

        # offset for plotting:
        # offset for plotting:
        if self.session_name == "mjc163R4R_0114":
            x_off = 30
            y_off = 35
            cheeseboard_center = np.array([35, 22])
        elif self.session_name == "mjc163R2R_0114":
            x_off = 0
            y_off = 0
        elif self.session_name == "mjc169R1R_0114":
            x_off = 25
            y_off = 15
        elif self.session_name == "mjc169R4R_0114":
            x_off = 25
            y_off = 15
        elif self.session_name == "mjc163R1L_0114":
            x_off = 38
            y_off = 5
        elif self.session_name == "mjc148R4R_0113":
            x_off = 32
            y_off = 5
        elif self.session_name == "mjc163R3L_0114":
            x_off = 25
            y_off = 20

        if plot_for_control:
            plt.imshow(model_dic["occ_map"].T)
            for g_l in gl:
                plt.scatter((g_l[0] - x_off) / 5,
                            (g_l[1] - y_off) / 5,
                            label="goal locations", color="white", s=30)
            handles, labels = plt.gca().get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            plt.gca().legend(by_label.values(), by_label.keys())
            circle1 = plt.Circle((cheeseboard_center[0], cheeseboard_center[1]), cheeseboard_radius,
                                 facecolor="None", edgecolor='grey', lw=1)
            plt.gca().add_patch(circle1)
            plt.xlim(0,80)
            plt.ylim(-20,60)
            plt.show()

        if nr_event_to_save is not None:
            event = pre_prob[nr_event_to_save]
            plt.style.use('default')
            plt.figure(figsize=(5, 5))
            tr_count = 0
            for nr, bin in enumerate(event[fragment_id_to_save*bins_per_fragment:(fragment_id_to_save+1)*bins_per_fragment,:]):
                b = np.reshape(bin, (res_2d[0], res_2d[1]))
                # find maximum
                pos = np.asarray(np.unravel_index(b.argmax(), b.shape))
                plt.scatter(pos[0], pos[1], color="red", s=10, label="decoded position")
                if tr_count == 0:
                    plt.scatter(pos[0], pos[1], color="red", label="start", s=30, marker="x")

                if tr_count > 0:
                    plt.plot([pos[0], pos_prev[0]], [pos[1], pos_prev[1]], color="red", alpha=0.5, linestyle="--")
                pos_prev = pos
                tr_count += 1
            circle1 = plt.Circle((cheeseboard_center[0], cheeseboard_center[1]), cheeseboard_radius,
                                 facecolor="None", edgecolor='grey', lw=1)
            for g_l in gl:
                plt.scatter((g_l[0] - x_off) / 5,
                            (g_l[1] - y_off) / 5,
                            label="goal locations", color="black", s=30, marker="+")
            handles, labels = plt.gca().get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            plt.gca().legend(by_label.values(), by_label.keys())
            handles, labels = plt.gca().get_legend_handles_labels()
            plt.gca().add_patch(circle1)
            plt.xlim(cheeseboard_center[0] - cheeseboard_radius - 5, cheeseboard_center[0] + cheeseboard_radius + 5)
            plt.xlim(cheeseboard_center[1] - cheeseboard_radius - 5, cheeseboard_center[1] + cheeseboard_radius + 5)
            plt.xticks(np.arange(0, 80, 10), np.round(np.arange(0, 80, 10) * 2.25).astype(int))
            plt.yticks(np.arange(-10, 70, 10), np.round(np.arange(0, 80, 10) * 2.25).astype(int))
            plt.xlabel("X (cm)")
            plt.ylabel("Y (cm)")
            plt.axis("equal")
            plt.rcParams['svg.fonttype'] = 'none'
            plt.savefig(os.path.join(save_path, "bayesian_decoding_rem_fragment_epoch_"+
                                     str(nr_event_to_save)+"_frag_"+str(fragment_id_to_save)+".svg"),
                        transparent="True")

        else:

            for event_id, event in enumerate(pre_prob):
                frag_id = 0
                plt.figure(figsize=(5, 5))
                # plt.imshow(model_dic["occ_map"].T)
                tr_count = 0
                tr_list = []
                if 0 < tr_count and tr_count < bins_per_fragment:
                    plt.close()

                for nr, bin in enumerate(event):
                    b = np.reshape(bin,(res_2d[0], res_2d[1]))
                    # find maximum
                    pos = np.asarray(np.unravel_index(b.argmax(), b.shape))
                    plt.scatter(pos[0], pos[1], color="red", s=10, label="decoded position")
                    if tr_count == 0:
                        plt.scatter(pos[0], pos[1], color="red", label="start", s=30, marker="x")

                    if tr_count > 0:
                        plt.plot([pos[0], pos_prev[0]], [pos[1], pos_prev[1]], color="red", alpha=0.5, linestyle="--")
                        # if np.linalg.norm(pos - pos_prev) < max_jump:
                        #     plt.plot([pos[0], pos_prev[0]], [pos[1], pos_prev[1]], color="red", alpha=0.5, zorder=1000, label="Small jumps")

                    # if tr_count > 0:
                    #     # check if current position is close to previous position
                    #     if np.linalg.norm(pos - pos_prev) < max_jump:
                    #         tr_list.append(pos)
                    #         tr_count += 1
                    #     else:
                    #
                    #         # check if at least two points in trajectory list
                    #         if len(tr_list) > min_nr_points:
                    #             # plot trajectory
                    #             traj = np.vstack(tr_list)
                    #             # colors = plt.cm.Reds(np.linspace(0, 1, traj.shape[0]-1))
                    #             for i in range(traj.shape[0]-1):
                    #                 plt.plot(traj[i:(i+2),0], traj[i:(i+2),1], color="blue", zorder=2000, label="Trajectories")
                    #             # handles, labels = plt.gca().get_legend_handles_labels()
                    #             # by_label = OrderedDict(zip(labels, handles))
                    #             # plt.gca().legend(by_label.values(), by_label.keys())
                    #             # plt.xlim(0, res_2d[0])
                    #             # plt.ylim(0, res_2d[1])
                    #             # plt.gca().invert_yaxis()
                    #             # plt.show()
                    #
                    #         # empty trajectory list
                    #         tr_list = []

                    pos_prev = pos
                    tr_count += 1
                    print(tr_count)
                    if tr_count > bins_per_fragment:
                        circle1 = plt.Circle((cheeseboard_center[0], cheeseboard_center[1]), cheeseboard_radius,
                                             facecolor="None", edgecolor='grey',lw=1)

                        for g_l in gl:
                            plt.scatter((g_l[0] - x_off) / 5,
                                        (g_l[1] - y_off) / 5,
                                        label="goal locations", color="white", s=30, marker="+")
                        handles, labels = plt.gca().get_legend_handles_labels()
                        by_label = OrderedDict(zip(labels, handles))
                        plt.gca().legend(by_label.values(), by_label.keys())
                        handles, labels = plt.gca().get_legend_handles_labels()
                        plt.gca().add_patch(circle1)
                        plt.title("REM epoch "+str(event_id)+", frag="+str(frag_id))
                        plt.xlim(cheeseboard_center[0]-cheeseboard_radius-5, cheeseboard_center[0]+cheeseboard_radius+5)
                        plt.xlim(cheeseboard_center[1]-cheeseboard_radius-5, cheeseboard_center[1]+cheeseboard_radius+5)
                        plt.xticks(np.arange(0,80,10), np.round(np.arange(0,80,10)*2.25).astype(int))
                        plt.yticks(np.arange(-10, 70, 10), np.round(np.arange(0, 80, 10) * 2.25).astype(int))
                        plt.xlabel("X (cm)")
                        plt.ylabel("Y (cm)")
                        plt.axis("equal")
                        plt.show()
                        frag_id +=1
                        plt.figure(figsize=(5, 5))
                        tr_count = 0

    def bayesian_decoding_nrem(self, pre_file_name=None, post_file_name=None, max_jump=15,min_nr_points=2,
                          nrem_pop_vec_threshold=5, plotting=False, cells_to_use="all", save_fig=False,
                          cheeseboard_radius=27, plot_for_control=False, nr_event_to_save=None):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_ - np.mean(dec, axis=0)[0]vec: int
        :param nrem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type nrem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param only_stable_cells: whether to only use stable cells
        :type only_stable_cells: bool
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """
        # get length of sleep in seconds

        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob, _, event_times = \
            self.memory_drift_long_sleep_get_raw_results(template_type="ising", pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=nrem_pop_vec_threshold, cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        # pre_prob_nrem, post_prob_nrem, event_times_nrem  = \
        #     self.memory_drift_long_sleep_get_raw_results(template_type=template_type, pre_file_name=pre_file_name,
        #                                              post_file_name=post_file_name, part_to_analyze="nrem",
        #                                              pop_vec_threshold=2, cells_to_use=cells_to_use)

        # need to get model to convert array to 2D matrix for locations

        template_file_name = self.long_sleep[0].session_params.default_pre_ising_model

        with open(self.params.pre_proc_dir + 'awake_ising_maps/' + template_file_name + '.pkl', 'rb') as f:
            model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        res_2d = model_dic["occ_map"].shape

        # factor a.u. to cm
        # scaling_fac = self.session_params.data_params_dictionary["spatial_factor"]

        # ising maps are decoded using 5 a.u.
        # spatial_bin_size = 5 * scaling_fac

        # goal locations
        gl = self.long_sleep[0].session_params.goal_locations

        # offset for plotting:
        # offset for plotting:
        if self.session_name == "mjc163R4R_0114":
            x_off = 30
            y_off = 35
            cheeseboard_center = np.array([35, 22])
        elif self.session_name == "mjc163R2R_0114":
            x_off = 0
            y_off = 0
        elif self.session_name == "mjc169R1R_0114":
            x_off = 25
            y_off = 15
        elif self.session_name == "mjc169R4R_0114":
            x_off = 25
            y_off = 15
        elif self.session_name == "mjc163R1L_0114":
            x_off = 38
            y_off = 5
        elif self.session_name == "mjc148R4R_0113":
            x_off = 32
            y_off = 5
        elif self.session_name == "mjc163R3L_0114":
            x_off = 25
            y_off = 20

        if plot_for_control:
            plt.imshow(model_dic["occ_map"].T)
            for g_l in gl:
                plt.scatter((g_l[0] - x_off) / 5,
                            (g_l[1] - y_off) / 5,
                            label="goal locations", color="white", s=30)
            handles, labels = plt.gca().get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            plt.gca().legend(by_label.values(), by_label.keys())
            circle1 = plt.Circle((cheeseboard_center[0], cheeseboard_center[1]), cheeseboard_radius,
                                 facecolor="None", edgecolor='grey', lw=1)
            plt.gca().add_patch(circle1)
            plt.xlim(0,80)
            plt.ylim(-20,60)
            plt.show()

        if nr_event_to_save is not None:
            plt.style.use('default')
            event = pre_prob[nr_event_to_save]
            plt.figure(figsize=(5, 5))
            tr_count = 0
            tr_list = []
            for nr, bin in enumerate(event):
                b = np.reshape(bin, (res_2d[0], res_2d[1]))
                # find maximum
                pos = np.asarray(np.unravel_index(b.argmax(), b.shape))
                plt.scatter(pos[0], pos[1], color="red", s=10, label="decoded position")
                if tr_count == 0:
                    plt.scatter(pos[0], pos[1], color="red", label="1st decoded position", s=30, marker="x")

                if tr_count > 0:
                    plt.plot([pos[0], pos_prev[0]], [pos[1], pos_prev[1]], color="red", alpha=0.5, linestyle="--")
                pos_prev = pos
                tr_count += 1
            circle1 = plt.Circle((cheeseboard_center[0], cheeseboard_center[1]), cheeseboard_radius,
                                 facecolor="None", edgecolor='grey', lw=1)
            for g_l in gl:
                plt.scatter((g_l[0] - x_off) / 5,
                            (g_l[1] - y_off) / 5,
                            label="goal locations", color="black", s=30, marker="+")
            handles, labels = plt.gca().get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            plt.gca().legend(by_label.values(), by_label.keys())
            handles, labels = plt.gca().get_legend_handles_labels()
            plt.gca().add_patch(circle1)
            plt.xlim(cheeseboard_center[0] - cheeseboard_radius - 5, cheeseboard_center[0] + cheeseboard_radius + 5)
            plt.xlim(cheeseboard_center[1] - cheeseboard_radius - 5, cheeseboard_center[1] + cheeseboard_radius + 5)
            plt.xticks(np.arange(0, 80, 10), np.round(np.arange(0, 80, 10) * 2.25).astype(int))
            plt.yticks(np.arange(-10, 70, 10), np.round(np.arange(0, 80, 10) * 2.25).astype(int))
            plt.xlabel("X (cm)")
            plt.ylabel("Y (cm)")
            plt.axis("equal")
            plt.rcParams['svg.fonttype'] = 'none'
            plt.savefig(os.path.join(save_path, "bayesian_decoding_nrem_id_"+str(nr_event_to_save)+".svg"),
                        transparent="True")

        else:
            for event_id, event in enumerate(pre_prob):
                plt.figure(figsize=(5, 5))
                # plt.imshow(model_dic["occ_map"].T)
                tr_count = 0
                tr_list = []
                for nr, bin in enumerate(event):
                    b = np.reshape(bin,(res_2d[0], res_2d[1]))
                    # find maximum
                    pos = np.asarray(np.unravel_index(b.argmax(), b.shape))
                    plt.scatter(pos[0], pos[1], color="red", s=10, label="decoded position")
                    if tr_count == 0:
                        plt.scatter(pos[0], pos[1], color="red", label="start", s=30, marker="x")

                    if tr_count > 0:
                        plt.plot([pos[0], pos_prev[0]], [pos[1], pos_prev[1]], color="red", alpha=0.5, linestyle="--")
                        # if np.linalg.norm(pos - pos_prev) < max_jump:
                        #     plt.plot([pos[0], pos_prev[0]], [pos[1], pos_prev[1]], color="red", alpha=0.5, zorder=1000, label="Small jumps")

                    if tr_count > 0:
                        # check if current position is close to previous position
                        if np.linalg.norm(pos - pos_prev) < max_jump:
                            tr_list.append(pos)
                            tr_count += 1
                        else:

                            # check if at least two points in trajectory list
                            if len(tr_list) > min_nr_points:
                                # plot trajectory
                                traj = np.vstack(tr_list)
                                # colors = plt.cm.Reds(np.linspace(0, 1, traj.shape[0]-1))
                                for i in range(traj.shape[0]-1):
                                    plt.plot(traj[i:(i+2),0], traj[i:(i+2),1], color="blue", zorder=2000, label="Trajectories")
                                # handles, labels = plt.gca().get_legend_handles_labels()
                                # by_label = OrderedDict(zip(labels, handles))
                                # plt.gca().legend(by_label.values(), by_label.keys())
                                # plt.xlim(0, res_2d[0])
                                # plt.ylim(0, res_2d[1])
                                # plt.gca().invert_yaxis()
                                # plt.show()
                            tr_count = 0
                            # empty trajectory list
                            tr_list = []

                    if nr==0:
                        tr_count += 1
                        tr_list.append(pos)
                    pos_prev = pos
                    tr_count += 1
                circle1 = plt.Circle((cheeseboard_center[0], cheeseboard_center[1]), cheeseboard_radius,
                                     facecolor="None", edgecolor='grey',lw=1)

                for g_l in gl:
                    plt.scatter((g_l[0] - x_off) / 5,
                                (g_l[1] - y_off) / 5,
                                label="goal locations", color="white", s=30, marker="+")
                handles, labels = plt.gca().get_legend_handles_labels()
                by_label = OrderedDict(zip(labels, handles))
                plt.gca().legend(by_label.values(), by_label.keys())
                handles, labels = plt.gca().get_legend_handles_labels()
                plt.gca().add_patch(circle1)
                plt.title("NREM epoch "+str(event_id))
                plt.xlim(cheeseboard_center[0]-cheeseboard_radius-5, cheeseboard_center[0]+cheeseboard_radius+5)
                plt.xlim(cheeseboard_center[1]-cheeseboard_radius-5, cheeseboard_center[1]+cheeseboard_radius+5)
                plt.xticks(np.arange(0,80,10), np.round(np.arange(0,80,10)*2.25).astype(int))
                plt.yticks(np.arange(-10, 70, 10), np.round(np.arange(0, 80, 10) * 2.25).astype(int))
                plt.xlabel("X (cm)")
                plt.ylabel("Y (cm)")
                plt.axis("equal")
                plt.show()

    def bayesian_decoding_jump_size(self, pre_file_name=None, post_file_name=None,
                          rem_pop_vec_threshold=10, nrem_pop_vec_threshold=2, plotting=False, cells_to_use="all"):
        """
        distance between subsequently decoded points using Bayesian decoding

        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :return: rem_distance, nrem_distance
        :rtype:
        """
        # get length of sleep in seconds

        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_rem, _, event_times_rem = \
            self.memory_drift_long_sleep_get_raw_results(template_type="ising", pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, cells_to_use=cells_to_use)

        # get nrem data
        pre_prob_nrem, _, event_times_nrem = \
            self.memory_drift_long_sleep_get_raw_results(template_type="ising", pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=nrem_pop_vec_threshold, cells_to_use=cells_to_use)

        template_file_name = self.long_sleep[0].session_params.default_pre_ising_model

        with open(self.params.pre_proc_dir + 'awake_ising_maps/' + template_file_name + '.pkl', 'rb') as f:
            model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        res_2d = model_dic["occ_map"].shape

        # factor a.u. to cm
        scaling_fac = self.session_params.data_params_dictionary["spatial_factor"]

        distances_rem = []

        for event in pre_prob_rem:
            tr_count = 0
            tr_list = []
            for nr, bin in enumerate(event):
                b = np.reshape(bin,(res_2d[0], res_2d[1]))
                # find maximum
                pos = np.asarray(np.unravel_index(b.argmax(), b.shape))
                if tr_count > 0:
                    distances_rem.append(np.linalg.norm(pos - pos_prev))
                    tr_count += 1

                pos_prev = pos
                tr_count += 1

        distances_nrem = []

        for event in pre_prob_nrem:
            tr_count = 0
            tr_list = []
            for nr, bin in enumerate(event):
                b = np.reshape(bin,(res_2d[0], res_2d[1]))
                # find maximum
                pos = np.asarray(np.unravel_index(b.argmax(), b.shape))

                if tr_count > 0:
                    distances_nrem.append(np.linalg.norm(pos - pos_prev))
                    tr_count += 1

                pos_prev = pos
                tr_count += 1

        # ising maps for bayesian decoding use 5 a.u. bins --> *0.45 (spatial factor)
        rem_dist = np.array(distances_rem)*5*scaling_fac
        nrem_dist = np.array(distances_nrem)*5*scaling_fac

        if plotting:
            plt.subplot(2,1,1)
            plt.title("REM")
            plt.hist(rem_dist, density=True, bins=20)
            plt.ylabel("Density")
            plt.subplot(2,1,2)
            plt.title("NREM")
            plt.hist(nrem_dist, density=True, bins=20)
            plt.xlabel("cm")
            plt.ylabel("Density")
            plt.show()
        else:
            return rem_dist, nrem_dist

    def bayesian_decoding_jump_distance_probability_per_gap(self, pre_file_name=None, post_file_name=None,
                                                            rem_pop_vec_threshold=10, cells_to_use="all", jump_size=10):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_ - np.mean(dec, axis=0)[0]vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param only_stable_cells: whether to only use stable cells
        :type only_stable_cells: bool
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """
        # get length of sleep in seconds

        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_rem, _, event_times_rem = \
            self.memory_drift_long_sleep_get_raw_results(template_type="ising", pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, cells_to_use=cells_to_use)

        pre_prob_nrem, _, event_times_nrem = \
            self.memory_drift_long_sleep_get_raw_results(template_type="ising", pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        # pre_prob_nrem, post_prob_nrem, event_times_nrem  = \
        #     self.memory_drift_long_sleep_get_raw_results(template_type=template_type, pre_file_name=pre_file_name,
        #                                              post_file_name=post_file_name, part_to_analyze="nrem",
        #                                              pop_vec_threshold=2, cells_to_use=cells_to_use)

        # need to get model to convert array to 2D matrix for locations

        template_file_name = self.long_sleep[0].session_params.default_pre_ising_model

        with open(self.params.pre_proc_dir + 'awake_ising_maps/' + template_file_name + '.pkl', 'rb') as f:
            model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        res_2d = model_dic["occ_map"].shape

        # factor a.u. to cm
        # scaling_fac = self.session_params.data_params_dictionary["spatial_factor"]

        # ising maps are decoded using 5 a.u.
        # spatial_bin_size = 5 * scaling_fac

        # goal locations
        gl = self.long_sleep[0].session_params.goal_locations

        # offset for plotting:
        if self.session_name == "mjc163R4R_0114":
            x_off = 26
            y_off = 32
        elif self.session_name == "mjc163R2R_0114":
            x_off = 35
            y_off = 15
        elif self.session_name == "mjc169R1R_0114":
            x_off = 25
            y_off = 25

        plt.imshow(model_dic["occ_map"].T)
        for g_l in gl:
            plt.scatter((g_l[0] - x_off) / 5,
                        (g_l[1] - y_off) / 5,
                        label="goal locations", color="white", s=30)
        handles, labels = plt.gca().get_legend_handles_labels()
        by_label = OrderedDict(zip(labels, handles))
        plt.gca().legend(by_label.values(), by_label.keys())

        plt.show()

        distances_rem = []

        def distance_between_points(x, offset):
            if offset == x.shape[0]:
                return None
            else:
                return np.linalg.norm(x[:-offset, :]-x[offset:, :], axis=1)

        # first compute all locations
        rem_locations = []
        rem_locations_per_event = []
        for event in pre_prob_rem:
            temp_event = np.zeros((event.shape[0], 2))
            for nr, bin in enumerate(event):
                b = np.reshape(bin,(res_2d[0], res_2d[1]))
                # find maximum
                rem_locations.append(np.asarray(np.unravel_index(b.argmax(), b.shape)))
                temp_event[nr] = np.asarray(np.unravel_index(b.argmax(), b.shape))
            rem_locations_per_event.append(temp_event)
        rem_locations= np.vstack(rem_locations)


        max_offset_gap = 10
        prob_distances_per_rem_event = np.zeros((max_offset_gap, len(rem_locations_per_event)))
        for n_event, event_loc in enumerate(rem_locations_per_event):
            for n_offset, offset in enumerate(range(1, max_offset_gap)):
                distance_with_offset_gap = distance_between_points(event_loc, offset)
                prob_distances_per_rem_event[n_offset, n_event] = np.count_nonzero(distance_with_offset_gap < jump_size) / \
                                               distance_with_offset_gap.shape[0]


        # first compute all locations
        nrem_locations = []
        nrem_locations_per_event = []
        for event in pre_prob_nrem:
            temp_event = np.zeros((event.shape[0], 2))
            for nr, bin in enumerate(event):
                b = np.reshape(bin,(res_2d[0], res_2d[1]))
                # find maximum
                nrem_locations.append(np.asarray(np.unravel_index(b.argmax(), b.shape)))
                temp_event[nr] = np.asarray(np.unravel_index(b.argmax(), b.shape))
            nrem_locations_per_event.append(temp_event)
        nrem_locations= np.vstack(nrem_locations)

        max_offset_gap = 10
        prob_distances_per_nrem_event = np.zeros((max_offset_gap, len(nrem_locations_per_event)))
        for n_event, event_loc in enumerate(nrem_locations_per_event):
            for n_offset, offset in enumerate(range(1, max_offset_gap)):
                distance_with_offset_gap = distance_between_points(event_loc, offset)
                if distance_with_offset_gap.shape is None:
                    prob_distances_per_nrem_event[n_offset, n_event] = np.nan
                else:
                    prob_distances_per_nrem_event[n_offset, n_event] = np.count_nonzero(distance_with_offset_gap < jump_size) / \
                                                   distance_with_offset_gap.shape[0]

        # with open("mjc163R4R_0114_REM_decoded_pos", "wb") as fp:  # Pickling
        #     pickle.dump(rem_locations_per_event, fp)
        # with open("mjc163R4R_0114_NREM_decoded_pos", "wb") as fp:  # Pickling
        #     pickle.dump(nrem_locations_per_event, fp)


        all_res = []

        for rem_res, nrem_res in zip(prob_distances_per_rem_event, prob_distances_per_nrem_event):
            all_res.append(rem_res)
            all_res.append(nrem_res)

        labels=[]
        for i in range(int(len(all_res)/2)):
            labels.append(str(i)+",REM")
            labels.append(str(i)+",NREM")

        c = "white"
        bplot = plt.boxplot(all_res, positions=np.arange(len(all_res)), patch_artist=True,
                            boxprops=dict(color=c),
                            capprops=dict(color=c),
                            labels=labels,
                            whiskerprops=dict(color=c),
                            flierprops=dict(color=c, markeredgecolor=c),
                            medianprops=dict(color=c), showfliers=False
                            )
        plt.ylabel("Probability of distance < "+str(jump_size))
        plt.grid(color="grey", axis="y")
        colors = ["magenta", "blue"] * 10
        for patch, color in zip(bplot['boxes'], colors):
            patch.set_facecolor(color)
        degrees = 45
        plt.xticks(rotation=degrees)
        # check significance
        all_data_ind = np.arange(len(all_res))[::2]
        for i in all_data_ind:
            try:
                if mannwhitneyu(all_res[i], all_res[i+1])[1] < 0.001:
                    plt.hlines(1.05, i, i+1, color="white")
                    plt.text(i+0.4, 1.06, "***")

                elif mannwhitneyu(all_res[i], all_res[i+1])[1] < 0.01:
                    plt.hlines(1.05, i, i + 1, color="white")
                    plt.text(i+0.4, 1.06, "**")

                elif mannwhitneyu(all_res[i], all_res[i + 1])[1] < 0.05:
                    plt.hlines(1.05, i, i + 1, color="white")
                    plt.text(i + 0.4, 1.06, "*")
            except:
                print("error for "+str(i))
                continue
        plt.ylim(0, 1.2)
        plt.title("Probability of distance < " + str(jump_size))
        plt.show()

        plt.subplot(2,1,1)
        c = "white"
        res = prob_distances_per_rem_event.T
        bplot = plt.boxplot(res, positions=np.arange(1,max_offset_gap+1), patch_artist=True,
                            boxprops=dict(color=c),
                            capprops=dict(color=c),
                            whiskerprops=dict(color=c),
                            flierprops=dict(color=c, markeredgecolor=c),
                            medianprops=dict(color=c), showfliers=False
                            )
        plt.title("REM")
        plt.ylabel("Probability of distance < "+str(jump_size))
        plt.grid(color="grey", axis="y")

        plt.subplot(2,1,2)
        res = prob_distances_per_nrem_event.T
        bplot = plt.boxplot(res, positions=np.arange(1,max_offset_gap+1), patch_artist=True,
                            boxprops=dict(color=c),
                            capprops=dict(color=c),
                            whiskerprops=dict(color=c),
                            flierprops=dict(color=c, markeredgecolor=c),
                            medianprops=dict(color=c), showfliers=False
                            )
        plt.title("NREM")
        plt.ylabel("Probability of distance < "+str(jump_size))
        plt.xlabel("Gap (nr. spike windows)")
        plt.grid(color="grey", axis="y")
        plt.show()

    def bayesian_decoding_save_decoded_locations(self, pre_file_name=None, post_file_name=None,
                                                            rem_pop_vec_threshold=10, cells_to_use="all", jump_size=10):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_ - np.mean(dec, axis=0)[0]vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param only_stable_cells: whether to only use stable cells
        :type only_stable_cells: bool
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """
        # get length of sleep in seconds

        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_rem, _, event_times_rem = \
            self.memory_drift_long_sleep_get_raw_results(template_type="ising", pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, cells_to_use=cells_to_use)

        pre_prob_nrem, _, event_times_nrem = \
            self.memory_drift_long_sleep_get_raw_results(template_type="ising", pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        # pre_prob_nrem, post_prob_nrem, event_times_nrem  = \
        #     self.memory_drift_long_sleep_get_raw_results(template_type=template_type, pre_file_name=pre_file_name,
        #                                              post_file_name=post_file_name, part_to_analyze="nrem",
        #                                              pop_vec_threshold=2, cells_to_use=cells_to_use)

        # need to get model to convert array to 2D matrix for locations

        template_file_name = self.long_sleep[0].session_params.default_pre_ising_model

        with open(self.params.pre_proc_dir + 'awake_ising_maps/' + template_file_name + '.pkl', 'rb') as f:
            model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        res_2d = model_dic["occ_map"].shape

        # first compute all locations
        rem_locations = []
        rem_locations_per_event = []
        for event in pre_prob_rem:
            temp_event = np.zeros((event.shape[0], 2))
            for nr, bin in enumerate(event):
                b = np.reshape(bin,(res_2d[0], res_2d[1]))
                # find maximum
                rem_locations.append(np.asarray(np.unravel_index(b.argmax(), b.shape)))
                temp_event[nr] = np.asarray(np.unravel_index(b.argmax(), b.shape))
            rem_locations_per_event.append(temp_event)
        rem_locations= np.vstack(rem_locations)


        # first compute all locations
        nrem_locations = []
        nrem_locations_per_event = []
        for event in pre_prob_nrem:
            temp_event = np.zeros((event.shape[0], 2))
            for nr, bin in enumerate(event):
                b = np.reshape(bin,(res_2d[0], res_2d[1]))
                # find maximum
                nrem_locations.append(np.asarray(np.unravel_index(b.argmax(), b.shape)))
                temp_event[nr] = np.asarray(np.unravel_index(b.argmax(), b.shape))
            nrem_locations_per_event.append(temp_event)
        nrem_locations= np.vstack(nrem_locations)

        with open(self.session_name+"_REM_decoded_pos", "wb") as fp:  # Pickling
            pickle.dump(rem_locations_per_event, fp)
        with open(self.session_name+"_NREM_decoded_pos", "wb") as fp:  # Pickling
            pickle.dump(nrem_locations_per_event, fp)

    def bayesian_decoding_jump_distance_gaussian_fits(self, pre_file_name=None, post_file_name=None,
                                                            rem_pop_vec_threshold=10, cells_to_use="all", jump_size=10):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_ - np.mean(dec, axis=0)[0]vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param only_stable_cells: whether to only use stable cells
        :type only_stable_cells: bool
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """
        # get length of sleep in seconds

        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_rem, _, event_times_rem = \
            self.memory_drift_long_sleep_get_raw_results(template_type="ising", pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, cells_to_use=cells_to_use)

        pre_prob_nrem, _, event_times_nrem = \
            self.memory_drift_long_sleep_get_raw_results(template_type="ising", pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        # pre_prob_nrem, post_prob_nrem, event_times_nrem  = \
        #     self.memory_drift_long_sleep_get_raw_results(template_type=template_type, pre_file_name=pre_file_name,
        #                                              post_file_name=post_file_name, part_to_analyze="nrem",
        #                                              pop_vec_threshold=2, cells_to_use=cells_to_use)

        # need to get model to convert array to 2D matrix for locations

        template_file_name = self.long_sleep[0].session_params.default_pre_ising_model

        with open(self.params.pre_proc_dir + 'awake_ising_maps/' + template_file_name + '.pkl', 'rb') as f:
            model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        res_2d = model_dic["occ_map"].shape

        # factor a.u. to cm
        # scaling_fac = self.session_params.data_params_dictionary["spatial_factor"]

        # ising maps are decoded using 5 a.u.
        # spatial_bin_size = 5 * scaling_fac

        # goal locations
        gl = self.long_sleep[0].session_params.goal_locations

        # offset for plotting:
        if self.session_name == "mjc163R4R_0114":
            x_off = 26
            y_off = 32
        elif self.session_name == "mjc163R2R_0114":
            x_off = 35
            y_off = 15
        elif self.session_name == "mjc169R1R_0114":
            x_off = 25
            y_off = 25

        # plt.imshow(model_dic["occ_map"].T)
        # for g_l in gl:
        #     plt.scatter((g_l[0] - x_off) / 5,
        #                 (g_l[1] - y_off) / 5,
        #                 label="goal locations", color="white", s=30)
        # handles, labels = plt.gca().get_legend_handles_labels()
        # by_label = OrderedDict(zip(labels, handles))
        # plt.gca().legend(by_label.values(), by_label.keys())

        # plt.show()

        # first compute all locations
        rem_locations = []
        rem_locations_per_event = []
        for event in pre_prob_rem:
            temp_event = np.zeros((event.shape[0], 2))
            for nr, bin in enumerate(event):
                b = np.reshape(bin,(res_2d[0], res_2d[1]))
                # find maximum
                rem_locations.append(np.asarray(np.unravel_index(b.argmax(), b.shape)))
                temp_event[nr] = np.asarray(np.unravel_index(b.argmax(), b.shape))
            rem_locations_per_event.append(temp_event)
        rem= rem_locations_per_event

        # first compute all locations
        nrem_locations = []
        nrem_locations_per_event = []
        for event in pre_prob_nrem:
            temp_event = np.zeros((event.shape[0], 2))
            for nr, bin in enumerate(event):
                b = np.reshape(bin,(res_2d[0], res_2d[1]))
                # find maximum
                nrem_locations.append(np.asarray(np.unravel_index(b.argmax(), b.shape)))
                temp_event[nr] = np.asarray(np.unravel_index(b.argmax(), b.shape))
            nrem_locations_per_event.append(temp_event)
        nrem= nrem_locations_per_event

        n_lags = 50

        gau_mean = np.zeros((n_lags, 5))
        gau_weig = np.zeros((n_lags, 5))
        gau_vari = np.zeros((n_lags, 5))

        for diag in range(1, n_lags + 1):

            dists = ()
            for ep in range(int(len(rem) / 1)):
                DD = squareform(pdist(rem[ep]))
                dists = np.concatenate((dists, np.diagonal(DD, diag)), axis=None)
            # mirror the values to fit gaussians on a symmetric (around 0) distribution  
            dists_mirr = np.concatenate((dists, dists * -1), axis=None)
            if len(dists_mirr) > 10:
                # fit 5 gaussians (1 central plus 2 for the first-neighbour goal plus 2 for the second-neighbour goal)
                gmm = GaussianMixture(n_components=5).fit(dists_mirr.reshape(-1, 1))
                # sort gaussians according to their position (so the 3rd one is the central peak)
                s_idx = np.argsort(gmm.means_, axis=None)
                gau_mean[diag - 1, :] = gmm.means_[s_idx].flatten()  # get gaussian parameters 
                gau_weig[diag - 1, :] = gmm.weights_[s_idx].flatten()
                gau_vari[diag - 1, :] = gmm.covariances_.flatten()[s_idx].flatten()

                gau_vari_rem = gau_vari

        gau_pos_rem = gau_mean
        color_list = ["yellow", "orange","red", "orange", "yellow"]
        label_list = ["3rd Gaussian", "2nd Gaussian", "1st Gaussian", "2nd Gaussian", "3rd Gaussian"]
        for i in range(5):
            plt.plot(gau_mean[:,i], color_list[i], label=label_list[i])
        plt.title("REM: Mean Gaussian")
        handles, labels = plt.gca().get_legend_handles_labels()
        by_label = dict(zip(labels, handles))
        plt.legend(by_label.values(), by_label.keys())
        plt.xlabel("Lag (#spike bins)")
        plt.ylabel("Mean (cm)")
        plt.show()

        gau_vari_rem = gau_vari
        plt.title("REM: Variance Gaussian")
        color_list = ["yellow", "orange","red", "orange", "yellow"]
        label_list = ["3rd Gaussian", "2nd Gaussian", "1st Gaussian", "2nd Gaussian", "3rd Gaussian"]
        for i in range(5):
            plt.plot(gau_vari_rem[:,i], color_list[i], label=label_list[i])
        handles, labels = plt.gca().get_legend_handles_labels()
        by_label = dict(zip(labels, handles))
        plt.legend(by_label.values(), by_label.keys())
        plt.xlabel("Lag (#spike bins)")
        plt.ylabel("Variance (cm)")
        plt.show()

        gau_weig_rem = gau_weig
        plt.title("REM: Weight Gaussian")
        color_list = ["yellow", "orange","red", "orange", "yellow"]
        label_list = ["3rd Gaussian", "2nd Gaussian", "1st Gaussian", "2nd Gaussian", "3rd Gaussian"]
        for i in range(5):
            plt.plot(gau_weig_rem[:,i], color_list[i], label=label_list[i])
        handles, labels = plt.gca().get_legend_handles_labels()
        by_label = dict(zip(labels, handles))
        plt.legend(by_label.values(), by_label.keys())
        plt.xlabel("Lag (#spike bins)")
        plt.ylabel("Weight")
        plt.show()

        n_lags = 20

        gau_mean = np.zeros((n_lags, 5))
        gau_weig = np.zeros((n_lags, 5))
        gau_vari = np.zeros((n_lags, 5))

        for diag in range(1, n_lags + 1):

            dists = ()
            for ep in range(int(len(nrem) / 1)):
                DD = squareform(pdist(nrem[ep]))
                dists = np.concatenate((dists, np.diagonal(DD, diag)), axis=None)
            dists_mirr = np.concatenate((dists, dists * -1), axis=None)
            if len(dists_mirr) > 10:
                gmm = GaussianMixture(n_components=5).fit(dists_mirr.reshape(-1, 1))
                s_idx = np.argsort(gmm.means_, axis=None)
                gau_mean[diag - 1, :] = gmm.means_[s_idx].flatten()
                gau_weig[diag - 1, :] = gmm.weights_[s_idx].flatten()
                gau_vari[diag - 1, :] = gmm.covariances_.flatten()[s_idx].flatten()

        gau_pos_nrem = gau_mean
        color_list = ["yellow", "orange","red", "orange", "yellow"]
        label_list = ["3rd Gaussian", "2nd Gaussian", "1st Gaussian", "2nd Gaussian", "3rd Gaussian"]
        for i in range(5):
            plt.plot(gau_pos_nrem[:,i], color_list[i], label=label_list[i])
        plt.title("NREM: Mean Gaussian")
        handles, labels = plt.gca().get_legend_handles_labels()
        by_label = dict(zip(labels, handles))
        plt.legend(by_label.values(), by_label.keys())
        plt.xlabel("Lag (#spike bins)")
        plt.ylabel("Mean (cm)")
        plt.show()

        gau_vari_nrem = gau_vari
        plt.title("NREM: Variance Gaussian")
        color_list = ["yellow", "orange","red", "orange", "yellow"]
        label_list = ["3rd Gaussian", "2nd Gaussian", "1st Gaussian", "2nd Gaussian", "3rd Gaussian"]
        for i in range(5):
            plt.plot(gau_vari_nrem[:,i], color_list[i], label=label_list[i])
        handles, labels = plt.gca().get_legend_handles_labels()
        by_label = dict(zip(labels, handles))
        plt.legend(by_label.values(), by_label.keys())
        plt.xlabel("Lag (#spike bins)")
        plt.ylabel("Variance (cm)")
        plt.show()

        gau_weig_nrem = gau_weig
        plt.title("NREM: Weight Gaussian")
        color_list = ["yellow", "orange","red", "orange", "yellow"]
        label_list = ["3rd Gaussian", "2nd Gaussian", "1st Gaussian", "2nd Gaussian", "3rd Gaussian"]
        for i in range(5):
            plt.plot(gau_weig_nrem[:,i], color_list[i], label=label_list[i])
        handles, labels = plt.gca().get_legend_handles_labels()
        by_label = dict(zip(labels, handles))
        plt.legend(by_label.values(), by_label.keys())
        plt.xlabel("Lag (#spike bins)")
        plt.ylabel("Weight")
        plt.show()

    # </editor-fold>

    # <editor-fold desc="NREM/REM analysis">

    def memory_drift_plot_rem_nrem(self, template_type, measure="normalized_ratio", pre_file_name=None,
                                   post_file_name=None, n_moving_average_pop_vec=400, rem_pop_vec_threshold=10,
                                   plotting=False, cells_to_use="all", save_fig=False, return_cum=True):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_ - np.mean(dec, axis=0)[0]vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param only_stable_cells: whether to only use stable cells
        :type only_stable_cells: bool
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """

        # get length of sleep in seconds
        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, _, _ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, _ ,_, = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, measure=measure,
                                                     cells_to_use=cells_to_use)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------_decoding_similarity_temporal------------------------

        all_events_ratio = ratio_per_rem_event + ratio_per_nrem_event
        all_events_length = event_lengths_rem + event_lengths_nrem
        all_events_length_s = np.hstack((event_duration_rem_in_s, event_duration_nrem_in_s))
        labels_events = np.zeros(len(ratio_per_rem_event)+len(ratio_per_nrem_event))
        labels_events[:len(ratio_per_rem_event)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_ratio_list = [x for _, x in sorted(zip(all_times, all_events_ratio))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_lengths_s = [x for _, x in sorted(zip(all_times, all_events_length_s))]
        sorted_pop_vec_ratio = np.hstack(sorted_events_ratio_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_length_s = []
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat==1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat==-1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))
        merged_events_length_s = np.array(merged_events_length_s)
        merged_events_times = np.vstack(merged_events_times)

        # merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        # merged_nrem_event_times = merged_events_times[merged_events_labels == 0]
        merged_events_length_s = merged_events_times[:,1]-merged_events_times[:,0]


        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps==1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre_post ratio) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_merged_rem_event = []
        ratio_per_merged_nrem_event = []
        ratio_rem_nrem_events = []
        rem_nrem_events_label = []
        ratio_rem_nrem_pop_vec = []
        rem_nrem_pop_vec_label = []

        for start_event, end_event in zip(start, end):
            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                ratio_per_merged_rem_event.append(sorted_pop_vec_ratio[start_event:end_event])

            # nrem event
            else:
                ratio_per_merged_nrem_event.append(sorted_pop_vec_ratio[start_event:end_event])

            ratio_rem_nrem_events.append(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])

            ratio_rem_nrem_pop_vec.extend(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_pop_vec_label.extend(labels_per_pop_vec[start_event:end_event])


        # smooth across population vectors --> that means also across REM/NREM epochs
        # --------------------------------------------------------------------------------------------------------------
        len_new_events = [x.shape[0] for x in ratio_rem_nrem_events]
        ratio_per_pop_vec_new = np.hstack(ratio_rem_nrem_events)
        ratio_per_pop_vec_new_smooth = moving_average(a=np.array(ratio_per_pop_vec_new), n=n_moving_average_pop_vec)

        ratio_rem_nrem_events_smooth = []
        spike_bin_timings_rem_nrem_events_smooth = []

        first = 0
        for event_id in range(len(len_new_events)):
            # check if data is still available (due to smoothing --> data gets shorter)
            # if not --> leave loop
            if first >= ratio_per_pop_vec_new_smooth.shape[0]:
                break
            end_event = min(first + len_new_events[event_id], ratio_per_pop_vec_new_smooth.shape[0])
            ratio_rem_nrem_events_smooth.append(ratio_per_pop_vec_new_smooth[first:end_event])
            first += len_new_events[event_id]

        # need to delete indices of events that happend after the last data point after smoothing
        rem_nrem_events_label = rem_nrem_events_label[:len(ratio_rem_nrem_events_smooth)]
        merged_events_length_s = merged_events_length_s[:len(ratio_rem_nrem_events_smooth)]

        # get rem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        rem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 1))
        ds_rem = []
        ds_rem_smoothed_within = []
        merged_events_rem_length = []
        ratio_rem_events_smooth = []
        for rem_index in rem_events_indices:
            ds_rem.append(ratio_rem_nrem_events_smooth[rem_index][-1]-ratio_rem_nrem_events_smooth[rem_index][0])
            smooth_event = moving_average(a=ratio_rem_nrem_events[rem_index], n=10)
            ds_rem_smoothed_within.append(smooth_event[-1]-smooth_event[0])
            merged_events_rem_length.append(ratio_rem_nrem_events_smooth[rem_index].shape[0])
            ratio_rem_events_smooth.append(ratio_rem_nrem_events_smooth[rem_index])

        # get nrem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        nrem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 0))
        ds_nrem = []
        ds_nrem_smoothed_within = []
        merged_events_nrem_length = []
        ratio_nrem_events_smooth = []
        for nrem_index in nrem_events_indices:
            ds_nrem.append(ratio_rem_nrem_events_smooth[nrem_index][-1]-ratio_rem_nrem_events_smooth[nrem_index][0])
            merged_events_nrem_length.append(ratio_rem_nrem_events_smooth[nrem_index].shape[0])
            ratio_nrem_events_smooth.append(ratio_rem_nrem_events_smooth[nrem_index])

        # compute delta score sum & cum sum
        # --------------------------------------------------------------------------------------------------------------
        ds_nrem_sum_smoothed_within = np.sum(np.array(ds_nrem_smoothed_within))
        ds_rem_sum_smoothed_within = np.sum(np.array(ds_rem_smoothed_within))

        # do mann-whitney-u test
        print(self.session_name+" , NREM vs. REM delta scores (MWU): \n")
        print(mannwhitneyu(ds_rem, ds_nrem, alternative="less"))

        ds_nrem_cum = np.cumsum(np.array(ds_nrem))
        ds_rem_cum = np.cumsum(np.array(ds_rem))

        # compute cross correlation of delta scores NREM/REM
        # --------------------------------------------------------------------------------------------------------------
        min_len = min(len(ds_rem), len(ds_nrem))
        ds_rem_arr = np.array(ds_rem)[:min_len]
        ds_nrem_arr = np.array(ds_nrem)[:min_len]
        corr_list_pos = []
        corr_list_neg = []
        shift_array_cross_corr = [0, 1, 2, 3, 4, 5, 6]
        for shift in shift_array_cross_corr:
            corr_list_pos.append(
                np.round(pearsonr(ds_rem_arr[shift:], ds_nrem_arr[:ds_nrem_arr.shape[0] - shift])[0], 2))
            corr_list_neg.append(
                np.round(pearsonr(ds_nrem_arr[shift:], ds_rem_arr[:ds_rem_arr.shape[0] - shift])[0], 2))
        corr_list = np.hstack((np.flip(np.array(corr_list_neg)), np.array(corr_list_pos)))

        # get mean score per event
        # --------------------------------------------------------------------------------------------------------------
        mean_nrem = []
        for i, event in enumerate(ratio_per_merged_nrem_event):
            mean_nrem.append(np.mean(event))
        mean_rem = []
        for i, event in enumerate(ratio_per_merged_rem_event):
            mean_rem.append(np.mean(event))

        # get data smoothed only across REM events and only across NREM events
        # --------------------------------------------------------------------------------------------------------------
        ratio_merged_nrem_events_arr = np.hstack(ratio_per_merged_nrem_event)
        ratio_merged_nrem_events_arr_smooth = moving_average(a=ratio_merged_nrem_events_arr, n=n_moving_average_pop_vec)
        ratio_merged_rem_events_arr = np.hstack(ratio_per_merged_rem_event)
        ratio_merged_rem_events_arr_smooth = moving_average(a=ratio_merged_rem_events_arr, n=n_moving_average_pop_vec)

        # compute oscillations of REM/NREM periods
        # --------------------------------------------------------------------------------------------------------------
        # moving window
        window_size = 100
        step_size = 100

        window_start = np.arange(0, ratio_merged_rem_events_arr_smooth.shape[0] - window_size, step_size)
        min_max_rem = []
        std_rem = []
        for w_s in window_start:
            min_max_rem.append(max(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size])
                               - min(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size]))
            std_rem.append(np.std(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size]))
        window_start = np.arange(0, ratio_merged_nrem_events_arr_smooth.shape[0] - window_size, step_size)
        min_max_nrem = []
        std_nrem = []
        for w_s in window_start:
            min_max_nrem.append(max(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size])
                                - min(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size]))
            std_nrem.append(np.std(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size]))

        if plotting or save_fig:
            # check which phase came first
            if rem_nrem_events_label[0] == 1:
                # first event was a rem event
                rem_x_axis = np.arange(0, 2 * len(ds_rem), 2)
                nrem_x_axis = np.arange(1, 2 * len(ds_nrem) + 1, 2)
            elif rem_nrem_events_label[0] == 0:
                # first event was a nrem event
                nrem_x_axis = np.arange(0, 2 * len(ds_nrem), 2)
                rem_x_axis = np.arange(1, 2 * len(ds_rem) + 1, 2)

            fig = plt.figure()
            ax = fig.add_subplot()
            start = 0
            for event, label, event_length in zip(ratio_rem_nrem_events_smooth, rem_nrem_events_label,
                                                 merged_events_length_s):
                x_data = np.linspace(start, start+event_length, event.shape[0])
                if label:
                    ax.plot(x_data, event, c="r", label="REM")
                else:
                    ax.plot(x_data, event, c="b", label="NREM", zorder=10000)
                start += event_length
            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())

            plt.grid(axis="y")
            plt.ylabel("sim_ratio")
            plt.xlabel("Duration (h)")
            plt.yticks([-1, -0.5, 0, 0.5, 1],["-1","-0.5","0","0.5","1"])
            plt.xticks([0,start/3, 2*start/3, start], ["0", "7", "14", "21"])
            plt.xlim(0,start)
            plt.rcParams['svg.fonttype'] = 'none'
            # plt.savefig("rem_nrem.svg", transparent="True")
            plt.show()

            fig = plt.figure()
            ax = fig.add_subplot()
            start = 0
            for event, label, event_length in zip(ratio_rem_nrem_events_smooth, rem_nrem_events_label,
                                                 merged_events_length_s):
                x_data = np.linspace(start, start+event_length, event.shape[0])
                if label:
                    ax.plot(x_data, event, c="r", label="REM")
                else:
                    ax.plot(x_data, event, c="b", label="NREM", zorder=10000)
                start += event_length
            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())

            plt.ylabel("sim_ratio")
            plt.xlabel("Duration (h)")
            # plt.yticks([-1, -0.5, 0, 0.5, 1],["-1","-0.5","0","0.5","1"])
            # plt.xticks([0,start/3, 2*start/3, start], ["0", "7", "14", "21"])
            plt.xlim(46900, 52700)
            plt.ylim(-0.35, 0.2)
            plt.rcParams['svg.fonttype'] = 'none'
            # plt.savefig("rem_nrem.svg", transparent="True")
            plt.show()

            # plotting
            # ------------------------------------------------------------------------
            fig = plt.figure()
            ax = fig.add_subplot()
            start = 0
            for event, label in zip(ratio_rem_nrem_events_smooth, rem_nrem_events_label):
                event_length = event.shape[0]
                if label:
                    ax.plot(np.arange(start, start + event_length), event, c="r", label="REM", zorder=-10000, linewidth=1)
                else:
                    ax.plot(np.arange(start, start + event_length), event, c="b", label="NREM", zorder=10000, linewidth=1)
                start += event_length
            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())
            plt.grid(axis="y")
            plt.ylabel("sim_ratio")

            plt.xlabel("Duration (h)")
            plt.yticks([-1, -0.5, 0, 0.5, 1],["-1","-0.5","0","0.5","1"])
            plt.xticks([0,start/3, 2*start/3, start], ["0", "7", "14", "21"])
            plt.xlim(0,start)
            plt.rcParams['svg.fonttype'] = 'none'
            # plt.savefig("rem_nrem.svg", transparent="True")
            plt.show()

            fig = plt.figure()
            ax = fig.add_subplot()
            start = 0
            for event, label in zip(ratio_rem_nrem_events_smooth, rem_nrem_events_label):
                event_length = event.shape[0]
                if label:
                    ax.plot(np.arange(start, start + event_length), event, c="r", label="REM")
                else:
                    ax.plot(np.arange(start, start + event_length), event, c="b", label="NREM")
                start += event_length
            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            plt.xlim(101600,102879)
            plt.ylim(-0.34, -0.14)
            plt.xticks([])
            plt.yticks([])
            plt.rcParams['svg.fonttype'] = 'none'
            #plt.savefig("rem_nrem_zoom.svg", transparent="True")
            plt.show()
            if save_fig:
                plt.style.use('default')

            # check which phase came first
            if rem_nrem_events_label[0] == 1:
                # first event was a rem event
                rem_x_axis = np.arange(0, 2 * len(ds_rem), 2)
                nrem_x_axis = np.arange(1, 2 * len(ds_nrem) + 1, 2)
            elif rem_nrem_events_label[0] == 0:
                # first event was a nrem event
                nrem_x_axis = np.arange(0, 2 * len(ds_nrem), 2)
                rem_x_axis = np.arange(1, 2 * len(ds_rem) + 1, 2)


            fig = plt.figure()
            ax = fig.add_subplot()
            start = 0
            for event, label, event_length in zip(ratio_rem_nrem_events_smooth, rem_nrem_events_label,
                                                 merged_events_length_s):
                x_data = np.linspace(start, start+event_length, event.shape[0])
                if label:
                    ax.plot(x_data, event, c="r", label="REM")
                else:
                    ax.plot(x_data, event, c="b", label="NREM", zorder=10000)
                start += event_length
            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())

            plt.grid(axis="y")
            plt.ylabel("sim_ratio")
            plt.xlabel("Duration (h)")
            plt.yticks([-1, -0.5, 0, 0.5, 1],["-1","-0.5","0","0.5","1"])
            plt.xticks([0,start/3, 2*start/3, start], ["0", "7", "14", "21"])
            plt.xlim(0,start)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "sim_ratio_nrem_rem_example.svg"), transparent="True")
                plt.close()
            else:
                plt.show()

            fig = plt.figure()
            ax = fig.add_subplot()
            start = 0
            for event, label, event_length in zip(ratio_rem_nrem_events_smooth, rem_nrem_events_label,
                                                 merged_events_length_s):
                x_data = np.linspace(start, start+event_length, event.shape[0])
                if label:
                    ax.plot(x_data, event, c="r", label="REM")
                else:
                    ax.plot(x_data, event, c="b", label="NREM", zorder=10000)
                start += event_length
            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())

            plt.ylabel("sim_ratio")
            plt.xlabel("Duration (h)")
            # plt.yticks([-1, -0.5, 0, 0.5, 1],["-1","-0.5","0","0.5","1"])
            # plt.xticks([0,start/3, 2*start/3, start], ["0", "7", "14", "21"])
            plt.xlim(46900, 52700)
            plt.ylim(-0.35, 0.2)
            plt.rcParams['svg.fonttype'] = 'none'
            plt.savefig(os.path.join(save_path, "sim_ratio_nrem_rem_zoom.svg"), transparent="True")
            plt.close()

            fig = plt.figure(figsize=(5,2))
            ax = fig.add_subplot()
            start = 0
            for event, label, event_length in zip(ratio_rem_nrem_events_smooth, rem_nrem_events_label,
                                                 merged_events_length_s):
                x_data = np.linspace(start, start+event_length, event.shape[0])
                if label:
                    ax.plot(x_data, event, c="r", label="REM")
                else:
                    ax.plot(x_data, event, c="b", label="NREM", zorder=10000)
                start += event_length
            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())

            plt.ylabel("sim_ratio")
            plt.xlabel("Duration (h)")
            # plt.yticks([-1, -0.5, 0, 0.5, 1],["-1","-0.5","0","0.5","1"])
            # plt.xticks([0,start/3, 2*start/3, start], ["0", "7", "14", "21"])
            plt.xlim(46900, 52700)
            plt.xticks([46900, 52700],np.round(np.array([46900, 52700])/60))
            plt.ylim(-0.35, 0.35)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "sim_ratio_nrem_rem_zoom_small.svg"), transparent="True")
                plt.close()
            else:
                plt.show()

        if return_cum:
            return ds_rem_cum[-1], ds_nrem_cum[-1]
        else:
            return ratio_rem_nrem_events_smooth, rem_nrem_events_label, merged_events_length_s

    def memory_drift_plot_rem_nrem_and_firing(self, save_fig=True):
        """

        """

        ratio_rem_nrem_events_smooth, rem_nrem_events_label, merged_events_length_s = \
            self.memory_drift_plot_rem_nrem(template_type="phmm", return_cum=False, n_moving_average_pop_vec=800)

        inc_smooth, stable_smooth, dec_smooth, first_event_label = \
            self.firing_rate_changes(return_raw_firing=True, smoothing=800)

        if not ((rem_nrem_events_label[0] == 0 and first_event_label == "nrem") or
                (rem_nrem_events_label[0] == 1 and first_event_label == "rem")):
            raise Exception("Data is not aligned")

        data_per_event = [x.shape[0] for x in inc_smooth]
        inc_smooth_arr_z = zscore(np.hstack(inc_smooth))
        dec_smooth_arr_z = zscore(np.hstack(dec_smooth))
        stable_smooth_arr_z = zscore(np.hstack(stable_smooth))
        inc_smooth_z = []
        dec_smooth_z = []
        stable_smooth_z = []
        start = 0
        for x_len in data_per_event:
            inc_smooth_z.append(inc_smooth_arr_z[start:(start+x_len)])
            dec_smooth_z.append(dec_smooth_arr_z[start:(start+x_len)])
            stable_smooth_z.append(stable_smooth_arr_z[start:(start+x_len)])
            start += x_len

        if save_fig:
            plt.style.use('default')
        fig = plt.figure()
        ax = fig.add_subplot()
        ax2 = ax.twinx()
        start = 0
        for event, label, inc, dec, stable, event_length in zip(ratio_rem_nrem_events_smooth, rem_nrem_events_label,
                                                                inc_smooth_z, dec_smooth_z, stable_smooth_z,
                                                                merged_events_length_s):
            x_data = np.linspace(start, start + event_length, event.shape[0])
            if label:
                ax.plot(x_data, event, c="grey", label="REM")
            else:
                ax.plot(x_data, event, c="black", label="NREM", zorder=10000)
            x_data_inc = np.linspace(start, start + event_length, inc.shape[0])
            ax2.plot(x_data_inc, inc, color="orange", label="Increasing")
            ax2.plot(x_data_inc, dec, color="turquoise", label="Decreasing")
            # ax2.plot(x_data_inc, stable, color="violet", label="Stable")
            start += event_length
        handles, labels = ax.get_legend_handles_labels()
        by_label = OrderedDict(zip(labels, handles))
        ax.legend(by_label.values(), by_label.keys())
        handles, labels = ax2.get_legend_handles_labels()
        by_label = OrderedDict(zip(labels, handles))
        ax2.legend(by_label.values(), by_label.keys(), loc=3)
        ax.set_ylabel("sim_ratio")
        ax2.set_ylabel("Firing probability (z-scored)")
        plt.xlabel("Duration (h)")
        # plt.yticks([-1, -0.5, 0, 0.5, 1],["-1","-0.5","0","0.5","1"])
        # plt.xticks([0,start/3, 2*start/3, start], ["0", "7", "14", "21"])
        plt.xlim(46200, 52700)
        plt.xticks([46800, 52100], ["13h00m", "14h28m"])
        ax.set_xlabel("Time during rest")
        # plt.ylim(-0.35, 0.2)
        plt.rcParams['svg.fonttype'] = 'none'
        ax.set_ylim(-1.5, 0.15)
        ax2.set_ylim(-3.5, 3)
        if save_fig:
            plt.savefig(os.path.join(save_path, "sim_ratio_firing_prob_nrem_rem_example.svg"), transparent="True")
            plt.close()
        else:
            plt.show()

    def memory_drift_plot_rem_nrem_at_exact_bin_times(self, template_type, measure="normalized_ratio", pre_file_name=None,
                                   post_file_name=None, n_moving_average_pop_vec=400, rem_pop_vec_threshold=10,
                                   plotting=False, cells_to_use="all"):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_ - np.mean(dec, axis=0)[0]vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param only_stable_cells: whether to only use stable cells
        :type only_stable_cells: bool
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """

        # get length of sleep in seconds

        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, _, _, \
            spike_bin_timings_rem = \
            self.memory_drift_long_sleep_get_results_and_bin_times(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, _ ,_, \
            spike_bin_timings_nrem = \
            self.memory_drift_long_sleep_get_results_and_bin_times(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, measure=measure,
                                                     cells_to_use=cells_to_use)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------_decoding_similarity_temporal------------------------

        all_events_ratio = ratio_per_rem_event + ratio_per_nrem_event
        all_events_spike_bin_timings = spike_bin_timings_rem + spike_bin_timings_nrem
        all_events_length = event_lengths_rem + event_lengths_nrem
        all_events_length_s = np.hstack((event_duration_rem_in_s, event_duration_nrem_in_s))
        labels_events = np.zeros(len(ratio_per_rem_event)+len(ratio_per_nrem_event))
        labels_events[:len(ratio_per_rem_event)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_ratio_list = [x for _, x in sorted(zip(all_times, all_events_ratio))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_lengths_s = [x for _, x in sorted(zip(all_times, all_events_length_s))]
        sorted_spike_bin_timings = [x for _, x in sorted(zip(all_times, all_events_spike_bin_timings))]
        sorted_spike_bin_timings = np.hstack(sorted_spike_bin_timings)
        sorted_pop_vec_ratio = np.hstack(sorted_events_ratio_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_length_s = []
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat==1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat==-1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))
        merged_events_length_s = np.array(merged_events_length_s)
        merged_events_times = np.vstack(merged_events_times)

        # merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        # merged_nrem_event_times = merged_events_times[merged_events_labels == 0]
        merged_events_length_s = merged_events_times[:,1]-merged_events_times[:,0]


        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps==1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre_post ratio) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_merged_rem_event = []
        ratio_per_merged_nrem_event = []
        spike_bin_timings_per_merged_rem_event = []
        spike_bin_timings_per_merged_nrem_event = []
        spike_bin_timings_rem_nrem_events = []
        ratio_rem_nrem_events = []
        rem_nrem_events_label = []
        ratio_rem_nrem_pop_vec = []
        rem_nrem_pop_vec_label = []

        for start_event, end_event in zip(start, end):
            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                ratio_per_merged_rem_event.append(sorted_pop_vec_ratio[start_event:end_event])
                spike_bin_timings_per_merged_rem_event.append(sorted_spike_bin_timings[start_event:end_event])
            # nrem event
            else:
                ratio_per_merged_nrem_event.append(sorted_pop_vec_ratio[start_event:end_event])
                spike_bin_timings_per_merged_nrem_event.append(sorted_spike_bin_timings[start_event:end_event])

            ratio_rem_nrem_events.append(sorted_pop_vec_ratio[start_event:end_event])
            spike_bin_timings_rem_nrem_events.append(sorted_spike_bin_timings[start_event:end_event])
            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])

            ratio_rem_nrem_pop_vec.extend(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_pop_vec_label.extend(labels_per_pop_vec[start_event:end_event])


        # smooth across population vectors --> that means also across REM/NREM epochs
        # --------------------------------------------------------------------------------------------------------------
        len_new_events = [x.shape[0] for x in ratio_rem_nrem_events]
        ratio_per_pop_vec_new = np.hstack(ratio_rem_nrem_events)
        ratio_per_pop_vec_new_smooth = moving_average(a=np.array(ratio_per_pop_vec_new), n=n_moving_average_pop_vec)

        spike_bin_timings_rem_nrem_events = np.hstack(spike_bin_timings_rem_nrem_events)
        ratio_rem_nrem_events_smooth = []
        spike_bin_timings_rem_nrem_events_smooth = []

        first = 0
        for event_id in range(len(len_new_events)):
            # check if data is still available (due to smoothing --> data gets shorter)
            # if not --> leave loop
            if first >= ratio_per_pop_vec_new_smooth.shape[0]:
                break
            end_event = min(first + len_new_events[event_id], ratio_per_pop_vec_new_smooth.shape[0])
            ratio_rem_nrem_events_smooth.append(ratio_per_pop_vec_new_smooth[first:end_event])
            spike_bin_timings_rem_nrem_events_smooth.append(spike_bin_timings_rem_nrem_events[first:end_event])
            first += len_new_events[event_id]

        # need to delete indices of events that happend after the last data point after smoothing
        rem_nrem_events_label = rem_nrem_events_label[:len(ratio_rem_nrem_events_smooth)]

        # get rem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        rem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 1))
        ds_rem = []
        ds_rem_smoothed_within = []
        merged_events_rem_length = []
        ratio_rem_events_smooth = []
        for rem_index in rem_events_indices:
            ds_rem.append(ratio_rem_nrem_events_smooth[rem_index][-1]-ratio_rem_nrem_events_smooth[rem_index][0])
            smooth_event = moving_average(a=ratio_rem_nrem_events[rem_index], n=10)
            ds_rem_smoothed_within.append(smooth_event[-1]-smooth_event[0])
            merged_events_rem_length.append(ratio_rem_nrem_events_smooth[rem_index].shape[0])
            ratio_rem_events_smooth.append(ratio_rem_nrem_events_smooth[rem_index])

        # get nrem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        nrem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 0))
        ds_nrem = []
        ds_nrem_smoothed_within = []
        merged_events_nrem_length = []
        ratio_nrem_events_smooth = []
        for nrem_index in nrem_events_indices:
            ds_nrem.append(ratio_rem_nrem_events_smooth[nrem_index][-1]-ratio_rem_nrem_events_smooth[nrem_index][0])
            merged_events_nrem_length.append(ratio_rem_nrem_events_smooth[nrem_index].shape[0])
            ratio_nrem_events_smooth.append(ratio_rem_nrem_events_smooth[nrem_index])

        # compute delta score sum & cum sum
        # --------------------------------------------------------------------------------------------------------------
        ds_nrem_sum_smoothed_within = np.sum(np.array(ds_nrem_smoothed_within))
        ds_rem_sum_smoothed_within = np.sum(np.array(ds_rem_smoothed_within))

        # do mann-whitney-u test
        print(self.session_name+" , NREM vs. REM delta scores (MWU): \n")
        print(mannwhitneyu(ds_rem, ds_nrem, alternative="less"))

        ds_nrem_cum = np.cumsum(np.array(ds_nrem))
        ds_rem_cum = np.cumsum(np.array(ds_rem))

        # compute cross correlation of delta scores NREM/REM
        # --------------------------------------------------------------------------------------------------------------
        min_len = min(len(ds_rem), len(ds_nrem))
        ds_rem_arr = np.array(ds_rem)[:min_len]
        ds_nrem_arr = np.array(ds_nrem)[:min_len]
        corr_list_pos = []
        corr_list_neg = []
        shift_array_cross_corr = [0, 1, 2, 3, 4, 5, 6]
        for shift in shift_array_cross_corr:
            corr_list_pos.append(
                np.round(pearsonr(ds_rem_arr[shift:], ds_nrem_arr[:ds_nrem_arr.shape[0] - shift])[0], 2))
            corr_list_neg.append(
                np.round(pearsonr(ds_nrem_arr[shift:], ds_rem_arr[:ds_rem_arr.shape[0] - shift])[0], 2))
        corr_list = np.hstack((np.flip(np.array(corr_list_neg)), np.array(corr_list_pos)))

        # get mean score per event
        # --------------------------------------------------------------------------------------------------------------
        mean_nrem = []
        for i, event in enumerate(ratio_per_merged_nrem_event):
            mean_nrem.append(np.mean(event))
        mean_rem = []
        for i, event in enumerate(ratio_per_merged_rem_event):
            mean_rem.append(np.mean(event))

        # get data smoothed only across REM events and only across NREM events
        # --------------------------------------------------------------------------------------------------------------
        ratio_merged_nrem_events_arr = np.hstack(ratio_per_merged_nrem_event)
        ratio_merged_nrem_events_arr_smooth = moving_average(a=ratio_merged_nrem_events_arr, n=n_moving_average_pop_vec)
        ratio_merged_rem_events_arr = np.hstack(ratio_per_merged_rem_event)
        ratio_merged_rem_events_arr_smooth = moving_average(a=ratio_merged_rem_events_arr, n=n_moving_average_pop_vec)

        # compute oscillations of REM/NREM periods
        # --------------------------------------------------------------------------------------------------------------
        # moving window
        window_size = 100
        step_size = 100

        window_start = np.arange(0, ratio_merged_rem_events_arr_smooth.shape[0] - window_size, step_size)
        min_max_rem = []
        std_rem = []
        for w_s in window_start:
            min_max_rem.append(max(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size])
                               - min(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size]))
            std_rem.append(np.std(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size]))
        window_start = np.arange(0, ratio_merged_nrem_events_arr_smooth.shape[0] - window_size, step_size)
        min_max_nrem = []
        std_nrem = []
        for w_s in window_start:
            min_max_nrem.append(max(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size])
                                - min(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size]))
            std_nrem.append(np.std(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size]))

        if plotting:
            # check which phase came first
            if rem_nrem_events_label[0] == 1:
                # first event was a rem event
                rem_x_axis = np.arange(0, 2 * len(ds_rem), 2)
                nrem_x_axis = np.arange(1, 2 * len(ds_nrem) + 1, 2)
            elif rem_nrem_events_label[0] == 0:
                # first event was a nrem event
                nrem_x_axis = np.arange(0, 2 * len(ds_nrem), 2)
                rem_x_axis = np.arange(1, 2 * len(ds_rem) + 1, 2)


            fig = plt.figure()
            ax = fig.add_subplot()
            start = 0
            for event, label, spike_bin_t in zip(ratio_rem_nrem_events_smooth, rem_nrem_events_label,
                                                 spike_bin_timings_rem_nrem_events_smooth):
                event_length = event.shape[0]
                if label:
                    ax.plot(spike_bin_t, event, c="r", label="REM")
                else:
                    ax.plot(spike_bin_t, event, c="b", label="NREM", zorder=10000)
                start += event_length
            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())

            plt.grid(axis="y")
            plt.ylabel("sim_ratio")
            plt.title("RESULTS AT ACCURATE TIMES")
            plt.xlabel("Duration (h)")
            plt.yticks([-1, -0.5, 0, 0.5, 1],["-1","-0.5","0","0.5","1"])
            # plt.xticks([0,start/3, 2*start/3, start], ["0", "7", "14", "21"])
            # plt.xlim(22000, 23000)
            plt.rcParams['svg.fonttype'] = 'none'
            # plt.savefig("rem_nrem.svg", transparent="True")
            plt.show()

            # plotting
            fig = plt.figure()
            ax = fig.add_subplot()
            start = 0
            for event, label in zip(ratio_rem_nrem_events_smooth, rem_nrem_events_label):
                event_length = event.shape[0]
                if label:
                    ax.plot(np.arange(start, start + event_length), event, c="r", label="REM")
                else:
                    ax.plot(np.arange(start, start + event_length), event, c="b", label="NREM", zorder=10000)
                start += event_length
            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())
            plt.grid(axis="y")
            plt.ylabel("sim_ratio")

            plt.xlabel("Duration (h)")
            plt.yticks([-1, -0.5, 0, 0.5, 1],["-1","-0.5","0","0.5","1"])
            plt.xticks([0,start/3, 2*start/3, start], ["0", "7", "14", "21"])
            plt.xlim(0,start)
            plt.rcParams['svg.fonttype'] = 'none'
            # plt.savefig("rem_nrem.svg", transparent="True")
            plt.show()

            fig = plt.figure()
            ax = fig.add_subplot()
            start = 0
            for event, label in zip(ratio_rem_nrem_events_smooth, rem_nrem_events_label):
                event_length = event.shape[0]
                if label:
                    ax.plot(np.arange(start, start + event_length), event, c="r", label="REM")
                else:
                    ax.plot(np.arange(start, start + event_length), event, c="b", label="NREM")
                start += event_length
            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            plt.xlim(101600,102879)
            plt.ylim(-0.34, -0.14)
            plt.xticks([])
            plt.yticks([])
            plt.rcParams['svg.fonttype'] = 'none'
            #plt.savefig("rem_nrem_zoom.svg", transparent="True")
            plt.show()
        else:
            return ds_rem_cum[-1], ds_nrem_cum[-1]

    def memory_drift_get_rem_nrem_at_exact_bin_times(self, template_type, measure="normalized_ratio", pre_file_name=None,
                                                      post_file_name=None, n_moving_average_pop_vec=400, rem_pop_vec_threshold=10,
                                                      plotting=False, cells_to_use="all"):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_ - np.mean(dec, axis=0)[0]vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param only_stable_cells: whether to only use stable cells
        :type only_stable_cells: bool
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """

        # get length of sleep in seconds

        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, _, _, \
            spike_bin_timings_rem = \
            self.memory_drift_long_sleep_get_results_and_bin_times(template_type=template_type, pre_file_name=pre_file_name,
                                                                   post_file_name=post_file_name, part_to_analyze="rem",
                                                                   pop_vec_threshold=rem_pop_vec_threshold, measure=measure,
                                                                   cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, _ ,_, \
            spike_bin_timings_nrem = \
            self.memory_drift_long_sleep_get_results_and_bin_times(template_type=template_type, pre_file_name=pre_file_name,
                                                                   post_file_name=post_file_name, part_to_analyze="nrem",
                                                                   pop_vec_threshold=2, measure=measure,
                                                                   cells_to_use=cells_to_use)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------_decoding_similarity_temporal------------------------

        all_events_ratio = ratio_per_rem_event + ratio_per_nrem_event
        all_events_spike_bin_timings = spike_bin_timings_rem + spike_bin_timings_nrem
        all_events_length = event_lengths_rem + event_lengths_nrem
        all_events_length_s = np.hstack((event_duration_rem_in_s, event_duration_nrem_in_s))
        labels_events = np.zeros(len(ratio_per_rem_event)+len(ratio_per_nrem_event))
        labels_events[:len(ratio_per_rem_event)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_ratio_list = [x for _, x in sorted(zip(all_times, all_events_ratio))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_lengths_s = [x for _, x in sorted(zip(all_times, all_events_length_s))]
        sorted_spike_bin_timings = [x for _, x in sorted(zip(all_times, all_events_spike_bin_timings))]
        sorted_spike_bin_timings = np.hstack(sorted_spike_bin_timings)
        sorted_pop_vec_ratio = np.hstack(sorted_events_ratio_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_length_s = []
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat==1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat==-1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))
        merged_events_length_s = np.array(merged_events_length_s)
        merged_events_times = np.vstack(merged_events_times)

        # merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        # merged_nrem_event_times = merged_events_times[merged_events_labels == 0]
        merged_events_length_s = merged_events_times[:,1]-merged_events_times[:,0]


        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps==1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre_post ratio) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_merged_rem_event = []
        ratio_per_merged_nrem_event = []
        spike_bin_timings_per_merged_rem_event = []
        spike_bin_timings_per_merged_nrem_event = []
        spike_bin_timings_rem_nrem_events = []
        ratio_rem_nrem_events = []
        rem_nrem_events_label = []
        ratio_rem_nrem_pop_vec = []
        rem_nrem_pop_vec_label = []

        for start_event, end_event in zip(start, end):
            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                ratio_per_merged_rem_event.append(sorted_pop_vec_ratio[start_event:end_event])
                spike_bin_timings_per_merged_rem_event.append(sorted_spike_bin_timings[start_event:end_event])
            # nrem event
            else:
                ratio_per_merged_nrem_event.append(sorted_pop_vec_ratio[start_event:end_event])
                spike_bin_timings_per_merged_nrem_event.append(sorted_spike_bin_timings[start_event:end_event])

            ratio_rem_nrem_events.append(sorted_pop_vec_ratio[start_event:end_event])
            spike_bin_timings_rem_nrem_events.append(sorted_spike_bin_timings[start_event:end_event])
            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])

            ratio_rem_nrem_pop_vec.extend(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_pop_vec_label.extend(labels_per_pop_vec[start_event:end_event])


        # smooth across population vectors --> that means also across REM/NREM epochs
        # --------------------------------------------------------------------------------------------------------------
        len_new_events = [x.shape[0] for x in ratio_rem_nrem_events]
        ratio_per_pop_vec_new = np.hstack(ratio_rem_nrem_events)
        ratio_per_pop_vec_new_smooth = moving_average(a=np.array(ratio_per_pop_vec_new), n=n_moving_average_pop_vec)

        spike_bin_timings_rem_nrem_events = np.hstack(spike_bin_timings_rem_nrem_events)
        ratio_rem_nrem_events_smooth = []
        spike_bin_timings_rem_nrem_events_smooth = []

        first = 0
        for event_id in range(len(len_new_events)):
            # check if data is still available (due to smoothing --> data gets shorter)
            # if not --> leave loop
            if first >= ratio_per_pop_vec_new_smooth.shape[0]:
                break
            end_event = min(first + len_new_events[event_id], ratio_per_pop_vec_new_smooth.shape[0])
            ratio_rem_nrem_events_smooth.append(ratio_per_pop_vec_new_smooth[first:end_event])
            spike_bin_timings_rem_nrem_events_smooth.append(spike_bin_timings_rem_nrem_events[first:end_event])
            first += len_new_events[event_id]

        # need to delete indices of events that happend after the last data point after smoothing
        rem_nrem_events_label = rem_nrem_events_label[:len(ratio_rem_nrem_events_smooth)]

        # get rem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        rem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 1))
        ds_rem = []
        ds_rem_smoothed_within = []
        merged_events_rem_length = []
        ratio_rem_events_smooth = []
        for rem_index in rem_events_indices:
            ds_rem.append(ratio_rem_nrem_events_smooth[rem_index][-1]-ratio_rem_nrem_events_smooth[rem_index][0])
            smooth_event = moving_average(a=ratio_rem_nrem_events[rem_index], n=10)
            ds_rem_smoothed_within.append(smooth_event[-1]-smooth_event[0])
            merged_events_rem_length.append(ratio_rem_nrem_events_smooth[rem_index].shape[0])
            ratio_rem_events_smooth.append(ratio_rem_nrem_events_smooth[rem_index])

        # get nrem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        nrem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 0))
        ds_nrem = []
        ds_nrem_smoothed_within = []
        merged_events_nrem_length = []
        ratio_nrem_events_smooth = []
        for nrem_index in nrem_events_indices:
            ds_nrem.append(ratio_rem_nrem_events_smooth[nrem_index][-1]-ratio_rem_nrem_events_smooth[nrem_index][0])
            merged_events_nrem_length.append(ratio_rem_nrem_events_smooth[nrem_index].shape[0])
            ratio_nrem_events_smooth.append(ratio_rem_nrem_events_smooth[nrem_index])

        # compute delta score sum & cum sum
        # --------------------------------------------------------------------------------------------------------------
        ds_nrem_sum_smoothed_within = np.sum(np.array(ds_nrem_smoothed_within))
        ds_rem_sum_smoothed_within = np.sum(np.array(ds_rem_smoothed_within))

        # do mann-whitney-u test
        print(self.session_name+" , NREM vs. REM delta scores (MWU): \n")
        print(mannwhitneyu(ds_rem, ds_nrem, alternative="less"))

        ds_nrem_cum = np.cumsum(np.array(ds_nrem))
        ds_rem_cum = np.cumsum(np.array(ds_rem))

        # compute cross correlation of delta scores NREM/REM
        # --------------------------------------------------------------------------------------------------------------
        min_len = min(len(ds_rem), len(ds_nrem))
        ds_rem_arr = np.array(ds_rem)[:min_len]
        ds_nrem_arr = np.array(ds_nrem)[:min_len]
        corr_list_pos = []
        corr_list_neg = []
        shift_array_cross_corr = [0, 1, 2, 3, 4, 5, 6]
        for shift in shift_array_cross_corr:
            corr_list_pos.append(
                np.round(pearsonr(ds_rem_arr[shift:], ds_nrem_arr[:ds_nrem_arr.shape[0] - shift])[0], 2))
            corr_list_neg.append(
                np.round(pearsonr(ds_nrem_arr[shift:], ds_rem_arr[:ds_rem_arr.shape[0] - shift])[0], 2))
        corr_list = np.hstack((np.flip(np.array(corr_list_neg)), np.array(corr_list_pos)))

        # get mean score per event
        # --------------------------------------------------------------------------------------------------------------
        mean_nrem = []
        for i, event in enumerate(ratio_per_merged_nrem_event):
            mean_nrem.append(np.mean(event))
        mean_rem = []
        for i, event in enumerate(ratio_per_merged_rem_event):
            mean_rem.append(np.mean(event))

        # get data smoothed only across REM events and only across NREM events
        # --------------------------------------------------------------------------------------------------------------
        ratio_merged_nrem_events_arr = np.hstack(ratio_per_merged_nrem_event)
        ratio_merged_nrem_events_arr_smooth = moving_average(a=ratio_merged_nrem_events_arr, n=n_moving_average_pop_vec)
        ratio_merged_rem_events_arr = np.hstack(ratio_per_merged_rem_event)
        ratio_merged_rem_events_arr_smooth = moving_average(a=ratio_merged_rem_events_arr, n=n_moving_average_pop_vec)

        # compute oscillations of REM/NREM periods
        # --------------------------------------------------------------------------------------------------------------
        # moving window
        window_size = 100
        step_size = 100

        window_start = np.arange(0, ratio_merged_rem_events_arr_smooth.shape[0] - window_size, step_size)
        min_max_rem = []
        std_rem = []
        for w_s in window_start:
            min_max_rem.append(max(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size])
                               - min(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size]))
            std_rem.append(np.std(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size]))
        window_start = np.arange(0, ratio_merged_nrem_events_arr_smooth.shape[0] - window_size, step_size)
        min_max_nrem = []
        std_nrem = []
        for w_s in window_start:
            min_max_nrem.append(max(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size])
                                - min(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size]))
            std_nrem.append(np.std(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size]))

        if plotting:
            # check which phase came first
            if rem_nrem_events_label[0] == 1:
                # first event was a rem event
                rem_x_axis = np.arange(0, 2 * len(ds_rem), 2)
                nrem_x_axis = np.arange(1, 2 * len(ds_nrem) + 1, 2)
            elif rem_nrem_events_label[0] == 0:
                # first event was a nrem event
                nrem_x_axis = np.arange(0, 2 * len(ds_nrem), 2)
                rem_x_axis = np.arange(1, 2 * len(ds_rem) + 1, 2)


            fig = plt.figure()
            ax = fig.add_subplot()
            start = 0
            for event, label, spike_bin_t in zip(ratio_rem_nrem_events_smooth, rem_nrem_events_label,
                                                 spike_bin_timings_rem_nrem_events_smooth):
                event_length = event.shape[0]
                if label:
                    ax.plot(spike_bin_t, event, c="r", label="REM")
                else:
                    ax.plot(spike_bin_t, event, c="b", label="NREM", zorder=10000)
                start += event_length
            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())

            plt.grid(axis="y")
            plt.ylabel("sim_ratio")
            plt.title("RESULTS AT ACCURATE TIMES")
            plt.xlabel("Duration (h)")
            plt.yticks([-1, -0.5, 0, 0.5, 1],["-1","-0.5","0","0.5","1"])
            # plt.xticks([0,start/3, 2*start/3, start], ["0", "7", "14", "21"])
            # plt.xlim(22000, 23000)
            plt.rcParams['svg.fonttype'] = 'none'
            # plt.savefig("rem_nrem.svg", transparent="True")
            plt.show()

            # plotting
            fig = plt.figure()
            ax = fig.add_subplot()
            start = 0
            for event, label in zip(ratio_rem_nrem_events_smooth, rem_nrem_events_label):
                event_length = event.shape[0]
                if label:
                    ax.plot(np.arange(start, start + event_length), event, c="r", label="REM")
                else:
                    ax.plot(np.arange(start, start + event_length), event, c="b", label="NREM", zorder=10000)
                start += event_length
            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())
            plt.grid(axis="y")
            plt.ylabel("sim_ratio")

            plt.xlabel("Duration (h)")
            plt.yticks([-1, -0.5, 0, 0.5, 1],["-1","-0.5","0","0.5","1"])
            plt.xticks([0,start/3, 2*start/3, start], ["0", "7", "14", "21"])
            plt.xlim(0,start)
            plt.rcParams['svg.fonttype'] = 'none'
            # plt.savefig("rem_nrem.svg", transparent="True")
            plt.show()

            fig = plt.figure()
            ax = fig.add_subplot()
            start = 0
            for event, label in zip(ratio_rem_nrem_events_smooth, rem_nrem_events_label):
                event_length = event.shape[0]
                if label:
                    ax.plot(np.arange(start, start + event_length), event, c="r", label="REM")
                else:
                    ax.plot(np.arange(start, start + event_length), event, c="b", label="NREM")
                start += event_length
            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            plt.xlim(101600,102879)
            plt.ylim(-0.34, -0.14)
            plt.xticks([])
            plt.yticks([])
            plt.rcParams['svg.fonttype'] = 'none'
            #plt.savefig("rem_nrem_zoom.svg", transparent="True")
            plt.show()
        else:
            return ds_rem_cum[-1], ds_nrem_cum[-1]

    def memory_drift_rem_nrem_merged_event_times_interval_between(self, template_type="phmm", measure="normalized_ratio", pre_file_name=None,
                                                      post_file_name=None, rem_pop_vec_threshold=10,
                                                      plotting=False, cells_to_use="all", plot_for_control=False):
        # get length of sleep in seconds

        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, _, _, \
            spike_bin_timings_rem = \
            self.memory_drift_long_sleep_get_results_and_bin_times(template_type=template_type, pre_file_name=pre_file_name,
                                                                   post_file_name=post_file_name, part_to_analyze="rem",
                                                                   pop_vec_threshold=rem_pop_vec_threshold, measure=measure,
                                                                   cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, _ ,_, \
            spike_bin_timings_nrem = \
            self.memory_drift_long_sleep_get_results_and_bin_times(template_type=template_type, pre_file_name=pre_file_name,
                                                                   post_file_name=post_file_name, part_to_analyze="nrem",
                                                                   pop_vec_threshold=2, measure=measure,
                                                                   cells_to_use=cells_to_use)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------_decoding_similarity_temporal------------------------

        all_events_ratio = ratio_per_rem_event + ratio_per_nrem_event
        all_events_spike_bin_timings = spike_bin_timings_rem + spike_bin_timings_nrem
        all_events_length = event_lengths_rem + event_lengths_nrem
        all_events_length_s = np.hstack((event_duration_rem_in_s, event_duration_nrem_in_s))
        labels_events = np.zeros(len(ratio_per_rem_event)+len(ratio_per_nrem_event))
        labels_events[:len(ratio_per_rem_event)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_ratio_list = [x for _, x in sorted(zip(all_times, all_events_ratio))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_lengths_s = [x for _, x in sorted(zip(all_times, all_events_length_s))]
        sorted_spike_bin_timings = [x for _, x in sorted(zip(all_times, all_events_spike_bin_timings))]
        sorted_spike_bin_timings = np.hstack(sorted_spike_bin_timings)
        sorted_pop_vec_ratio = np.hstack(sorted_events_ratio_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        if plot_for_control:
            # plot for control
            for i, (start, end) in enumerate(zip(sorted_times, sorted_end_times)):
                plt.hlines(i, start, end)
            plt.xlabel("Time")
            plt.ylabel("Event ID")
            plt.show()

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_length_s = []
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat==1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat==-1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))
        merged_events_length_s = np.array(merged_events_length_s)
        merged_events_times = np.vstack(merged_events_times)
        # labels --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        rem_event_times = merged_events_times[merged_events_labels==1]
        nrem_event_times = merged_events_times[merged_events_labels==0]

        if plot_for_control:
            # plot for control
            for i, et in enumerate(merged_events_times):
                plt.hlines(i, et[0], et[1])

            for i, et in enumerate(nrem_event_times):
                plt.hlines(-10, et[0], et[1], color="blue")
            for i, et in enumerate(rem_event_times):
                plt.hlines(-20, et[0], et[1], color="red")
            plt.xlim(0,5000)
            plt.xlabel("Time")
            plt.ylabel("Event ID")
            plt.tight_layout()
            plt.show()

        # cut to same length
        if nrem_event_times.shape[0] < rem_event_times.shape[0]:
            rem_event_times = rem_event_times[:nrem_event_times.shape[0], :]
        elif rem_event_times.shape[0] < nrem_event_times.shape[0]:
            nrem_event_times = nrem_event_times[:rem_event_times.shape[0], :]

        if merged_events_labels[0] == 1:
            # if first merged event is rem
            rem_to_nrem_interval = nrem_event_times[:, 0] - rem_event_times[:, 1]
            nrem_to_rem_interval = rem_event_times[1:, 0] - nrem_event_times[:-1, 1]
        elif merged_events_labels[0] == 0:
            # if first merged event is nrem
            nrem_to_rem_interval = rem_event_times[:, 0] - nrem_event_times[:, 1]
            rem_to_nrem_interval = nrem_event_times[1:, 0] - rem_event_times[:-1, 1]


        if plotting:
            plt.hist(rem_to_nrem_interval, label="rem->nrem interval")
            plt.hist(nrem_to_rem_interval, label="nrem->rem interval")
            plt.legend()
            plt.show()

            plt.scatter(np.arange(rem_to_nrem_interval.shape[0]), rem_to_nrem_interval)
            plt.title("rem->nrem")
            plt.show()
            plt.scatter(np.arange(nrem_to_rem_interval.shape[0]), nrem_to_rem_interval)
            plt.title("nrem->rem")
            plt.show()

        print(np.count_nonzero(rem_to_nrem_interval < 0))
        print(np.count_nonzero(nrem_to_rem_interval < 0))

        # remove negative values
        rem_to_nrem_interval = rem_to_nrem_interval[rem_to_nrem_interval > 0]
        nrem_to_rem_interval = nrem_to_rem_interval[nrem_to_rem_interval > 0]

        return rem_to_nrem_interval, nrem_to_rem_interval

    def memory_drift_rem_nrem_equalized_rem_nrem_pairs(self, template_type="phmm", measure="normalized_ratio", pre_file_name=None,
                                                                      post_file_name=None, rem_pop_vec_threshold=10,
                                                                      plotting=False, cells_to_use="all"):
        # get length of sleep in seconds

        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())
        len_sleep_sec =np.copy(len_sleep)
        len_sleep = np.sum(np.array(len_sleep))

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, _, _, \
            spike_bin_timings_rem = \
            self.memory_drift_long_sleep_get_results_and_bin_times(template_type=template_type, pre_file_name=pre_file_name,
                                                                   post_file_name=post_file_name, part_to_analyze="rem",
                                                                   pop_vec_threshold=rem_pop_vec_threshold, measure=measure,
                                                                   cells_to_use=cells_to_use)
        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, _ ,_, \
            spike_bin_timings_nrem = \
            self.memory_drift_long_sleep_get_results_and_bin_times(template_type=template_type, pre_file_name=pre_file_name,
                                                                   post_file_name=post_file_name, part_to_analyze="nrem",
                                                                   pop_vec_threshold=2, measure=measure,
                                                                   cells_to_use=cells_to_use)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------_decoding_similarity_temporal------------------------

        all_events_ratio = ratio_per_rem_event + ratio_per_nrem_event
        all_events_spike_bin_timings = spike_bin_timings_rem + spike_bin_timings_nrem
        all_events_length = event_lengths_rem + event_lengths_nrem
        all_events_length_s = np.hstack((event_duration_rem_in_s, event_duration_nrem_in_s))
        labels_events = np.zeros(len(ratio_per_rem_event)+len(ratio_per_nrem_event))
        labels_events[:len(ratio_per_rem_event)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_ratio_list = [x for _, x in sorted(zip(all_times, all_events_ratio))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_lengths_s = [x for _, x in sorted(zip(all_times, all_events_length_s))]
        sorted_spike_bin_timings = [x for _, x in sorted(zip(all_times, all_events_spike_bin_timings))]
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_length_s = []
        merged_events_labels = []
        merged_events_times = []
        merged_events_intervals_within = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat==1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
                merged_events_intervals_within.append([sorted_times[first:(first + trans)],
                                                       sorted_end_times[first:(first + trans)]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat==-1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
                merged_events_intervals_within.append([sorted_times[first:(first + trans)],
                                                       sorted_end_times[first:(first + trans)]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))
        merged_events_length_s = np.array(merged_events_length_s)
        merged_events_times = np.vstack(merged_events_times)
        # labels --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        rem_event_times = merged_events_times[merged_events_labels==1]
        nrem_event_times = merged_events_times[merged_events_labels==0]
        rem_event_times_intervals_within = [merged_events_intervals_within[i] for i in np.squeeze(np.argwhere(merged_events_labels==1))]
        nrem_event_times_intervals_within = [merged_events_intervals_within[i] for i in np.squeeze(np.argwhere(merged_events_labels==0))]
        rem_events_length_s = merged_events_length_s[merged_events_labels==1]
        nrem_events_length_s = merged_events_length_s[merged_events_labels==0]

        # want to start with NREM
        if merged_events_labels[0] == 1:
            # if first merged event is rem
            rem_event_times = rem_event_times[1:, :]
            rem_event_times_intervals_within = rem_event_times_intervals_within[1:]
            rem_events_length_s = rem_events_length_s[1:]

        # need to cut to the same length --> want NREM-REM pairs
        if rem_event_times.shape[0] > nrem_event_times.shape[0]:
            nrem_event_times = nrem_event_times[:rem_event_times.shape[0],:]
            nrem_events_length_s = nrem_events_length_s[:rem_event_times.shape[0]]
            nrem_event_times_intervals_within = nrem_event_times_intervals_within[:rem_event_times.shape[0]]
        elif rem_event_times.shape[0] < nrem_event_times.shape[0]:
            rem_event_times = rem_event_times[:nrem_event_times.shape[0],:]
            rem_event_times_intervals_within = rem_event_times_intervals_within[:nrem_event_times.shape[0]]
            rem_events_length_s = rem_events_length_s[:nrem_event_times.shape[0]]

        # need to split into single file data
        sleep_part_times =np.cumsum(len_sleep_sec)
        sleep_part_times =np.insert(sleep_part_times, 0, 0)

        # 1. need to check for each NREM-REM pair in which sleep file the data is
        # 2. get spike timings for the NREM and REM epoch
        # 3. equalize firing rate per cell
        # 4. get constant spike bins

        event_spike_rasters_epoch_1 = []
        event_spike_rasters_epoch_2 = []


        for i, l_s in enumerate(self.long_sleep):
            # get start time and end time of current sleep file
            start_file = sleep_part_times[i]
            end_file = sleep_part_times[i+1]

            # go through all NREM-REM pairs
            for nrem_start_end, nrem_intervals, nrem_length_s, rem_start_end, rem_intervals, rem_length_s in zip(nrem_event_times,
                                                                                                                 nrem_event_times_intervals_within,
                                                                                                                 nrem_events_length_s, rem_event_times,
                                                                                                                 rem_event_times_intervals_within, rem_events_length_s):

                # check if NREM and REM intervals are part of the current file
                nrem_inside = np.logical_and(nrem_start_end[0]>start_file, nrem_start_end[1]<end_file)
                rem_inside = np.logical_and(rem_start_end[0]>start_file, rem_start_end[1]<end_file)

                if nrem_inside and rem_inside:
                    # need to offset using start time of file
                    
                    nrem_intervals = np.vstack(nrem_intervals) - start_file
                    rem_intervals = np.vstack(rem_intervals) - start_file

                    # get equalized bins
                    # ------------------------------------------------------------------------------------------------------
                    event_spike_rasters_epoch_1_, event_spike_rasters_epoch_2_ = \
                        l_s.get_spike_binned_raster_equalized_between_two_epochs(event_times_epoch_1=nrem_intervals,
                                                                                 event_times_epoch_2=rem_intervals)

                    event_spike_rasters_epoch_1.append(event_spike_rasters_epoch_1_)
                    event_spike_rasters_epoch_2.append(event_spike_rasters_epoch_2_)

        bins_per_epoch_1 = [x.shape[1] for x in event_spike_rasters_epoch_1]
        bins_per_epoch_2 = [x.shape[1] for x in event_spike_rasters_epoch_2]

        event_spike_rasters_epoch_1_arr = np.hstack(event_spike_rasters_epoch_1)
        event_spike_rasters_epoch_2_arr = np.hstack(event_spike_rasters_epoch_2)

        # get pre and post model
        pre_file_name = self.session_params.default_pre_phmm_model
        post_file_name = self.session_params.default_post_phmm_model

        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        # decode epoch 1 first
        # --------------------------------------------------------------------------------------------------------
        pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                  event_spike_rasters=event_spike_rasters_epoch_1_arr,
                                                  compression_factor=compression_factor)

        post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                   event_spike_rasters=event_spike_rasters_epoch_1_arr,
                                                   compression_factor=compression_factor)

        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        sim_ratio_epoch_1 = (max_post_likeli - max_pre_likeli) / (max_post_likeli + max_pre_likeli)

        # decode epoch 2
        # --------------------------------------------------------------------------------------------------------
        pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                  event_spike_rasters=event_spike_rasters_epoch_2_arr,
                                                  compression_factor=compression_factor)

        post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                   event_spike_rasters=event_spike_rasters_epoch_2_arr,
                                                   compression_factor=compression_factor)

        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)
        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        sim_ratio_epoch_2 = (max_post_likeli - max_pre_likeli) / (max_post_likeli + max_pre_likeli)

        # split into single epochs again
        sim_ratio_epoch_1_split = np.split(sim_ratio_epoch_1, np.cumsum(bins_per_epoch_1))[:-1]
        sim_ratio_epoch_2_split = np.split(sim_ratio_epoch_2, np.cumsum(bins_per_epoch_2))[:-1]

        all_data = []
        bins_for_both_epochs = []
        for nrem_, rem_, bins_nrem, bins_rem in zip(sim_ratio_epoch_1_split, sim_ratio_epoch_2_split,
                                                    bins_per_epoch_1, bins_per_epoch_2):
            all_data.append(nrem_)
            all_data.append(rem_)
            bins_for_both_epochs.append(bins_nrem)
            bins_for_both_epochs.append(bins_rem)
            #m = np.hstack((nrem_, rem_))
            # plt.plot(moving_average(nrem_rem, 20))
            # plt.show()

        all_data = np.hstack(all_data)
        all_data_smooth = moving_average(all_data, 20)

        nrem_rem_smooth = np.split(all_data_smooth, np.cumsum(bins_for_both_epochs))[:-1]

        ds_nrem = []
        ds_rem = []

        for i_event, event in enumerate(nrem_rem_smooth):
            # need to check if there is still NREM and REM data
            if event.shape[0] > 1:
                # rem are the uneven events
                if np.remainder(i_event, 2):
                    ds_rem.append(event[-1]-event[0])
                else:
                    ds_nrem.append(event[-1]-event[0])
            else:
                # rem are the uneven events
                if np.remainder(i_event, 2):
                    ds_rem.append(np.nan)
                else:
                    ds_nrem.append(np.nan)

        ds_rem = np.array(ds_rem)
        ds_nrem = np.array(ds_nrem)

        good_pairs = ~ np.logical_or(np.isnan(ds_rem), np.isnan(ds_nrem))

        ds_rem = ds_rem[good_pairs]
        ds_nrem = ds_nrem[good_pairs]

        if plotting:
            def make_square_axes(ax):
                """Make an axes square in screen units.

                Should be called after plotting.
                """
                ax.set_aspect(1 / ax.get_data_ratio())

            plt.scatter(ds_rem, ds_nrem)
            plt.xlabel("Delta drift score REM")
            plt.ylabel("Delta drift score NREM")
            plt.xlim(-1.5, 1.5)
            plt.ylim(-1.5, 1.5)
            plt.text(-1, 1.2, "R="+str(np.round(pearsonr(ds_rem, ds_nrem)[0], 3)))
            make_square_axes(plt.gca())
            plt.tight_layout()
            plt.show()

            plt.plot(all_data_smooth)
            plt.xlabel("Time")
            plt.ylabel("Drift score")
            plt.show()

        return ds_rem, ds_nrem

    def memory_drift_plot_rem_nrem_delta_score(self, template_type, measure="normalized_ratio", pre_file_name=None,
                                               post_file_name=None, n_moving_average_pop_vec=20,
                                               rem_pop_vec_threshold=10, plotting=False, cells_to_use="all",
                                               save_fig=False):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_ - np.mean(dec, axis=0)[0]vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param only_stable_cells: whether to only use stable cells
        :type only_stable_cells: bool
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """

        # get length of sleep in seconds

        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, _, _ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, _ ,_ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, measure=measure,
                                                     cells_to_use=cells_to_use)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------_decoding_similarity_temporal------------------------

        all_events_ratio = ratio_per_rem_event + ratio_per_nrem_event
        all_events_length = event_lengths_rem + event_lengths_nrem
        all_events_length_s = np.hstack((event_duration_rem_in_s, event_duration_nrem_in_s))
        labels_events = np.zeros(len(ratio_per_rem_event)+len(ratio_per_nrem_event))
        labels_events[:len(ratio_per_rem_event)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_ratio_list = [x for _, x in sorted(zip(all_times, all_events_ratio))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_lengths_s = [x for _, x in sorted(zip(all_times, all_events_length_s))]
        sorted_pop_vec_ratio = np.hstack(sorted_events_ratio_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_length_s = []
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat==1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat==-1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))
        merged_events_length_s = np.array(merged_events_length_s)
        merged_events_times = np.vstack(merged_events_times)

        # merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        # merged_nrem_event_times = merged_events_times[merged_events_labels == 0]
        merged_events_length_s = merged_events_times[:,1]-merged_events_times[:,0]


        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps==1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre_post ratio) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_merged_rem_event = []
        ratio_per_merged_nrem_event = []
        ratio_rem_nrem_events = []
        rem_nrem_events_label = []
        ratio_rem_nrem_pop_vec = []
        rem_nrem_pop_vec_label = []

        for start_event, end_event in zip(start, end):
            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                ratio_per_merged_rem_event.append(sorted_pop_vec_ratio[start_event:end_event])
            # nrem event
            else:
                ratio_per_merged_nrem_event.append(sorted_pop_vec_ratio[start_event:end_event])

            ratio_rem_nrem_events.append(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])

            ratio_rem_nrem_pop_vec.extend(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_pop_vec_label.extend(labels_per_pop_vec[start_event:end_event])


        # smooth across population vectors --> that means also across REM/NREM epochs
        # --------------------------------------------------------------------------------------------------------------
        len_new_events = [x.shape[0] for x in ratio_rem_nrem_events]
        ratio_per_pop_vec_new = np.hstack(ratio_rem_nrem_events)
        ratio_per_pop_vec_new_smooth = moving_average(a=np.array(ratio_per_pop_vec_new), n=n_moving_average_pop_vec)

        ratio_rem_nrem_events_smooth = []
        first = 0
        for event_id in range(len(len_new_events)):
            # check if data is still available (due to smoothing --> data gets shorter)
            # if not --> leave loop
            if first >= ratio_per_pop_vec_new_smooth.shape[0]:
                break
            end_event = min(first + len_new_events[event_id], ratio_per_pop_vec_new_smooth.shape[0])
            ratio_rem_nrem_events_smooth.append(ratio_per_pop_vec_new_smooth[first:end_event])
            first += len_new_events[event_id]

        # need to delete indices of events that happend after the last data point after smoothing
        rem_nrem_events_label = rem_nrem_events_label[:len(ratio_rem_nrem_events_smooth)]

        # get rem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        rem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 1))
        ds_rem = []
        ds_rem_smoothed_within = []
        merged_events_rem_length = []
        ratio_rem_events_smooth = []
        for rem_index in rem_events_indices:
            ds_rem.append(ratio_rem_nrem_events_smooth[rem_index][-1]-ratio_rem_nrem_events_smooth[rem_index][0])
            smooth_event = moving_average(a=ratio_rem_nrem_events[rem_index], n=10)
            ds_rem_smoothed_within.append(smooth_event[-1]-smooth_event[0])
            merged_events_rem_length.append(ratio_rem_nrem_events_smooth[rem_index].shape[0])
            ratio_rem_events_smooth.append(ratio_rem_nrem_events_smooth[rem_index])

        # get nrem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        nrem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 0))
        ds_nrem = []
        ds_nrem_smoothed_within = []
        merged_events_nrem_length = []
        ratio_nrem_events_smooth = []
        for nrem_index in nrem_events_indices:
            ds_nrem.append(ratio_rem_nrem_events_smooth[nrem_index][-1]-ratio_rem_nrem_events_smooth[nrem_index][0])
            merged_events_nrem_length.append(ratio_rem_nrem_events_smooth[nrem_index].shape[0])
            ratio_nrem_events_smooth.append(ratio_rem_nrem_events_smooth[nrem_index])

        # compute delta score sum & cum sum
        # --------------------------------------------------------------------------------------------------------------
        ds_nrem_sum_smoothed_within = np.sum(np.array(ds_nrem_smoothed_within))
        ds_rem_sum_smoothed_within = np.sum(np.array(ds_rem_smoothed_within))
        ds_nrem_sum = np.sum(np.array(ds_nrem))
        ds_rem_sum = np.sum(np.array(ds_rem))
        ds_nrem_cum = np.cumsum(np.array(ds_nrem))
        ds_rem_cum = np.cumsum(np.array(ds_rem))

        # compute cross correlation of delta scores NREM/REM
        # --------------------------------------------------------------------------------------------------------------
        min_len = min(len(ds_rem), len(ds_nrem))
        ds_rem_arr = np.array(ds_rem)[:min_len]
        ds_nrem_arr = np.array(ds_nrem)[:min_len]
        corr_list_pos = []
        corr_list_neg = []
        shift_array_cross_corr = [0, 1, 2, 3, 4, 5, 6]
        for shift in shift_array_cross_corr:
            corr_list_pos.append(
                np.round(pearsonr(ds_rem_arr[shift:], ds_nrem_arr[:ds_nrem_arr.shape[0] - shift])[0], 2))
            corr_list_neg.append(
                np.round(pearsonr(ds_nrem_arr[shift:], ds_rem_arr[:ds_rem_arr.shape[0] - shift])[0], 2))
        corr_list = np.hstack((np.flip(np.array(corr_list_neg)), np.array(corr_list_pos)))

        # get mean score per event
        # --------------------------------------------------------------------------------------------------------------
        mean_nrem = []
        for i, event in enumerate(ratio_per_merged_nrem_event):
            mean_nrem.append(np.mean(event))
        mean_rem = []
        for i, event in enumerate(ratio_per_merged_rem_event):
            mean_rem.append(np.mean(event))

        # get data smoothed only across REM events and only across NREM events
        # --------------------------------------------------------------------------------------------------------------
        ratio_merged_nrem_events_arr = np.hstack(ratio_per_merged_nrem_event)
        ratio_merged_nrem_events_arr_smooth = moving_average(a=ratio_merged_nrem_events_arr, n=n_moving_average_pop_vec)
        ratio_merged_rem_events_arr = np.hstack(ratio_per_merged_rem_event)
        ratio_merged_rem_events_arr_smooth = moving_average(a=ratio_merged_rem_events_arr, n=n_moving_average_pop_vec)

        # compute oscillations of REM/NREM periods
        # --------------------------------------------------------------------------------------------------------------
        # moving window
        window_size = 100
        step_size = 100

        window_start = np.arange(0, ratio_merged_rem_events_arr_smooth.shape[0] - window_size, step_size)
        min_max_rem = []
        std_rem = []
        for w_s in window_start:
            min_max_rem.append(max(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size])
                               - min(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size]))
            std_rem.append(np.std(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size]))
        window_start = np.arange(0, ratio_merged_nrem_events_arr_smooth.shape[0] - window_size, step_size)
        min_max_nrem = []
        std_nrem = []
        for w_s in window_start:
            min_max_nrem.append(max(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size])
                                - min(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size]))
            std_nrem.append(np.std(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size]))

        if plotting or save_fig:
            # check which phase came first
            if rem_nrem_events_label[0] == 1:
                # first event was a rem event
                rem_x_axis = np.arange(0, 2 * len(ds_rem), 2)
                nrem_x_axis = np.arange(1, 2 * len(ds_nrem) + 1, 2)
            elif rem_nrem_events_label[0] == 0:
                # first event was a nrem event
                nrem_x_axis = np.arange(0, 2 * len(ds_nrem), 2)
                rem_x_axis = np.arange(1, 2 * len(ds_rem) + 1, 2)

            if save_fig:
                plt.style.use('default')
            fig = plt.figure(figsize=(5,3))
            ax = fig.add_subplot()
            plt.plot(rem_x_axis, ds_rem, marker=".", label="REM", color="r")
            plt.plot(nrem_x_axis, ds_nrem, marker=".", label="NREM", color="b")
            plt.legend()
            plt.hlines(0, 0, rem_x_axis.shape[0] + nrem_x_axis.shape[0], color="gray")
            plt.ylabel("Delta score per epoch")
            #plt.title("PER EVENT DELTA SCORE")
            plt.xlim(0, len(ds_rem)+len(ds_nrem))
            plt.xticks([0, (len(ds_rem)+len(ds_nrem))/2, len(ds_rem)+len(ds_nrem)],
                       [0,np.round(len_sleep_h/2,0).astype(int), np.round(len_sleep_h,0).astype(int)])
            plt.tight_layout()
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "delta_score_"+template_type+".svg"), transparent="True")
            else:
                plt.show()
            plt.close()
            fig = plt.figure(figsize=(5,2))
            ax = fig.add_subplot()
            plt.plot(ds_nrem_cum, color="b", label="NREM")
            plt.plot(ds_rem_cum, color="r", label="REM")
            plt.xlabel("Duration (h)")
            plt.legend()
            plt.ylabel("Cumulative sum")
            plt.xlim(0, max(len(ds_nrem_cum), len(ds_rem_cum)))
            plt.yticks([min(ds_rem_cum), 0, max(ds_nrem_cum)],[np.round(min(ds_rem_cum),0).astype(int), 0, np.round(max(ds_nrem_cum),0).astype(int)])
            plt.ylim(min(ds_rem_cum), max(ds_nrem_cum))
            plt.xticks([0, (max(len(ds_nrem_cum), len(ds_rem_cum)))/2, max(len(ds_nrem_cum), len(ds_rem_cum))],
                       [0,np.round(len_sleep_h/2,0).astype(int), np.round(len_sleep_h,0).astype(int)])
            plt.hlines(0, 0, rem_x_axis.shape[0] + nrem_x_axis.shape[0], color="gray")
            plt.tight_layout()
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "cumulative_delta_score_"+template_type+".svg"),
                            transparent="True")
            else:
                plt.show()

        else:
            return ds_rem_cum[-1], ds_nrem_cum[-1]
    
    def memory_drift_rem_nrem_delta_score_smoothing(self):
            
        n_smoothing = [5, 20, 40, 100, 200, 400, 800, 2000]
        cum_delta_nrem = []
        cum_delta_rem = []

        for sm in n_smoothing:
            cd_rem, cd_nrem = self.memory_drift_plot_rem_nrem_delta_score(template_type="phmm",
                                                                          n_moving_average_pop_vec=sm)
            cum_delta_nrem.append(cd_nrem)
            cum_delta_rem.append(cd_rem)
        plt.style.use('default')
        plt.scatter(n_smoothing, cum_delta_nrem, color="blue", label="NREM SWRs")
        plt.scatter(n_smoothing, cum_delta_rem, color="red", label="REM")
        plt.xlabel("Smoothing window size (n)")
        plt.ylabel("Cumulative \n Delta Drift score")
        plt.legend()
        plt.hlines(0, n_smoothing[0], n_smoothing[-1], color="grey", linestyle="--")
        plt.xticks([ 5, 50, 100, 200, 400, 800, 2000], [ 5, 50, 100, 200, 400, 800, 2000], rotation=45)
        plt.tight_layout()
        plt.show()
        print("HERE")
    
    def memory_drift_neighbouring_epochs(self, template_type, measure="normalized_ratio", pre_file_name=None,
                                              post_file_name=None, n_moving_average_pop_vec=20,
                                              rem_pop_vec_threshold=10, plotting=False, cells_to_use="all",
                                         first_type="nrem"):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_ - np.mean(dec, axis=0)[0]vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param only_stable_cells: whether to only use stable cells
        :type only_stable_cells: bool
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """

        # get length of sleep in seconds

        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, _, _ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, _ ,_ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, measure=measure,
                                                     cells_to_use=cells_to_use)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------_decoding_similarity_temporal------------------------

        all_events_ratio = ratio_per_rem_event + ratio_per_nrem_event
        all_events_length = event_lengths_rem + event_lengths_nrem
        all_events_length_s = np.hstack((event_duration_rem_in_s, event_duration_nrem_in_s))
        labels_events = np.zeros(len(ratio_per_rem_event)+len(ratio_per_nrem_event))
        labels_events[:len(ratio_per_rem_event)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_ratio_list = [x for _, x in sorted(zip(all_times, all_events_ratio))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_lengths_s = [x for _, x in sorted(zip(all_times, all_events_length_s))]
        sorted_pop_vec_ratio = np.hstack(sorted_events_ratio_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_length_s = []
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat==1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat==-1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))
        merged_events_length_s = np.array(merged_events_length_s)
        merged_events_times = np.vstack(merged_events_times)

        # merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        # merged_nrem_event_times = merged_events_times[merged_events_labels == 0]
        merged_events_length_s = merged_events_times[:,1]-merged_events_times[:,0]


        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps==1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre_post ratio) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_merged_rem_event = []
        ratio_per_merged_nrem_event = []
        ratio_rem_nrem_events = []
        rem_nrem_events_label = []
        ratio_rem_nrem_pop_vec = []
        rem_nrem_pop_vec_label = []

        for start_event, end_event in zip(start, end):
            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                ratio_per_merged_rem_event.append(sorted_pop_vec_ratio[start_event:end_event])
            # nrem event
            else:
                ratio_per_merged_nrem_event.append(sorted_pop_vec_ratio[start_event:end_event])

            ratio_rem_nrem_events.append(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])

            ratio_rem_nrem_pop_vec.extend(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_pop_vec_label.extend(labels_per_pop_vec[start_event:end_event])

        # smooth across population vectors --> that means also across REM/NREM epochs
        # --------------------------------------------------------------------------------------------------------------
        len_new_events = [x.shape[0] for x in ratio_rem_nrem_events]
        ratio_per_pop_vec_new = np.hstack(ratio_rem_nrem_events)
        ratio_per_pop_vec_new_smooth = moving_average(a=np.array(ratio_per_pop_vec_new), n=n_moving_average_pop_vec)

        ratio_rem_nrem_events_smooth = []
        first = 0
        for event_id in range(len(len_new_events)):
            # check if data is still available (due to smoothing --> data gets shorter)
            # if not --> leave loop
            if first >= ratio_per_pop_vec_new_smooth.shape[0]:
                break
            end_event = min(first + len_new_events[event_id], ratio_per_pop_vec_new_smooth.shape[0])
            ratio_rem_nrem_events_smooth.append(ratio_per_pop_vec_new_smooth[first:end_event])
            first += len_new_events[event_id]

        # need to delete indices of events that happend after the last data point after smoothing
        rem_nrem_events_label = rem_nrem_events_label[:len(ratio_rem_nrem_events_smooth)]

        # get rem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        rem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 1))
        ds_rem = []
        ds_rem_smoothed_within = []
        merged_events_rem_length = []
        ratio_rem_events_smooth = []
        for rem_index in rem_events_indices:
            ds_rem.append(ratio_rem_nrem_events_smooth[rem_index][-1]-ratio_rem_nrem_events_smooth[rem_index][0])
            smooth_event = moving_average(a=ratio_rem_nrem_events[rem_index], n=10)
            ds_rem_smoothed_within.append(smooth_event[-1]-smooth_event[0])
            merged_events_rem_length.append(ratio_rem_nrem_events_smooth[rem_index].shape[0])
            ratio_rem_events_smooth.append(ratio_rem_nrem_events_smooth[rem_index])

        # get nrem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        nrem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 0))
        ds_nrem = []
        ds_nrem_smoothed_within = []
        merged_events_nrem_length = []
        ratio_nrem_events_smooth = []
        for nrem_index in nrem_events_indices:
            ds_nrem.append(ratio_rem_nrem_events_smooth[nrem_index][-1]-ratio_rem_nrem_events_smooth[nrem_index][0])
            merged_events_nrem_length.append(ratio_rem_nrem_events_smooth[nrem_index].shape[0])
            ratio_nrem_events_smooth.append(ratio_rem_nrem_events_smooth[nrem_index])

        # compute delta score sum & cum sum
        # --------------------------------------------------------------------------------------------------------------
        ds_nrem_sum_smoothed_within = np.sum(np.array(ds_nrem_smoothed_within))
        ds_rem_sum_smoothed_within = np.sum(np.array(ds_rem_smoothed_within))
        ds_nrem_sum = np.sum(np.array(ds_nrem))
        ds_rem_sum = np.sum(np.array(ds_rem))
        ds_nrem_cum = np.cumsum(np.array(ds_nrem))
        ds_rem_cum = np.cumsum(np.array(ds_rem))

        # compute cross correlation of delta scores NREM/REM
        # --------------------------------------------------------------------------------------------------------------
        min_len = min(len(ds_rem), len(ds_nrem))
        ds_rem_arr = np.array(ds_rem)[:min_len]
        ds_nrem_arr = np.array(ds_nrem)[:min_len]
        corr_list_pos = []
        corr_list_neg = []
        shift_array_cross_corr = [0, 1, 2, 3, 4, 5, 6]
        for shift in shift_array_cross_corr:
            corr_list_pos.append(
                np.round(pearsonr(ds_rem_arr[shift:], ds_nrem_arr[:ds_nrem_arr.shape[0] - shift])[0], 2))
            corr_list_neg.append(
                np.round(pearsonr(ds_nrem_arr[shift:], ds_rem_arr[:ds_rem_arr.shape[0] - shift])[0], 2))
        corr_list = np.hstack((np.flip(np.array(corr_list_neg)), np.array(corr_list_pos)))

        # get mean score per event
        # --------------------------------------------------------------------------------------------------------------
        mean_nrem = []
        for i, event in enumerate(ratio_per_merged_nrem_event):
            mean_nrem.append(np.mean(event))
        mean_rem = []
        for i, event in enumerate(ratio_per_merged_rem_event):
            mean_rem.append(np.mean(event))

        # get data smoothed only across REM events and only across NREM events
        # --------------------------------------------------------------------------------------------------------------
        ratio_merged_nrem_events_arr = np.hstack(ratio_per_merged_nrem_event)
        ratio_merged_nrem_events_arr_smooth = moving_average(a=ratio_merged_nrem_events_arr, n=n_moving_average_pop_vec)
        ratio_merged_rem_events_arr = np.hstack(ratio_per_merged_rem_event)
        ratio_merged_rem_events_arr_smooth = moving_average(a=ratio_merged_rem_events_arr, n=n_moving_average_pop_vec)

        # compute oscillations of REM/NREM periods
        # --------------------------------------------------------------------------------------------------------------
        # moving window
        window_size = 100
        step_size = 100

        window_start = np.arange(0, ratio_merged_rem_events_arr_smooth.shape[0] - window_size, step_size)
        min_max_rem = []
        std_rem = []
        for w_s in window_start:
            min_max_rem.append(max(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size])
                               - min(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size]))
            std_rem.append(np.std(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size]))
        window_start = np.arange(0, ratio_merged_nrem_events_arr_smooth.shape[0] - window_size, step_size)
        min_max_nrem = []
        std_nrem = []
        for w_s in window_start:
            min_max_nrem.append(max(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size])
                                - min(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size]))
            std_nrem.append(np.std(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size]))

        if not first_type == "random":
            if first_type == "nrem":
                # check which phase came first
                if rem_nrem_events_label[0] == 1:
                    # first event was a rem event
                    rem_x_axis = np.arange(0, 2 * len(ds_rem), 2)
                    nrem_x_axis = np.arange(1, 2 * len(ds_nrem) + 1, 2)
                    ds_rem_arr = ds_rem_arr[1:]
                    ds_nrem_arr = ds_nrem_arr[:-1]
                    print("First epoch: REM --> deleted first REM and last NREM epoch")
                elif rem_nrem_events_label[0] == 0:
                    # first event was a nrem event
                    nrem_x_axis = np.arange(0, 2 * len(ds_nrem), 2)
                    rem_x_axis = np.arange(1, 2 * len(ds_rem) + 1, 2)
                    print("First epoch: NREM --> nothing was done")
            elif first_type == "rem":
                # check which phase came first
                if rem_nrem_events_label[0] == 1:
                    # first event was a rem event
                    rem_x_axis = np.arange(0, 2 * len(ds_rem), 2)
                    nrem_x_axis = np.arange(1, 2 * len(ds_nrem) + 1, 2)
                    print("First epoch: REM --> nothing was done")
                elif rem_nrem_events_label[0] == 0:
                    # first event was a nrem event
                    nrem_x_axis = np.arange(0, 2 * len(ds_nrem), 2)
                    rem_x_axis = np.arange(1, 2 * len(ds_rem) + 1, 2)

                    ds_rem_arr = ds_rem_arr[:-1]
                    ds_nrem_arr = ds_nrem_arr[1:]
                    print("First epoch: NREM --> delete first NREM and last REM epoch")

        if plotting:
            def make_square_axes(ax):
                """Make an axes square in screen units.

                Should be called after plotting.
                """
                ax.set_aspect(1 / ax.get_data_ratio())

            plt.scatter(ds_rem_arr, ds_nrem_arr)
            plt.title("NEIGHBOURING PERIODS, R=" + str(np.round(pearsonr(ds_rem_arr, ds_nrem_arr)[0], 2)))
            plt.xlabel("DELTA SCORE REM")
            plt.ylabel("DELTA SCORE NREM")
            make_square_axes(plt.gca())
            plt.show()
        else:
            return ds_rem_arr, ds_nrem_arr

    # </editor-fold>

    # <editor-fold desc="Memory drift: additional analysis">

    def memory_drift_parameter_influence(self, template_type, n_moving_average_pop_vec,
                                                   sleep_classification_method="std"):
        fig = plt.figure()
        ax = fig.add_subplot()
        for i, rem_pop_vec_threshold in enumerate([20, 50, 100, 150, 200, 300, 400, 500, 600]):
            ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event =\
                self.memory_drift(template_type=template_type, n_moving_average_pop_vec=n_moving_average_pop_vec,
                                  rem_pop_vec_threshold=rem_pop_vec_threshold,
                                  sleep_classification_method=sleep_classification_method)
            ax.scatter(rem_pop_vec_threshold, ds_nrem_sum, color="b", label="NREM", zorder=1000)
            ax.scatter(rem_pop_vec_threshold, ds_rem_sum, color="r", label="REM", zorder=1000)

        handles, labels = ax.get_legend_handles_labels()
        by_label = OrderedDict(zip(labels, handles))
        ax.legend(by_label.values(), by_label.keys())
        plt.grid()
        plt.ylabel("CUMULATIVE DELTA SCORE")
        plt.xlabel("REM THRESHOLD ( POP.VEC. < THRS. ARE DISCARDED)")
        plt.title("SMOOTHING, n="+str(n_moving_average_pop_vec))
        plt.show()

    def memory_drift_spatial_content_progression_swr(self, pHMM_file_pre, pHMM_file_post,
                                   plot_for_control=False, n_moving_average_swr=10, n_moving_average_pop_vec=40):

        print("SPATIAL CONTENT PROGRESSION \n")

        # get pre-post similarity
        pre_SWR_prob, post_SWR_prob, event_times = self.sleep_fam.content_based_memory_drift_phmm(
                        pHMM_file_pre=pHMM_file_pre,
                        pHMM_file_post=pHMM_file_post,
                        plot_for_control=plot_for_control, return_results=True)

        # get spatial information for each mode from PRE
        sparsity_pre, skaggs_pre = self.exploration_fam.phmm_mode_spatial_information_from_model(file_name=pHMM_file_pre,
                                                                                          plot_for_control=plot_for_control,
                                                                                         spatial_resolution=10)

        # get spatial information for each mode from POST
        sparsity_post, skaggs_post = self.exploration_novel.phmm_mode_spatial_information_from_model(file_name=pHMM_file_post,
                                                                                          plot_for_control=plot_for_control,
                                                                                         spatial_resolution=10)

        plt.hist(skaggs_post, label="POST MODES")
        plt.hist(skaggs_pre, color="r", alpha=0.5, label="PRE MODES")
        plt.xlabel("SKAGGS INFO")
        plt.ylabel("COUNTS")
        plt.legend()
        plt.show()

        # get model from PRE and transition matrix
        model_pre = self.exploration_fam.load_poisson_hmm(file_name=pHMM_file_pre)
        transmat_pre = model_pre.transmat_

        nr_modes_pre = pre_SWR_prob[0].shape[1]
        nr_modes_post = post_SWR_prob[0].shape[1]

        # per SWR results
        swr_pre_sparsity = []
        swr_pre_skaggs = []
        swr_post_sparsity = []
        swr_post_skaggs = []
        swr_pre_post_ratio = []
        swr_pre_prob = []
        swr_post_prob = []
        swr_seq_similarity = []
        swr_len_seq = []

        # per population vector results
        pop_vec_pre_post_ratio = []
        pre_seq_list = []
        post_seq_list = []
        pre_seq_list_prob = []
        post_seq_list_prob = []
        pop_vec_post_prob = []
        pop_vec_pre_prob = []

        # go trough all SWR
        for pre_array, post_array in zip(pre_SWR_prob, post_SWR_prob):

            # make sure that there is any data for the current SWR

            if pre_array.shape[0] > 0:
                pre_sequence = np.argmax(pre_array, axis=1)
                pre_sequence_prob = np.max(pre_array, axis=1)
                post_sequence = np.argmax(post_array, axis=1)
                post_sequence_prob = np.max(post_array, axis=1)
                pre_seq_list.extend(pre_sequence)
                post_seq_list.extend(post_sequence)
                pre_seq_list_prob.extend(pre_sequence_prob)
                post_seq_list_prob.extend(post_sequence_prob)

                # check how likely observed sequence is considering transitions from model (awake behavior)
                mode_before = pre_sequence[:-1]
                mode_after = pre_sequence[1:]
                transition_prob = 0
                # go trough each transition of the sequence
                for bef, aft in zip(mode_before, mode_after):
                    transition_prob += np.log(transmat_pre[bef, aft])

                swr_seq_similarity.append(np.exp(transition_prob))
                swr_len_seq.append(pre_sequence.shape[0])

                # per SWR computations
                # ----------------------------------------------------------------------------------------------------------
                # compute spatial information per SWR
                swr_pre_sparsity.append(np.mean(sparsity_pre[pre_sequence]))
                swr_pre_skaggs.append(np.mean(skaggs_pre[pre_sequence]))
                swr_post_sparsity.append(np.mean(sparsity_post[post_sequence]))
                swr_post_skaggs.append(np.mean(skaggs_post[post_sequence]))
                # arrays: [nr_pop_vecs_per_SWR, nr_time_spatial_time_bins]
                # get maximum value per population vector and take average across the SWR
                if pre_array.shape[0] > 0:
                    # save pre and post probabilities
                    swr_pre_prob.append(np.mean(np.max(pre_array, axis=1)))
                    swr_post_prob.append(np.mean(np.max(post_array, axis=1)))
                    # compute ratio
                    prob_pre = np.mean(np.max(pre_array, axis=1))
                    prob_post = np.mean(np.max(post_array, axis=1))
                    swr_pre_post_ratio.append((prob_post - prob_pre) / (prob_post + prob_pre))
                else:
                    swr_pre_prob.append(np.nan)
                    swr_post_prob.append(np.nan)
                    swr_pre_post_ratio.append(np.nan)

                # per population vector computations
                # ----------------------------------------------------------------------------------------------------------
                # compute per population vector similarity score
                prob_post = np.max(post_array, axis=1)
                prob_pre = np.max(pre_array, axis=1)
                pop_vec_pre_post_ratio.extend((prob_post - prob_pre) / (prob_post + prob_pre))
                pop_vec_pre_prob.extend(np.max(pre_array, axis=1))
                if pre_array.shape[0] > 0:
                    pop_vec_pre_prob.extend(np.max(pre_array, axis=1))
                    pop_vec_post_prob.extend(np.max(post_array, axis=1))
                else:
                    pop_vec_pre_prob.extend([np.nan])
                    pop_vec_post_prob.extend([np.nan])

        # smoothen per SWR info
        # --------------------------------------------------------------------------------------------------------------
        swr_pre_sparsity = moving_average(a=np.array(swr_pre_sparsity), n=n_moving_average_swr)
        swr_pre_skaggs = moving_average(a=np.array(swr_pre_skaggs), n=n_moving_average_swr)
        swr_post_skaggs = moving_average(a=np.array(swr_post_skaggs), n=n_moving_average_swr)
        swr_post_sparsity = moving_average(a=np.array(swr_post_sparsity), n=n_moving_average_swr)
        swr_pre_post_ratio = moving_average(a=np.array(swr_pre_post_ratio), n=n_moving_average_swr)

        # plot per SWR info
        # --------------------------------------------------------------------------------------------------------------
        plt.plot(swr_pre_post_ratio)
        plt.title("PRE-POST RATIO FOR EACH SWR: PHMM")
        plt.xlabel("SWR ID")
        plt.ylabel("PRE-POST SIMILARITY")
        plt.ylim(-1, 1)
        plt.grid()
        plt.show()

        plt.subplot(2,1,1)
        plt.plot(swr_pre_skaggs)
        plt.title("SPATIAL INFORMATION PER SWR: PRE, SKAGGS")
        plt.xlabel("SWR ID")
        plt.ylabel("AVG. SKAGGS INFO")
        plt.subplot(2,1,2)
        plt.plot(swr_pre_sparsity)
        plt.title("SPATIAL INFORMATION PER SWR: PRE, SPARSITY")
        plt.xlabel("SWR ID")
        plt.ylabel("AVG. SPARSITY")
        plt.show()

        plt.scatter(swr_pre_post_ratio, swr_pre_skaggs)
        plt.title("PER SWR CORRELATION: SCORE vs. SKAGGS\n"+str(pearsonr(swr_pre_post_ratio, swr_pre_skaggs)))
        plt.xlabel("SIMILARITY SCORE")
        plt.ylabel("SKAGGS INFORMATION")
        plt.show()
        plt.scatter(swr_pre_post_ratio, swr_pre_sparsity)
        plt.title("PER SWR CORRELATION: SCORE vs. SPARSITY\n"+str(pearsonr(swr_pre_post_ratio, swr_pre_sparsity)))
        plt.xlabel("SIMILARITY SCORE")
        plt.ylabel("SKAGGS SPARSITY")
        plt.show()

        plt.plot(swr_pre_skaggs, c="r", label="PRE")
        plt.plot(swr_post_skaggs, label="POST")
        plt.title("SPATIAL INFORMATION PER SWR: PRE, SKAGGS")
        plt.xlabel("SWR ID")
        plt.ylabel("AVG. SKAGGS INFO")
        plt.legend()
        plt.show()

        plt.plot(swr_pre_sparsity, c="r", label="PRE")
        plt.plot(swr_post_sparsity, label="POST")
        plt.title("SPATIAL INFORMATION PER SWR: PRE, SPARSITY")
        plt.xlabel("SWR ID")
        plt.ylabel("AVG. SPARSITY")
        plt.legend()
        plt.show()

        # sequence probability
        swr_seq_similarity = moving_average(a=np.array(swr_seq_similarity), n=n_moving_average_pop_vec)
        plt.plot(swr_seq_similarity)
        plt.title("PROBABILITY SWR PHMM MODE SEQUENCES \n USING AWAKE TRANSITION PROB. PRE")
        plt.ylabel("JOINT PROBABILITY")
        plt.xlabel("SWR ID")
        plt.show()

        # plot per population vector info
        # --------------------------------------------------------------------------------------------------------------
        pop_vec_pre_post_ratio = np.array(pop_vec_pre_post_ratio)
        # compute moving average to smooth signal
        pop_vec_pre_post_ratio_smooth = moving_average(a=pop_vec_pre_post_ratio, n=n_moving_average_pop_vec)
        plt.plot(pop_vec_pre_post_ratio_smooth)
        plt.title("PRE-POST RATIO FOR EACH POP. VECTOR: PHMM")
        plt.xlabel("POP.VEC. ID")
        plt.ylabel("PRE-POST SIMILARITY")
        plt.ylim(-1, 1)
        plt.grid()
        plt.show()

        # compute per mode info
        # --------------------------------------------------------------------------------------------------------------
        pre_seq = np.array(pre_seq_list)
        post_seq = np.array(post_seq_list)
        mode_score_mean_pre = np.zeros(pre_SWR_prob[0].shape[1])
        mode_score_std_pre = np.zeros(pre_SWR_prob[0].shape[1])
        mode_score_mean_post = np.zeros(post_SWR_prob[0].shape[1])
        mode_score_std_post = np.zeros(post_SWR_prob[0].shape[1])

        # go through all pre modes and check the average score
        for i in range(pre_SWR_prob[0].shape[1]):
            ind_sel = np.where(pre_seq == i)[0]
            if ind_sel.size == 0 or ind_sel.size == 1:
                mode_score_mean_pre[i] = np.nan
                mode_score_std_pre[i] = np.nan
            else:
                # delete all indices that are too large (becaue of moving average)
                ind_sel = ind_sel[ind_sel < pop_vec_pre_post_ratio.shape[0]]
                mode_score_mean_pre[i] = np.mean(pop_vec_pre_post_ratio[ind_sel])
                mode_score_std_pre[i] = np.std(pop_vec_pre_post_ratio[ind_sel])

        # go through all post modes and check the average score
        for i in range(post_SWR_prob[0].shape[1]):
            ind_sel = np.where(post_seq == i)[0]
            if ind_sel.size == 0 or ind_sel.size == 1:
                mode_score_mean_post[i] = np.nan
                mode_score_std_post[i] = np.nan
            else:
                # delete all indices that are too large (becaue of moving average)
                ind_sel = ind_sel[ind_sel < pop_vec_pre_post_ratio.shape[0]]
                mode_score_mean_post[i] = np.mean(pop_vec_pre_post_ratio[ind_sel])
                mode_score_std_post[i] = np.std(pop_vec_pre_post_ratio[ind_sel])

        low_score_modes = np.argsort(mode_score_mean_pre)
        # need to skip nans
        nr_nans = np.count_nonzero(np.isnan(mode_score_mean_pre))
        high_score_modes = np.flip(low_score_modes)[nr_nans:]

        plt.errorbar(range(pre_SWR_prob[0].shape[1]), mode_score_mean_pre, yerr=mode_score_std_pre, linestyle="")
        plt.scatter(range(pre_SWR_prob[0].shape[1]), mode_score_mean_pre)
        plt.title("PRE-POST SCORE PER MODE: PRE")
        plt.xlabel("MODE ID")
        plt.ylabel("PRE-POST SCORE: MEAN AND STD")
        plt.show()

        plt.errorbar(range(post_SWR_prob[0].shape[1]), mode_score_mean_post, yerr=mode_score_std_post, linestyle="")
        plt.scatter(range(post_SWR_prob[0].shape[1]), mode_score_mean_post)
        plt.title("PRE-POST SCORE PER MODE: POST")
        plt.xlabel("MODE ID")
        plt.ylabel("PRE-POST SCORE: MEAN AND STD")
        plt.show()

        plt.scatter(mode_score_mean_pre, sparsity_pre)
        plt.title("PER MODE: CORRELATION SPARSITY - PRE_POST SCORE\n"+
                  str(pearsonr(np.nan_to_num(mode_score_mean_pre), np.nan_to_num(sparsity_pre))))
        plt.xlabel("PRE_POST SCORE")
        plt.ylabel("MEAN SPARSITY")
        plt.show()
        plt.scatter(mode_score_mean_pre, skaggs_pre)
        plt.title("PER MODE: CORRELATION SKAGGS - PRE_POST SCORE\n"+
                  str(pearsonr(np.nan_to_num(mode_score_mean_pre), np.nan_to_num(skaggs_pre))))
        plt.xlabel("PRE_POST SCORE")
        plt.ylabel("MEAN SKAGGS INFO")
        plt.show()

        # check if modes get more often/less often reactivated over time
        pre_seq_list = np.array(pre_seq_list)
        nr_pop_vec = 20
        nr_windows = int(pre_seq_list.shape[0]/nr_pop_vec)
        occurence_modes = np.zeros((nr_modes_pre, nr_windows))
        for i in range(nr_windows):
            seq = pre_seq_list[i*nr_pop_vec:(i+1)*nr_pop_vec]
            mode, counts = np.unique(seq, return_counts=True)
            occurence_modes[mode,i] = counts

        plt.imshow(occurence_modes, interpolation='nearest', aspect='auto')
        plt.ylabel("MODE ID")
        plt.xlabel("WINDOW ID")
        a = plt.colorbar()
        a.set_label("#WINS/"+str(nr_pop_vec)+" POP. VEC. WINDOW")
        plt.title("OCCURENCE (#WINS) OF MODES IN WINDOWS OF FIXED LENGTH")
        plt.show()

    def memory_drift_spatial_content_progression_window(self, pHMM_file_pre, pHMM_file_post,
                                   plot_for_control=False, n_moving_average=10, sliding_window=True):

        print("SPATIAL CONTENT PROGRESSION USING WINDOWS \n")

        # get pre-post similarity
        pre_SWR_prob, post_SWR_prob, event_times = self.sleep_fam.content_based_memory_drift_phmm(
                        pHMM_file_pre=pHMM_file_pre,
                        pHMM_file_post=pHMM_file_post,
                        plot_for_control=plot_for_control, return_results=True)

        # get spatial information for each mode from PRE
        sparsity_pre, skaggs_pre = self.exploration_fam.phmm_mode_spatial_information_from_model(file_name=pHMM_file_pre,
                                                                                          plot_for_control=plot_for_control,
                                                                                         spatial_resolution=10)

        # get spatial information for each mode from POST
        sparsity_post, skaggs_post = self.exploration_novel.phmm_mode_spatial_information_from_model(file_name=pHMM_file_post,
                                                                                          plot_for_control=plot_for_control,
                                                                                         spatial_resolution=10)

        # get model from PRE and transition matrix
        model_pre = self.exploration_fam.load_poisson_hmm(file_name=pHMM_file_pre)
        transmat_pre = model_pre.transmat_


        pre_SWR_prob_arr = np.vstack(pre_SWR_prob)
        post_SWR_prob_arr = np.vstack(post_SWR_prob)

        win_pre_sparsity = []
        win_pre_skaggs = []
        win_post_sparsity = []
        win_post_skaggs = []
        win_pre_post_ratio = []
        win_seq_similarity = []


        # use sliding window to go over SWR pop. vec
        n_slid_win = 20
        overlap = 5

        if sliding_window:

            for i in np.arange(0,pre_SWR_prob_arr.shape[0] - n_slid_win +1, overlap):
                pre_array = pre_SWR_prob_arr[i:(i + n_slid_win)]
                pre_sequence = np.argmax(pre_array, axis=1)
                post_array = post_SWR_prob_arr[i:(i + n_slid_win)]
                post_sequence = np.argmax(post_array, axis=1)
                win_pre_sparsity.append(np.mean(sparsity_pre[pre_sequence]))
                win_pre_skaggs.append(np.mean(skaggs_pre[pre_sequence]))
                win_post_sparsity.append(np.mean(sparsity_post[post_sequence]))
                win_post_skaggs.append(np.mean(skaggs_post[post_sequence]))
                prob_pre = np.mean(np.max(pre_array, axis=1))
                prob_post = np.mean(np.max(post_array, axis=1))
                win_pre_post_ratio.append((prob_post - prob_pre) / (prob_post + prob_pre))

                # check how likely observed sequence is considering transitions from model (awake behavior)
                mode_before = pre_sequence[:-1]
                mode_after = pre_sequence[1:]
                transition_prob = 0
                # go trough each transition of the sequence
                for bef, aft in zip(mode_before, mode_after):
                    transition_prob += np.log(transmat_pre[bef, aft])

                win_seq_similarity.append(np.exp(transition_prob))

        else:

            for i in range(round(pre_SWR_prob_arr.shape[0]/n_slid_win)):
                pre_array = pre_SWR_prob_arr[i*n_slid_win:(i+1)*n_slid_win]
                pre_sequence = np.argmax(pre_array, axis=1)
                post_array = post_SWR_prob_arr[i*n_slid_win:(i+1)*n_slid_win]
                post_sequence = np.argmax(post_array, axis=1)
                win_pre_sparsity.append(np.mean(sparsity_pre[pre_sequence]))
                win_pre_skaggs.append(np.mean(skaggs_pre[pre_sequence]))
                win_post_sparsity.append(np.mean(sparsity_post[post_sequence]))
                win_post_skaggs.append(np.mean(skaggs_post[post_sequence]))
                prob_pre = np.mean(np.max(pre_array, axis=1))
                prob_post = np.mean(np.max(post_array, axis=1))
                win_pre_post_ratio.append((prob_post - prob_pre) / (prob_post + prob_pre))

                # check how likely observed sequence is considering transitions from model (awake behavior)
                mode_before = pre_sequence[:-1]
                mode_after = pre_sequence[1:]
                transition_prob = 0
                # go trough each transition of the sequence
                for bef, aft in zip(mode_before, mode_after):
                    transition_prob += np.log(transmat_pre[bef, aft])

                win_seq_similarity.append(np.exp(transition_prob))

        win_pre_skaggs = moving_average(a=np.array(win_pre_skaggs), n=n_moving_average)
        win_pre_sparsity = moving_average(a=np.array(win_pre_sparsity), n=n_moving_average)
        win_post_skaggs = moving_average(a=np.array(win_post_skaggs), n=n_moving_average)
        win_post_sparsity = moving_average(a=np.array(win_post_sparsity), n=n_moving_average)
        win_pre_post_ratio = moving_average(a=np.array(win_pre_post_ratio), n=n_moving_average)
        # win_seq_similarity = moving_average(a=np.array(win_seq_similarity), n=30)

        # plot per SWR info
        # --------------------------------------------------------------------------------------------------------------
        plt.plot(win_pre_post_ratio)
        plt.title("PRE-POST RATIO FOR EACH WINDOW: PHMM")
        plt.xlabel("WINDOW ID")
        plt.ylabel("PRE-POST SIMILARITY")
        plt.ylim(-1, 1)
        plt.grid()
        plt.show()

        plt.subplot(2,1,1)
        plt.plot(win_pre_skaggs)
        plt.title("SPATIAL INFORMATION PER WINDOW: PRE, SKAGGS")
        plt.xlabel("WINDOW ID")
        plt.ylabel("AVG. SKAGGS INFO")
        plt.subplot(2,1,2)
        plt.plot(win_pre_sparsity)
        plt.title("SPATIAL INFORMATION PER WINDOW: PRE, SPARSITY")
        plt.xlabel("WINDOW ID")
        plt.ylabel("AVG. SPARSITY")
        plt.show()

        plt.scatter(win_pre_post_ratio, win_pre_skaggs)
        plt.title("PER WINDOW CORRELATION: SCORE vs. SKAGGS\n"+str(pearsonr(win_pre_post_ratio, win_pre_skaggs)))
        plt.xlabel("SIMILARITY SCORE")
        plt.ylabel("SKAGGS INFORMATION")
        plt.show()
        plt.scatter(win_pre_post_ratio, win_pre_sparsity)
        plt.title("PER WINDOW CORRELATION: SCORE vs. SPARSITY\n"+str(pearsonr(win_pre_post_ratio, win_pre_sparsity)))
        plt.xlabel("SIMILARITY SCORE")
        plt.ylabel("SKAGGS SPARSITY")
        plt.show()

        plt.plot(win_pre_skaggs, c="r", label="PRE")
        plt.plot(win_post_skaggs, label="POST")
        plt.title("SPATIAL INFORMATION PER WINDOW: PRE, SKAGGS")
        plt.xlabel("WINDOW ID")
        plt.ylabel("AVG. SKAGGS INFO")
        plt.legend()
        plt.show()

        plt.plot(win_pre_sparsity, c="r", label="PRE")
        plt.plot(win_post_sparsity, label="POST")
        plt.title("SPATIAL INFORMATION PER WINDOW: PRE, SPARSITY")
        plt.xlabel("WINDOW ID")
        plt.ylabel("AVG. SPARSITY")
        plt.legend()
        plt.show()

        plt.plot(win_seq_similarity)
        plt.title("SEQUENCE PROBABILITY: PHMM")
        plt.xlabel("WINDOW ID")
        plt.ylabel("PROBABILITY")
        plt.grid()
        plt.show()

    def memory_drift_rem_nrem_and_awake_visualization(self, template_type, pre_file_name=None, post_file_name=None,
                                                      rem_pop_vec_threshold=100, log_transform=False):
        # get awake results first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_awake = self.learning_cheeseboard[0].decode_awake_activity()
        pre_prob_awake = np.vstack(pre_prob_awake)

        # get rem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_rem, post_prob_rem = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem = self.memory_drift_long_sleep_get_results(template_type=template_type,
                                                                  pre_file_name=pre_file_name,
                                                                  post_file_name=post_file_name, part_to_analyze="nrem",
                                                                  pop_vec_threshold=2)
        if log_transform:
            pre_prob_rem_log = np.log(pre_prob_rem)
            pre_prob_nrem_log = np.log(pre_prob_nrem)
            pre_prob_awake_log = np.log(pre_prob_awake)

        else:
            pre_prob_rem_log = pre_prob_rem
            pre_prob_nrem_log = pre_prob_nrem
            pre_prob_awake_log = pre_prob_awake

        pre_prob_rem_log_mean = np.mean(pre_prob_rem_log, axis=0)
        pre_prob_nrem_log_mean = np.mean(pre_prob_nrem_log, axis=0)
        pre_prob_awake_log_mean = np.mean(pre_prob_awake_log, axis=0)
        pre_prob_rem_mean = np.mean(pre_prob_rem, axis=0)
        pre_prob_nrem_mean = np.mean(pre_prob_nrem, axis=0)
        pre_prob_awake_mean = np.mean(pre_prob_awake, axis=0)

        pre_prob_rem_log_samples = pre_prob_rem_log[0::50, :]
        pre_prob_nrem_log_samples = pre_prob_nrem_log[0::50, :]
        pre_prob_awake_log_samples = pre_prob_awake_log[0::20, :]

        sep_1 = pre_prob_rem_log_samples.shape[0]
        sep_2 = pre_prob_rem_log_samples.shape[0] + pre_prob_nrem_log_samples.shape[0]
        comb = np.vstack((pre_prob_rem_log_samples, pre_prob_nrem_log_samples, pre_prob_awake_log_samples)).T
        sep_3 = comb.shape[1]

        result = multi_dim_scaling(act_mat=comb, param_dic=self.params)
        # result = perform_isomap(act_mat=comb, param_dic=self.params)
        # result = perform_PCA(act_mat=comb, param_dic=self.params)[0]
        # result = perform_TSNE(act_mat=comb, param_dic=self.params)


        # split again into REM/NREM/AWAKE
        result_rem = result[:sep_1,:]
        result_nrem = result[sep_1:sep_2,:]
        result_awake = result[sep_2:,:]

        fig, axs = plt.subplots(2, 2)
        axs[0, 0].scatter(result_rem[:,0], result_rem[:,1], color="r", s=1)
        axs[0, 0].set_title("REM")
        axs[0, 1].scatter(result_nrem[:,0], result_nrem[:,1], color="b", s=1)
        axs[0, 1].set_title("NREM")
        axs[1, 0].scatter(result_awake[:,0], result_awake[:,1], color="y", s=1)
        axs[1, 0].set_title("AWAKE")
        axs[1, 1].scatter(result_rem[:,0], result_rem[:,1], color="r", s=1)
        axs[1, 1].scatter(result_nrem[:, 0], result_nrem[:, 1], color="b", s=1)
        axs[1, 1].scatter(result_awake[:, 0], result_awake[:, 1], color="y", s=1)

        axs[1, 1].set_title("ALL")
        plt.show()
        exit()



        if self.params.dr_method_p2 == 3:
            # create figure instance
            fig = plt.figure()
            ax = fig.add_subplot(111, projection='3d')
            plot_3D_scatter(ax=ax, mds=result, params=self.params, data_sep=np.array([sep_1, sep_2]), data_sep2=sep_2)
        else:
            fig = plt.figure()
            ax = fig.add_subplot(111)
            plot_2D_scatter(ax=ax, mds=result, params=self.params, data_sep=np.array([sep_1, sep_2, sep_3]),
                            labels=["REM",
                                    "NREM",
                                    "AWAKE"])
        handles, labels = ax.get_legend_handles_labels()
        by_label = OrderedDict(zip(labels, handles))
        ax.legend(by_label.values(), by_label.keys())
        plt.show()

        exit()
        comb_min = np.min(np.hstack((pre_prob_nrem_log_mean, pre_prob_rem_log_mean, pre_prob_awake_log_mean)))
        comb_max = np.max(np.hstack((pre_prob_nrem_log_mean, pre_prob_rem_log_mean, pre_prob_awake_log_mean)))
        plt.subplot(1, 3, 1)
        plt.imshow(np.expand_dims(pre_prob_nrem_log_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        plt.ylabel("MODE ID")
        plt.title("NREM")
        plt.gca().set_xticklabels([])
        plt.subplot(1, 3, 2)
        plt.imshow(np.expand_dims(pre_prob_rem_log_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        plt.title("REM")
        plt.gca().set_xticklabels([])
        plt.subplot(1, 3, 3)
        plt.imshow(np.expand_dims(pre_prob_awake_log_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        a = plt.colorbar()
        a.set_label("MEAN LOG-PROB.")
        plt.title("AWAKE")
        plt.gca().set_xticklabels([])
        plt.show()

        comb_min = np.min(np.hstack((pre_prob_nrem_mean, pre_prob_rem_mean, pre_prob_awake_mean)))
        comb_max = np.max(np.hstack((pre_prob_nrem_mean, pre_prob_rem_mean, pre_prob_awake_mean)))
        plt.subplot(1, 3, 1)
        plt.imshow(np.expand_dims(pre_prob_nrem_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        plt.ylabel("MODE ID")
        plt.title("NREM")
        plt.gca().set_xticklabels([])
        plt.subplot(1, 3, 2)
        plt.imshow(np.expand_dims(pre_prob_rem_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        plt.title("REM")
        plt.gca().set_xticklabels([])
        plt.subplot(1, 3, 3)
        plt.imshow(np.expand_dims(pre_prob_awake_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        a = plt.colorbar()
        a.set_label("MEAN PROB.")
        plt.title("AWAKE")
        plt.gca().set_xticklabels([])
        plt.show()

        plt.imshow(pre_prob_rem_log.T, interpolation='nearest', aspect='auto')
        a = plt.colorbar()
        a.set_label("LOG(PROB)")
        plt.xlabel("POP.VEC.ID")
        plt.ylabel("MODE ID")
        plt.title("REM")
        plt.show()

        plt.imshow(pre_prob_nrem_log.T, interpolation='nearest', aspect='auto')
        plt.xlabel("POP.VEC.ID")
        plt.ylabel("MODE ID")
        a = plt.colorbar()
        a.set_label("LOG(PROB)")
        plt.title("NREM")
        plt.show()

        plt.imshow(pre_prob_awake_log.T, interpolation='nearest', aspect='auto')
        plt.xlabel("POP.VEC.ID")
        plt.ylabel("MODE ID")
        a = plt.colorbar()
        a.set_label("LOG(PROB)")
        plt.title("AWAKE")
        plt.show()

    def memory_drift_rem_nrem_and_awake_fluctuations(self, template_type, pre_file_name=None, post_file_name=None,
                                                     rem_pop_vec_threshold=100):
        # get awake results first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_awake = self.learning_cheeseboard[0].decode_awake_activity()
        pre_prob_awake = np.vstack(pre_prob_awake)
        pre_prob_awake_log = np.log(pre_prob_awake).T

        # get rem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_rem, post_prob_rem = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold)

        pre_prob_rem = np.vstack(pre_prob_rem).T
        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem = self.memory_drift_long_sleep_get_results(template_type=template_type,
                                                                  pre_file_name=pre_file_name,
                                                                  post_file_name=post_file_name, part_to_analyze="nrem",
                                                                  pop_vec_threshold=2)

        pre_prob_nrem = np.vstack(pre_prob_nrem).T

        pre_prob_rem_log = np.log(pre_prob_rem)
        pre_prob_nrem_log = np.log(pre_prob_nrem)
        pre_prob_awake_log = np.log(pre_prob_awake)

        ab_angle, rel_angle = angle_between_col_vectors(pre_prob_nrem_log)
        plt.plot(rel_angle)
        plt.show()

    def memory_drift_rem_nrem_decoding_similarity(self, template_type, pre_file_name=None, post_file_name=None,
                                                  rem_pop_vec_threshold=100, cells_to_use="all", plotting=True,
                                                  control=False, nr_shuffles=20, pre_or_post="pre"):

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        _, _, _, _, pre_prob_rem, post_prob_rem= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold,
                                                     cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        _, _, _, _, pre_prob_nrem, \
        post_prob_nrem= self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, cells_to_use=cells_to_use)

        # need to compute control if want to plot
        if plotting:
            control=True
        if control is False:
            rem_mode_freq_norm_odd = None
            rem_mode_freq_norm_even = None
            nrem_mode_freq_norm_odd = None
            nrem_mode_freq_norm_even = None


        # pre_prob_rem_norm = pre_prob_rem / np.sum(pre_prob_rem, axis=1, keepdims=True)
        # pre_prob_nrem_norm = pre_prob_nrem / np.sum(pre_prob_nrem, axis=1, keepdims=True)

        # use PRE or POST results --> REM
        if pre_or_post=="pre":
            # rem pre map results and normalize
            rem_mode_freq = np.zeros(pre_prob_rem.shape[1])
            map_result_rem = np.argmax(pre_prob_rem, axis=1)
            prob_rem = pre_prob_rem

        elif pre_or_post=="post":
            # rem post map results and normalize
            rem_mode_freq = np.zeros(post_prob_rem.shape[1])
            map_result_rem = np.argmax(post_prob_rem, axis=1)
            prob_rem = post_prob_rem

        rem_mode_id, rem_mode_count = np.unique(map_result_rem, return_counts=True)
        rem_mode_freq[rem_mode_id] = rem_mode_count
        rem_mode_freq_norm = rem_mode_freq / np.sum(rem_mode_freq)

        if control:
            corr_within_rem = np.zeros(nr_shuffles)
            # check rem reactivation correlation with itself
            for shuffle_id in range(nr_shuffles):
                shuffled_ind = np.random.permutation(np.arange(map_result_rem.shape[0]))
                map_result_rem_even = map_result_rem[shuffled_ind[:int(0.5*shuffled_ind.shape[0])]]
                map_result_rem_odd = map_result_rem[shuffled_ind[int(0.5*shuffled_ind.shape[0]):]]
                # pre_map_result_rem_even = pre_map_result_rem[::2]
                # pre_map_result_rem_odd = pre_map_result_rem[1:][::2]
                rem_mode_id_even, rem_mode_count_even = np.unique(map_result_rem_even, return_counts=True)
                rem_mode_id_odd, rem_mode_count_odd = np.unique(map_result_rem_odd, return_counts=True)
                rem_mode_freq_even = np.zeros(prob_rem.shape[1])
                rem_mode_freq_even[rem_mode_id_even] = rem_mode_count_even
                rem_mode_freq_norm_even = rem_mode_freq_even / np.sum(rem_mode_freq_even)

                rem_mode_freq_odd = np.zeros(prob_rem.shape[1])
                rem_mode_freq_odd[rem_mode_id_odd] = rem_mode_count_odd
                rem_mode_freq_norm_odd = rem_mode_freq_odd / np.sum(rem_mode_freq_odd)

                corr_within_rem[shuffle_id] = pearsonr(rem_mode_freq_norm_even , rem_mode_freq_norm_odd)[0]
            #
            # plt.scatter(pre_rem_mode_freq_norm_even, pre_rem_mode_freq_norm_odd)
            # plt.title("REM: even vs. odd\n Pearson:" + str(
            #     pearsonr(pre_rem_mode_freq_norm_even, pre_rem_mode_freq_norm_odd)[0]) + "\n Spear:" +
            #           str(spearmanr(pre_rem_mode_freq_norm_even, pre_rem_mode_freq_norm_odd)[0]))
            # plt.show()

            # get nrem pre map results and normalize

        # use PRE or POST results --> REM
        if pre_or_post == "pre":
            # rem pre map results and normalize
            nrem_mode_freq = np.zeros(pre_prob_nrem.shape[1])
            map_result_nrem = np.argmax(pre_prob_nrem, axis=1)
            prob_nrem = pre_prob_nrem

        elif pre_or_post == "post":
            # rem post map results and normalize
            nrem_mode_freq = np.zeros(post_prob_nrem.shape[1])
            map_result_nrem = np.argmax(post_prob_nrem, axis=1)
            prob_nrem = post_prob_nrem

        nrem_mode_id, nrem_mode_count = np.unique(map_result_nrem, return_counts=True)
        nrem_mode_freq[nrem_mode_id] = nrem_mode_count
        nrem_mode_freq_norm = nrem_mode_freq / np.sum(nrem_mode_freq)

        if control:
            # check nrem reactivation correlation with itself
            corr_within_nrem = np.zeros(nr_shuffles)
            # check rem reactivation correlation with itself
            for shuffle_id in range(nr_shuffles):
                shuffled_ind = np.random.permutation(np.arange(map_result_nrem.shape[0]))
                map_result_nrem_even = map_result_nrem[shuffled_ind[:int(0.5*shuffled_ind.shape[0])]]
                map_result_nrem_odd = map_result_nrem[shuffled_ind[int(0.5*shuffled_ind.shape[0]):]]
                # pre_map_result_nrem_even = pre_map_result_nrem[::2]
                # pre_map_result_nrem_odd = pre_map_result_nrem[1:][::2]
                nrem_mode_id_even, nrem_mode_count_even = np.unique(map_result_nrem_even, return_counts=True)
                nrem_mode_id_odd, nrem_mode_count_odd = np.unique(map_result_nrem_odd, return_counts=True)
                nrem_mode_freq_even = np.zeros(prob_nrem.shape[1])
                nrem_mode_freq_even[nrem_mode_id_even] = nrem_mode_count_even
                nrem_mode_freq_norm_even = nrem_mode_freq_even / np.sum(nrem_mode_freq_even)

                nrem_mode_freq_odd = np.zeros(prob_nrem.shape[1])
                nrem_mode_freq_odd[nrem_mode_id_odd] = nrem_mode_count_odd
                nrem_mode_freq_norm_odd = nrem_mode_freq_odd / np.sum(nrem_mode_freq_odd)

                corr_within_nrem[shuffle_id] = pearsonr(nrem_mode_freq_norm_even, nrem_mode_freq_norm_odd)[0]
            # plt.scatter(pre_nrem_mode_freq_norm_even, pre_nrem_mode_freq_norm_odd)
            # plt.title("NREM: even vs. odd\n Pearson:" + str(
            #     pearsonr(pre_nrem_mode_freq_norm_even, pre_nrem_mode_freq_norm_odd)[0]) + "\n Spear:" +
            #           str(spearmanr(pre_nrem_mode_freq_norm_even, pre_nrem_mode_freq_norm_odd)[0]))
            # plt.show()

            # pre_rem_mode_freq_norm = pre_rem_mode_freq_norm[ pre_rem_mode_freq_norm<0.2]
            # pre_nrem_mode_freq_norm = pre_nrem_mode_freq_norm[pre_nrem_mode_freq_norm < 0.2]
        if plotting:
            binwidth=0.0001

            y1,x1,_ = plt.hist(corr_within_nrem, bins=np.arange(min(corr_within_nrem), max(corr_within_nrem) + binwidth, binwidth),
                     color="blue", label="within nrem", density=True)
            y2,x2,_ = plt.hist(corr_within_rem, bins=np.arange(min(corr_within_rem), max(corr_within_rem) + binwidth, binwidth),
                     color="red", label="within rem", density=True)

            r_across = pearsonr(rem_mode_freq_norm, nrem_mode_freq_norm)[0]
            plt.title(template_type+", Pearson R across = " +str(r_across))
            plt.legend()
            plt.xlabel("Pearson R")
            plt.show()

            plt.scatter(rem_mode_freq_norm, nrem_mode_freq_norm)
            plt.xlabel("REM")
            plt.ylabel("NREM")
            plt.title(template_type+", REM vs. NREM\n Pearson:"+str(pearsonr(rem_mode_freq_norm, nrem_mode_freq_norm)[0])+"\n Spear:"+
                      str(spearmanr(rem_mode_freq_norm, nrem_mode_freq_norm)[0]))

            plt.show()
        else:
            return rem_mode_freq_norm, nrem_mode_freq_norm, rem_mode_freq_norm_odd, rem_mode_freq_norm_even, \
                   nrem_mode_freq_norm_odd, nrem_mode_freq_norm_even

    def memory_drift_rem_nrem_decoding_cleanliness_per_mode(self, template_type, pre_file_name=None, post_file_name=None,
                                                  rem_pop_vec_threshold=100, cells_to_use="all", plotting=True,
                                                  nr_shuffles=500, control_data="rem"):

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        _, _, _, _, pre_prob_rem, post_prob_rem= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold,
                                                     cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        _, _, _, _, pre_prob_nrem, \
        post_prob_nrem= self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, cells_to_use=cells_to_use)

        pre_prob_rem_norm = pre_prob_rem / np.sum(pre_prob_rem, axis=1, keepdims=True)
        pre_prob_nrem_norm = pre_prob_nrem / np.sum(pre_prob_nrem, axis=1, keepdims=True)

        pre_prob_rem_map_mode = np.argmax(pre_prob_rem, axis=1)
        pre_prob_nrem_map_mode = np.argmax(pre_prob_nrem, axis=1)

        pre_prob_rem_map_val = np.max(pre_prob_rem_norm, axis=1)
        pre_prob_nrem_map_val = np.max(pre_prob_nrem_norm, axis=1)

        mean_ratio_per_mode = np.zeros(pre_prob_rem.shape[1])
        mean_ratio_per_mode[:] = np.nan
        mean_post_prob_rem = np.zeros(pre_prob_rem.shape[1])
        mean_post_prob_nrem = np.zeros(pre_prob_rem.shape[1])
        post_prob_rem = []
        post_prob_nrem = []
        for mode_id in range(pre_prob_rem.shape[1]):
            if np.count_nonzero(pre_prob_rem_map_val[pre_prob_rem_map_mode == mode_id]) and \
                np.count_nonzero(pre_prob_nrem_map_val[pre_prob_nrem_map_mode == mode_id]):
                mean_ratio_per_mode[mode_id] = np.mean(pre_prob_rem_map_val[pre_prob_rem_map_mode == mode_id])/\
                                           np.mean(pre_prob_nrem_map_val[pre_prob_nrem_map_mode == mode_id])

                mean_post_prob_rem[mode_id] = np.mean(pre_prob_rem_map_val[pre_prob_rem_map_mode == mode_id])
                mean_post_prob_nrem[mode_id] = np.mean(pre_prob_nrem_map_val[pre_prob_nrem_map_mode == mode_id])
                post_prob_rem.append(pre_prob_rem_map_val[pre_prob_rem_map_mode == mode_id])
                post_prob_nrem.append(pre_prob_nrem_map_val[pre_prob_nrem_map_mode == mode_id])

        # mean_ratio_per_mode = mean_ratio_per_mode[~np.isnan(mean_ratio_per_mode)]

        shuffled_rem_per_mode = []
        shuffled_nrem_per_mode = []

        for mode_id in range(pre_prob_rem.shape[1]):
            mean_ratio_per_mode_rem = np.zeros(nr_shuffles)
            mean_ratio_per_mode_rem[:] = np.nan
            mean_ratio_per_mode_nrem = np.zeros(nr_shuffles)
            mean_ratio_per_mode_nrem[:] = np.nan
            all_rem = pre_prob_rem_map_val[pre_prob_rem_map_mode == mode_id]
            all_nrem = pre_prob_nrem_map_val[pre_prob_nrem_map_mode == mode_id]

            for shuffle_id in range(nr_shuffles):

                all_rem_shuffled = np.random.permutation(all_rem)
                mean_ratio_per_mode_rem[shuffle_id] = np.mean(all_rem_shuffled[:int(all_rem_shuffled.shape[0]*0.5)])/\
                                                   np.mean(all_rem_shuffled[int(all_rem_shuffled.shape[0]*0.5):])

                all_nrem_shuffled = np.random.permutation(all_nrem)
                mean_ratio_per_mode_nrem[shuffle_id] = np.mean(all_nrem_shuffled[:int(all_nrem_shuffled.shape[0]*0.5)])/\
                                                   np.mean(all_nrem_shuffled[int(all_nrem_shuffled.shape[0]*0.5):])

            shuffled_rem_per_mode.append(mean_ratio_per_mode_rem)
            shuffled_nrem_per_mode.append(mean_ratio_per_mode_nrem)

        # go through all modes and check if they are signficantly greater than within
        sign_diff_modes = np.zeros(pre_prob_rem.shape[1])

        for mode_id, (mode_ratio, nrem_control, rem_control) in enumerate(zip(mean_ratio_per_mode, shuffled_nrem_per_mode,
                                                                    shuffled_rem_per_mode)):

            if control_data == "nrem":
                control = nrem_control
            elif control_data == "rem":
                control = rem_control

            if np.count_nonzero(np.isnan(control)) > 0:
                continue

            if mode_ratio < np.mean(control) - 2 * np.std(control) or \
                np.mean(control)+2*np.std(control) < mode_ratio:

                sign_diff_modes[mode_id] = 1

                # plt.hist(nrem_control, color="gray")
                # plt.vlines(mode_ratio, 0, 20, color="red", label="data")
                # plt.vlines(np.mean(nrem_control) + 2 * np.std(nrem_control), 0, 20, color="yellow", label="2std")
                # plt.vlines(np.mean(nrem_control) - 2 * np.std(nrem_control), 0, 20, color="yellow")
                # plt.title("mode" + str(mode_id))
                # plt.legend()
                # plt.show()

        # only select modes that are significantly different
        mean_ratio_per_mode_significant = mean_ratio_per_mode[sign_diff_modes.astype(bool)]
        nr_sign_diff_modes = np.count_nonzero(sign_diff_modes)
        nr_modes = pre_prob_rem.shape[1]
        mean_post_prob_rem_significant = mean_post_prob_rem[sign_diff_modes.astype(bool)]
        mean_post_prob_nrem_significant = mean_post_prob_nrem[sign_diff_modes.astype(bool)]

        post_prob_rem_significant = []
        post_prob_nrem_significant = []

        for rem_dat, nrem_dat, signif_ind in zip(post_prob_rem, post_prob_nrem, sign_diff_modes):
            if signif_ind:
                post_prob_rem_significant.extend(rem_dat)
                post_prob_nrem_significant.extend(nrem_dat)

        if plotting:
            g = sns.kdeplot(mean_post_prob_rem_significant, fill=True, color="red", label="REM")
            max_y = g.viewLim.bounds[3]
            g = sns.kdeplot(mean_post_prob_nrem_significant, fill=True, color="blue", label="NREM")
            max_y = g.viewLim.bounds[3]
            plt.show()
            plt.hist(mean_ratio_per_mode_significant)
            plt.show()
        else:
            return mean_ratio_per_mode_significant, mean_ratio_per_mode, mean_post_prob_rem_significant, \
                   mean_post_prob_nrem_significant, post_prob_rem_significant, post_prob_nrem_significant, \
                   nr_sign_diff_modes, nr_modes

    def memory_drift_rem_nrem_decoding_cleanliness(self, template_type, pre_file_name=None, post_file_name=None,
                                                  rem_pop_vec_threshold=100, cells_to_use="all", plotting=True):

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        _, _, _, _, pre_prob_rem, post_prob_rem= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold,
                                                     cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        _, _, _, _, pre_prob_nrem, \
        post_prob_nrem= self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, cells_to_use=cells_to_use)

        # compute mean of all likelihoods to threshold
        mean_likeli_nrem = np.mean(pre_prob_nrem.flatten())
        pre_prob_nrem = pre_prob_nrem[np.max(pre_prob_nrem, axis=1) > mean_likeli_nrem]

        # compute ratio of highest and second highest likelihood NREM
        max_likeli_ind_nrem = np.argmax(pre_prob_nrem, axis=1)
        max_likeli_nrem = np.max(pre_prob_nrem, axis=1)

        m, n = pre_prob_nrem.shape
        pre_prob_nrem_wo_max = pre_prob_nrem[np.arange(n) != np.array(max_likeli_ind_nrem)[:, None]].reshape(m, -1)
        second_max_likeli_nrem = np.max(pre_prob_nrem_wo_max, axis=1)

        nrem_first_second_max_ratio = max_likeli_nrem / second_max_likeli_nrem
        

        # compute mean of likelihoods
        mean_likeli_rem = np.mean(pre_prob_rem.flatten())
        pre_prob_rem = pre_prob_rem[np.max(pre_prob_rem, axis=1) > mean_likeli_rem]


        # compute ratio of highest and second highest likelihood REM
        max_likeli_ind_rem = np.argmax(pre_prob_rem, axis=1)
        max_likeli_rem = np.max(pre_prob_rem, axis=1)

        m, n = pre_prob_rem.shape
        pre_prob_rem_wo_max = pre_prob_rem[np.arange(n) != np.array(max_likeli_ind_rem)[:, None]].reshape(m, -1)
        second_max_likeli_rem = np.max(pre_prob_rem_wo_max, axis=1)

        rem_first_second_max_ratio = max_likeli_rem / second_max_likeli_rem

        p_ratio_rem = 1. * np.arange(rem_first_second_max_ratio.shape[0]) / (rem_first_second_max_ratio.shape[0] - 1)
        p_ratio_nrem = 1. * np.arange(nrem_first_second_max_ratio.shape[0]) / (nrem_first_second_max_ratio.shape[0] - 1)

        pre_prob_nrem_norm = pre_prob_nrem / np.sum(pre_prob_nrem, axis=1, keepdims=True)
        pre_prob_rem_norm = pre_prob_rem / np.sum(pre_prob_rem, axis=1, keepdims=True)

        max_prob_nrem = np.max(pre_prob_nrem_norm, axis=1)
        max_prob_rem = np.max(pre_prob_rem_norm, axis=1)

        p_prob_rem = 1. * np.arange(max_prob_rem.shape[0]) / (max_prob_rem.shape[0] - 1)
        p_prob_nrem = 1. * np.arange(max_prob_nrem.shape[0]) / (max_prob_nrem.shape[0] - 1)

        if plotting:

            plt.plot(np.sort(rem_first_second_max_ratio), p_ratio_rem, label="REM")
            plt.plot(np.sort(nrem_first_second_max_ratio), p_ratio_nrem, label="NREM")
            plt.xscale("log")
            plt.legend()
            plt.xlabel("max. likeli / second largest likeli")
            plt.ylabel("cdf")
            plt.show()
            print(mannwhitneyu(rem_first_second_max_ratio, nrem_first_second_max_ratio, alternative="greater"))

            plt.plot(np.sort(max_prob_rem), p_prob_rem, label="REM")
            plt.plot(np.sort(max_prob_nrem), p_prob_nrem, label="NREM")
            plt.ylabel("cdf")
            plt.xlabel("max. prob.")
            plt.legend()
            plt.show()

            print(mannwhitneyu(max_prob_rem, max_prob_nrem, alternative="greater"))

        else:
            return rem_first_second_max_ratio, nrem_first_second_max_ratio, max_prob_rem, max_prob_nrem

    def memory_drift_rem_nrem_decoding_visualization(self, template_type, pre_file_name=None, post_file_name=None,
                                                  rem_pop_vec_threshold=100, only_stable_cells=False):

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_rem, post_prob_rem= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold,
                                                     only_stable_cells=only_stable_cells)


        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem= self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, only_stable_cells=only_stable_cells)

        pre_prob_rem_log = np.log(pre_prob_rem)
        pre_prob_nrem_log = np.log(pre_prob_nrem)

        pre_prob_rem_log_mean = np.mean(pre_prob_rem_log, axis=0)
        pre_prob_nrem_log_mean = np.mean(pre_prob_nrem_log, axis=0)
        pre_prob_rem_mean = np.mean(pre_prob_rem, axis=0)
        pre_prob_nrem_mean = np.mean(pre_prob_nrem, axis=0)

        pre_prob_rem = pre_prob_rem_log[0::50,:]
        pre_prob_nrem = pre_prob_nrem_log[0::50,:]

        comb_mean = np.vstack((pre_prob_rem, pre_prob_nrem)).T
        sep = np.array([pre_prob_rem.shape[0], comb_mean.shape[1]])

        result = multi_dim_scaling(act_mat=comb_mean, param_dic=self.params)

        if self.params.dr_method_p2 == 3:
            # create figure instance
            fig = plt.figure()
            ax = fig.add_subplot(111, projection='3d')
            plot_3D_scatter(ax=ax, mds=result, params=self.params, data_sep=sep)
        else:
            fig = plt.figure()
            ax = fig.add_subplot(111)
            plot_2D_scatter(ax=ax, mds=result, params=self.params, data_sep=sep, labels=["REM", "NREM"])
        handles, labels = ax.get_legend_handles_labels()
        by_label = OrderedDict(zip(labels, handles))
        ax.legend(by_label.values(), by_label.keys())
        plt.show()

        comb_min = np.min(np.hstack((pre_prob_nrem_log_mean, pre_prob_rem_log_mean)))
        comb_max = np.max(np.hstack((pre_prob_nrem_log_mean, pre_prob_rem_log_mean)))
        plt.subplot(1,2,1)
        plt.imshow(np.expand_dims(pre_prob_nrem_log_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        a=plt.colorbar()
        a.set_label("MEAN LOG-PROB.")
        plt.ylabel("MODE ID")
        plt.title("NREM")
        plt.gca().set_xticklabels([])
        plt.subplot(1,2,2)
        plt.imshow(np.expand_dims(pre_prob_rem_log_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        a=plt.colorbar()
        a.set_label("MEAN LOG-PROB.")
        plt.title("REM")
        plt.gca().set_xticklabels([])
        plt.show()

        comb_min = np.min(np.hstack((pre_prob_nrem_mean, pre_prob_rem_mean)))
        comb_max = np.max(np.hstack((pre_prob_nrem_mean, pre_prob_rem_mean)))
        plt.subplot(1,2,1)
        plt.imshow(np.expand_dims(pre_prob_nrem_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        a=plt.colorbar()
        a.set_label("MEAN PROB.")
        plt.ylabel("MODE ID")
        plt.title("NREM")
        plt.gca().set_xticklabels([])
        plt.subplot(1,2,2)
        plt.imshow(np.expand_dims(pre_prob_rem_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        a=plt.colorbar()
        a.set_label("MEAN PROB.")
        plt.title("REM")
        plt.gca().set_xticklabels([])
        plt.show()

        plt.imshow(pre_prob_rem_log.T, interpolation='nearest', aspect='auto')
        a = plt.colorbar()
        a.set_label("LOG(PROB)")
        plt.xlabel("POP.VEC.ID")
        plt.ylabel("MODE ID")
        plt.title("REM")
        plt.show()

        plt.imshow(pre_prob_nrem_log.T, interpolation='nearest', aspect='auto')
        plt.xlabel("POP.VEC.ID")
        plt.ylabel("MODE ID")
        a = plt.colorbar()
        a.set_label("LOG(PROB)")
        plt.title("NREM")
        plt.show()

    def memory_drift_rem_nrem_decoding_temporal_visualization(self, template_type, pre_file_name=None,
                                                              post_file_name=None, only_stable_cells=False):

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_list_rem, post_prob_list_rem, event_times_list_rem= \
            self.memory_drift_long_sleep_get_raw_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                         only_stable_cells=only_stable_cells)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_list_nrem, post_prob_list_nrem, event_times_list_nrem= self.memory_drift_long_sleep_get_raw_results(
            template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     only_stable_cells=only_stable_cells)


        time_per_rem_vec = []
        # assign temporal label to all vectors
        for prob_l, e_t in zip(pre_prob_list_rem, event_times_list_rem):
            time_per_rem_vec.append(np.linspace(e_t[0],e_t[1], prob_l.shape[0]))

        time_per_rem_vec = np.hstack(time_per_rem_vec)
        pre_prob_rem = np.vstack(pre_prob_list_rem)


        time_per_nrem_vec = []
        # assign temporal label to all vectors
        for prob_l, e_t in zip(pre_prob_list_nrem, event_times_list_nrem):
            time_per_nrem_vec.append(np.linspace(e_t[0],e_t[1], prob_l.shape[0]))

        time_per_nrem_vec = np.hstack(time_per_nrem_vec)
        pre_prob_nrem = np.vstack(pre_prob_list_nrem)

        sampling = 50

        pre_prob_rem_log = np.log(pre_prob_rem)
        pre_prob_nrem_log = np.log(pre_prob_nrem)

        pre_prob_rem_log = pre_prob_rem_log[0::sampling,:]
        time_per_rem_vec = time_per_rem_vec[0::sampling]
        pre_prob_nrem_log = pre_prob_nrem_log[0::sampling,:]
        time_per_nrem_vec = time_per_nrem_vec[0::sampling]

        comb = np.vstack((pre_prob_rem_log, pre_prob_nrem_log)).T
        # sep = np.array([pre_prob_rem_log.shape[0], comb.shape[1]])
        sep = pre_prob_rem_log.shape[0]

        result = multi_dim_scaling(act_mat=comb, param_dic=self.params)

        rem_res = result[:sep,:]
        nrem_res = result[sep:, :]

        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')

        ax.scatter(rem_res[:,0], rem_res[:,1], time_per_rem_vec / 60, color="r", alpha=0.5)
        ax.scatter(nrem_res[:, 0], nrem_res[:, 1], time_per_nrem_vec / 60, color="b", alpha=0.5)
        for plot_c in range(rem_res.shape[0]-1):
            ax.plot(rem_res[plot_c:plot_c+2,0], rem_res[plot_c:plot_c+2,1], time_per_rem_vec[plot_c:plot_c+2] / 60,
                    c="lightcoral", alpha=0.5)

        for plot_c in range(nrem_res.shape[0]-1):
            ax.plot(nrem_res[plot_c:plot_c+2,0], nrem_res[plot_c:plot_c+2,1], time_per_nrem_vec[plot_c:plot_c+2] / 60,
                    c="royalblue", alpha=0.8)

        # hide labels
        ax.set_yticklabels([])
        ax.set_xticklabels([])
        ax.set_zlabel("Time / min")
        # set pane alpha value to zero --> transparent
        ax.w_xaxis.set_pane_color((0.8, 0.8, 0.8, 0.0))
        ax.w_yaxis.set_pane_color((0.8, 0.8, 0.8, 0.0))
        ax.w_zaxis.set_pane_color((0.8, 0.8, 0.8, 0.0))
        plt.show()

    def memory_drift_decoding_similarity_temporal(self, template_type, pre_file_name=None, samples_per_epoch = 40,
                                                  compare_with_previous=True, post_file_name=None,
                                                  only_stable_cells=False):

        # with open(self.params.pre_proc_dir+"temp_data/"+"test", 'rb') as f:
        #     per_event_max_corr = pickle.load(f)
        # start = 0
        # for i, event in enumerate(per_event_max_corr):
        #     length_event = event.shape[0]
        #     plt.plot(range(start, start+length_event),event)
        #     start += length_event
        #     plt.xlabel("Pop.Vec.ID (REM)")
        #     plt.ylabel("Max. corr. with previous NREM")
        #     plt.title("REM EPOCH "+str(i))
        #     plt.show()
        # exit()
        #
        #
        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_list_rem, post_prob_list_rem, event_times_list_rem= \
            self.memory_drift_long_sleep_get_raw_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                         pop_vec_threshold=10, only_stable_cells=only_stable_cells)

        # pre_prob_arr_rem = np.vstack(pre_prob_list_rem)
        length_per_event_rem = [x.shape[0] for x in pre_prob_list_rem]
        event_times_rem = np.vstack(event_times_list_rem)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_list_nrem, post_prob_list_nrem, event_times_list_nrem= \
            self.memory_drift_long_sleep_get_raw_results(template_type=template_type, pre_file_name=pre_file_name,
                                                         post_file_name=post_file_name, part_to_analyze="nrem",
                                                         only_stable_cells=only_stable_cells)

        length_per_event_nrem = [x.shape[0] for x in pre_prob_list_nrem]
        event_times_nrem = np.vstack(event_times_list_nrem)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------------------------------

        all_events_pre_prob = pre_prob_list_rem + pre_prob_list_nrem
        all_events_length = length_per_event_rem + length_per_event_nrem
        labels_events = np.zeros(len(pre_prob_list_rem) + len(pre_prob_list_nrem))
        labels_events[:len(pre_prob_list_rem)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:, 0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_pre_prob_list = [x for _, x in sorted(zip(all_times, all_events_pre_prob))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_pre_prob = np.vstack(sorted_events_pre_prob_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat == 1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0] + 1
                merged_events_labels.append(np.unique(sorted_labels_events[first:first+trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat == -1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0] + 1
                merged_events_labels.append(np.unique(sorted_labels_events[first:first+trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))

        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps == 1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre_post ratio) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_per_merged_rem_event = []
        pre_prob_per_merged_nrem_event = []
        pre_prob_rem_nrem_events = []
        rem_nrem_events_label = []
        pre_prob_rem_nrem_pop_vec = []
        rem_nrem_pop_vec_label = []

        for start_event, end_event in zip(start, end):
            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                pre_prob_per_merged_rem_event.append(sorted_pre_prob[start_event:end_event])
            # nrem event
            else:
                pre_prob_per_merged_nrem_event.append(sorted_pre_prob[start_event:end_event])

            pre_prob_rem_nrem_events.append(sorted_pre_prob[start_event:end_event])
            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])

            pre_prob_rem_nrem_pop_vec.extend(sorted_pre_prob[start_event:end_event])
            rem_nrem_pop_vec_label.extend(labels_per_pop_vec[start_event:end_event])

        merged_events_times = np.vstack(merged_events_times)

        merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        merged_nrem_event_times = merged_events_times[merged_events_labels == 0]

        # take samples from each epoch --> otherwise MDS won't work (too much data)
        pre_prob_per_merged_rem_event_samples = []
        for epoch in pre_prob_per_merged_rem_event:
            sample_ind = np.random.randint(0, epoch.shape[0], size=samples_per_epoch)
            pre_prob_per_merged_rem_event_samples.append(epoch[sample_ind, :])

        pre_prob_per_merged_nrem_event_samples = []
        for epoch in pre_prob_per_merged_nrem_event:
            sample_ind = np.random.randint(0, epoch.shape[0], size=samples_per_epoch)
            pre_prob_per_merged_nrem_event_samples.append(epoch[sample_ind, :])

        nr_rem_epochs = len(pre_prob_per_merged_rem_event_samples)
        nr_nrem_epochs = len(pre_prob_per_merged_nrem_event_samples)

        merged_data = np.vstack((np.vstack(pre_prob_per_merged_rem_event_samples),
                                 np.vstack(pre_prob_per_merged_nrem_event_samples)))

        merged_data = np.log(merged_data)
        # apply multidimensional scaling using correlations
        D = pairwise_distances(merged_data, metric="correlation")

        model = MDS(n_components=2, dissimilarity='precomputed', random_state=1)
        results = model.fit_transform(D)

        plt.scatter(results[:(samples_per_epoch*nr_rem_epochs),0], results[:(samples_per_epoch*nr_rem_epochs),1],
                    color="r")
        plt.scatter(results[(samples_per_epoch * nr_rem_epochs):, 0], results[(samples_per_epoch * nr_rem_epochs):, 1],
                    color="b")
        plt.show()

        # split again into REM and NREM
        split_results = np.split(results, nr_rem_epochs+nr_nrem_epochs)
        rem_results = split_results[:nr_rem_epochs]
        nrem_results = split_results[nr_rem_epochs:]

        # go through all epochs and compute spread
        rem_area = []
        rem_centers = []
        for rem_epoch in rem_results:
            # find mean
            m = np.mean(rem_epoch, axis=0)
            # apply pca to find ellipse that spans the data
            centered = rem_epoch - m
            # covariance
            c = centered.transpose() @ centered
            ev = np.linalg.eig(c)
            trans = centered @ ev[1]
            ellipse_wh = np.max(np.abs(trans), axis=0)
            area = np.round(np.pi*ellipse_wh[0]*ellipse_wh[1],3)
            # fig, ax = plt.subplots()
            # from matplotlib.patches import Ellipse
            # ax.scatter(trans[:,0], trans[:,1], color="red", label="LIKELIHOOD VECTORS, MDS")
            # e = Ellipse(xy=[0,0], width=2*ellipse_wh[0], height=2*ellipse_wh[1], facecolor="None", edgecolor="r")
            # ax.add_artist(e)
            # plt.xlim(-1.5, 1.5)
            # plt.ylim(-1.5, 1.5)
            # plt.xlabel("FIRST PC")
            # plt.ylabel("SECOND PC")
            # plt.legend()
            # plt.title("AREA ELLIPSE = "+str(area))
            # plt.show()

            rem_area.append(area)
            rem_centers.append(m)

        # go through all epochs and compute spread
        nrem_area = []
        nrem_centers = []
        for nrem_epoch in nrem_results:
            # find mean
            m = np.mean(nrem_epoch, axis=0)
            # apply pca to find ellipse that spans the data
            centered = nrem_epoch - m
            # covariance
            c = centered.transpose() @ centered
            ev = np.linalg.eig(c)
            trans = centered @ ev[1]
            ellipse_wh = np.max(np.abs(trans), axis=0)
            area = np.round(np.pi*ellipse_wh[0]*ellipse_wh[1],3)
            # fig, ax = plt.subplots()
            # from matplotlib.patches import Ellipse
            # ax.scatter(trans[:,0], trans[:,1], color="blue", label="LIKELIHOOD VECTORS, MDS")
            # e = Ellipse(xy=[0,0], width=2*ellipse_wh[0], height=2*ellipse_wh[1], facecolor="None", edgecolor="b")
            # ax.add_artist(e)
            # plt.xlim(-1.5, 1.5)
            # plt.ylim(-1.5, 1.5)
            # plt.xlabel("FIRST PC")
            # plt.ylabel("SECOND PC")
            # plt.legend()
            # plt.title("AREA ELLIPSE = "+str(area))
            # plt.show()
            nrem_area.append(area)
            nrem_centers.append(m)

        # Create figure and axes
        fig, ax = plt.subplots()
        prev_center = np.array([0,0])
        for i, (area, center, event_time) in enumerate(zip(nrem_area, nrem_centers, merged_nrem_event_times)):
            # center nrem around zero
            dist_from_prev = 0
            ax.hlines(dist_from_prev, event_time[0], event_time[1], colors="b", zorder=1000, label="CENTER NREM")
            rect = patches.Rectangle((event_time[0], dist_from_prev-0.5*area), event_time[1]-event_time[0], area, linewidth=1,
                                     edgecolor='None', facecolor='lightblue', zorder=800, label="SPREAD NREM")
            ax.add_patch(rect)

        nrem_centers = np.array(nrem_centers)
        rem_centers = np.array(rem_centers)
        # compute distance between rem and previous nrem
        if merged_events_labels[0] == 1:
            # first event is a REM event
            rem_centers = rem_centers[1:,:]
            rem_area = rem_area[1:]
            merged_rem_event_times = merged_rem_event_times[1:,:]
            nrem_centers = nrem_centers[:rem_centers.shape[0],:]
        elif merged_events_labels[0] == 0:
            nrem_centers = nrem_centers[:rem_centers.shape[0],:]

        dist = np.linalg.norm(nrem_centers-rem_centers, axis=1)

        for i, (area, d, event_time) in enumerate(zip(rem_area, dist, merged_rem_event_times)):
            # compute correlations within
            ax.hlines(d, event_time[0], event_time[1], colors="r", zorder=1000, label="REM DIST. TO PREV. NREM")
            rect = patches.Rectangle((event_time[0], d-0.5*area), event_time[1]-event_time[0], area, linewidth=1,
                                     edgecolor='None', facecolor='mistyrose', zorder=800, label="SPREAD REM")
            ax.add_patch(rect)

        handles, labels = plt.gca().get_legend_handles_labels()
        by_label = OrderedDict(zip(labels, handles))
        plt.legend(by_label.values(), by_label.keys())
        plt.xlabel("TIME / s")
        plt.ylabel("CENTER & SPREAD OF ELLIPSE")
        plt.show()

        per_event_max_corr = []

    def memory_drift_rem_nrem_decoding_linear_separability(self, template_type, pre_file_name=None, post_file_name=None,
                                                  rem_pop_vec_threshold=100, log_transform=False):

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_rem, post_prob_rem= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold)
        ratio_rem = np.hstack(ratio_per_rem_event)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem= self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2)
        ratio_nrem = np.hstack(ratio_per_nrem_event)

        # dat_1 = np.log(pre_prob_rem[0::50, :])
        # dat_2 = np.log(pre_prob_nrem[0::50, :])

        nr_samples = 2000

        ind_rem = np.random.choice(pre_prob_rem.shape[0], nr_samples, replace=False)
        ind_nrem = np.random.choice(pre_prob_nrem.shape[0], nr_samples, replace=False)

        dat_1 = pre_prob_rem[ind_rem, :]
        dat_2 = pre_prob_nrem[ind_nrem, :]

        if log_transform:
            dat_1 = np.log(dat_1)
            dat_2 = np.log(dat_2)

        # dat_1 = pre_prob_rem[0::20, :]
        # dat_2 = pre_prob_nrem[0::20, :]

        # linear separability of probability vectors
        data = np.vstack((dat_1, dat_2)).T
        labels = np.zeros(dat_1.shape[0]+dat_2.shape[0])
        # rem --> 1
        labels[:dat_1.shape[0]+1] = 1

        acc_rem_list = []
        acc_nrem_list = []
        acc_overal_list = []
        # try linear separability for different test/training set
        for iter in range(10):
            acc_rem, acc_nrem, acc = MlMethodsOnePopulation(params=self.params).linear_separability(
                input_data=data, input_labels=labels)
            acc_rem_list.append(acc_rem)
            acc_nrem_list.append(acc_nrem)
            acc_overal_list.append(acc)

        c = "white"

        acc_nrem = np.array(acc_nrem_list)
        acc_rem = np.array(acc_rem_list)
        acc_overal = np.array(acc_overal_list)
        res = np.vstack((acc_nrem, acc_rem, acc_overal)).T

        bplot=plt.boxplot(res, positions=[1, 2, 3], patch_artist=True,
                    labels=["NREM", "REM", "ALL"],
                    boxprops=dict(color=c),
                    capprops=dict(color=c),
                    whiskerprops=dict(color=c),
                    flierprops=dict(color=c, markeredgecolor=c),
                    medianprops=dict(color=c),
                    )
        colors = ['blue', 'red', "grey"]
        for patch, color in zip(bplot['boxes'], colors):
            patch.set_facecolor(color)
        if log_transform:
            plt.title("LINEAR SEPARABILITY OF LOG-LIKELIHOOD VECTORS")
        else:
            plt.title("LINEAR SEPARABILITY OF LIKELIHOOD VECTORS")
        plt.ylabel("ACCURACY (10 SPLITS)")
        plt.grid(color="grey", axis="y")
        plt.show()

    def memory_drift_rem_nrem_likelihoods(self, template_type, pre_file_name=None, post_file_name=None,
                                                  rem_pop_vec_threshold=100, plotting=True, save_fig=False):


        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_rem, post_prob_rem= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem= self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                      post_file_name=post_file_name, part_to_analyze="nrem",
                                                      pop_vec_threshold=2)

        pre_likeli_rem_max = np.max(pre_prob_rem, axis=1)
        pre_likeli_nrem_max = np.max(pre_prob_nrem, axis=1)

        # normalize --> maximal posterior probability
        pre_posterior_prob_rem = pre_prob_rem / np.sum(pre_prob_rem, axis=1, keepdims=True)
        pre_posterior_prob_rem_max = np.max(pre_posterior_prob_rem, axis=1)

        pre_posterior_prob_nrem = pre_prob_nrem / np.sum(pre_prob_nrem, axis=1, keepdims=True)
        pre_posterior_prob_nrem_max = np.max(pre_posterior_prob_nrem, axis=1)

        pre_likeli_rem_flat = pre_prob_rem.flatten()
        pre_likeli_nrem_flat = pre_prob_nrem.flatten()

        if plotting or save_fig:

            if save_fig:
                plt.style.use('default')

            p_mwu = mannwhitneyu(pre_likeli_rem_max, pre_likeli_nrem_max, alternative="greater")
            print("Max. likelihoods, MWU-test: p-value = " + str(p_mwu))


            pre_prob_rem_max_sorted = np.sort(pre_likeli_rem_max)
            pre_prob_nrem_max_sorted = np.sort(pre_likeli_nrem_max)

            p_rem = 1. * np.arange(pre_likeli_rem_max.shape[0]) / (pre_likeli_rem_max.shape[0] - 1)
            p_nrem = 1. * np.arange(pre_likeli_nrem_max.shape[0]) / (pre_likeli_nrem_max.shape[0] - 1)
            plt.plot(pre_prob_rem_max_sorted, p_rem, color="red", label="REM")
            plt.plot(pre_prob_nrem_max_sorted, p_nrem, color="blue", label="NREM")
            plt.gca().set_xscale("log")
            plt.xlabel("max. likelihood per PV")
            plt.ylabel("CDF")
            plt.legend()
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "decoding_max_likelihoods"+self.session_name+".svg"),
                            transparent="True")
                plt.close()
            else:
                plt.show()

            pre_prob_rem_flat_sorted = np.sort(pre_likeli_rem_flat)
            pre_prob_nrem_flat_sorted = np.sort(pre_likeli_nrem_flat)

            p_rem_flat = 1. * np.arange(pre_likeli_rem_flat.shape[0]) / (pre_likelib_rem_flat.shape[0] - 1)
            p_nrem_flat = 1. * np.arange(pre_likeli_nrem_flat.shape[0]) / (pre_likeli_nrem_flat.shape[0] - 1)
            plt.plot(pre_prob_rem_flat_sorted, p_rem_flat, color="red", label="REM")
            plt.plot(pre_prob_nrem_flat_sorted, p_nrem_flat, color="blue", label="NREM")
            plt.gca().set_xscale("log")
            plt.xlabel("Likelihoods per PV")
            plt.ylabel("CDF")
            plt.legend()
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "decoding_likelihoods"+self.session_name+".svg"),
                            transparent="True")
            else:
                plt.show()

        else:
            return pre_likeli_rem_max, pre_likeli_nrem_max, pre_likeli_rem_flat, pre_likeli_nrem_flat, \
                   pre_posterior_prob_nrem_max, pre_posterior_prob_rem_max

    def memory_drift_rem_nrem_autocorrelation_temporal(self, template_type, pre_file_name=None, post_file_name=None,
                                                       rem_pop_vec_threshold=100, plot_for_control=True, plotting=True,
                                                       bootstrapping=False, duration_for_autocorrelation_rem=10,
                                                       duration_for_autocorrelation_nrem=10, save_fig=False):

        # get median bin duration of constant spike bins
        nrem_bin_dur, rem_bin_dur = self.get_constant_spike_bin_length(plotting=False, return_median=True)

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_rem, post_prob_rem = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold)
        ratio_rem = np.hstack(ratio_per_rem_event)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem = self.memory_drift_long_sleep_get_results(template_type=template_type,
                                                                  pre_file_name=pre_file_name,
                                                                  post_file_name=post_file_name, part_to_analyze="nrem",
                                                                  pop_vec_threshold=2)
        ratio_nrem = np.hstack(ratio_per_nrem_event)

        auto_corr_rem = []
        shift_array_nrem = np.arange(int(-duration_for_autocorrelation_nrem/nrem_bin_dur),
                                     int(duration_for_autocorrelation_nrem/nrem_bin_dur))
        shift_array_rem = np.arange(int(-duration_for_autocorrelation_rem/rem_bin_dur),
                                    int(duration_for_autocorrelation_rem/rem_bin_dur))
        for mode in range(pre_prob_rem.shape[1]):
            ac, _ = cross_correlate(pre_prob_rem[:, mode], pre_prob_rem[:, mode], shift_array=shift_array_rem)
            auto_corr_rem.append(ac)
        auto_corr_rem = np.vstack(auto_corr_rem)
        mean_auto_corr_rem = np.mean(auto_corr_rem, axis=0)

        auto_corr_nrem = []
        for mode in range(pre_prob_rem.shape[1]):
            ac, _ = cross_correlate(pre_prob_nrem[:, mode], pre_prob_nrem[:, mode], shift_array=shift_array_nrem)
            auto_corr_nrem.append(ac)
        auto_corr_nrem = np.vstack(auto_corr_nrem)
        mean_auto_corr_nrem = np.mean(auto_corr_nrem, axis=0)

        # auto_corr_raw = np.correlate(ratio_all, ratio_all, mode="full")
        auto_corr_raw_rem, shift_array = cross_correlate(ratio_rem, ratio_rem, shift_array=shift_array_rem)
        auto_corr_raw_nrem, _ = cross_correlate(ratio_nrem, ratio_nrem, shift_array=shift_array_nrem)

        # z-score data
        auto_corr_raw_nrem_z = (auto_corr_raw_nrem - np.mean(auto_corr_raw_nrem[:50]))/np.std(auto_corr_raw_nrem[:50])
        auto_corr_raw_rem_z = (auto_corr_raw_rem - np.mean(auto_corr_raw_rem[:50])) / np.std(auto_corr_raw_rem[:50])

        auto_corr_raw_nrem_z[int(auto_corr_raw_nrem.shape[0] / 2)] = np.nan
        auto_corr_raw_rem_z[int(auto_corr_raw_rem.shape[0] / 2)] = np.nan

        if plotting or save_fig:

            if save_fig:
                plt.style.use('default')
            plt.plot(shift_array_nrem*nrem_bin_dur, auto_corr_raw_nrem_z, c="b", label="NREM")
            plt.plot(shift_array_rem*rem_bin_dur, auto_corr_raw_rem_z, c="r", label="REM")
            plt.title("Auto-correlation of sim_ratio")
            plt.xlabel("Time (s)")
            plt.ylabel("z-scored Pearson correlation of sim_ratio")
            plt.xscale("symlog")
            plt.legend()
            # plt.xlim(-4, 4)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "sim_ratio_autocorr_temporal.svg"), transparent="True")
                plt.close()
            else:
                plt.show()

        nrem_test_data = auto_corr_raw_nrem_z[int(auto_corr_raw_nrem.shape[0] / 2)+1:]
        rem_test_data = auto_corr_raw_rem_z[int(auto_corr_raw_rem.shape[0] / 2) + 1:]

        def exponential(x, a, k, b):
            return a * np.exp(x * k) + b

        popt_exponential_rem, pcov_exponential_rem = optimize.curve_fit(exponential, np.arange(rem_test_data.shape[0])*rem_bin_dur,
                                                                        rem_test_data, p0=[1, -0.5, 1])
        popt_exponential_nrem, pcov_exponential_nrem = optimize.curve_fit(exponential, np.arange(nrem_test_data.shape[0])*nrem_bin_dur,
                                                                        nrem_test_data, p0=[1, -0.5, 1])
        if plotting or save_fig:

            if save_fig:
                plt.style.use('default')
            # plot fits
            plt.text(3, 10, "k = " +str(np.round(popt_exponential_rem[1], 2)), c="red" )
            plt.scatter(np.arange(rem_test_data.shape[0])*rem_bin_dur, rem_test_data, c="salmon", label="REM data")
            plt.plot((np.arange(rem_test_data.shape[0])*rem_bin_dur)[1:], exponential((np.arange(rem_test_data.shape[0])*rem_bin_dur)[1:],
                                                                    a=popt_exponential_rem[0], k=popt_exponential_rem[1],
                                                                    b=popt_exponential_rem[2]), c="red", label="REM fit")
            plt.text(0.05, 5, "k = " +str(np.round(popt_exponential_nrem[1], 2)), c="blue" )
            plt.scatter(np.arange(nrem_test_data.shape[0])*nrem_bin_dur, nrem_test_data, c="lightblue", label="NREM data")
            plt.plot((np.arange(nrem_test_data.shape[0])*nrem_bin_dur)[1:], exponential((np.arange(nrem_test_data.shape[0])*nrem_bin_dur)[1:],
                                                                    a=popt_exponential_nrem[0], k=popt_exponential_nrem[1],
                                                                    b=popt_exponential_nrem[2]), c="blue", label="NREM fit")

            plt.legend(loc=2)
            plt.xscale("log")
            plt.ylabel("Pearson R (z-scored)")
            plt.xlabel("Time (s)")
            plt.yticks([0, 5, 10, 15])
            plt.ylim(-3, 18)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "exponential_fit_example.svg"), transparent="True")
                plt.close()
            else:
                plt.show()

        if bootstrapping:

            # bootstrapping
            n_boots = 500
            n_samples_perc = 0.8

            nrem_exp = []
            rem_exp = []

            for boots_id in range(n_boots):
                per_ind = np.random.permutation(np.arange(rem_test_data.shape[0]))
                sel_ind = per_ind[:int(n_samples_perc*per_ind.shape[0])]
                # select subset
                x_rem = np.arange(nrem_test_data.shape[0])[sel_ind]*rem_bin_dur
                x_nrem = np.arange(nrem_test_data.shape[0])[sel_ind]*nrem_bin_dur
                y_rem = rem_test_data[sel_ind]
                y_nrem = nrem_test_data[sel_ind]
                try:
                    popt_exponential_rem, _ = optimize.curve_fit(exponential,x_rem, y_rem, p0=[1, -0.5, 1])
                    popt_exponential_nrem, _ = optimize.curve_fit(exponential, x_nrem, y_nrem, p0=[1, -0.5, 1])
                except:
                    continue

                rem_exp.append(popt_exponential_rem[1])
                nrem_exp.append(popt_exponential_nrem[1])

            if plotting:
                plt.hist(rem_exp, label="rem", color="red", bins=10, density=True)
                plt.xlabel("k from exp. function")
                plt.ylabel("density")
                plt.legend()
                plt.show()
                plt.hist(nrem_exp, label="nrem", color="blue", alpha=0.8, bins=10, density=True)
                # plt.xlim(-2,0.1)
                # plt.title("k from exponential fit (bootstrapped)\n"+"Ttest one-sided: p="+\
                #           str(ttest_ind(rem_exp, nrem_exp, alternative="greater")[1]))
                # plt.xscale("log")
                plt.show()
            else:
                return np.median(np.array(rem_exp)), np.median(np.array(nrem_exp))
        else:
            return popt_exponential_rem[1], popt_exponential_nrem[1]

    def memory_drift_rem_nrem_autocorrelation_spikes_sim_ratio(self, template_type, pre_file_name=None,
                                                               post_file_name=None, rem_pop_vec_threshold=100,
                                                               plot_for_control=True, plotting=True,
                                                               bootstrapping=False, nr_pop_vecs=100, save_fig=False):

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_rem, post_prob_rem = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold)
        ratio_rem = np.hstack(ratio_per_rem_event)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem = self.memory_drift_long_sleep_get_results(template_type=template_type,
                                                                  pre_file_name=pre_file_name,
                                                                  post_file_name=post_file_name, part_to_analyze="nrem",
                                                                  pop_vec_threshold=2)
        ratio_nrem = np.hstack(ratio_per_nrem_event)

        auto_corr_rem = []
        shift_array = np.arange(-1*int(nr_pop_vecs),
                                     int(nr_pop_vecs))

        # for mode in range(pre_prob_rem.shape[1]):
        #     ac, _ = cross_correlate(pre_prob_rem[:, mode], pre_prob_rem[:, mode], shift_array=shift_array)
        #     auto_corr_rem.append(ac)
        # auto_corr_rem = np.vstack(auto_corr_rem)
        # mean_auto_corr_rem = np.mean(auto_corr_rem, axis=0)
        #
        # auto_corr_nrem = []
        # for mode in range(pre_prob_rem.shape[1]):
        #     ac, _ = cross_correlate(pre_prob_nrem[:, mode], pre_prob_nrem[:, mode], shift_array=shift_array)
        #     auto_corr_nrem.append(ac)
        # auto_corr_nrem = np.vstack(auto_corr_nrem)
        # mean_auto_corr_nrem = np.mean(auto_corr_nrem, axis=0)

        # auto_corr_raw = np.correlate(ratio_all, ratio_all, mode="full")
        auto_corr_raw_rem, shift_array = cross_correlate(ratio_rem, ratio_rem, shift_array=shift_array)
        auto_corr_raw_nrem, _ = cross_correlate(ratio_nrem, ratio_nrem, shift_array=shift_array)

        # z-score data
        auto_corr_raw_nrem_z = (auto_corr_raw_nrem - np.mean(auto_corr_raw_nrem[:50]))/np.std(auto_corr_raw_nrem[:50])
        auto_corr_raw_rem_z = (auto_corr_raw_rem - np.mean(auto_corr_raw_rem[:50])) / np.std(auto_corr_raw_rem[:50])

        auto_corr_raw_nrem_z[int(auto_corr_raw_nrem.shape[0] / 2)] = np.nan
        auto_corr_raw_rem_z[int(auto_corr_raw_rem.shape[0] / 2)] = np.nan

        if plotting or save_fig:

            if save_fig:
                plt.style.use('default')
            plt.plot(shift_array, auto_corr_raw_nrem_z, c="b", label="NREM")
            plt.plot(shift_array, auto_corr_raw_rem_z, c="r", label="REM")
            plt.xlabel("Shift (#spikes)")
            plt.ylabel("z-scored Pearson correlation of sim_ratio")
            plt.legend()
            plt.xticks([-100, -75, -50, -25, 0, 25, 50, 75, 100], np.array([-100, -75, -50, -25, 0, 25, 50, 75, 100]) * 12)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "sim_ratio_autocorr_spikes.svg"), transparent="True")
                plt.close()
            else:
                plt.show()

        nrem_test_data = auto_corr_raw_nrem_z[int(auto_corr_raw_nrem.shape[0] / 2)+1:]
        rem_test_data = auto_corr_raw_rem_z[int(auto_corr_raw_rem.shape[0] / 2) + 1:]

        def exponential(x, a, k, b):
            return a * np.exp(x * k) + b

        popt_exponential_rem, pcov_exponential_rem = optimize.curve_fit(exponential, np.arange(rem_test_data.shape[0]),
                                                                        rem_test_data, p0=[1, -0.5, 1])
        popt_exponential_nrem, pcov_exponential_nrem = optimize.curve_fit(exponential, np.arange(nrem_test_data.shape[0]),
                                                                        nrem_test_data, p0=[1, -0.5, 1])
        if plotting or save_fig:

            if save_fig:
                plt.style.use('default')
            # plot fits
            plt.text(3, 10, "k = " +str(np.round(popt_exponential_rem[1], 2)), c="red" )
            plt.scatter(np.arange(rem_test_data.shape[0]), rem_test_data, c="salmon", label="REM data")
            plt.plot((np.arange(rem_test_data.shape[0]))[1:], exponential((np.arange(rem_test_data.shape[0]))[1:],
                                                                    a=popt_exponential_rem[0], k=popt_exponential_rem[1],
                                                                    b=popt_exponential_rem[2]), c="red", label="REM fit")
            plt.text(0.05, 5, "k = " +str(np.round(popt_exponential_nrem[1], 2)), c="blue" )
            plt.scatter(np.arange(nrem_test_data.shape[0]), nrem_test_data, c="lightblue", label="NREM data")
            plt.plot((np.arange(nrem_test_data.shape[0]))[1:], exponential((np.arange(nrem_test_data.shape[0]))[1:],
                                                                    a=popt_exponential_nrem[0], k=popt_exponential_nrem[1],
                                                                    b=popt_exponential_nrem[2]), c="blue", label="NREM fit")

            plt.legend(loc=2)
            plt.ylabel("Pearson R (z-scored)")
            plt.xlabel("nr. spikes")
            plt.ylim(-3, 18)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "exponential_fit_spikes.svg"), transparent="True")
                plt.close()
            else:
                plt.show()

        if bootstrapping:

            # bootstrapping
            n_boots = 500
            n_samples_perc = 0.8

            nrem_exp = []
            rem_exp = []

            for boots_id in range(n_boots):
                per_ind = np.random.permutation(np.arange(rem_test_data.shape[0]))
                sel_ind = per_ind[:int(n_samples_perc*per_ind.shape[0])]
                # select subset
                x_rem = np.arange(nrem_test_data.shape[0])[sel_ind]*rem_bin_dur
                x_nrem = np.arange(nrem_test_data.shape[0])[sel_ind]*nrem_bin_dur
                y_rem = rem_test_data[sel_ind]
                y_nrem = nrem_test_data[sel_ind]
                try:
                    popt_exponential_rem, _ = optimize.curve_fit(exponential,x_rem, y_rem, p0=[1, -0.5, 1])
                    popt_exponential_nrem, _ = optimize.curve_fit(exponential, x_nrem, y_nrem, p0=[1, -0.5, 1])
                except:
                    continue

                rem_exp.append(popt_exponential_rem[1])
                nrem_exp.append(popt_exponential_nrem[1])

            if plotting:
                plt.hist(rem_exp, label="rem", color="red", bins=10, density=True)
                plt.xlabel("k from exp. function")
                plt.ylabel("density")
                plt.legend()
                plt.show()
                plt.hist(nrem_exp, label="nrem", color="blue", alpha=0.8, bins=10, density=True)
                # plt.xlim(-2,0.1)
                # plt.title("k from exponential fit (bootstrapped)\n"+"Ttest one-sided: p="+\
                #           str(ttest_ind(rem_exp, nrem_exp, alternative="greater")[1]))
                # plt.xscale("log")
                plt.show()
            else:
                return np.median(np.array(rem_exp)), np.median(np.array(nrem_exp))
        else:
            return popt_exponential_rem[1], popt_exponential_nrem[1]

    def memory_drift_rem_nrem_autocorrelation_spikes_likelihood_vectors(self, template_type, pre_file_name=None,
                                                                        post_file_name=None, rem_pop_vec_threshold=100,
                                                                        plot_for_control=True, plotting=True,
                                                                        bootstrapping=False, nr_pop_vecs=10, save_fig=False):

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_list_rem, post_prob_list_rem, _ = \
            self.memory_drift_long_sleep_get_raw_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold)
        pre_prob_rem = np.vstack(pre_prob_list_rem)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_list_nrem, post_prob_list_nrem, _ = self.memory_drift_long_sleep_get_raw_results(template_type=template_type,
                                                                  pre_file_name=pre_file_name,
                                                                  post_file_name=post_file_name, part_to_analyze="nrem",
                                                                  pop_vec_threshold=2)
        pre_prob_nrem = np.vstack(pre_prob_list_nrem)


        # compute correlations
        # --------------------------------------------------------------------------------------------------------------
        shift_array = np.arange(-1*int(nr_pop_vecs), int(nr_pop_vecs)+1)
        print("Computing rem autocorrelation ...")
        auto_corr_rem, _ = cross_correlate_matrices(pre_prob_rem.T, pre_prob_rem.T, shift_array=shift_array)
        print("... done!")
        print("Computing nrem autocorrelation ...")
        auto_corr_nrem, _ = cross_correlate_matrices(pre_prob_nrem.T, pre_prob_nrem.T, shift_array=shift_array)
        print("... done!")

        auto_corr_nrem_norm = (auto_corr_nrem-auto_corr_nrem[-1])/(1-auto_corr_nrem[-1])
        auto_corr_rem_norm = (auto_corr_rem-auto_corr_rem[-1])/(1-auto_corr_rem[-1])

        if plotting or save_fig:
            if save_fig:
                plt.style.use('default')
            plt.plot(shift_array, (auto_corr_nrem-auto_corr_nrem[-1])/(1-auto_corr_nrem[-1]), c="b", label="NREM")
            plt.plot(shift_array, (auto_corr_rem-auto_corr_rem[-1])/(1-auto_corr_rem[-1]), c="r", label="REM")
            plt.xlabel("Shift (#spikes)")
            plt.ylabel("Avg. Pearson correlation of likelihood vectors")
            plt.legend()
            # plt.xticks([-100, -75, -50, -25, 0, 25, 50, 75, 100], np.array([-100, -75, -50, -25, 0, 25, 50, 75, 100]) * 12)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "sim_ratio_autocorr_spikes.svg"), transparent="True")
                plt.close()
            else:
                plt.show()

        # fitting exponential
        # --------------------------------------------------------------------------------------------------------------
        # only take positive part (symmetric) --> exclude first data point
        nrem_test_data = auto_corr_nrem[int(auto_corr_nrem.shape[0] / 2):][1:]
        rem_test_data = auto_corr_rem[int(auto_corr_rem.shape[0] / 2):][1:]

        def exponential(x, a, k, b):
            return a * np.exp(x * k) + b

        popt_exponential_rem, pcov_exponential_rem = optimize.curve_fit(exponential, np.arange(rem_test_data.shape[0]),
                                                                        rem_test_data, p0=[1, -0.5, 1])
        popt_exponential_nrem, pcov_exponential_nrem = optimize.curve_fit(exponential, np.arange(nrem_test_data.shape[0]),
                                                                        nrem_test_data, p0=[1, -0.5, 1])
        if plotting or save_fig:

            if save_fig:
                plt.style.use('default')
            # plot fits
            plt.text(3, 10, "k = " +str(np.round(popt_exponential_rem[1], 2)), c="red" )
            plt.scatter(np.arange(rem_test_data.shape[0]), rem_test_data, c="salmon", label="REM data")
            plt.plot((np.arange(rem_test_data.shape[0]))[1:], exponential((np.arange(rem_test_data.shape[0]))[1:],
                                                                    a=popt_exponential_rem[0], k=popt_exponential_rem[1],
                                                                    b=popt_exponential_rem[2]), c="red", label="REM fit")
            plt.text(0.05, 5, "k = " +str(np.round(popt_exponential_nrem[1], 2)), c="blue" )
            plt.scatter(np.arange(nrem_test_data.shape[0]), nrem_test_data, c="lightblue", label="NREM data")
            plt.plot((np.arange(nrem_test_data.shape[0]))[1:], exponential((np.arange(nrem_test_data.shape[0]))[1:],
                                                                    a=popt_exponential_nrem[0], k=popt_exponential_nrem[1],
                                                                    b=popt_exponential_nrem[2]), c="blue", label="NREM fit")

            plt.legend(loc=2)
            plt.ylabel("Pearson R (z-scored)")
            plt.xlabel("nr. spikes")
            plt.ylim(-3, 18)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "exponential_fit_spikes.svg"), transparent="True")
                plt.close()
            else:
                plt.show()

        if bootstrapping:

            # bootstrapping
            n_boots = 500
            n_samples_perc = 0.8

            nrem_exp = []
            rem_exp = []

            for boots_id in range(n_boots):
                per_ind = np.random.permutation(np.arange(rem_test_data.shape[0]))
                sel_ind = per_ind[:int(n_samples_perc*per_ind.shape[0])]
                # select subset
                x_rem = np.arange(nrem_test_data.shape[0])[sel_ind]
                x_nrem = np.arange(nrem_test_data.shape[0])[sel_ind]
                y_rem = rem_test_data[sel_ind]
                y_nrem = nrem_test_data[sel_ind]
                try:
                    popt_exponential_rem, _ = optimize.curve_fit(exponential,x_rem, y_rem, p0=[1, -0.5, 1])
                    popt_exponential_nrem, _ = optimize.curve_fit(exponential, x_nrem, y_nrem, p0=[1, -0.5, 1])
                except:
                    continue

                rem_exp.append(popt_exponential_rem[1])
                nrem_exp.append(popt_exponential_nrem[1])

            if plotting:
                plt.hist(rem_exp, label="rem", color="red", bins=10, density=True)
                plt.xlabel("k from exp. function")
                plt.ylabel("density")
                plt.legend()
                plt.show()
                plt.hist(nrem_exp, label="nrem", color="blue", alpha=0.8, bins=10, density=True)
                # plt.xlim(-2,0.1)
                # plt.title("k from exponential fit (bootstrapped)\n"+"Ttest one-sided: p="+\
                #           str(ttest_ind(rem_exp, nrem_exp, alternative="greater")[1]))
                # plt.xscale("log")
                plt.show()
            else:
                return np.median(np.array(rem_exp)), np.median(np.array(nrem_exp))
        else:
            return auto_corr_rem_norm, auto_corr_nrem_norm, popt_exponential_rem[1], popt_exponential_nrem[1]

    def memory_drift_rem_nrem_spatial_decoding_autocorrelation(self, rem_pop_vec_threshold=100, plot_for_control=False,
                                                               save_fig=False, duration_for_autocorrelation_rem=22,
                                                               duration_for_autocorrelation_nrem=22):

        # get median bin duration of constant spike bins
        nrem_bin_dur, rem_bin_dur = self.get_constant_spike_bin_length(plotting=False)

        shift_array_nrem = np.arange(int(-duration_for_autocorrelation_nrem/nrem_bin_dur),
                                     int(duration_for_autocorrelation_nrem/nrem_bin_dur))
        shift_array_rem = np.arange(int(-duration_for_autocorrelation_rem/rem_bin_dur),
                                    int(duration_for_autocorrelation_rem/rem_bin_dur))

        # load model from PRE
        with open(self.params.pre_proc_dir + 'awake_ising_maps/' + self.session_params.default_pre_ising_model + '.pkl',
                  'rb') as f:
            model_dic = pickle.load(f)

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_list_rem, post_prob_list_rem, event_times_list_rem = \
            self.memory_drift_long_sleep_get_raw_results(template_type="ising", pre_file_name=None,
                                                         post_file_name=None, part_to_analyze="rem",
                                                         pop_vec_threshold=rem_pop_vec_threshold)

        pre_likeli_rem = np.vstack(pre_prob_list_rem)

        # reshape likelihoods to be spatially like the pre model
        pre_likeli_rem_spatial = pre_likeli_rem.reshape(-1, model_dic["occ_map"].shape[0], model_dic["occ_map"].shape[1])

        if plot_for_control:
            # plot 5 random likelihood maps
            for i in range(5):
                plt.imshow(pre_likeli_rem_spatial[np.random.choice(np.arange(pre_likeli_rem_spatial.shape[0])),:,:]), \
                plt.show()

        dec_loc = np.zeros((pre_likeli_rem_spatial.shape[0], 2))
        for time_bin_id, time_bin in enumerate(pre_likeli_rem_spatial):
            # for each time bin --> get location of max
            dec_loc[time_bin_id] = np.unravel_index(time_bin.argmax(), time_bin.shape)


        dist_rem = np.zeros(shift_array_rem.shape[0])

        for i, shift in enumerate(shift_array_rem):
            if shift == 0:
                dist_rem[i] = np.nan
            elif shift > 0:
                dist_rem[i] = np.mean(np.linalg.norm(dec_loc[:-1*shift,:] - dec_loc[shift:,:], axis=1))
            else:
                shift = np.abs(shift)
                dist_rem[i] = np.mean(np.linalg.norm(dec_loc[shift:,:] - dec_loc[:-1*shift,:], axis=1))


        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_list_nrem, post_prob_list_nrem, event_times_list_nrem = \
            self.memory_drift_long_sleep_get_raw_results(template_type="ising", pre_file_name=None,
                                                         post_file_name=None, part_to_analyze="nrem",
                                                         pop_vec_threshold=2)

        pre_likeli_nrem = np.vstack(pre_prob_list_nrem)

        # reshape likelihoods to be spatially like the pre model
        pre_likeli_nrem_spatial = pre_likeli_nrem.reshape(-1, model_dic["occ_map"].shape[0], model_dic["occ_map"].shape[1])

        if plot_for_control:
            # plot 5 random likelihood maps
            for i in range(5):
                plt.imshow(pre_likeli_nrem_spatial[np.random.choice(np.arange(pre_likeli_nrem_spatial.shape[0])),:,:]), \
                plt.show()

        dec_loc = np.zeros((pre_likeli_nrem_spatial.shape[0], 2))
        for time_bin_id, time_bin in enumerate(pre_likeli_nrem_spatial):
            # for each time bin --> get location of max
            dec_loc[time_bin_id] = np.unravel_index(time_bin.argmax(), time_bin.shape)

        dist_nrem = np.zeros(shift_array_nrem.shape[0])

        for i, shift in enumerate(shift_array_nrem):
            if shift == 0:
                dist_nrem[i] = np.nan
            elif shift > 0:
                dist_nrem[i] = np.mean(np.linalg.norm(dec_loc[:-1*shift,:] - dec_loc[shift:,:], axis=1))
            else:
                shift = np.abs(shift)
                dist_nrem[i] = np.mean(np.linalg.norm(dec_loc[shift:,:] - dec_loc[:-1*shift,:], axis=1))

        if save_fig:
            plt.style.use('default')
        # bins are 5 a.u. --> 2.25 cm
        plt.plot(shift_array_nrem * nrem_bin_dur, dist_nrem * 2.25, c="b", label="NREM")
        plt.plot(shift_array_rem * rem_bin_dur, dist_rem *2.25, c="r", label="REM")
        plt.ylabel("Distance between decoded locations (cm)")
        plt.xlabel("Time (s)")
        plt.xscale("symlog")
        plt.legend()
        plt.xlim(-duration_for_autocorrelation_rem/2, duration_for_autocorrelation_rem/2)
        if save_fig:
            plt.rcParams['svg.fonttype'] = 'none'
            plt.savefig(os.path.join(save_path, "autocorr_distance.svg"), transparent="True")
            plt.close()
        else:
            plt.show()

    def memory_drift_analyze_nrem(self, template_type, pre_file_name=None, post_file_name=None,
                                                      rem_pop_vec_threshold=100, log_transform=False):
        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem = self.memory_drift_long_sleep_get_results(template_type=template_type,
                                                                  pre_file_name=pre_file_name,
                                                                  post_file_name=post_file_name, part_to_analyze="nrem",
                                                                  pop_vec_threshold=2)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        _, _, _, _, pre_prob_rem, \
        post_prob_rem = self.memory_drift_long_sleep_get_results(template_type=template_type,
                                                                  pre_file_name=pre_file_name,
                                                                  post_file_name=post_file_name, part_to_analyze="rem",
                                                                  pop_vec_threshold=2)

        # get most likely mode
        ml_mode_nrem = np.argmax(pre_prob_nrem, axis=1)
        ml_mode_rem = np.argmax(pre_prob_rem, axis=1)

        with open("nrem_spikes","rb") as fp:
            nrem_spikes = pickle.load(fp)

        nrem_spikes = np.hstack(nrem_spikes)

        with open("rem_spikes","rb") as fp:
            rem_spikes = pickle.load(fp)

        rem_spikes = np.hstack(rem_spikes)

        mode_to_plot_1 = 5
        mode_to_plot_2 = 2
        # select only one mode
        nrem_spikes_sel_1 = nrem_spikes[:, ml_mode_nrem==mode_to_plot_1]
        rem_spikes_sel_1 = rem_spikes[:, ml_mode_rem == mode_to_plot_1]
        nrem_spikes_sel_2= nrem_spikes[:, ml_mode_nrem==mode_to_plot_2]
        rem_spikes_sel_2 = rem_spikes[:, ml_mode_rem == mode_to_plot_2]


        comb = np.hstack((rem_spikes_sel_1, rem_spikes_sel_2,
                          nrem_spikes_sel_1, nrem_spikes_sel_2))

        # active and non active set

        comb[comb>0] = 1

        print(comb.shape)
        DD = pairwise_distances(comb.T, metric="jaccard")
        seed = np.random.RandomState(seed=3)
        mds = MDS(n_components=2, max_iter=3000, eps=1e-9, random_state=seed,
                           dissimilarity="precomputed", n_jobs = 1)
        pos = mds.fit(DD).embedding_
        nmds = MDS(n_components=2, metric=False, max_iter=3000, eps=1e-12,
                            dissimilarity="precomputed", random_state=seed, n_jobs=1,
                                                                                       n_init = 1)
        npos = nmds.fit_transform(DD, init=pos)

        # npos = TSNE(n_components=2, perplexity=5).fit_transform(comb.T)

        sep_1 = rem_spikes_sel_1.shape[1]
        sep_2 = sep_1 + rem_spikes_sel_2.shape[1]
        sep_3 = sep_2 + nrem_spikes_sel_1.shape[1]

        rem_res_1 = npos[:sep_1,:]
        rem_res_2 = npos[sep_1:sep_2, :]
        nrem_res_1 = npos[sep_2:sep_3, :]
        nrem_res_2 = npos[sep_3:, :]

        plt.scatter(rem_res_1[:,0], rem_res_1[:,1], color="lightcoral", marker="*")
        plt.scatter(rem_res_2[:, 0], rem_res_2[:, 1], edgecolors="red", facecolors="none")
        plt.scatter(nrem_res_1[:,0], nrem_res_1[:,1], color="cornflowerblue", marker="*", alpha=0.5)
        plt.scatter(nrem_res_2[:, 0], nrem_res_2[:, 1], edgecolors="blue", facecolors="none")
        plt.show()

        # compute transition matrix
        # trans_mat = transition_matrix(ml_mode)
        print("HERE")

    def phmm_mode_occurrence(self, part_to_analyze="rem", n_smoothing=2000, data_length=1):

        pre_prob_list, _, _ = \
            self.memory_drift_long_sleep_get_raw_results(template_type="phmm", part_to_analyze=part_to_analyze)
        pre_prob = np.vstack(pre_prob_list)
        pre_prob = pre_prob[:int(data_length*pre_prob.shape[0]), :]

        active_mode = np.argmax(pre_prob, axis=1)
        mode, mode_occ = np.unique(active_mode, return_counts=True)
        mode_occurrence = np.zeros(pre_prob.shape[1])
        mode_occurrence[mode] = mode_occ
        smooth_post_prob = []
        m = []
        # compute probabilites in moving window
        for mode_post_prob in pre_prob.T:
            mode_post_prob_smooth = moving_average(a=mode_post_prob, n=n_smoothing)
            mode_post_prob_smooth_norm = mode_post_prob_smooth / np.max(mode_post_prob_smooth)
            smooth_post_prob.append(mode_post_prob_smooth_norm)
            coef = np.polyfit(np.linspace(0, 1, mode_post_prob_smooth_norm.shape[0]), mode_post_prob_smooth_norm, 1)
            m.append(coef[0])
            # plt.plot(mode_post_prob_smooth_norm)
            # poly1d_fn = np.poly1d(coef)
            # plt.plot(np.arange(mode_post_prob_smooth_norm.shape[0]),
            # poly1d_fn(np.linspace(0,1,mode_post_prob_smooth_norm.shape[0])), '--w')
            # plt.title(coef[0])
            # plt.show()

        m = np.array(m)

        return m, mode_occurrence

    def memory_drift_noise_correlations(self, template_type="phmm", part_to_analyze="nrem", pop_vec_threshold=2,
                                            measure="normalized_ratio", pre_file_name=None, post_file_name=None,
                                            cells_to_use="all", shuffling=False, sleep_classification_method="std",
                                            return_pre_prob_list=False):

        # rem: pop_vec_threshold = 100
        # nrem: pop_vec_threshold = 2

        first = 0
        pre_prob_list = []
        post_prob_list = []
        event_times_list = []
        spike_bin_timings = []
        spike_bins = []
        for i, l_s in enumerate(self.long_sleep):
            if not i:
                default_pre_phmm_model, default_post_phmm_model,_ ,_ = l_s.get_pre_post_templates()
            duration = l_s.get_duration_sec()
            event_spike_rasters, event_spike_window_lengths, bin_times = l_s.get_event_spike_rasters_and_times(part_to_analyze=part_to_analyze)
            spike_bin_timings.extend([x + first for x in bin_times])
            pre_prob, post_prob, ev_t, _ = l_s.decode_activity_using_pre_post(template_type=template_type,
                                                                              pre_file_name=pre_file_name,
                                                                              post_file_name=post_file_name,
                                                                              part_to_analyze=part_to_analyze,
                                                                              cells_to_use=cells_to_use,
                                                                              shuffling=shuffling,
                                                                              sleep_classification_method=
                                                                              sleep_classification_method)
            pre_prob_list.extend(pre_prob)
            post_prob_list.extend(post_prob)
            event_times_list.extend(ev_t + first)
            first += duration
            spike_bins.append(np.hstack(event_spike_rasters))

        pre_prob_arr = np.vstack(pre_prob_list).astype(np.float32)
        post_prob_arr = np.vstack(post_prob_list).astype(np.float32)
        spike_bins = np.hstack(spike_bins)

        result_all = (np.max(post_prob_arr, axis=1) - np.max(pre_prob_arr, axis=1)) / \
                     (np.max(pre_prob_arr, axis=1) + np.max(post_prob_arr, axis=1))
        plt.plot(moving_average(result_all, 100))
        plt.xlabel("Pop vec")
        plt.ylabel("sim_ratio")
        plt.ylim(-1,1)
        plt.tight_layout()
        plt.show()

        # for each mode --> compute mean across "reactivations" and use this to compute residuals
        decoded_mode = np.argmax(pre_prob_arr, axis=1)
        decoded_modes = np.unique(decoded_mode)

        # subtract mean activity to get residuals
        residuals = np.copy(spike_bins)
        for mode_id in decoded_modes:
            # select all pop-vecs when this mode was decoded and compute mean
            dat = spike_bins[:, decoded_mode == mode_id]
            mean_mode_vec = np.mean(dat, axis=1)
            residuals[:, decoded_mode == mode_id] = (residuals[:, decoded_mode == mode_id] -
                                                     np.tile(mean_mode_vec, (dat.shape[1], 1)).T)

        # attention: negative firing rates!!! (because residuals can be negative)
        # compute correlations from equalized rasters --> 60 seconds was a good estimate to get full rank corr. matrices
        len_corr_chunk = 100
        nr_chunks_corr = int(residuals.shape[1] / len_corr_chunk)
        chunk_size = np.round(residuals.shape[1] / nr_chunks_corr).astype(int)

        corr_matrices_sleep = []

        for i in range(int(nr_chunks_corr)):
            corr_matrices_sleep.append(
                upper_tri_without_diag(np.corrcoef(residuals[:, i * chunk_size:(i + 1) * chunk_size])))

        corr_matrices_sleep = np.vstack(corr_matrices_sleep)
        # compute mean of first 10 and mean of last 10 to compute ratio
        mean_first = np.mean(corr_matrices_sleep[:10, :], axis=0)
        mean_last = np.mean(corr_matrices_sleep[-10:, :], axis=0)

        sim_first =cdist(corr_matrices_sleep[10:-10], np.expand_dims(mean_first, 0), metric="correlation").flatten()
        sim_last = cdist(corr_matrices_sleep[10:-10], np.expand_dims(mean_last, 0), metric="correlation").flatten()

        ratio = (sim_last - sim_first)/(sim_last + sim_first)
        plt.xlabel("Window ID")
        plt.ylabel("")
        plt.plot(sim_last, label="distance_last")
        plt.plot(sim_first, label="distance_first")
        plt.ylabel("Pearson R")
        plt.legend()
        plt.tight_layout()
        plt.show()

        plt.xlabel("Window ID")
        plt.ylabel("ratio")
        plt.plot(ratio)
        plt.legend()
        plt.tight_layout()
        plt.show()

    # </editor-fold>

    # <editor-fold desc="Non-stationarity">

    # population vectors

    def cos_distance_during_sleep(self, part_to_analyze="all"):

        if part_to_analyze == "all":
            raster = []
            for l_s in self.long_sleep:
                r = l_s.get_raster()
                raster.append(r)
            raster = np.hstack(raster)

        elif part_to_analyze in ["nrem", "rem", "all_swr"]:
            raster = []
            for l_s in self.long_sleep:
                r = l_s.get_event_time_bin_rasters(sleep_phase=part_to_analyze)
                raster.append(r)
            raster = np.hstack(raster)

        # split into 4 pieces
        first = raster[:, :int(raster.shape[1]*0.25)]
        second = raster[:, int(raster.shape[1]*0.25):int(raster.shape[1]*0.5)]
        third = raster[:, int(raster.shape[1]*0.5):int(raster.shape[1]*0.75)]
        fourth = raster[:, int(raster.shape[1]*0.75):]

        distances = []
        for x in [first, second, third, fourth]:
            # select random pairs and compute distance
            rand_ind = np.random.permutation(np.arange(x.shape[1]))[:10000]
            x_shuff = x[:, rand_ind]
            distance_mat_1 = distance.squareform(distance.pdist(x_shuff.T, metric="cosine"))
            dist = np.nan_to_num(np.diag(v=distance_mat_1, k=1))
            distances.append(dist)


        plt.subplot(1,4,1)
        plt.hist(distances[0], orientation='horizontal', density=True, bins=50)
        plt.ylim(0,1)
        plt.title("1st part sleep")
        plt.ylabel("Cos distance between pairs")
        plt.xlabel("Density")
        plt.subplot(1,4,2)
        plt.hist(distances[1], orientation='horizontal', density=True, bins=50)
        plt.ylim(0,1)
        plt.title("2nd part sleep")
        plt.xlabel("Density")
        plt.subplot(1,4,3)
        plt.hist(distances[2], orientation='horizontal', density=True, bins=50)
        plt.ylim(0,1)
        plt.title("3rd part sleep")
        plt.xlabel("Density")
        plt.subplot(1,4,4)
        plt.xlabel("Density")
        plt.hist(distances[3], orientation='horizontal', density=True, bins=50)
        plt.ylim(0,1)
        plt.title("4th part sleep")
        plt.tight_layout()
        plt.show()


        print("HERE")
        # # load only stable cells
        # with open(self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
        #           "rb") as f:
        #     class_dic = pickle.load(f)
        #
        # stable_ids = class_dic["stable_cell_ids"]
        # inc_ids = class_dic["increase_cell_ids"]

    def predict_time_progression_const_spikes(self, part_to_analyze="all"):

        if part_to_analyze == "all":
            raster = []
            times = []
            first = 0
            for l_s in self.long_sleep:
                duration = l_s.get_duration_sec()
                r, t = l_s.get_spike_binned_raster(return_estimated_times=True)
                raster.append(r)
                times.append(t + first)
                first += duration

            raster = np.hstack(raster)
            times = np.hstack(times)

            # load only stable cells
            with open(self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                      "rb") as f:
                class_dic = pickle.load(f)

            stable_ids = class_dic["stable_cell_ids"]
            inc_ids = class_dic["increase_cell_ids"]
            raster_stable = raster[stable_ids, :]
            raster_wo_stable = np.delete(raster, stable_ids, axis=0)

            new_ml = MlMethodsOnePopulation()
            all_r2 = []
            stable_r2 = []
            wo_stable = []

            r2 = new_ml.ridge_time_bin_progress(x=raster, y=times, new_time_bin_size=" 50 SPIKE", alpha=100,
                                                alpha_fitting=False, plotting=True)

            for i in range(15):
                r2 = new_ml.ridge_time_bin_progress(x=raster, y=times, new_time_bin_size="CONST #SPIKES", alpha=100,
                                                    alpha_fitting=False, plotting=False)
                all_r2.append(r2)

            for i in range(15):
                r2 = new_ml.ridge_time_bin_progress(x=raster_wo_stable, y=times, new_time_bin_size="CONST #SPIKES",
                                                    alpha=100,
                                                    alpha_fitting=False, plotting=False)
                wo_stable.append(r2)

            for i in range(15):
                r2 = new_ml.ridge_time_bin_progress(x=raster_stable, y=times, new_time_bin_size="CONST #SPIKES",
                                                    alpha=100,
                                                    alpha_fitting=False, plotting=False)
                stable_r2.append(r2)

            c = "white"

            stable_r2 = np.array(stable_r2)
            wo_stable = np.array(wo_stable)
            all_r2 = np.array(all_r2)

            res = np.vstack((all_r2, wo_stable, stable_r2)).T

            bplot = plt.boxplot(res, positions=[1, 2, 3], patch_artist=True,
                                labels=["ALL", "W/O STABLE", "ONLY STABLE"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c),
                                )
            colors = ["yellow", 'blue', 'red']
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            plt.title("R2 VALUES OF RIDGE REGRESSION")
            plt.ylabel("R2 (15 SPLITS)")
            plt.grid(color="grey", axis="y")
            plt.show()



        else:
            # check how many cells
            nr_cells = self.long_sleep[0].get_raster().shape[0]
            all_event_rasters = []
            all_raster_lengths = []
            start_times = []
            end_times = []

            # need to offset each sleep file by duration of previous sleep files
            first = 0
            for l_s in self.long_sleep:
                duration = l_s.get_duration_sec()
                # all_event_rasters = all_event_rasters + l_s.get_event_spike_rasters(part_to_analyze=part_to_analyze)[0]
                new_event_raster, start_time, end_time = l_s.get_event_time_bin_rasters(part_to_analyze=part_to_analyze,
                                                                                        time_bin_size=0.01)
                all_event_rasters = all_event_rasters + new_event_raster
                start_times.append(start_time + first)
                end_times.append(end_time + first)
                first += duration

            start_times = np.hstack(start_times)
            end_times = np.hstack(end_times)

            new_time_stamps = []

            for event, start, end in zip(all_event_rasters, start_times, end_times):
                new_time_stamps.extend(np.linspace(start, end, event.shape[1]))

            new_time_stamps = np.expand_dims(np.array(new_time_stamps), 0)
            all_event_rasters = np.hstack(all_event_rasters)

            scaler = int(self.time_bin_size / 0.01)

            down_sampled = down_sample_array_sum(x=all_event_rasters, chunk_size=scaler)
            times_down_sampled = down_sample_array_mean(x=new_time_stamps, chunk_size=scaler)
            times_down_sampled = np.squeeze(times_down_sampled)

            new_ml = MlMethodsOnePopulation()
            new_ml.ridge_time_bin_progress(x=down_sampled, y=times_down_sampled,
                                           new_time_bin_size=0.5, alpha_fitting=True)

    def predict_time_progression_time_bin(self, part_to_analyze="all", time_bin_size=0.1, nr_fits=5):

        if part_to_analyze == "all":
            raster = []
            first = 0
            for l_s in self.long_sleep:
                duration = l_s.get_duration_sec()
                r = l_s.get_raster()
                raster.append(r)
                first += duration

            raster = np.hstack(raster)
            scaler = int(time_bin_size / self.params.time_bin_size)
            raster = down_sample_array_sum(x=raster, chunk_size=scaler)

            print(raster.shape)
            #
            times = np.arange(0, raster.shape[1]) * time_bin_size

            # load only stable cells
            with open(self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                      "rb") as f:
                class_dic = pickle.load(f)

            stable_ids = class_dic["stable_cell_ids"]
            inc_ids = class_dic["increase_cell_ids"]
            raster_stable = raster[stable_ids, :]
            raster_wo_stable = np.delete(raster, stable_ids, axis=0)

            new_ml = MlMethodsOnePopulation()
            all_r2 = []
            stable_r2 = []
            wo_stable = []
            for i in range(nr_fits):
                r2 = new_ml.ridge_time_bin_progress(x=raster, y=times, new_time_bin_size=time_bin_size, alpha=100,
                                                    alpha_fitting=False, plotting=False)
                all_r2.append(r2)

            for i in range(nr_fits):
                r2 = new_ml.ridge_time_bin_progress(x=raster_wo_stable, y=times, new_time_bin_size="CONST #SPIKES",
                                                    alpha=100,
                                                    alpha_fitting=False, plotting=False)
                wo_stable.append(r2)

            for i in range(nr_fits):
                r2 = new_ml.ridge_time_bin_progress(x=raster_stable, y=times, new_time_bin_size="CONST #SPIKES",
                                                    alpha=100,
                                                    alpha_fitting=False, plotting=False)
                stable_r2.append(r2)

            c = "white"

            stable_r2 = np.array(stable_r2)
            wo_stable = np.array(wo_stable)
            all_r2 = np.array(all_r2)

            res = np.vstack((all_r2, wo_stable, stable_r2)).T

            bplot = plt.boxplot(res, positions=[1, 2, 3], patch_artist=True,
                                labels=["ALL", "W/O STABLE", "ONLY STABLE"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c),
                                )
            colors = ["yellow", 'blue', 'red']
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            plt.title("R2 VALUES OF RIDGE REGRESSION")
            plt.ylabel("R2 (15 SPLITS)")
            plt.grid(color="grey", axis="y")
            plt.show()

        else:
            # check how many cells
            nr_cells = self.long_sleep[0].get_raster().shape[0]
            all_event_rasters = []
            all_raster_lengths = []
            start_times = []
            end_times = []

            # need to offset each sleep file by duration of previous sleep files
            first = 0
            for l_s in self.long_sleep:
                duration = l_s.get_duration_sec()
                # all_event_rasters = all_event_rasters + l_s.get_event_spike_rasters(part_to_analyze=part_to_analyze)[0]
                new_event_raster, start_time, end_time = l_s.get_event_time_bin_rasters(part_to_analyze=part_to_analyze,
                                                                                        time_bin_size=0.01)
                all_event_rasters = all_event_rasters + new_event_raster
                start_times.append(start_time + first)
                end_times.append(end_time + first)
                first += duration

            start_times = np.hstack(start_times)
            end_times = np.hstack(end_times)

            new_time_stamps = []

            for event, start, end in zip(all_event_rasters, start_times, end_times):
                new_time_stamps.extend(np.linspace(start, end, event.shape[1]))

            new_time_stamps = np.expand_dims(np.array(new_time_stamps), 0)
            all_event_rasters = np.hstack(all_event_rasters)

            scaler = int(time_bin_size / 0.01)

            down_sampled = down_sample_array_sum(x=all_event_rasters, chunk_size=scaler)
            times_down_sampled = down_sample_array_mean(x=new_time_stamps, chunk_size=scaler)
            times_down_sampled = np.squeeze(times_down_sampled)

            new_ml = MlMethodsOnePopulation()
            new_ml.ridge_time_bin_progress(x=down_sampled, y=times_down_sampled,
                                           new_time_bin_size=0.5, alpha_fitting=True)

    def optimal_time_bin_size_to_predict_time_progression(self, part_to_analyze="all", nr_fits=100):

        if part_to_analyze == "all":
            raster = []
            first = 0
            for l_s in self.long_sleep:
                duration = l_s.get_duration_sec()
                r = l_s.get_raster()
                raster.append(r)
                first += duration

            raster_orig = np.hstack(raster)

            time_bin_size_array = [5, 10, 20, 30, 60, 80, 100, 120, 140, 160, 180]
            # time_bin_size_array = [0.1, 1]

            res_mean = []
            res_std = []
            for time_bin_size in time_bin_size_array:
                print("COMPUTING RESULTS FOR TIME BIN SIZE: " + str(time_bin_size))

                scaler = int(time_bin_size / self.params.time_bin_size)
                raster = down_sample_array_sum(x=raster_orig, chunk_size=scaler)

                #
                times = np.arange(0, raster.shape[1]) * time_bin_size

                new_ml = MlMethodsOnePopulation()

                res_temp = []
                for i in range(nr_fits):
                    r2 = new_ml.ridge_time_bin_progress(x=raster, y=times, new_time_bin_size=time_bin_size, alpha=100,
                                                        alpha_fitting=False, plotting=False)
                    res_temp.append(r2)

                c = "white"

                res_mean.append(np.mean(np.array(res_temp)))
                res_std.append(np.std(np.array(res_temp)))

            plt.scatter(x=time_bin_size_array, y=res_mean, color="red")
            plt.errorbar(x=time_bin_size_array, y=res_mean, yerr=res_std, ls="none", color="red")
            plt.title("R2 VALUES OF RIDGE REGRESSION")
            plt.ylabel("R2: MEAN +- STD (" + str(nr_fits) + " FITS)")
            plt.xlabel("TIME BIN SIZE (s)")
            plt.grid(color="grey")
            plt.show()

        else:
            # check how many cells
            nr_cells = self.long_sleep[0].get_raster().shape[0]
            all_event_rasters = []
            all_raster_lengths = []
            start_times = []
            end_times = []

            # need to offset each sleep file by duration of previous sleep files
            first = 0
            for l_s in self.long_sleep:
                duration = l_s.get_duration_sec()
                # all_event_rasters = all_event_rasters + l_s.get_event_spike_rasters(part_to_analyze=part_to_analyze)[0]
                new_event_raster, start_time, end_time = l_s.get_event_time_bin_rasters(part_to_analyze=part_to_analyze,
                                                                                        time_bin_size=0.01)
                all_event_rasters = all_event_rasters + new_event_raster
                start_times.append(start_time + first)
                end_times.append(end_time + first)
                first += duration

            start_times = np.hstack(start_times)
            end_times = np.hstack(end_times)

            new_time_stamps = []

            for event, start, end in zip(all_event_rasters, start_times, end_times):
                new_time_stamps.extend(np.linspace(start, end, event.shape[1]))

            new_time_stamps = np.expand_dims(np.array(new_time_stamps), 0)
            all_event_rasters = np.hstack(all_event_rasters)

            scaler = int(time_bin_size / 0.01)

            down_sampled = down_sample_array_sum(x=all_event_rasters, chunk_size=scaler)
            times_down_sampled = down_sample_array_mean(x=new_time_stamps, chunk_size=scaler)
            times_down_sampled = np.squeeze(times_down_sampled)

            new_ml = MlMethodsOnePopulation()
            new_ml.ridge_time_bin_progress(x=down_sampled, y=times_down_sampled,
                                           new_time_bin_size=0.5, alpha_fitting=True)

    # correlation matrices

    def predict_time_progression_time_bin_correlations(self, plot_file_name="test_1", only_upper_triangle=True,
                                                       part_to_analyze="all",   use_pca=True,
                                                       bins_per_corr_matrix=20, only_stable_cells=False):

        if part_to_analyze == "all":

            # load only stable cells
            if only_stable_cells:
                with open(
                        self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                        "rb") as f:
                    class_dic = pickle.load(f)

                stable_ids = class_dic["stable_cell_ids"]
                print("ONLY STABLE CELLS!!!")

                nr_cells = stable_ids.shape[0]

                if only_upper_triangle is True:
                    corr_mat = np.zeros((int(nr_cells * (nr_cells - 1) / 2), 0))
                else:
                    corr_mat = np.zeros((nr_cells ** 2, 0))

                for l_s in self.long_sleep:
                    c_m = l_s.get_correlation_matrices(bins_per_corr_matrix=bins_per_corr_matrix,
                                                       only_upper_triangle=only_upper_triangle, cell_selection=stable_ids)
                    corr_mat = np.hstack((corr_mat, c_m))

            else:

                nr_cells = self.long_sleep[0].get_nr_cells()

                if only_upper_triangle is True:
                    corr_mat = np.zeros((int(nr_cells * (nr_cells - 1) / 2), 0))
                else:
                    corr_mat = np.zeros((nr_cells ** 2, 0))

                for l_s in self.long_sleep:
                    c_m = l_s.get_correlation_matrices(bins_per_corr_matrix=bins_per_corr_matrix,
                                                       only_upper_triangle=only_upper_triangle)
                    corr_mat = np.hstack((corr_mat, c_m))

            times = np.arange(0, corr_mat.shape[1]) * self.params.time_bin_size * bins_per_corr_matrix

            # apply PCA to only use first n-th principal components
            if use_pca == True:
                res_mean = []
                res_std = []
                n_comp_array = np.arange(1,30,2)
                for n_components in n_comp_array:
                    pca = PCA(n_components=n_components)
                    pca_data = pca.fit_transform(X=corr_mat.T).T
                    new_ml = MlMethodsOnePopulation()
                    print("\nSTARTING RIDGE ... \n")
                    fit_res = []
                    for nr_fits in range(10):
                        r2 = new_ml.ridge_time_bin_progress(x=pca_data, y=times, new_time_bin_size="CONST #SPIKES",
                                                        alpha_fitting=True, plotting=False)
                        fit_res.append(r2)
                    res_mean.append(np.mean(np.array(fit_res)))
                    res_std.append(np.std(np.array(fit_res)))
                plt.scatter(x=n_comp_array, y=res_mean, color="red", label= "R2")
                plt.errorbar(x=n_comp_array, y=res_mean, yerr=res_std, ls="none", color="red")
                plt.title("PREDICTIVE POWER USING PC OF CORRELATIONS")
                plt.ylabel("R2 (MEAN +- STD, 10 SPLITS)")
                plt.xlabel("n-th PRINCIPAL COMPONENTS")
                plt.grid(color="grey")
                plt.show()
                pca = PCA(n_components=30)
                pca.fit_transform(X=corr_mat.T).T
                v_ex = pca.explained_variance_ratio_
                plt.plot(v_ex)
                plt.ylabel("VARIANCE EXPLAINED")
                plt.xlabel("PC")
                plt.title("PCA OF CORRELATIONS (UPPER TRIANGLE): VARIANCE EXPLAINED")
                plt.show()

            else:
                new_ml = MlMethodsOnePopulation()
                print("\nSTARTING RIDGE ... \n")
                r2 = new_ml.ridge_time_bin_progress(x=corr_mat, y=times, new_time_bin_size="CONST #SPIKES",
                                                    alpha_fitting=True, plotting=True)

                plt.savefig(os.path.join(save_path, self.params.pre_proc_dir + plot_file_name))

        else:
            # check how many cells
            nr_cells = self.long_sleep[0].get_raster().shape[0]
            all_event_rasters = []
            all_raster_lengths = []
            start_times = []
            end_times = []

            # need to offset each sleep file by duration of previous sleep files
            first = 0
            for l_s in self.long_sleep:
                duration = l_s.get_duration_sec()
                # all_event_rasters = all_event_rasters + l_s.get_event_spike_rasters(part_to_analyze=part_to_analyze)[0]
                new_event_raster, start_time, end_time = l_s.get_event_time_bin_rasters(part_to_analyze=part_to_analyze,
                                                                                        time_bin_size=0.01)
                all_event_rasters = all_event_rasters + new_event_raster
                start_times.append(start_time + first)
                end_times.append(end_time + first)
                first += duration

            start_times = np.hstack(start_times)
            end_times = np.hstack(end_times)

            new_time_stamps = []

            for event, start, end in zip(all_event_rasters, start_times, end_times):
                new_time_stamps.extend(np.linspace(start, end, event.shape[1]))

            new_time_stamps = np.expand_dims(np.array(new_time_stamps), 0)
            all_event_rasters = np.hstack(all_event_rasters)

            scaler = int(time_bin_size / 0.01)

            down_sampled = down_sample_array_sum(x=all_event_rasters, chunk_size=scaler)
            times_down_sampled = down_sample_array_mean(x=new_time_stamps, chunk_size=scaler)
            times_down_sampled = np.squeeze(times_down_sampled)

            new_ml = MlMethodsOnePopulation()
            new_ml.ridge_time_bin_progress(x=down_sampled, y=times_down_sampled,
                                           new_time_bin_size=0.5, alpha_fitting=True)

    def compute_rank_correlation_matrices(self):

        ranks_mean = []
        ranks_std = []
        window_size_list = [20, 50, 100, 200, 400, 600, 800, 1000, 1200]
        for bins_per_corr_matrix in window_size_list:
            nr_cells = self.long_sleep[0].get_nr_cells()

            ranks_per_window_size = []

            for l_s in self.long_sleep:
                c_m = l_s.get_correlation_matrices(bins_per_corr_matrix=bins_per_corr_matrix,
                                                   exclude_diagonal=False)
                for corr_mat in c_m.T:
                    corr_mat = np.reshape(corr_mat, (nr_cells, nr_cells))
                    ranks_per_window_size.append(np.linalg.matrix_rank(corr_mat))

            ranks_mean.append(np.mean(np.array(ranks_per_window_size)))
            ranks_std.append(np.std(np.array(ranks_per_window_size)))

        plt.scatter(x=np.array(window_size_list)*0.1, y=ranks_mean, color="red")
        plt.errorbar(x=np.array(window_size_list)*0.1, y=ranks_mean, yerr=ranks_std, ls="none", color="red")
        plt.title("RANK OF CORRELATION MATRICES")
        plt.ylabel("RANK (MEAN +- STD)")
        plt.xlabel("WINDOW SIZE (s)")
        plt.hlines(nr_cells, 0, max(np.array(window_size_list)*0.1), color="red", linestyle="--", label="FULL RANK")
        plt.legend(loc="lower right")
        plt.grid(color="grey")
        plt.show()

    def predict_time_progression_pop_vec_and_corr(self, part_to_analyze="all", time_bin_and_window_size_s=60, nr_pcs=15,
                                                  only_stable_cells=False):

        if part_to_analyze == "all":

            # load only stable cells
            if only_stable_cells:
                bins_per_corr_matrix = int(time_bin_and_window_size_s / self.params.time_bin_size)

                if self.params.stable_cell_method == "k_means":
                    # load only stable cells
                    with open(self.params.pre_proc_dir + "cell_classification/" +
                              self.params.session_name + "_k_means.pickle", "rb") as f:
                        class_dic = pickle.load(f)
                    stable_ids = class_dic["stable_cell_ids"].flatten()

                elif self.params.stable_cell_method == "mean_firing_awake":
                    # load only stable cells
                    with open(self.params.pre_proc_dir + "cell_classification/" +
                              self.params.session_name + "_mean_firing_awake.pickle", "rb") as f:
                        class_dic = pickle.load(f)

                    stable_ids = class_dic["stable_cell_ids"].flatten()

                print("ONLY STABLE CELLS!!!")

                nr_cells = stable_ids.shape[0]

                corr_mat = np.zeros((int(nr_cells * (nr_cells - 1) / 2), 0))

                # get population vectors and correlations
                raster = []
                first = 0
                for l_s in self.long_sleep:
                    duration = l_s.get_duration_sec()
                    r = l_s.get_raster()
                    raster.append(r)
                    first += duration
                    c_m = l_s.get_correlation_matrices(bins_per_corr_matrix=bins_per_corr_matrix,
                                                       only_upper_triangle=True, cell_selection=stable_ids)
                    corr_mat = np.hstack((corr_mat, c_m))

                # apply PCA to correlation matrices
                pca = PCA(n_components=nr_pcs)
                pca_data = pca.fit_transform(X=corr_mat.T).T

                raster = np.hstack(raster)
                scaler = int(time_bin_and_window_size_s / self.params.time_bin_size)
                raster = down_sample_array_sum(x=raster, chunk_size=scaler)

                # only select stable cells
                raster = raster[stable_ids,:]

                times = np.arange(0, raster.shape[1]) * time_bin_and_window_size_s
                new_ml = MlMethodsOnePopulation()

                # trim both to same length
                if raster.shape[1] > pca_data.shape[1]:
                    raster = raster[:, :pca_data.shape[1]]
                elif raster.shape[1] < pca_data.shape[1]:
                    pca_data = pca_data[:, :raster.shape[1]]

                # compute using only
                res_only_pop_vecs = []
                for nr_fits in range(100):
                    r2 = new_ml.ridge_time_bin_progress(x=raster, y=times, new_time_bin_size="CONST #SPIKES",
                                                        alpha_fitting=True, plotting=False)
                    res_only_pop_vecs.append(r2)

                raster_and_corr = np.vstack((pca_data, raster))
                res_pop_vec_and_corr = []
                for nr_fits in range(100):
                    r2 = new_ml.ridge_time_bin_progress(x=raster_and_corr, y=times, new_time_bin_size="CONST #SPIKES",
                                                        alpha_fitting=True, plotting=False)
                    res_pop_vec_and_corr.append(r2)

                res = np.vstack((np.array(res_only_pop_vecs), np.array(res_pop_vec_and_corr))).T

            else:
                bins_per_corr_matrix = int(time_bin_and_window_size_s / self.params.time_bin_size)

                nr_cells = self.long_sleep[0].get_nr_cells()

                corr_mat = np.zeros((int(nr_cells * (nr_cells - 1) / 2), 0))

                # get population vectors and correlations
                raster = []
                first = 0
                for l_s in self.long_sleep:
                    duration = l_s.get_duration_sec()
                    r = l_s.get_raster()
                    raster.append(r)
                    first += duration
                    c_m = l_s.get_correlation_matrices(bins_per_corr_matrix=bins_per_corr_matrix,
                                                       only_upper_triangle=True)
                    corr_mat = np.hstack((corr_mat, c_m))

                # apply PCA to correlation matrices
                pca = PCA(n_components=nr_pcs)
                pca_data = pca.fit_transform(X=corr_mat.T).T


                raster = np.hstack(raster)
                scaler = int(time_bin_and_window_size_s / self.params.time_bin_size)
                raster = down_sample_array_sum(x=raster, chunk_size=scaler)

                times = np.arange(0, raster.shape[1]) * time_bin_and_window_size_s
                new_ml = MlMethodsOnePopulation()

                # trim both to same length
                if raster.shape[1] > pca_data.shape[1]:
                    raster = raster[:,:pca_data.shape[1]]
                elif raster.shape[1] < pca_data.shape[1]:
                    pca_data = pca_data[:,:raster.shape[1]]

                # compute using only
                res_only_pop_vecs = []
                for nr_fits in range(100):
                    r2 = new_ml.ridge_time_bin_progress(x=raster, y=times, new_time_bin_size=time_bin_and_window_size_s,
                                                    alpha_fitting=True, plotting=False)
                    res_only_pop_vecs.append(r2)

                raster_and_corr = np.vstack((pca_data, raster))
                res_pop_vec_and_corr = []
                for nr_fits in range(100):
                    r2 = new_ml.ridge_time_bin_progress(x=raster_and_corr, y=times, new_time_bin_size="CONST #SPIKES",
                                                    alpha_fitting=True, plotting=False)
                    res_pop_vec_and_corr.append(r2)

                res = np.vstack((np.array(res_only_pop_vecs), np.array(res_pop_vec_and_corr))).T

            c = "white"

            bplot = plt.boxplot(res, positions=[1, 2], patch_artist=True,
                                labels=["ONLY POP. VEC", "POP. VEC. AND CORR."],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c),
                                )
            plt.ylabel("R2 (100 SPLITS)")
            plt.show()

    def drift_correlation_structure(self, plot_file_name="test", bins_per_corr_matrix=600,
                                    only_stable_cells=False, n_smoothing=40):
        # --------------------------------------------------------------------------------------------------------------
        # analyzes memory drift using correlation structure. Computes correlation matrix of awake behavior before and
        # correlation matrix of behavior after -> compares correlation matrix computed from sliding window during sleep
        # with before/after correlation matrix using Pearson correlation value
        #
        # parameters:   - correlation_window_size, int: size of sliding window (nr. time bins) to compute correlations
        #                 during sleep
        #
        # returns:      -
        # --------------------------------------------------------------------------------------------------------------

        nr_cells = self.long_sleep[0].get_nr_cells()

        if only_stable_cells:
            # load only stable cells
            with open(
                    self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                    "rb") as f:
                class_dic = pickle.load(f)

            stable_ids = class_dic["stable_cell_ids"]

            nr_cells = stable_ids.shape[0]

            corr_sleep = np.zeros((int(nr_cells * (nr_cells - 1) / 2), 0))

            for l_s in self.long_sleep:
                c_m = l_s.get_correlation_matrices(bins_per_corr_matrix=bins_per_corr_matrix,
                                                   cell_selection=stable_ids, only_upper_triangle=True)
                corr_sleep = np.hstack((corr_sleep, c_m))

        else:

            # corr_sleep = np.zeros((nr_cells ** 2, 0))
            # only off-diagonal elements
            corr_sleep = np.zeros((int(nr_cells * (nr_cells - 1) / 2), 0))

            for l_s in self.long_sleep:
                c_m = l_s.get_correlation_matrices(bins_per_corr_matrix=bins_per_corr_matrix, only_upper_triangle=True)
                corr_sleep = np.hstack((corr_sleep, c_m))

        # define first 10% and last 10% as template
        first_10_template = np.mean(corr_sleep[:,:int(corr_sleep.shape[1]*0.1)], axis=1)
        last_10_template = np.mean(corr_sleep[:,-int(corr_sleep.shape[1]*0.1):], axis=1)

        sleep_data = corr_sleep[:,int(corr_sleep.shape[1]*0.1):-int(corr_sleep.shape[1]*0.1)]

        sim_pearson = []

        # for each sliding window compute similarity with behavior before/after
        for corr in sleep_data.T:
            sim_post = abs(pearsonr(corr, last_10_template)[0])
            sim_pre = abs(pearsonr(corr, first_10_template)[0])
            sim_pearson.append((sim_post - sim_pre) / (sim_post + sim_pre))
            # TODO: how to deal with negative correlation values

        x_axis = (np.arange(len(sim_pearson)) * bins_per_corr_matrix * self.params.time_bin_size / 60) / 60
        fig = plt.figure()
        ax = fig.add_subplot()
        ax.plot(x_axis, sim_pearson, color="red", label="PEARSON")
        plt.title("CORRELATION STRUCTURE SIMILARITY: FIRST 10% - LAST 10%\n #BINS PER WINDOW: " + str(bins_per_corr_matrix))
        plt.xlabel("TIME (hours)")
        plt.ylabel("SIMILARITY FIRST 10% - LAST 10% / PEARSON")
        plt.ylim(-1, 1)
        plt.show()

        sim_pearson = np.array(sim_pearson)

        s = sim_pearson.copy()
        control = []
        # control --> do 50 shuffles
        for i in range(50):
            np.random.shuffle(s)
            s_smooth = moving_average(a=s, n=n_smoothing)
            control.append(s_smooth)

        x_axis = (np.arange(s_smooth.shape[0]) * bins_per_corr_matrix * self.params.time_bin_size / 60) / 60
        control = np.array(control)
        con_mean = np.mean(control, axis=0)
        con_std = np.std(control, axis=0)
        fig = plt.figure()
        ax = fig.add_subplot()
        # smoothing
        sim_pearson_s = moving_average(a=np.array(sim_pearson), n=n_smoothing)
        ax.plot(x_axis, con_mean, color="grey", label="CONTROL (MEAN +- STD), 50 SHUFFLES")
        ax.plot(x_axis, con_mean + con_std, color="grey", linestyle="dashed")
        ax.plot(x_axis, con_mean - con_std, color="grey", linestyle="dashed")

        ax.plot(x_axis, sim_pearson_s, color="red", label="DATA")
        plt.title("CORRELATION STRUCTURE SIMILARITY: FIRST 10% - LAST 10%\n #BINS PER WINDOW: " + str(bins_per_corr_matrix))
        plt.xlabel("TIME (hours)")
        plt.ylabel("FIRST 10% - LAST 10% SIMILARITY")
        plt.ylim(-0.33, 0.33)
        # plt.ylim(min(sim_pearson_s), -1*min(sim_pearson_s))
        plt.legend()
        plt.show()

        # speed_smooth = moving_average(a=speed, n=n_smoothing)
        # plt.plot(x_axis, speed_smooth)
        # plt.xlabel("TIME (hours)")
        # plt.ylabel("SPEED (cm/s)")
        # plt.show()

        exit()
        plt.savefig(os.path.join(save_path, self.params.pre_proc_dir + plot_file_name))

    def instant_change(self, part_to_analyze="all", only_stable_cells=False, distance_meas="cos"):

        if part_to_analyze == "all":

            # load only stable cells
            if only_stable_cells:
                bins_per_corr_matrix = int(time_bin_and_window_size_s / self.params.time_bin_size)

                if self.params.stable_cell_method == "k_means":
                    # load only stable cells
                    with open(self.params.pre_proc_dir + "cell_classification/" +
                              self.params.session_name + "_k_means.pickle", "rb") as f:
                        class_dic = pickle.load(f)
                    stable_ids = class_dic["stable_cell_ids"].flatten()

                elif self.params.stable_cell_method == "mean_firing_awake":
                    # load only stable cells
                    with open(self.params.pre_proc_dir + "cell_classification/" +
                              self.params.session_name + "_mean_firing_awake.pickle", "rb") as f:
                        class_dic = pickle.load(f)

                    stable_ids = class_dic["stable_cell_ids"].flatten()

                print("ONLY STABLE CELLS!!!")

                nr_cells = stable_ids.shape[0]

                corr_mat = np.zeros((int(nr_cells * (nr_cells - 1) / 2), 0))

                # get population vectors and correlations
                raster = []
                first = 0
                for l_s in self.long_sleep:
                    duration = l_s.get_duration_sec()
                    r = l_s.get_raster()
                    raster.append(r)
                    first += duration
                    c_m = l_s.get_correlation_matrices(bins_per_corr_matrix=bins_per_corr_matrix,
                                                       only_upper_triangle=True, cell_selection=stable_ids)
                    corr_mat = np.hstack((corr_mat, c_m))

                # apply PCA to correlation matrices
                pca = PCA(n_components=nr_pcs)
                pca_data = pca.fit_transform(X=corr_mat.T).T

                raster = np.hstack(raster)
                scaler = int(time_bin_and_window_size_s / self.params.time_bin_size)
                raster = down_sample_array_sum(x=raster, chunk_size=scaler)

                # only select stable cells
                raster = raster[stable_ids,:]

                times = np.arange(0, raster.shape[1]) * time_bin_and_window_size_s
                new_ml = MlMethodsOnePopulation()

                # trim both to same length
                if raster.shape[1] > pca_data.shape[1]:
                    raster = raster[:, :pca_data.shape[1]]
                elif raster.shape[1] < pca_data.shape[1]:
                    pca_data = pca_data[:, :raster.shape[1]]

                # compute using only
                res_only_pop_vecs = []
                for nr_fits in range(100):
                    r2 = new_ml.ridge_time_bin_progress(x=raster, y=times, new_time_bin_size="CONST #SPIKES",
                                                        alpha_fitting=True, plotting=False)
                    res_only_pop_vecs.append(r2)

                raster_and_corr = np.vstack((pca_data, raster))
                res_pop_vec_and_corr = []
                for nr_fits in range(100):
                    r2 = new_ml.ridge_time_bin_progress(x=raster_and_corr, y=times, new_time_bin_size="CONST #SPIKES",
                                                        alpha_fitting=True, plotting=False)
                    res_pop_vec_and_corr.append(r2)

                res = np.vstack((np.array(res_only_pop_vecs), np.array(res_pop_vec_and_corr))).T

            else:
                first = 0
                # get population vectors and correlations
                raster = []
                rem_phase = []
                nrem_phase = []
                for l_s in self.long_sleep:
                    duration = l_s.get_duration_sec()
                    r = l_s.get_raster()
                    raster.append(r)
                    rem_p = l_s.get_sleep_phase("rem") + first
                    nrem_p = l_s.get_sleep_phase("nrem") + first
                    rem_phase.append(rem_p)
                    nrem_phase.append(nrem_p)
                    first += duration
                raster = np.hstack(raster)
                rem_phase = np.vstack(rem_phase)
                nrem_phase = np.vstack(nrem_phase)

            r1 = raster[:, :10000]

            # compute distances
            distance_mat_1 = distance.squareform(distance.pdist(r1.T, metric=distance_meas))

            neigh_dist_1 = np.nan_to_num(np.diag(v=distance_mat_1, k=1))

            x_ = np.arange(neigh_dist_1.shape[0])
            # plot sleep phases
            rem_phase_mod = rem_phase[rem_phase[:, 1] < neigh_dist_1.shape[0]]
            nrem_phase_mod = nrem_phase[nrem_phase[:, 1] < neigh_dist_1.shape[0]]
            plt.plot(neigh_dist_1, color="gray")
            for rem_p in rem_phase_mod:
                plt.plot(x_[int(rem_p[0]):int(rem_p[1])], neigh_dist_1[int(rem_p[0]):int(rem_p[1])], color="red", label="REM")
            for nrem_p in nrem_phase_mod:
                plt.plot(x_[int(nrem_p[0]):int(nrem_p[1])], neigh_dist_1[int(nrem_p[0]):int(nrem_p[1])], color="blue", label="NREM")
            handles, labels = plt.gca().get_legend_handles_labels()
            by_label = dict(zip(labels, handles))
            plt.legend(by_label.values(), by_label.keys())
            plt.xlim(3000, 5000)
            plt.xlabel("Time (s)")
            plt.ylabel("Cos distance")
            plt.show()

            print("HERE")

        result_per_event, event_times, length_per_event, duration_event_in_s, pre_prob_arr, post_prob_arr = \
            self.memory_drift_phmm_results_event_times(event_times=nrem_phase_mod)
    # </editor-fold>

    # <editor-fold desc="Drift in correlations">
    def drift_correlation_structure_equalized_firing_rates_all_cells(self, len_chunk_s = 200,
                                                                     n_equalizing=1, n_smoothing=20,
                                                                     plot_for_control=False, plotting=True,
                                                                    first_n_matrices_as_ref=10):

        raster_sleep = []

        for l_s in self.long_sleep:
            r = l_s.get_raster()
            raster_sleep.append(r)

        raster_sleep = np.hstack(raster_sleep)

        # filter cells that are quite most of the time
        per_cell_percent_firing = np.count_nonzero(raster_sleep, axis=1)/raster_sleep.shape[1]
        cells_to_exclude = np.where(per_cell_percent_firing < 0.01)
        raster_sleep = np.delete(raster_sleep, cells_to_exclude, axis=0)

        nr_chunks = raster_sleep.shape[1]*self.params.time_bin_size/len_chunk_s
        chunk_size = np.round(raster_sleep.shape[1]/nr_chunks).astype(int)
        # split into equal size parts to find min. number of spikes
        raster_sleep_chunks = down_sample_array_sum(x=raster_sleep, chunk_size=chunk_size)

        # nr of windows with zero spikes
        nr_time_bins_more_than_zero_spikes = np.count_nonzero(np.where(raster_sleep_chunks==0, 1, 0), axis=0)
        # binarize
        nr_time_bins_more_than_zero_spikes = np.where(nr_time_bins_more_than_zero_spikes==0, 1, 0).astype(bool)

        # remove time time windows if there is a single cell that doesn't spike
        raster_sleep_chunks_filtered = raster_sleep_chunks[:, nr_time_bins_more_than_zero_spikes]

        # min. number of spikes per window
        min_nr_spikes = np.min(raster_sleep_chunks_filtered, axis=1).astype(int)

        # need to filter periods where there is no spike for at least one cell in the initial raster
        nr_time_bins_more_than_zero_spikes_orig_resolution = nr_time_bins_more_than_zero_spikes.repeat(chunk_size).astype(bool)
        raster_sleep_filtered = raster_sleep[:,:nr_time_bins_more_than_zero_spikes_orig_resolution.shape[0]]
        raster_sleep_filtered = raster_sleep_filtered[:,nr_time_bins_more_than_zero_spikes_orig_resolution]

        # need to generate many equalized rasters to compute mean later
        all_corr_matrices = []
        for n_eq in range(n_equalizing):
            additional_chunk_to_remove = []
            equalized_raster = np.zeros(raster_sleep_filtered.shape)
            # go through rasters and remove random spikes to equalize firing rates over time
            for chunk_id in range(int(np.round(nr_chunks))):
                # if end of chunk is bigger that filtered raster --> leave loop
                if (chunk_id+1)*chunk_size > raster_sleep_filtered.shape[1]:
                    additional_chunk_to_remove.append(chunk_id)
                    break
                # find all time bins with spikes
                raster_chunk = raster_sleep_filtered[:, chunk_id*chunk_size:(chunk_id+1)*chunk_size]
                raster_chunk_copy = np.copy(raster_chunk)
                # go trough all cells
                for cell_id, cell_min_nr_spikes in enumerate(min_nr_spikes):
                    # keep removing spikes until min. nr. spikes is reached
                    cell_chunk = np.copy(raster_chunk[cell_id,:]).astype(int)
                    nr_spikes_in_chunk = np.sum(cell_chunk)
                    while nr_spikes_in_chunk > cell_min_nr_spikes:
                        nr_spikes_to_remove = nr_spikes_in_chunk - cell_min_nr_spikes
                        # find time bins with spikes
                        cell_bins_with_spikes = np.argwhere(cell_chunk > 0).flatten()
                        # chunk with zero spikes for one cell
                        if cell_bins_with_spikes.shape[0] == 0:
                            additional_chunk_to_remove.append(chunk_id)
                            break
                        # there are less time bins with spikes than spikes to remove --> need to go through while loop
                        # another time
                        elif nr_spikes_to_remove > cell_bins_with_spikes.shape[0]:
                            spikes_to_remove = np.random.choice(a=cell_bins_with_spikes, size=cell_bins_with_spikes.shape[0],
                                                                replace=False)
                        else:
                            spikes_to_remove = np.random.choice(a=cell_bins_with_spikes, size=nr_spikes_to_remove,
                                                                replace=False)
                        # remove n spikes from bins to match min_nr_spikes
                        cell_chunk[spikes_to_remove] -= 1
                        # check if more spikes need to be removed
                        nr_spikes_in_chunk = np.sum(cell_chunk)

                    equalized_raster[cell_id, chunk_id*chunk_size:(chunk_id+1)*chunk_size] = cell_chunk

                # # if last chunk is too small --> do not add it
                # if chunk_size == raster_chunk_copy.shape[1]:
                #     equalized_raster[:, chunk_id*chunk_size:(chunk_id+1)*chunk_size] = raster_chunk_copy
                # else:
                #     break
            if len(additional_chunk_to_remove)>0:
                # remove additional chunks, TODO: might be useless
                min_chunks_to_remove = min(additional_chunk_to_remove)
                equalized_raster = equalized_raster[:, :min_chunks_to_remove*chunk_size]
            if plot_for_control:
                # check equalized raster
                equalized_raster_test = down_sample_array_sum(x=equalized_raster, chunk_size=chunk_size)
                plt.figure(figsize=(7,3))
                plt.imshow(equalized_raster_test)
                plt.title("Spikes per chunk")
                plt.xlabel("Chunk ID")
                plt.ylabel("Cell ID")
                plt.show()
                plt.figure(figsize=(9,2))
                plt.title("Spikes per time bin")
                plt.xlabel("Time bin")
                plt.ylabel("Cell ID")
                plt.imshow(equalized_raster[:,:500])
                plt.show()

            # compute correlations from equalized rasters --> 60 seconds was a good estimate to get full rank corr. matrices
            len_corr_chunk_s = 200
            nr_chunks_corr = equalized_raster.shape[1]*self.params.time_bin_size/len_corr_chunk_s
            chunk_size = np.round(equalized_raster.shape[1]/nr_chunks_corr).astype(int)

            corr_matrices_sleep = []

            for i in range(int(nr_chunks_corr)):
                corr_matrices_sleep.append(upper_tri_without_diag(np.corrcoef(equalized_raster[:, i*chunk_size:(i+1)*chunk_size])))

            all_corr_matrices.append(corr_matrices_sleep)


        all_sim_pearson = []
        for corr_matrices_sleep in all_corr_matrices:
            corr_matrices_sleep_arr = np.vstack(corr_matrices_sleep)
            ref_corr = np.mean(corr_matrices_sleep_arr[:first_n_matrices_as_ref, :], axis=0)
            pearson_corr = []
            # for each sliding window compute similarity with behavior before/after
            for corr in corr_matrices_sleep[first_n_matrices_as_ref:]:
                pearson_corr.append(pearsonr(corr, ref_corr)[0])
            all_sim_pearson.append(pearson_corr)

        all_sim_pearson = np.vstack(all_sim_pearson)

        all_sim_pearson_smooth = []
        for sim_pearson in all_sim_pearson:
            # apply some smoothing
            all_sim_pearson_smooth.append(moving_average(a=np.array(sim_pearson), n=n_smoothing))

        all_sim_pearson_smooth = np.vstack(all_sim_pearson_smooth)

        if n_equalizing > 1:
            mean = np.mean(all_sim_pearson_smooth, axis=0)
            std = np.std(all_sim_pearson_smooth, axis=0)
        else:
            mean = all_sim_pearson_smooth
            std=None

        if plotting:
            x_axis = (np.arange(len(all_sim_pearson[0])) * len_corr_chunk_s)/60/60
            fig = plt.figure()
            ax = fig.add_subplot()
            ax.errorbar(x=x_axis, y=np.mean(all_sim_pearson, axis=0), yerr=np.std(all_sim_pearson, axis=0),
                        color="red", label="PEARSON")
            plt.title("CORRELATION STRUCTURE SIMILARITY: BEFORE - AFTER\n #Duration to compute corr: " +
                      str(np.round(len_corr_chunk_s/60,1))+" min")
            plt.xlabel("TIME (h)")
            plt.ylabel("SIMILARITY WITH FIRST PART OF SLEEP")
            plt.ylim(-1, 1)
            plt.show()

            x_axis = (np.arange(all_sim_pearson_smooth.shape[1]) * len_corr_chunk_s/200)/60/60
            fig = plt.figure()
            ax = fig.add_subplot()
            ax.errorbar(x=x_axis, y=np.mean(all_sim_pearson_smooth, axis=0), yerr=np.std(all_sim_pearson_smooth, axis=0),
                        color="red", label="PEARSON")
            plt.title("CORRELATION STRUCTURE SIMILARITY: BEFORE - AFTER\n #Duration to compute corr: " +
                      str(np.round(len_corr_chunk_s/60,1))+" min - smoothed")
            plt.xlabel("TIME")
            plt.ylabel("SIMILARITY WITH FIRST PART OF SLEEP")
            plt.ylim(-1, 1)
            plt.show()
        else:
            return mean, std

    def drift_correlation_structure_all_cells(self, len_chunk_s = 200, n_smoothing=20,
                                                                     plot_for_control=False, plotting=True,
                                                                    first_n_matrices_as_ref=10):

        raster_sleep = []

        for l_s in self.long_sleep:
            r = l_s.get_raster()
            raster_sleep.append(r)

        raster_sleep = np.hstack(raster_sleep)

        # filter cells that are quiet most of the time
        per_cell_percent_firing = np.count_nonzero(raster_sleep, axis=1)/raster_sleep.shape[1]
        cells_to_exclude = np.where(per_cell_percent_firing < 0.01)
        raster_sleep = np.delete(raster_sleep, cells_to_exclude, axis=0)

        nr_chunks = raster_sleep.shape[1]*self.params.time_bin_size/len_chunk_s
        chunk_size = np.round(raster_sleep.shape[1]/nr_chunks).astype(int)
        # split into equal size parts to find min. number of spikes
        raster_sleep_chunks = down_sample_array_sum(x=raster_sleep, chunk_size=chunk_size)

        # nr of windows with zero spikes
        nr_time_bins_more_than_zero_spikes = np.count_nonzero(np.where(raster_sleep_chunks==0, 1, 0), axis=0)
        # binarize
        nr_time_bins_more_than_zero_spikes = np.where(nr_time_bins_more_than_zero_spikes==0, 1, 0).astype(bool)

        # remove time time windows if there is a single cell that doesn't spike
        raster_sleep_chunks_filtered = raster_sleep_chunks[:, nr_time_bins_more_than_zero_spikes]

        # min. number of spikes per window
        min_nr_spikes = np.min(raster_sleep_chunks_filtered, axis=1).astype(int)

        # need to filter periods where there is no spike for at least one cell in the initial raster
        nr_time_bins_more_than_zero_spikes_orig_resolution = nr_time_bins_more_than_zero_spikes.repeat(chunk_size).astype(bool)
        raster_sleep_filtered = raster_sleep[:,:nr_time_bins_more_than_zero_spikes_orig_resolution.shape[0]]
        raster_sleep_filtered = raster_sleep_filtered[:,nr_time_bins_more_than_zero_spikes_orig_resolution]


        # compute correlations from equalized rasters --> 60 seconds was a good estimate to get full rank corr. matrices
        len_corr_chunk_s = 200
        nr_chunks_corr = raster_sleep_filtered.shape[1]*self.params.time_bin_size/len_corr_chunk_s
        chunk_size = np.round(raster_sleep_filtered.shape[1]/nr_chunks_corr).astype(int)

        corr_matrices_sleep = []

        for i in range(int(nr_chunks_corr)):
            corr_matrices_sleep.append(upper_tri_without_diag(np.corrcoef(raster_sleep_filtered[:, i*chunk_size:(i+1)*chunk_size])))

        all_sim_pearson = []

        corr_matrices_sleep_arr = np.vstack(corr_matrices_sleep)
        ref_corr = np.mean(corr_matrices_sleep_arr[:first_n_matrices_as_ref, :], axis=0)
        pearson_corr = []
        # for each sliding window compute similarity with behavior before/after
        for corr in corr_matrices_sleep[first_n_matrices_as_ref:]:
            pearson_corr.append(pearsonr(corr, ref_corr)[0])
        all_sim_pearson.append(pearson_corr)

        all_sim_pearson = np.vstack(all_sim_pearson)

        all_sim_pearson_smooth = []
        for sim_pearson in all_sim_pearson:
            # apply some smoothing
            all_sim_pearson_smooth.append(moving_average(a=np.array(sim_pearson), n=n_smoothing))

        all_sim_pearson_smooth = np.vstack(all_sim_pearson_smooth)

        mean = all_sim_pearson_smooth
        std=None

        if plotting:
            x_axis = (np.arange(len(all_sim_pearson[0])) * len_corr_chunk_s)/60/60
            fig = plt.figure()
            ax = fig.add_subplot()
            ax.errorbar(x=x_axis, y=np.mean(all_sim_pearson, axis=0), yerr=np.std(all_sim_pearson, axis=0),
                        color="red", label="PEARSON")
            plt.title("CORRELATION STRUCTURE SIMILARITY: BEFORE - AFTER\n #Duration to compute corr: " +
                      str(np.round(len_corr_chunk_s/60,1))+" min")
            plt.xlabel("TIME (h)")
            plt.ylabel("SIMILARITY WITH FIRST PART OF SLEEP")
            plt.ylim(-1, 1)
            plt.show()

            x_axis = (np.arange(all_sim_pearson_smooth.shape[1]) * len_corr_chunk_s/200)/60/60
            fig = plt.figure()
            ax = fig.add_subplot()
            ax.errorbar(x=x_axis, y=np.mean(all_sim_pearson_smooth, axis=0), yerr=np.std(all_sim_pearson_smooth, axis=0),
                        color="red", label="PEARSON")
            plt.title("CORRELATION STRUCTURE SIMILARITY: BEFORE - AFTER\n #Duration to compute corr: " +
                      str(np.round(len_corr_chunk_s/60,1))+" min - smoothed")
            plt.xlabel("TIME")
            plt.ylabel("SIMILARITY WITH FIRST PART OF SLEEP")
            plt.ylim(-1, 1)
            plt.show()
        else:
            return mean, std

    def drift_correlation_structure_equalized_firing_rates_vs_not_all_cells(self, len_chunk_s = 200,
                                                                     n_equalizing=10, n_smoothing=20,
                                                                     plot_for_control=False, plotting=True,
                                                                    first_n_matrices_as_ref=10):

        raster_sleep = []

        for l_s in self.long_sleep:
            r = l_s.get_raster()
            raster_sleep.append(r)

        raster_sleep = np.hstack(raster_sleep)

        # filter cells that are quite most of the time
        per_cell_percent_firing = np.count_nonzero(raster_sleep, axis=1)/raster_sleep.shape[1]
        cells_to_exclude = np.where(per_cell_percent_firing < 0.01)
        raster_sleep = np.delete(raster_sleep, cells_to_exclude, axis=0)

        nr_chunks = raster_sleep.shape[1]*self.params.time_bin_size/len_chunk_s
        chunk_size = np.round(raster_sleep.shape[1]/nr_chunks).astype(int)
        # split into equal size parts to find min. number of spikes
        raster_sleep_chunks = down_sample_array_sum(x=raster_sleep, chunk_size=chunk_size)

        # nr of windows with zero spikes
        nr_time_bins_more_than_zero_spikes = np.count_nonzero(np.where(raster_sleep_chunks==0, 1, 0), axis=0)
        # binarize
        nr_time_bins_more_than_zero_spikes = np.where(nr_time_bins_more_than_zero_spikes==0, 1, 0).astype(bool)

        # remove time time windows if there is a single cell that doesn't spike
        raster_sleep_chunks_filtered = raster_sleep_chunks[:, nr_time_bins_more_than_zero_spikes]

        # min. number of spikes per window
        min_nr_spikes = np.min(raster_sleep_chunks_filtered, axis=1).astype(int)

        # need to filter periods where there is no spike for at least one cell in the initial raster
        nr_time_bins_more_than_zero_spikes_orig_resolution = nr_time_bins_more_than_zero_spikes.repeat(chunk_size).astype(bool)
        raster_sleep_filtered = raster_sleep[:,:nr_time_bins_more_than_zero_spikes_orig_resolution.shape[0]]
        raster_sleep_filtered = raster_sleep_filtered[:,nr_time_bins_more_than_zero_spikes_orig_resolution]


        corr_matrices_sleep_non_eq = []
        len_corr_chunk_s = 200
        nr_chunks_corr = raster_sleep_filtered.shape[1] * self.params.time_bin_size / len_corr_chunk_s
        chunk_size = np.round(raster_sleep_filtered.shape[1] / nr_chunks_corr).astype(int)
        for i in range(int(nr_chunks_corr)):
            corr_matrices_sleep_non_eq.append(upper_tri_without_diag(np.corrcoef(raster_sleep_filtered[:, i*chunk_size:(i+1)*chunk_size])))

        # need to generate many equalized rasters to compute mean later
        all_corr_matrices_equalized = []
        for n_eq in range(n_equalizing):
            additional_chunk_to_remove = []
            equalized_raster = np.zeros(raster_sleep_filtered.shape)
            # go through rasters and remove random spikes to equalize firing rates over time
            for chunk_id in range(int(np.round(nr_chunks))):
                # if end of chunk is bigger that filtered raster --> leave loop
                if (chunk_id+1)*chunk_size > raster_sleep_filtered.shape[1]:
                    additional_chunk_to_remove.append(chunk_id)
                    break
                # find all time bins with spikes
                raster_chunk = raster_sleep_filtered[:, chunk_id*chunk_size:(chunk_id+1)*chunk_size]
                raster_chunk_copy = np.copy(raster_chunk)
                # go trough all cells
                for cell_id, cell_min_nr_spikes in enumerate(min_nr_spikes):
                    # keep removing spikes until min. nr. spikes is reached
                    cell_chunk = np.copy(raster_chunk[cell_id,:]).astype(int)
                    nr_spikes_in_chunk = np.sum(cell_chunk)
                    while nr_spikes_in_chunk > cell_min_nr_spikes:
                        nr_spikes_to_remove = nr_spikes_in_chunk - cell_min_nr_spikes
                        # find time bins with spikes
                        cell_bins_with_spikes = np.argwhere(cell_chunk > 0).flatten()
                        # chunk with zero spikes for one cell
                        if cell_bins_with_spikes.shape[0] == 0:
                            additional_chunk_to_remove.append(chunk_id)
                            break
                        # there are less time bins with spikes than spikes to remove --> need to go through while loop
                        # another time
                        elif nr_spikes_to_remove > cell_bins_with_spikes.shape[0]:
                            spikes_to_remove = np.random.choice(a=cell_bins_with_spikes, size=cell_bins_with_spikes.shape[0],
                                                                replace=False)
                        else:
                            spikes_to_remove = np.random.choice(a=cell_bins_with_spikes, size=nr_spikes_to_remove,
                                                                replace=False)
                        # remove n spikes from bins to match min_nr_spikes
                        cell_chunk[spikes_to_remove] -= 1
                        # check if more spikes need to be removed
                        nr_spikes_in_chunk = np.sum(cell_chunk)

                    equalized_raster[cell_id, chunk_id*chunk_size:(chunk_id+1)*chunk_size] = cell_chunk

                # # if last chunk is too small --> do not add it
                # if chunk_size == raster_chunk_copy.shape[1]:
                #     equalized_raster[:, chunk_id*chunk_size:(chunk_id+1)*chunk_size] = raster_chunk_copy
                # else:
                #     break
            if len(additional_chunk_to_remove)>0:
                # remove additional chunks, TODO: might be useless
                min_chunks_to_remove = min(additional_chunk_to_remove)
                equalized_raster = equalized_raster[:, :min_chunks_to_remove*chunk_size]
            if plot_for_control:
                # check equalized raster
                equalized_raster_test = down_sample_array_sum(x=equalized_raster, chunk_size=chunk_size)
                plt.figure(figsize=(7,3))
                plt.imshow(equalized_raster_test)
                plt.title("Spikes per chunk")
                plt.xlabel("Chunk ID")
                plt.ylabel("Cell ID")
                plt.show()
                plt.figure(figsize=(9,2))
                plt.title("Spikes per time bin")
                plt.xlabel("Time bin")
                plt.ylabel("Cell ID")
                plt.imshow(equalized_raster[:,:500])
                plt.show()

            # compute correlations from equalized rasters --> 60 seconds was a good estimate to get full rank corr. matrices
            len_corr_chunk_s = 200
            nr_chunks_corr = equalized_raster.shape[1]*self.params.time_bin_size/len_corr_chunk_s
            chunk_size = np.round(equalized_raster.shape[1]/nr_chunks_corr).astype(int)

            corr_matrices_sleep = []

            for i in range(int(nr_chunks_corr)):
                corr_matrices_sleep.append(upper_tri_without_diag(np.corrcoef(equalized_raster[:, i*chunk_size:(i+1)*chunk_size])))

            all_corr_matrices_equalized.append(corr_matrices_sleep)

        all_res = []
        for eq_corr in all_corr_matrices_equalized:
            corr_res = np.zeros(len(eq_corr))
            for window_id, (eq_corr_window, non_eq_corr_window) in enumerate(zip(eq_corr, corr_matrices_sleep_non_eq)):
                corr_res[window_id] = pearsonr(eq_corr_window, non_eq_corr_window)[0]
            all_res.append(corr_res)

        all_res = np.vstack(all_res)

        plt.errorbar(x=np.arange(all_res.shape[1]), y=np.mean(all_res, axis=0), yerr=np.std(all_res, axis=0))
        plt.ylabel("Pearson R: \n eq_firing_corr vs. firing_corr")
        plt.xlabel("Window id (time)")
        plt.tight_layout()
        plt.show()
        plt.plot(moving_average(np.mean(all_res, axis=0), 10))
        plt.ylabel("Pearson R: \n eq_firing_corr vs. firing_corr")
        plt.xlabel("Window id (time)")
        plt.title("Smooth")
        plt.tight_layout()
        plt.show()

    def drift_correlation_structure_equalized_firing_rates(self, len_chunk_s = 200, n_equalizing=1,
                                                           plot_for_control=False, plotting=True, n_smoothing=20,
                                                                      cells_to_use="stable", first_n_matrices_as_ref=10):
        # --------------------------------------------------------------------------------------------------------------
        # analyzes memory drift using correlation structure. Computes correlation matrix of awake behavior before and
        # correlation matrix of behavior after -> compares correlation matrix computed from sliding window during sleep
        # with before/after correlation matrix using Pearson correlation value
        #
        # parameters:   - correlation_window_size, int: size of sliding window (nr. time bins) to compute correlations
        #                 during sleep
        #
        # returns:      -
        # --------------------------------------------------------------------------------------------------------------

        # check if cheeseboard data or exploration data is used

        # get rasters from exploration before/after

        # load only stable cells
        with open(
                self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                "rb") as f:
            class_dic = pickle.load(f)

        if cells_to_use == "stable":
            cell_ids = class_dic["stable_cell_ids"]
        elif cells_to_use == "increasing":
            cell_ids = class_dic["increase_cell_ids"]
        elif cells_to_use == "decreasing":
            cell_ids = class_dic["decrease_cell_ids"]
        else:
           raise Exception("Cell subset name not defined!")

        raster_sleep = []

        for l_s in self.long_sleep:
            r = l_s.get_raster()
            raster_sleep.append(r)

        raster_sleep = np.hstack(raster_sleep)

        # only select stable cells
        raster_sleep = raster_sleep[cell_ids, :]

        # filter cells that are quiet most of the time
        per_cell_percent_firing = np.count_nonzero(raster_sleep, axis=1)/raster_sleep.shape[1]
        cells_to_exclude = np.where(per_cell_percent_firing < 0.01)
        raster_sleep = np.delete(raster_sleep, cells_to_exclude, axis=0)

        nr_chunks = raster_sleep.shape[1]*self.params.time_bin_size/len_chunk_s
        chunk_size = np.round(raster_sleep.shape[1]/nr_chunks).astype(int)
        # split into equal size parts to find min. number of spikes
        raster_sleep_chunks = down_sample_array_sum(x=raster_sleep, chunk_size=chunk_size)

        # nr of windows with zero spikes
        nr_time_bins_more_than_zero_spikes = np.count_nonzero(np.where(raster_sleep_chunks==0, 1, 0), axis=0)
        # binarize
        nr_time_bins_more_than_zero_spikes = np.where(nr_time_bins_more_than_zero_spikes==0, 1, 0).astype(bool)

        # remove time time windows if there is a single cell that doesn't spike
        raster_sleep_chunks_filtered = raster_sleep_chunks[:, nr_time_bins_more_than_zero_spikes]

        # min. number of spikes per window
        min_nr_spikes = np.min(raster_sleep_chunks_filtered, axis=1).astype(int)


        # need to filter periods where there is no spike for at least one cell in the initial raster
        nr_time_bins_more_than_zero_spikes_orig_resolution = nr_time_bins_more_than_zero_spikes.repeat(chunk_size).astype(bool)
        raster_sleep_filtered = raster_sleep[:,:nr_time_bins_more_than_zero_spikes_orig_resolution.shape[0]]
        raster_sleep_filtered = raster_sleep_filtered[:,nr_time_bins_more_than_zero_spikes_orig_resolution]


        # need to generate many equalized rasters to compute mean later
        all_corr_matrices = []
        for n_eq in range(n_equalizing):
            additional_chunk_to_remove = []
            equalized_raster = np.zeros(raster_sleep_filtered.shape)
            # go through rasters and remove random spikes to equalize firing rates over time
            for chunk_id in range(int(np.round(nr_chunks))):
                # if end of chunk is bigger that filtered raster --> leave loop
                if (chunk_id+1)*chunk_size > raster_sleep_filtered.shape[1]:
                    additional_chunk_to_remove.append(chunk_id)
                    break
                # find all time bins with spikes
                raster_chunk = raster_sleep_filtered[:, chunk_id*chunk_size:(chunk_id+1)*chunk_size]
                raster_chunk_copy = np.copy(raster_chunk)
                # go trough all cells
                for cell_id, cell_min_nr_spikes in enumerate(min_nr_spikes):
                    # keep removing spikes until min. nr. spikes is reached
                    cell_chunk = np.copy(raster_chunk[cell_id,:]).astype(int)
                    nr_spikes_in_chunk = np.sum(cell_chunk)
                    while nr_spikes_in_chunk > cell_min_nr_spikes:
                        nr_spikes_to_remove = nr_spikes_in_chunk - cell_min_nr_spikes
                        # find time bins with spikes
                        cell_bins_with_spikes = np.argwhere(cell_chunk > 0).flatten()
                        # chunk with zero spikes for one cell
                        if cell_bins_with_spikes.shape[0] == 0:
                            additional_chunk_to_remove.append(chunk_id)
                            break
                        # there are less time bins with spikes than spikes to remove --> need to go through while loop
                        # another time
                        elif nr_spikes_to_remove > cell_bins_with_spikes.shape[0]:
                            spikes_to_remove = np.random.choice(a=cell_bins_with_spikes, size=cell_bins_with_spikes.shape[0],
                                                                replace=False)
                        else:
                            spikes_to_remove = np.random.choice(a=cell_bins_with_spikes, size=nr_spikes_to_remove,
                                                                replace=False)
                        # remove n spikes from bins to match min_nr_spikes
                        cell_chunk[spikes_to_remove] -= 1
                        # check if more spikes need to be removed
                        nr_spikes_in_chunk = np.sum(cell_chunk)

                    equalized_raster[cell_id, chunk_id*chunk_size:(chunk_id+1)*chunk_size] = cell_chunk

                # # if last chunk is too small --> do not add it
                # if chunk_size == raster_chunk_copy.shape[1]:
                #     equalized_raster[:, chunk_id*chunk_size:(chunk_id+1)*chunk_size] = raster_chunk_copy
                # else:
                #     break
            if len(additional_chunk_to_remove)>0:
                # remove additional chunks, TODO: might be useless
                min_chunks_to_remove = min(additional_chunk_to_remove)
                equalized_raster = equalized_raster[:, :min_chunks_to_remove*chunk_size]
            if plot_for_control:
                # check equalized raster
                equalized_raster_test = down_sample_array_sum(x=equalized_raster, chunk_size=chunk_size)
                plt.figure(figsize=(7,3))
                plt.imshow(equalized_raster_test)
                plt.title("Spikes per chunk")
                plt.xlabel("Chunk ID")
                plt.ylabel("Cell ID")
                plt.show()
                plt.figure(figsize=(9,2))
                plt.title("Spikes per time bin")
                plt.xlabel("Time bin")
                plt.ylabel("Cell ID")
                plt.imshow(equalized_raster[:,:500])
                plt.show()

            # compute correlations from equalized rasters --> 60 seconds was a good estimate to get full rank corr. matrices
            len_corr_chunk_s = 200
            nr_chunks_corr = equalized_raster.shape[1]*self.params.time_bin_size/len_corr_chunk_s
            chunk_size = np.round(equalized_raster.shape[1]/nr_chunks_corr).astype(int)

            corr_matrices_sleep = []

            for i in range(int(nr_chunks_corr)):
                corr_matrices_sleep.append(upper_tri_without_diag(np.corrcoef(equalized_raster[:, i*chunk_size:(i+1)*chunk_size])))

            all_corr_matrices.append(corr_matrices_sleep)


        all_sim_pearson = []
        for corr_matrices_sleep in all_corr_matrices:
            corr_matrices_sleep_arr = np.vstack(corr_matrices_sleep)
            ref_corr = np.mean(corr_matrices_sleep_arr[:first_n_matrices_as_ref, :], axis=0)
            pearson_corr = []
            # for each sliding window compute similarity with behavior before/after
            for corr in corr_matrices_sleep[first_n_matrices_as_ref:]:
                pearson_corr.append(pearsonr(corr, ref_corr)[0])
            all_sim_pearson.append(pearson_corr)

        all_sim_pearson = np.vstack(all_sim_pearson)


        all_sim_pearson_smooth = []
        for sim_pearson in all_sim_pearson:
            # apply some smoothing
            all_sim_pearson_smooth.append(moving_average(a=np.array(sim_pearson), n=n_smoothing))

        all_sim_pearson_smooth = np.vstack(all_sim_pearson_smooth)

        if n_equalizing > 1:
            mean = np.mean(all_sim_pearson_smooth, axis=0)
            std = np.std(all_sim_pearson_smooth, axis=0)
        else:
            mean = all_sim_pearson_smooth
            std=None

        if plotting:
            x_axis = (np.arange(len(all_sim_pearson[0])) * len_corr_chunk_s)/60/60
            fig = plt.figure()
            ax = fig.add_subplot()
            ax.errorbar(x=x_axis, y=np.mean(all_sim_pearson, axis=0), yerr=np.std(all_sim_pearson, axis=0),
                        color="red", label="PEARSON")
            plt.title("CORRELATION STRUCTURE SIMILARITY: BEFORE - AFTER\n #Duration to compute corr: " +
                      str(np.round(len_corr_chunk_s/60,1))+" min")
            plt.xlabel("TIME (h)")
            plt.ylabel("SIMILARITY BEFORE - AFTER / PEARSON")
            plt.ylim(-1, 1)
            plt.show()

            x_axis = (np.arange(all_sim_pearson_smooth.shape[1]) * len_corr_chunk_s/200)/60/60
            fig = plt.figure()
            ax = fig.add_subplot()
            ax.errorbar(x=x_axis, y=np.mean(all_sim_pearson_smooth, axis=0), yerr=np.std(all_sim_pearson_smooth, axis=0),
                        color="red", label="PEARSON")
            plt.title("CORRELATION STRUCTURE SIMILARITY: BEFORE - AFTER\n #Duration to compute corr: " +
                      str(np.round(len_corr_chunk_s/60,1))+" min - smoothed")
            plt.xlabel("TIME")
            plt.ylabel("SIMILARITY BEFORE - AFTER / PEARSON")
            plt.ylim(-1, 1)
            plt.show()
        else:
            return mean, std
    # </editor-fold>

    # <editor-fold desc="Others">
    def swr_frequency(self, window_size_min = 10, plotting=False):
        swr_times = []
        start_time = 0
        for l_s in self.long_sleep:
            swr_times_ = l_s.swr_times()
            swr_times.append(swr_times_+start_time)
            start_time += l_s.get_duration_sec()
        swr_times = np.hstack(swr_times)

        sleep_dur = start_time
        window_size_sec = window_size_min * 60
        n_windows = np.round(sleep_dur / window_size_sec).astype(int)

        swr_times_per_window = np.zeros(n_windows)
        for i_window in range(n_windows):
            swr_times_per_window[i_window] = np.count_nonzero(np.logical_and(i_window*window_size_sec<swr_times,
                                                                              swr_times < (i_window+1)*window_size_sec))
        if plotting:
            plt.plot(swr_times_per_window)
            plt.ylabel("#SWR / window ("+str(window_size_min)+"min)")
            plt.xlabel("Window ID (Time)")
            plt.tight_layout()
            plt.show()

            plt.plot(moving_average(swr_times_per_window,10))
            plt.ylabel("#SWR / window ("+str(window_size_min)+"min)")
            plt.xlabel("Window ID (Time)")
            plt.title("Smooth")
            plt.tight_layout()
            plt.show()
        return swr_times_per_window

    def swr_frequency_nrem(self, window_size_min=2, plotting=False):
        swr_frequency_nrem = []

        for l_s in self.long_sleep:
            swr_freq_ = l_s.get_swr_frequency_nrem()
            swr_frequency_nrem.append(swr_freq_)
        swr_frequency_nrem = np.hstack(swr_frequency_nrem)

        return swr_frequency_nrem

    def visualize_temporal_trend(self, debug=False, method="isomap", points_per_frame=10, save_fig=True):
        # mjc163R2R_0114 with time_bin_size =20

        cm_name = 'plasma'
        if debug:
            # datapoint for color

            cm = plt.get_cmap(cm_name, 100)
            C = [cm(n) for n in range(cm.N)]

            # Space Coordinate
            X = np.random.random((100,)) * 255 * 2 - 255
            Y = np.random.random((100,)) * 255 * 2 - 255
            Z = np.random.random((100,)) * 255 * 2 - 255

            # Magnitude of each point
            # M = np.random.random((100,))*-1+0.5
            M = np.random.randint(1, 70, size=100)
            # Time
            t = np.sort(np.random.random((100,)) * 10)

            # ID each point should be color coded. Moreover, each point belongs to a cluster `ID`
            ID = np.sort(np.round([np.random.random((100,)) * 5]))

            x = []
            y = []
            z = []
            m = []
            c = []

            def update_lines(i):
                #     for i in range (df_IS["EASTING [m]"].size):
                dx = X[i]
                dy = Y[i]
                dz = Z[i]
                dm = M[i]
                dc = C[i]
                #     text.set_text("{:d}: [{:.0f}] Mw[{:.2f}]".format(ID[i], t[i],ID[i]))  # for debugging
                x.append(dx)
                y.append(dy)
                z.append(dz)
                m.append(dm)
                c.append(dc)
                graph._offsets3d = (x, y, z)
                graph.set_facecolor(c)
                graph.set_edgecolor(c)
                ax.view_init(elev=10., azim=i)

                # graph.set_sizes(m)
                return graph,

            fig = plt.figure(figsize=(5, 5))
            ax = fig.add_subplot(111, projection="3d")
            graph = ax.scatter([], [], [], color='orange')  # s argument here # for debugging
            ax.view_init(elev=10., azim=90)
            ax.set_xlim3d(X.min(), X.max())
            ax.set_ylim3d(Y.min(), Y.max())
            ax.set_zlim3d(Z.min(), Z.max())

            # make grid and background transparent
            ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))
            ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))
            ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))
            ax.xaxis._axinfo["grid"]['color'] =  (1,1,1,0)
            ax.yaxis._axinfo["grid"]['color'] =  (1,1,1,0)
            ax.zaxis._axinfo["grid"]['color'] =  (1,1,1,0)
            ax.set_xticks([])
            ax.set_yticks([])
            ax.set_zticks([])
            ax.set_axis_off()

            for dir in range(3):
                coords=np.array([[-1, -1], [-1, -1], [-1, -1]])
                coords[dir, 1] = 0
                # plotting
                ax.plot3D(coords[0,:],coords[1,:], coords[2,:], color="grey")

            # Creating the Animation object
            ani = animation.FuncAnimation(fig, update_lines, frames=100, interval=50, blit=False, repeat=False)
            ani.save(self.session_name+"_mds_cosine_new.gif", dpi=150, writer=PillowWriter(fps=25))
            plt.show()

        raster = []
        for l_s in self.long_sleep:
            r = l_s.get_raster()
            raster.append(r)
        raster = np.hstack(raster)
        raster = raster[:, ::5]

        # plot population vectors
        # --------------------------------------------------------------------------------------------------------------
        raster_to_plot =raster[:, ::2]
        raster_to_plot =raster_to_plot[:, :5]

        # import matplotlib.cm as cm
        # cmap = cm.viridis
        # bounds = np.arange(np.max(np.max()))
        # norm = colors.BoundaryNorm(bounds, cmap.N)

        fig = plt.figure(figsize=(15, 15))
        gs = fig.add_gridspec(10, 10)
        ax1 = fig.add_subplot(gs[:9, :2])
        ax2 = fig.add_subplot(gs[:9, 2:4])
        ax3 = fig.add_subplot(gs[:9, 4:6])
        ax4 = fig.add_subplot(gs[:9, 6:8])
        ax5 = fig.add_subplot(gs[:9, 8:10])
        ax6 = fig.add_subplot(gs[9:, :8])
        axis_list = [ax1, ax2, ax3, ax4, ax5]

        for i, (pv, ax_) in enumerate(zip(raster_to_plot.T, axis_list)):
            if i == 0:
                # ax_.set_yticks([0,1,2], ["Cell 1", "Cell 2", "Cell 3"])
                m = ax_.imshow(np.expand_dims(pv, 1), interpolation='nearest', aspect='auto')
                # ax_.set_xticks([0.1], ["t1"])
            else:
                ax_.set_yticks([])
                ax_.imshow(np.expand_dims(pv, 1), interpolation='nearest', aspect='auto')
                # ax_.set_xticks([0.1], ["t"+str(i)])
            ax_.set_xticks([])
            # ax_.set_tick_params(left = False)
        a = fig.colorbar(mappable=m, cax=ax6, orientation="horizontal")
        a.set_label("#spikes")
        # a.set_ticks([0.5, 1.5, 2.5, 3.5, 4.5], ["1","2","3","4","5"])
        plt.tight_layout()
        if save_fig:
            plt.rcParams['svg.fonttype'] = 'none'
            plt.savefig("pop_vecs_for_mds.svg", transparent="True")
            plt.close()
        else:
            plt.show()

        if method == "mds":
            result = multi_dim_scaling(act_mat=raster, metric="euclidean")
        elif method == "isomap":
            result = perform_isomap(act_mat=raster)
        # pca_ = PCA(n_components=3)
        # result = pca_.fit_transform(raster[:, ::100].T)
        # result = result[::100, :]
        # result = perform_TSNE(act_mat=raster[:, ::100])
        #

        # Space Coordinate
        X = result[::2, 0]
        Y = result[::2, 1]
        Z = result[::2, 2]

        # find direction
        # a = np.mean(result[-5:,:], axis=0) - np.mean(result[:5, :], axis=0)

        # datapoint for color
        cm = plt.get_cmap(cm_name, X.shape[0])
        C = [cm(n) for n in range(cm.N)]


        #  generate plot with all data for colorbar
        a = np.array([[0,1]])
        plt.figure(figsize=(9, 1.5))
        img = plt.imshow(a, cmap=cm_name)
        plt.gca().set_visible(False)
        cax = plt.axes([0.1, 0.2, 0.8, 0.6])
        cbar = plt.colorbar(orientation="horizontal", cax=cax)
        cbar.set_ticks([0, 1])
        cbar.set_ticklabels(["early", "late"])
        cbar.ax.tick_params(labelsize=10)
        # a.set_label([0, 1], ["early", "late"])
        plt.rcParams['svg.fonttype'] = 'none'
        plt.savefig("color_bar.svg", transparent="True")


        # save figure using the first points
        fig = plt.figure(figsize=(5, 5))
        ax = fig.add_subplot(111, projection="3d")
        graph = ax.scatter(X[:5], Y[:5], Z[:5], color=C[:5])  # s argument here # for debugging
        ax.view_init(elev=20, azim=10, vertical_axis="z")
        ax.set_xlim3d(X.min(), X.max())
        ax.set_ylim3d(Y.min(), Y.max())
        ax.set_zlim3d(Z.min(), Z.max())
        # plot coordinate system
        offset = (X.min()- X.max())/10

        for dir in range(3):
            coords = np.array([[X.min()+(X.max()-X.min())/2, X.min()+(X.max()-X.min())/2], [Y.min()+(Y.max()-Y.min())/2,
                                                                                            Y.min()+(Y.max()-Y.min())/2],[Z.min()-offset, Z.min()-offset]])
            if method == "isomap" and dir ==2:
                coords[dir, 1] -= offset
            else:
                coords[dir, 1] += offset
            # plotting
            ax.plot3D(coords[0, :], coords[1, :], coords[2, :], color="grey")
        # make grid and background transparent
        ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))
        ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))
        ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))
        ax.xaxis._axinfo["grid"]['color'] = (1, 1, 1, 0)
        ax.yaxis._axinfo["grid"]['color'] = (1, 1, 1, 0)
        ax.zaxis._axinfo["grid"]['color'] = (1, 1, 1, 0)
        ax.set_xticks([])
        ax.set_yticks([])
        ax.set_zticks([])
        ax.set_axis_off()
        plt.savefig(method+"_viz_start.png", transparent="True")
        plt.close()

        x = []
        y = []
        z = []
        c = []

        def update_lines(i):
            #     for i in range (df_IS["EASTING [m]"].size):
            if ((i+1)*points_per_frame) < X.shape[0]:
                dx = X[i*points_per_frame:(i+1)*points_per_frame].tolist()
                dy = Y[i*points_per_frame:(i+1)*points_per_frame].tolist()
                dz = Z[i*points_per_frame:(i+1)*points_per_frame].tolist()
                dc = C[i*points_per_frame:(i+1)*points_per_frame]
                #     text.set_text("{:d}: [{:.0f}] Mw[{:.2f}]".format(ID[i], t[i],ID[i]))  # for debugging
                x.extend(dx)
                y.extend(dy)
                z.extend(dz)
                c.extend(dc)
                graph._offsets3d = (x, y, z)
                graph.set_facecolor(c)
                graph.set_edgecolor(c)
            ax.view_init(elev=20, azim=10+i/1.2, vertical_axis="z")
            # ax.view_init(elev=0, azim=-90, roll=i)
            # graph.set_sizes(m)
            return graph,

        fig = plt.figure(figsize=(5, 5))
        ax = fig.add_subplot(111, projection="3d")
        graph = ax.scatter([], [], [], color='orange')  # s argument here # for debugging
        ax.view_init(elev=20, azim=0, vertical_axis="z")
        ax.set_xlim3d(X.min(), X.max())
        ax.set_ylim3d(Y.min(), Y.max())
        ax.set_zlim3d(Z.min(), Z.max())

        # make grid and background transparent
        ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))
        ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))
        ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))
        ax.xaxis._axinfo["grid"]['color'] = (1, 1, 1, 0)
        ax.yaxis._axinfo["grid"]['color'] = (1, 1, 1, 0)
        ax.zaxis._axinfo["grid"]['color'] = (1, 1, 1, 0)
        ax.set_xticks([])
        ax.set_yticks([])
        ax.set_zticks([])
        ax.set_axis_off()

        for dir in range(3):
            coords = np.array([[X.min()+(X.max()-X.min())/2, X.min()+(X.max()-X.min())/2], [Y.min()+(Y.max()-Y.min())/2,
                                                                                            Y.min()+(Y.max()-Y.min())/2],[Z.min()-offset, Z.min()-offset]])
            if method == "isomap" and dir ==2:
                coords[dir, 1] -= offset
            else:
                coords[dir, 1] += offset
            # plotting
            ax.plot3D(coords[0, :], coords[1, :], coords[2, :], color="grey")

        # Creating the Animation object
        ani = animation.FuncAnimation(fig, update_lines, frames=999, interval=20, blit=False, repeat=False)
        ani.save(self.session_name+"_"+method+"_mds_cosine_new.gif", dpi=150, writer=PillowWriter(fps=100))
        plt.show()

    def swr_profile(self, plot_for_control=False, plotting=False, save_fig=False):

        mean_shape = []
        cells_per_tetrode = []

        for l_s in self.long_sleep:
            ms, cpt = l_s.swr_profile()
            mean_shape.append(ms)
            cells_per_tetrode.append(cpt)

        # compute mean shape across sleep parts
        mean_across_sleep = np.mean(np.array(mean_shape), axis=0)

        # get cell labels
        with open(self.params.pre_proc_dir + "cell_classification/" + self.session_name + "_k_means.pickle",
                "rb") as f:
            class_dic = pickle.load(f)

        stable_ids = class_dic["stable_cell_ids"]
        dec_ids = class_dic["decrease_cell_ids"]
        inc_ids = class_dic["increase_cell_ids"]

        stable_tetrode = cells_per_tetrode[0][stable_ids]
        dec_tetrode = cells_per_tetrode[0][dec_ids]
        inc_tetrode = cells_per_tetrode[0][inc_ids]

        shape_classifiyer = np.zeros(mean_across_sleep.shape[0])
        # go through swr shapes and classifiy them --> more positive around peak: +1, negative around peak: -1
        for i_tet, shape in enumerate(mean_across_sleep):
            # check values around the peak (center of window)
            shape_classifiyer[i_tet] = np.mean(shape[450:550])

            if plot_for_control:
                plt.plot(shape)
                plt.title(i_tet)
                plt.show()

        stable_shape = [shape_classifiyer[int(tet)] for tet in stable_tetrode]
        dec_shape = [shape_classifiyer[int(tet)] for tet in dec_tetrode]
        inc_shape = [shape_classifiyer[int(tet)] for tet in inc_tetrode]

        if plotting:
            plt.figure(figsize=(4,6))
            plt.subplot(2,1,1)
            if save_fig:
                plt.style.use('default')
            plt.plot(np.linspace(-500*(1/5e3)*1e3, 500*(1/5e3)*1e3, mean_across_sleep[11].shape[0]), mean_across_sleep[11])
            plt.ylabel("Amplitude (a.u.)")
            plt.ylim(-1800, 1800)
            plt.subplot(2,1,2)
            plt.plot(np.linspace(-500*(1/5e3)*1e3, 500*(1/5e3)*1e3, mean_across_sleep[5].shape[0]), mean_across_sleep[5])
            plt.xlabel("Time (ms)")
            plt.ylabel("Amplitude (a.u.)")
            plt.ylim(-1800, 1800)
            plt.tight_layout()
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig("swr_profile_example.svg", transparent="True")
            else:
                plt.show()

            c = "white"
            plt.figure(figsize=(4, 4))
            bplot = plt.boxplot([stable_shape, dec_shape, inc_shape], positions=[1, 2, 3], patch_artist=True,
                                labels=["persistent", "dec", "inc"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            plt.ylim(-2000, 2000)
            for patch, color in zip(bplot['boxes'], ["violet", "turquoise", "orange"]):
                patch.set_facecolor(color)
            plt.ylabel("SWR peak value")
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.show()

        return stable_shape, dec_shape, inc_shape
    # </editor-fold>

    # <editor-fold desc="Controls and decoding quality">
    def memory_drift_control_equalized_firing_rates(self, len_chunk_s = 200, n_equalizing=5, n_smoothing=20,
                                                    plot_for_control=False, plotting=True):

        raster_sleep = []

        for l_s in self.long_sleep:
            r = l_s.get_raster()
            raster_sleep.append(r)

        raster_sleep = np.hstack(raster_sleep)

        # get means of pre and post modes + compression factor
        pre_file_name = self.session_params.default_pre_phmm_model
        post_file_name = self.session_params.default_post_phmm_model
        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        # filter cells that are quiet most of the time
        per_cell_percent_firing = np.count_nonzero(raster_sleep, axis=1)/raster_sleep.shape[1]
        cells_to_exclude = np.where(per_cell_percent_firing < 0.01)
        raster_sleep = np.delete(raster_sleep, cells_to_exclude, axis=0)

        # also delete these cells from pre and post model means
        pre_mode_means = np.delete(pre_mode_means, cells_to_exclude, axis=1)
        post_mode_means = np.delete(post_mode_means, cells_to_exclude, axis=1)

        nr_chunks = raster_sleep.shape[1]*self.params.time_bin_size/len_chunk_s
        chunk_size = np.round(raster_sleep.shape[1]/nr_chunks).astype(int)
        # split into equal size parts to find min. number of spikes
        raster_sleep_chunks = down_sample_array_sum(x=raster_sleep, chunk_size=chunk_size)

        # nr of windows with zero spikes
        nr_time_bins_more_than_zero_spikes = np.count_nonzero(np.where(raster_sleep_chunks==0, 1, 0), axis=0)
        # binarize
        nr_time_bins_more_than_zero_spikes = np.where(nr_time_bins_more_than_zero_spikes==0, 1, 0).astype(bool)

        # remove time windows if there is a single cell that doesn't spike
        raster_sleep_chunks_filtered = raster_sleep_chunks[:, nr_time_bins_more_than_zero_spikes]

        # min. number of spikes per window
        min_nr_spikes = np.min(raster_sleep_chunks_filtered, axis=1).astype(int)

        # need to filter periods where there is no spike for at least one cell in the initial raster
        nr_time_bins_more_than_zero_spikes_orig_resolution = nr_time_bins_more_than_zero_spikes.repeat(chunk_size).astype(bool)
        raster_sleep_filtered = raster_sleep[:,:nr_time_bins_more_than_zero_spikes_orig_resolution.shape[0]]
        raster_sleep_filtered = raster_sleep_filtered[:,nr_time_bins_more_than_zero_spikes_orig_resolution]

        # equalize raster and do decoding
        all_results_eq = []

        for n_eq in range(n_equalizing):
            additional_chunk_to_remove = []
            equalized_raster = np.zeros(raster_sleep_filtered.shape)
            # go through rasters and remove random spikes to equalize firing rates over time
            print("started equalizing")
            start = time.time()
            for chunk_id in range(int(np.round(nr_chunks))):
                # if end of chunk is bigger that filtered raster --> leave loop
                if (chunk_id+1)*chunk_size > raster_sleep_filtered.shape[1]:
                    additional_chunk_to_remove.append(chunk_id)
                    break
                # find all time bins with spikes
                raster_chunk = raster_sleep_filtered[:, chunk_id*chunk_size:(chunk_id+1)*chunk_size]
                raster_chunk_copy = np.copy(raster_chunk)
                # go trough all cells
                for cell_id, cell_min_nr_spikes in enumerate(min_nr_spikes):
                    # keep removing spikes until min. nr. spikes is reached
                    cell_chunk = np.copy(raster_chunk[cell_id,:]).astype(int)
                    nr_spikes_in_chunk = np.sum(cell_chunk)
                    while nr_spikes_in_chunk > cell_min_nr_spikes:
                        nr_spikes_to_remove = nr_spikes_in_chunk - cell_min_nr_spikes
                        # find time bins with spikes
                        cell_bins_with_spikes = np.argwhere(cell_chunk > 0).flatten()
                        # chunk with zero spikes for one cell
                        if cell_bins_with_spikes.shape[0] == 0:
                            additional_chunk_to_remove.append(chunk_id)
                            break
                        # there are less time bins with spikes than spikes to remove --> need to go through while loop
                        # another time
                        elif nr_spikes_to_remove > cell_bins_with_spikes.shape[0]:
                            spikes_to_remove = np.random.choice(a=cell_bins_with_spikes, size=cell_bins_with_spikes.shape[0],
                                                                replace=False)
                        else:
                            spikes_to_remove = np.random.choice(a=cell_bins_with_spikes, size=nr_spikes_to_remove,
                                                                replace=False)
                        # remove n spikes from bins to match min_nr_spikes
                        cell_chunk[spikes_to_remove] -= 1
                        # check if more spikes need to be removed
                        nr_spikes_in_chunk = np.sum(cell_chunk)

                    equalized_raster[cell_id, chunk_id*chunk_size:(chunk_id+1)*chunk_size] = cell_chunk

                # # if last chunk is too small --> do not add it
                # if chunk_size == raster_chunk_copy.shape[1]:
                #     equalized_raster[:, chunk_id*chunk_size:(chunk_id+1)*chunk_size] = raster_chunk_copy
                # else:
                #     break
            if len(additional_chunk_to_remove)>0:
                # remove additional chunks, TODO: might be useless
                min_chunks_to_remove = min(additional_chunk_to_remove)
                equalized_raster = equalized_raster[:, :min_chunks_to_remove*chunk_size]
            if plot_for_control:
                # check equalized raster
                equalized_raster_test = down_sample_array_sum(x=equalized_raster, chunk_size=chunk_size)
                plt.figure(figsize=(7,3))
                plt.imshow(equalized_raster_test)
                plt.title("Spikes per chunk")
                plt.xlabel("Chunk ID")
                plt.ylabel("Cell ID")
                plt.show()
                plt.figure(figsize=(9,2))
                plt.title("Spikes per time bin")
                plt.xlabel("Time bin")
                plt.ylabel("Cell ID")
                plt.imshow(equalized_raster[:,:500])
                plt.show()
            end = time.time()
            print("Done (elapsed time: "+str(end-start)+"s)")
            # should maybe sub-sample equalized raster --> we end up with lots of 12 spike bins (== number of 0.1s bins)
            down_s_eq_raster = equalized_raster[:, ::1000]

            # do decoding using equalized raster --> need 12 spike bins to do that
            # generate 12-spike bins
            print("started 12-spike binning")
            start = time.time()
            spike_bins = constant_nr_spike_bin_from_mean_firing(mean_firing_vector=down_s_eq_raster,
                                                                return_mean_vector=True)
            end = time.time()
            print("Done (elapsed time: "+str(end-start)+"s)")
            # do decoding and plot result
            # ----------------------------------------------------------------------------------------------------------
            # do the decoding using pre and post model
            pre_likeli = decode_using_phmm_modes(mode_means=pre_mode_means,
                                                 event_spike_rasters=[spike_bins],
                                                 compression_factor=compression_factor,
                                                 cell_selection="all")[0]

            max_pre_likeli = np.max(pre_likeli, axis=1)

            post_likeli = decode_using_phmm_modes(mode_means=post_mode_means,
                                                  event_spike_rasters=[spike_bins],
                                                  compression_factor=compression_factor,
                                                  cell_selection="all")[0]

            max_post_likeli = np.max(post_likeli, axis=1)

            sim_ratio = (max_post_likeli - max_pre_likeli) / (max_post_likeli + max_pre_likeli)
            all_results_eq.append(sim_ratio)
            # plt.plot(sim_ratio)
            # plt.xlabel("relative time")
            # plt.xticks(ticks=np.arange(sim_ratio.shape[0])[::10],
            #            labels=np.round(np.linspace(0, 1, sim_ratio.shape[0])[::10], 1))
            # plt.ylabel("sim_ratio")
            # plt.tight_layout()
            # plt.show()

            dat = moving_average(sim_ratio, 20)
            plt.plot(dat)
            plt.xlabel("relative time")
            plt.xticks(ticks=np.arange(dat.shape[0])[::10], labels=np.round(np.linspace(0, 1, dat.shape[0])[::10], 1))
            plt.ylabel("sim_ratio - smooth")
            plt.tight_layout()
            plt.ylim(-1, 1)
            plt.show()

        # compute mean across equalization runs
        mean_sim = np.nanmean(np.vstack(all_results_eq), axis=0)
        std_sim = np.nanstd(np.vstack(all_results_eq), axis=0)
        dat_mean = moving_average(mean_sim, 20)
        dat_std = moving_average(std_sim, 20)
        plt.errorbar(x=np.arange(dat_mean.shape[0]), y=dat_mean, yerr=dat_std, zorder=-1000)
        plt.plot(np.arange(dat_mean.shape[0]), dat_mean, c="red")
        plt.xlabel("relative time")
        plt.ylabel("sim_ratio - smooth")
        plt.tight_layout()
        plt.ylim(-1, 1)
        plt.show()

    def memory_drift_control_jitter(self, len_chunk_s = 200, n_equalizing=5,
                                    plot_for_control=False, plotting=True, rem_pop_vec_threshold=10,
                                    nrem_pop_vec_threshold=2, n_moving_average_pop_vec=200):

        event_spike_rasters_rem = []
        event_spike_rasters_nrem = []
        event_spike_rasters_rem_jittered = []
        event_spike_rasters_nrem_jittered = []
        event_times_rem = []
        event_times_nrem = []

        first = 0
        for l_s in self.long_sleep[:2]:
            duration = l_s.get_duration_sec()

            # get jittered bins
            # ----------------------------------------------------------------------------------------------------------

            event_times_nrem_, event_spike_rasters_nrem_jittered_, _ = (
                l_s.get_spike_binned_raster_jittered_sleep_phase(sleep_phase="nrem"))

            event_times_rem_, event_spike_rasters_rem_jittered_, _ = (
                l_s.get_spike_binned_raster_jittered_sleep_phase(sleep_phase="rem"))

            # get original bins
            # ----------------------------------------------------------------------------------------------------------

            _, event_spike_rasters_nrem_ = (
                l_s.get_spike_binned_raster_sleep_phase(sleep_phase="nrem"))

            _, event_spike_rasters_rem_ = (
                l_s.get_spike_binned_raster_sleep_phase(sleep_phase="rem"))

            event_spike_rasters_rem.extend(event_spike_rasters_rem_)
            event_spike_rasters_nrem.extend(event_spike_rasters_nrem_)
            event_spike_rasters_rem_jittered.extend(event_spike_rasters_rem_jittered_)
            event_spike_rasters_nrem_jittered.extend(event_spike_rasters_nrem_jittered_)

            event_times_rem.extend(event_times_rem_ + first)
            event_times_nrem.extend(event_times_nrem_ + first)
            first += duration

        if plot_for_control:
            x_min = 0
            x_max = 20
            all_rem_rasters = np.hstack(event_spike_rasters_rem)
            all_rem_rasters_jittered = np.hstack(event_spike_rasters_rem_jittered)
            plt.subplot(2,1,1)
            plt.imshow(all_rem_rasters, interpolation='nearest', aspect='auto')
            plt.title("REM: Original")
            plt.xlim(x_min, x_max)
            plt.subplot(2,1,2)
            plt.imshow(all_rem_rasters_jittered, interpolation='nearest', aspect='auto')
            plt.xlim(x_min, x_max)
            plt.title("REM: Jittered")
            plt.tight_layout()
            plt.show()

            x_min = 0
            x_max = 20
            all_rem_rasters = np.hstack(event_spike_rasters_nrem)
            all_rem_rasters_jittered = np.hstack(event_spike_rasters_nrem_jittered)
            plt.subplot(2,1,1)
            plt.imshow(all_rem_rasters, interpolation='nearest', aspect='auto')
            plt.title("NREM: Original")
            plt.xlim(x_min, x_max)
            plt.subplot(2,1,2)
            plt.imshow(all_rem_rasters_jittered, interpolation='nearest', aspect='auto')
            plt.xlim(x_min, x_max)
            plt.title("NREM: Jittered")
            plt.tight_layout()
            plt.show()

        # need to sort events according to occurrence time and merge adjacent ones of same time
        labels_events = np.zeros(len(event_spike_rasters_rem)+len(event_spike_rasters_nrem))
        labels_events[:len(event_spike_rasters_rem)] = 1
        all_start_times = np.hstack((np.vstack(event_times_rem)[:, 0], np.vstack(event_times_nrem)[:,0]))
        all_end_times = np.hstack((np.vstack(event_times_rem)[:, 1], np.vstack(event_times_nrem)[:,2]))
        all_spike_rasters = event_spike_rasters_rem + event_spike_rasters_nrem
        all_spike_rasters_jittered = event_spike_rasters_rem_jittered + event_spike_rasters_nrem_jittered

        # sort events according to time
        sorted_spike_rasters = [x for _, x in sorted(zip(all_start_times, all_spike_rasters))]
        sorted_spike_rasters_jittered = [x for _, x in sorted(zip(all_start_times, all_spike_rasters_jittered))]
        sorted_labels_events = [x for _, x in sorted(zip(all_start_times, labels_events))]
        sorted_start_times = [x for x in sorted(all_start_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_start_times, all_end_times))]

        # merge events
        # --------------------------------------------------------------------------------------------------------------
        merged_events = []
        merged_events_jittered = []
        merged_events_labels = []
        merged_events_start_times = []
        merged_events_end_times = []

        curr_label = sorted_labels_events[0]
        ev_id = 0
        merged_event = []
        merged_event_jittered = []
        while ev_id < len(sorted_spike_rasters):
            if sorted_labels_events[ev_id] == curr_label:
                merged_event.append(sorted_spike_rasters[ev_id])
                merged_event_jittered.append(sorted_spike_rasters_jittered[ev_id])
            else:
                # need to append the event to merged_events --> use end time
                merged_events.append(np.hstack(merged_event))
                merged_events_jittered.append(np.hstack(merged_event_jittered))
                merged_events_labels.append(curr_label)
                merged_events_end_times.append(sorted_end_times[ev_id])
                # initialize new event --> use start time
                merged_event = []
                merged_event_jittered = []
                curr_label = sorted_labels_events[ev_id]
                # append current element
                merged_event.append(sorted_spike_rasters[ev_id])
                merged_event_jittered.append(sorted_spike_rasters_jittered[ev_id])
                merged_events_start_times.append(sorted_start_times[ev_id])
            ev_id += 1

        # need to append the LAST events to merged_events
        merged_events.append(np.hstack(merged_event))
        merged_events_jittered.append(np.hstack(merged_event_jittered))
        merged_events_labels.append(curr_label)
        merged_events_end_times.append(sorted_end_times[ev_id])


        # delete events which are too short --> TODO
        # --------------------------------------------------------------------------------------------------------------
        # nr_bins = np.array([x.shape[1] for x in merged_events])

        # get means of pre and post modes + compression factor
        pre_file_name = self.session_params.default_pre_phmm_model
        post_file_name = self.session_params.default_post_phmm_model
        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        # do decoding and plot result
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli = decode_using_phmm_modes(mode_means=pre_mode_means,
                                             event_spike_rasters=merged_events,
                                             compression_factor=compression_factor,
                                             cell_selection="all")



        post_likeli = decode_using_phmm_modes(mode_means=post_mode_means,
                                              event_spike_rasters=merged_events,
                                              compression_factor=compression_factor,
                                              cell_selection="all")

        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        sim_ratio = (max_post_likeli - max_pre_likeli) / (max_post_likeli + max_pre_likeli)

        # do decoding and plot result for jittered data
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_j = decode_using_phmm_modes(mode_means=pre_mode_means,
                                             event_spike_rasters=merged_events_jittered,
                                             compression_factor=compression_factor,
                                             cell_selection="all")

        post_likeli_j = decode_using_phmm_modes(mode_means=post_mode_means,
                                              event_spike_rasters=merged_events_jittered,
                                              compression_factor=compression_factor,
                                              cell_selection="all")

        pre_likeli_j = np.vstack(pre_likeli_j)
        post_likeli_j = np.vstack(post_likeli_j)

        max_pre_likeli_j = np.max(pre_likeli_j, axis=1)
        max_post_likeli_j = np.max(post_likeli_j, axis=1)

        sim_ratio_j = (max_post_likeli_j - max_pre_likeli_j) / (max_post_likeli_j + max_pre_likeli_j)

        plt.plot(max_pre_likeli, label="PRE: original")
        plt.plot(max_pre_likeli_j, label="PRE: jittered")
        plt.legend()
        plt.show()

        plt.plot(max_post_likeli, label="POST: original")
        plt.plot(max_post_likeli_j, label="POST: jittered")
        plt.legend()
        plt.show()

        plt.plot(moving_average(sim_ratio, n_moving_average_pop_vec), label="original")
        plt.plot(moving_average(sim_ratio_j, n_moving_average_pop_vec), label="spike jittered")
        plt.xlabel("Time")
        plt.ylabel("sim_ratio")
        plt.legend()
        plt.tight_layout()
        plt.show()

    def memory_drift_control_jitter_one_sleep_phase(self, sleep_phase="nrem",
                                        plot_for_control=False, n_moving_average_pop_vec=200,
                                                    nr_spikes_per_jitter_window=10000):

        event_spike_rasters = []
        event_spike_rasters_jittered = []
        event_times = []
        first = 0
        for l_s in self.long_sleep:
            duration = l_s.get_duration_sec()

            # get original bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_ = (
                l_s.get_spike_binned_raster_sleep_phase(sleep_phase=sleep_phase))

            # get jittered bins
            # ----------------------------------------------------------------------------------------------------------
            event_times_, event_spike_rasters_jittered_ = (
                l_s.get_spike_binned_raster_sleep_phase_jittered(sleep_phase=sleep_phase,
                                                                 nr_spikes_per_jitter_window=nr_spikes_per_jitter_window))

            if plot_for_control:
                orig = np.hstack(event_spike_rasters_)
                jit = event_spike_rasters_jittered_
                plt.subplot(2,1,1)
                plt.imshow(orig, interpolation='nearest', aspect='auto')
                plt.xlim(0, 20)
                plt.subplot(2,1,2)
                plt.imshow(jit, interpolation='nearest', aspect='auto')
                plt.xlim(0, 20)
                plt.show()

                # check if number of spikes are preserved per cell
                # assume 12 spikes per bin --> jitter along 200 spikes:
                a = np.sum(orig[:, :int(200/12)], axis=1)
                b = np.sum(jit[:, :int(200/12)], axis=1)
                print("Difference in spikes between original and jittered")
                print(a-b)

            event_spike_rasters.extend(event_spike_rasters_)
            event_spike_rasters_jittered.append(event_spike_rasters_jittered_)

            event_times.extend(event_times_ + first)
            first += duration

        # event_spike_rasters = np.hstack(event_spike_rasters)
        # event_spike_rasters_jittered = np.hstack(event_spike_rasters_jittered)

        # get means of pre and post modes + compression factor
        pre_file_name = self.session_params.default_pre_phmm_model
        post_file_name = self.session_params.default_post_phmm_model
        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        # decode using original rasters
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                             event_spike_rasters=event_spike_rasters,
                                             compression_factor=compression_factor)

        post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                              event_spike_rasters=event_spike_rasters,
                                              compression_factor=compression_factor)

        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        sim_ratio = (max_post_likeli - max_pre_likeli) / (max_post_likeli + max_pre_likeli)

        # do decoding and plot result for jittered data
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_j = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                               event_spike_rasters=event_spike_rasters_jittered,
                                               compression_factor=compression_factor)

        post_likeli_j = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                event_spike_rasters=event_spike_rasters_jittered,
                                                compression_factor=compression_factor)

        pre_likeli_j = np.vstack(pre_likeli_j)
        post_likeli_j = np.vstack(post_likeli_j)

        max_pre_likeli_j = np.max(pre_likeli_j, axis=1)
        max_post_likeli_j = np.max(post_likeli_j, axis=1)

        sim_ratio_j = (max_post_likeli_j - max_pre_likeli_j) / (max_post_likeli_j + max_pre_likeli_j)

        plt.plot(moving_average(max_pre_likeli,n_moving_average_pop_vec), color="red", label="PRE original")
        # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
        plt.plot(moving_average(max_pre_likeli_j, n_moving_average_pop_vec), color="salmon", linestyle="--", label="PRE jittered")
        plt.yscale("log")
        plt.ylabel("Max likelihood")
        plt.legend()
        plt.tight_layout()
        plt.show()

        plt.plot(moving_average(max_post_likeli, n_moving_average_pop_vec), color="white", label="POST original")
        # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
        plt.plot(moving_average(max_post_likeli_j, n_moving_average_pop_vec), color="lightblue", linestyle="--", label="POST jittered", alpha=0.8)
        plt.yscale("log")
        plt.ylabel("Max likelihood")
        plt.legend()
        plt.tight_layout()
        plt.show()

        plt.plot(moving_average(sim_ratio, n_moving_average_pop_vec), label="original", color="grey")
        plt.plot(moving_average(sim_ratio_j, n_moving_average_pop_vec), label="spike jittered", color="red", alpha=0.5)
        plt.xlabel("Time")
        plt.ylabel("sim_ratio")
        plt.legend()
        plt.tight_layout()
        plt.show()

        # cut to same length for the ratio
        # --------------------------------------------------------------------------------------------------------------
        if max_pre_likeli.shape[0] > max_pre_likeli_j.shape[0]:
           max_pre_likeli = max_pre_likeli[:max_pre_likeli_j.shape[0]]
           max_post_likeli = max_post_likeli[:max_post_likeli_j.shape[0]]
        else:
            max_pre_likeli_j = max_pre_likeli_j[:max_pre_likeli.shape[0]]
            max_post_likeli_j = max_post_likeli_j[:max_post_likeli.shape[0]]

        plt.plot(moving_average(max_post_likeli/max_post_likeli_j, 800), color="lightblue")
        plt.yscale("log")
        plt.ylabel("max post orig / max post jittered")
        plt.title("POST: orig / jittered")
        plt.tight_layout()
        plt.show()

        plt.plot(moving_average(max_pre_likeli/max_pre_likeli_j, 800), color="red")
        plt.yscale("log")
        plt.ylabel("max pre orig / max pre jittered")
        plt.title("PRE: orig / jittered")
        plt.tight_layout()
        plt.show()

        c = "white"
        res = [max_pre_likeli, max_pre_likeli_j, max_post_likeli, max_post_likeli_j]
        bplot = plt.boxplot(res, positions=[1, 2, 3, 4], patch_artist=True,
                            labels=["Max PRE", "Max PRE Jittered", "Max POST", "Max POST Jittered"],
                            boxprops=dict(color=c),
                            capprops=dict(color=c),
                            whiskerprops=dict(color=c),
                            flierprops=dict(color=c, markeredgecolor=c),
                            medianprops=dict(color=c), showfliers=False
                            )
        colors = ["magenta", 'magenta', "blue", "blue"]
        for patch, color in zip(bplot['boxes'], colors):
            patch.set_facecolor(color)
        plt.ylabel("Max. Likelihood")
        plt.grid(color="grey", axis="y")
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()

        c = "white"
        res = [pre_likeli.flatten(), pre_likeli_j.flatten(), post_likeli.flatten(), post_likeli_j.flatten()]
        bplot = plt.boxplot(res, positions=[1, 2, 3, 4], patch_artist=True,
                            labels=["PRE", "PRE Jittered", "POST", "POST Jittered"],
                            boxprops=dict(color=c),
                            capprops=dict(color=c),
                            whiskerprops=dict(color=c),
                            flierprops=dict(color=c, markeredgecolor=c),
                            medianprops=dict(color=c), showfliers=False
                            )
        colors = ["magenta", 'magenta', "blue", "blue"]
        for patch, color in zip(bplot['boxes'], colors):
            patch.set_facecolor(color)
        plt.ylabel("Likelihood")
        plt.grid(color="grey", axis="y")
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()

    def memory_drift_control_jitter_subsets_combine_sleep_phases(self, n_moving_average_pop_vec=800,
                                                                 time_interval_jitter_s=20, other_cell_pop="inc"):

        # get cell subsets

        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle",
                  "rb") as f:
            class_dic = pickle.load(f)

        stable = class_dic["stable_cell_ids"]
        if other_cell_pop == "inc":
            other = class_dic["increase_cell_ids"]
        elif other_cell_pop == "dec":
            other = class_dic["decrease_cell_ids"]

        # sub-sample to have matching numbers
        if stable.shape[0] > other.shape[0]:
            stable = np.random.choice(stable, replace=False, size=other.shape[0])
        else:
            other = np.random.choice(other, replace=False, size=stable.shape[0])

        stable_res = []
        other_res = []

        for l_s in self.long_sleep:

            # get jittered bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_jittered_stable_ = \
                (l_s.get_spike_binned_raster_combined_sleep_phases_jitter_subset(time_interval_jitter_s=
                                                                                 time_interval_jitter_s, cell_ids=stable))

            # get original bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_jittered_other_ = \
                (l_s.get_spike_binned_raster_combined_sleep_phases_jitter_subset(time_interval_jitter_s=
                                                                                 time_interval_jitter_s, cell_ids=other))

            stable_res.append(np.hstack(event_spike_rasters_jittered_stable_))
            other_res.append(np.hstack(event_spike_rasters_jittered_other_))


        all_spike_rasters_stable = np.hstack(stable_res)
        all_spike_rasters_other = np.hstack(other_res)

        # delete events which are too short --> TODO
        # --------------------------------------------------------------------------------------------------------------
        # nr_bins = np.array([x.shape[1] for x in merged_events])

        # get means of pre and post modes + compression factor
        pre_file_name = self.session_params.default_pre_phmm_model
        post_file_name = self.session_params.default_post_phmm_model
        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        # do decoding and plot result
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_stable = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                             event_spike_rasters=all_spike_rasters_stable,
                                             compression_factor=compression_factor)

        post_likeli_stable = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                              event_spike_rasters=all_spike_rasters_stable,
                                              compression_factor=compression_factor)

        pre_likeli_stable = np.vstack(pre_likeli_stable)
        post_likeli_stable = np.vstack(post_likeli_stable)

        max_pre_likeli_stable = np.max(pre_likeli_stable, axis=1)
        max_post_likeli_stable = np.max(post_likeli_stable, axis=1)

        # do decoding and plot result for jittered data
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_other = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                               event_spike_rasters=all_spike_rasters_other,
                                               compression_factor=compression_factor)

        post_likeli_other = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                event_spike_rasters=all_spike_rasters_other,
                                                compression_factor=compression_factor)

        pre_likeli_other = np.vstack(pre_likeli_other)
        post_likeli_other = np.vstack(post_likeli_other)

        max_pre_likeli_other = np.max(pre_likeli_other, axis=1)
        max_post_likeli_other = np.max(post_likeli_other, axis=1)

        sim_ratio_s = (max_post_likeli_stable - max_pre_likeli_stable)/(max_post_likeli_stable + max_pre_likeli_stable)
        sim_ratio_o = (max_post_likeli_other - max_pre_likeli_other)/(max_post_likeli_other + max_pre_likeli_other)

        # apply smoothing
        max_pre_likeli_s = moving_average(max_pre_likeli_stable, n_moving_average_pop_vec)
        max_post_likeli_s = moving_average(max_post_likeli_stable, n_moving_average_pop_vec)
        max_pre_likeli_o = moving_average(max_pre_likeli_other, n_moving_average_pop_vec)
        max_post_likeli_o = moving_average(max_post_likeli_other, n_moving_average_pop_vec)


        plt.plot(max_pre_likeli_s, color="red", label="PRE stable jittered")
        # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
        plt.plot(max_pre_likeli_o, color="salmon", linestyle="--", label="PRE "+other_cell_pop+" jittered")
        plt.yscale("log")
        plt.ylabel("Max likelihood")
        plt.legend()
        plt.tight_layout()
        plt.show()

        plt.plot(max_post_likeli_s, color="white", label="POST stable jittered")
        # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
        plt.plot(max_post_likeli_o, color="lightblue", linestyle="--", label="POST " +other_cell_pop+" jittered", alpha=0.8)
        plt.yscale("log")
        plt.ylabel("Max likelihood")
        plt.legend()
        plt.tight_layout()
        plt.show()

        plt.plot(moving_average(sim_ratio_s, n_moving_average_pop_vec), label="stable jittered", color="grey")
        plt.plot(moving_average(sim_ratio_o, n_moving_average_pop_vec), label=other_cell_pop+" jittered", color="red", alpha=0.5)
        plt.xlabel("Time")
        plt.ylabel("sim_ratio")
        plt.legend()
        plt.tight_layout()
        plt.show()

        # cut to same length for the ratio
        # --------------------------------------------------------------------------------------------------------------
        print("HERE")

        if max_pre_likeli_s.shape[0] > max_pre_likeli_o.shape[0]:
           max_pre_likeli_s = max_pre_likeli_s[:max_pre_likeli_o.shape[0]]
           max_post_likeli_s = max_post_likeli_s[:max_post_likeli_o.shape[0]]
        else:
            max_pre_likeli_o = max_pre_likeli_o[:max_pre_likeli_s.shape[0]]
            max_post_likeli_o = max_post_likeli_o[:max_post_likeli_s.shape[0]]

        plt.subplot(1,2,1)
        plt.plot(max_post_likeli_s/max_post_likeli_o, color="lightblue")
        plt.yscale("log")
        plt.ylabel("ratio")
        plt.title("POST: stable / "+other_cell_pop+" jittered")

        plt.subplot(1,2,2)
        plt.boxplot(max_post_likeli_s/max_post_likeli_o, positions=[1], showfliers=False)
        plt.tight_layout()
        plt.show()

        plt.subplot(1,2,1)
        plt.plot(max_pre_likeli_s/max_pre_likeli_o, color="red")
        plt.yscale("log")
        plt.ylabel("ratio")
        plt.title("PRE: stable / "+other_cell_pop+" jittered")

        plt.subplot(1,2,2)
        plt.boxplot(max_pre_likeli_s/max_pre_likeli_o, positions=[1], showfliers=False)
        plt.tight_layout()
        plt.show()

        c = "white"
        res = [max_pre_likeli_s, max_pre_likeli_o, max_post_likeli_s, max_post_likeli_o]
        bplot = plt.boxplot(res, positions=[1, 2, 3, 4], patch_artist=True,
                            labels=["Max PRE stable", "Max PRE other", "Max POST stable", "Max POST other"],
                            boxprops=dict(color=c),
                            capprops=dict(color=c),
                            whiskerprops=dict(color=c),
                            flierprops=dict(color=c, markeredgecolor=c),
                            medianprops=dict(color=c), showfliers=False
                            )
        colors = ["magenta", 'magenta', "blue", "blue"]
        for patch, color in zip(bplot['boxes'], colors):
            patch.set_facecolor(color)
        plt.ylabel("Max. Likelihood")
        plt.grid(color="grey", axis="y")
        plt.xticks(rotation=45)
        plt.yscale("log")
        plt.tight_layout()
        plt.show()

        c = "white"
        res = [pre_likeli_stable.flatten(), pre_likeli_other.flatten(),
               post_likeli_stable.flatten(), post_likeli_other.flatten()]
        bplot = plt.boxplot(res, positions=[1, 2, 3, 4], patch_artist=True,
                            labels=["PRE stable", "PRE other", "POST stable", "POST other"],
                            boxprops=dict(color=c),
                            capprops=dict(color=c),
                            whiskerprops=dict(color=c),
                            flierprops=dict(color=c, markeredgecolor=c),
                            medianprops=dict(color=c), showfliers=False
                            )
        colors = ["magenta", 'magenta', "blue", "blue"]
        for patch, color in zip(bplot['boxes'], colors):
            patch.set_facecolor(color)
        plt.ylabel("Likelihood")
        plt.yscale("log")
        plt.grid(color="grey", axis="y")
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()

    def memory_drift_control_jitter_combine_sleep_phases(self, n_moving_average_pop_vec=10000,
                                                             nr_spikes_per_jitter_window=10000,
                                                         save_likelihoods=False, plotting=False,
                                                         save_results=False, pre_model ="pre", post_model="post"):

        event_spike_rasters = []
        event_spike_rasters_jittered = []

        for l_s in self.long_sleep:

            # get original bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_ = (
                l_s.get_spike_binned_raster_combined_sleep_phases())

            # get jittered bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_jittered_ = (
                l_s.get_spike_binned_raster_combined_sleep_phases_jittered(nr_spikes_per_jitter_window=nr_spikes_per_jitter_window))
            #
            # _, event_spike_rasters_jittered_ = (
            #     l_s.get_spike_binned_raster_combined_sleep_phases_jittered(nr_spikes_per_jitter_window=np.hstack(event_spike_rasters_).shape[1]))

            event_spike_rasters.append(np.hstack(event_spike_rasters_))
            event_spike_rasters_jittered.append(event_spike_rasters_jittered_)


        all_spike_rasters_jittered = np.hstack(event_spike_rasters_jittered)
        all_spike_rasters = np.hstack(event_spike_rasters)

        # delete events which are too short --> TODO
        # --------------------------------------------------------------------------------------------------------------
        # nr_bins = np.array([x.shape[1] for x in merged_events])

        # get means of pre and post modes + compression factor

        if pre_model == "pre":
            pre_file_name = self.session_params.default_pre_phmm_model
        elif pre_model == "pre_familiar":
            pre_file_name = self.session_params.default_exp_fam_1_model
        if post_model ==  "post":
            post_file_name = self.session_params.default_post_phmm_model
        elif post_model == "post_familiar":
            post_file_name = self.session_params.default_exp_fam_2_model

        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms
        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_


        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        # do decoding and plot result
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                  event_spike_rasters=all_spike_rasters,
                                                  compression_factor=compression_factor)

        post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                   event_spike_rasters=all_spike_rasters,
                                                   compression_factor=compression_factor)

        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        sim_ratio = (max_post_likeli - max_pre_likeli) / (max_post_likeli + max_pre_likeli)

        # do decoding and plot result for jittered data
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_j = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                    event_spike_rasters=all_spike_rasters_jittered,
                                                    compression_factor=compression_factor)

        post_likeli_j = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                     event_spike_rasters=all_spike_rasters_jittered,
                                                     compression_factor=compression_factor)

        pre_likeli_j = np.vstack(pre_likeli_j)
        post_likeli_j = np.vstack(post_likeli_j)

        max_pre_likeli_j = np.max(pre_likeli_j, axis=1)
        max_post_likeli_j = np.max(post_likeli_j, axis=1)

        sim_ratio_j = (max_post_likeli_j - max_pre_likeli_j) / (max_post_likeli_j + max_pre_likeli_j)

        # check if likelihoods are supposed to be saved
        if save_likelihoods:

            with open(self.session_name+'_pre_likeli.pkl', 'wb') as f:
                pickle.dump(pre_likeli, f)
            with open(self.session_name+'_post_likeli.pkl', 'wb') as f:
                pickle.dump(post_likeli, f)
            with open(self.session_name+'_pre_likeli_jittered.pkl', 'wb') as f:
                pickle.dump(pre_likeli_j, f)
            with open(self.session_name+'_post_likeli_jittered.pkl', 'wb') as f:
                pickle.dump(post_likeli_j, f)

        # cut to same length for the ratio
        # --------------------------------------------------------------------------------------------------------------

        # take the log
        max_pre_log_likeli = np.log(max_pre_likeli)
        max_pre_log_likeli_j = np.log(max_pre_likeli_j)
        max_post_log_likeli = np.log(max_post_likeli)
        max_post_log_likeli_j = np.log(max_post_likeli_j)

        # interpolate missing values
        max_pre_log_likeli_j = np.interp(np.linspace(0, len(max_pre_log_likeli_j), len(max_pre_log_likeli)),
                                         np.arange(len(max_pre_log_likeli_j)), max_pre_log_likeli_j)

        max_post_log_likeli_j = np.interp(np.linspace(0, len(max_post_log_likeli_j), len(max_post_log_likeli)),
                                          np.arange(len(max_post_log_likeli_j)), max_post_log_likeli_j)

        # apply smoothing
        # max_pre_log_likeli_s = uniform_filter1d(max_pre_log_likeli, int(max_pre_log_likeli.shape[0]/5))
        # max_pre_log_likeli_j_s = uniform_filter1d(max_pre_log_likeli_j, int(max_pre_log_likeli.shape[0]/5))
        # max_post_log_likeli_s = uniform_filter1d(max_post_log_likeli, int(max_pre_log_likeli.shape[0]/5))
        # max_post_log_likeli_j_s = uniform_filter1d(max_post_log_likeli_j, int(max_pre_log_likeli.shape[0]/5))

        # compute difference
        diff_pre = max_pre_log_likeli - max_pre_log_likeli_j
        diff_post = max_post_log_likeli - max_post_log_likeli_j

        if plotting:

            plt.plot(moving_average(sim_ratio, n_moving_average_pop_vec), label="original", color="grey")
            plt.plot(moving_average(sim_ratio_j, n_moving_average_pop_vec), label="spike jittered", color="red", alpha=0.5)
            plt.xlabel("Time")
            plt.ylabel("sim_ratio")
            plt.legend()
            plt.tight_layout()
            plt.show()

            plt.plot(uniform_filter1d(diff_pre, int(diff_pre.shape[0]/5)), color="red", label="Diff PRE")
            plt.plot(uniform_filter1d(diff_post, int(diff_post.shape[0]/5)), color="blue", label="Diff POST")
            plt.legend()
            plt.xlabel("Time")
            plt.ylabel("Log-likeli: Original - jittered")
            plt.title("All bins")
            plt.show()

            diff_pre_dec =diff_pre[max_pre_log_likeli > max_post_log_likeli]
            diff_post_dec =diff_post[max_pre_log_likeli < max_post_log_likeli]

            plt.plot(uniform_filter1d(diff_pre_dec, int(diff_pre_dec.shape[0]/5)), color="red", label="Diff PRE")
            plt.plot(uniform_filter1d(diff_post_dec, int(diff_post_dec.shape[0]/5)), color="blue", label="Diff POST")
            plt.xlabel("Time")
            plt.ylabel("Log-likeli: Original - jittered")
            plt.legend()
            plt.title("Only when decoded")
            plt.show()


        if save_results:

            res_dic = {}
            res_dic["sim_ratio"] = sim_ratio
            res_dic["sim_ratio_j"] = sim_ratio_j
            res_dic["diff_pre"] = diff_pre
            res_dic["diff_post"] = diff_post
            res_dic["max_pre"] = max_pre_log_likeli
            res_dic["max_pre_jit"] = max_pre_log_likeli_j
            res_dic["max_post"] = max_post_log_likeli
            res_dic["max_post_jit"] = max_post_log_likeli_j

            if pre_model == "pre" and post_model == "post":
               save_path = os.path.dirname(os.path.realpath(__file__)) + "/../temp_data/jittered"
            elif pre_model == "pre_familiar" and post_model == "post_familiar":
                save_path = os.path.dirname(os.path.realpath(__file__)) + "/../temp_data/jittered/familiar_exploration"
            filename = save_path+ "/" + self.session_name

            outfile = open(filename, 'wb')
            pickle.dump(res_dic, outfile)
            outfile.close()

        return sim_ratio, sim_ratio_j, diff_pre, diff_post, max_pre_log_likeli, \
            max_pre_log_likeli_j, max_post_log_likeli, max_post_log_likeli_j

    def memory_drift_control_equalize_combine_sleep_phases(self, n_moving_average_pop_vec=10000,
                                                           nr_chunks=4, plotting=True,
                                                           save_fig=False, save_results=True, pre_model="pre",
                                                           post_model="post"):

        event_spike_rasters = []
        event_spike_rasters_jittered = []

        for l_s in self.long_sleep:

            # get equalized bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_jittered_ = (
                l_s.get_spike_binned_raster_combined_sleep_phases_equalized(nr_chunks=nr_chunks))

            # get original bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_ = (
                l_s.get_spike_binned_raster_combined_sleep_phases())

            #
            # _, event_spike_rasters_jittered_ = (
            #     l_s.get_spike_binned_raster_combined_sleep_phases_jittered(nr_spikes_per_jitter_window=np.hstack(event_spike_rasters_).shape[1]))

            event_spike_rasters.append(np.hstack(event_spike_rasters_))
            event_spike_rasters_jittered.append(event_spike_rasters_jittered_)


        all_spike_rasters_jittered = np.hstack(event_spike_rasters_jittered)
        all_spike_rasters = np.hstack(event_spike_rasters)

        # delete events which are too short --> TODO
        # --------------------------------------------------------------------------------------------------------------
        # nr_bins = np.array([x.shape[1] for x in merged_events])

        # get means of pre and post modes + compression factor
        if pre_model == "pre":
            pre_file_name = self.session_params.default_pre_phmm_model
        elif pre_model == "pre_familiar":
            pre_file_name = self.session_params.default_exp_fam_1_model
        if post_model ==  "post":
            post_file_name = self.session_params.default_post_phmm_model
        elif post_model == "post_familiar":
            post_file_name = self.session_params.default_exp_fam_2_model
        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        # do decoding and plot result
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                  event_spike_rasters=all_spike_rasters,
                                                  compression_factor=compression_factor)

        post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                   event_spike_rasters=all_spike_rasters,
                                                   compression_factor=compression_factor)

        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)


        sim_ratio = (max_post_likeli - max_pre_likeli) / (max_post_likeli + max_pre_likeli)

        # do decoding and plot result for jittered data
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_j = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                    event_spike_rasters=all_spike_rasters_jittered,
                                                    compression_factor=compression_factor)

        post_likeli_j = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                     event_spike_rasters=all_spike_rasters_jittered,
                                                     compression_factor=compression_factor)

        pre_likeli_j = np.vstack(pre_likeli_j)
        post_likeli_j = np.vstack(post_likeli_j)

        max_pre_likeli_j = np.max(pre_likeli_j, axis=1)
        max_post_likeli_j = np.max(post_likeli_j, axis=1)

        sim_ratio_j = (max_post_likeli_j - max_pre_likeli_j) / (max_post_likeli_j + max_pre_likeli_j)

        # apply smoothing
        # cut to same length for the ratio
        # --------------------------------------------------------------------------------------------------------------

        # take the log
        max_pre_log_likeli = np.log(max_pre_likeli)
        max_pre_log_likeli_j = np.log(max_pre_likeli_j)
        max_post_log_likeli = np.log(max_post_likeli)
        max_post_log_likeli_j = np.log(max_post_likeli_j)

        # interpolate values --> likelihoods
        # --------------------------------------------------------------------------------------------------------------
        max_pre_log_likeli_j = np.interp(np.linspace(0, len(max_pre_log_likeli_j), len(max_pre_log_likeli)),
                                         np.arange(len(max_pre_log_likeli_j)), max_pre_log_likeli_j)

        max_post_log_likeli_j = np.interp(np.linspace(0, len(max_post_log_likeli_j), len(max_post_log_likeli)),
                                          np.arange(len(max_post_log_likeli_j)), max_post_log_likeli_j)


        # interpolate values --> sim_ratio
        # --------------------------------------------------------------------------------------------------------------
        sim_ratio_j = np.interp(np.linspace(0, len(sim_ratio_j), len(sim_ratio)),
                                         np.arange(len(sim_ratio_j)), sim_ratio_j)

        # apply smoothing
        max_pre_log_likeli_s = uniform_filter1d(max_pre_log_likeli, n_moving_average_pop_vec)
        max_pre_log_likeli_j_s = uniform_filter1d(max_pre_log_likeli_j, n_moving_average_pop_vec)
        max_post_log_likeli_s = uniform_filter1d(max_post_log_likeli, n_moving_average_pop_vec)
        max_post_log_likeli_j_s = uniform_filter1d(max_post_log_likeli_j, n_moving_average_pop_vec)

        # compute difference
        diff_pre = max_pre_log_likeli_s - max_pre_log_likeli_j_s
        diff_post = max_post_log_likeli_s - max_post_log_likeli_j_s


        if plotting or save_fig:

            plt.figure(figsize=(4,3))
            if save_fig:
                plt.style.use('default')
            plt.plot(np.linspace(0,1,moving_average(sim_ratio, n_moving_average_pop_vec).shape[0]),
                     moving_average(sim_ratio, n_moving_average_pop_vec), label="original", color="grey")
            plt.plot(np.linspace(0,1,moving_average(sim_ratio_j, n_moving_average_pop_vec).shape[0]),
                     moving_average(sim_ratio_j, n_moving_average_pop_vec), label="equalized", color="red", alpha=0.5)
            plt.xlabel("Normalized duration")
            plt.ylabel("Drift score")
            plt.legend()
            plt.tight_layout()
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "drift_score_equalized"+self.session_name+".svg"), transparent="True")
            else:
                plt.show()

            plt.figure(figsize=(12,8))
            plt.subplot(2,2,1)
            plt.plot(max_pre_log_likeli_s, color="red", label="PRE original")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_pre_log_likeli_j_s, color="salmon", linestyle="--", label="PRE equalized")
            plt.xticks([])
            plt.ylabel("Max log likelihood")
            plt.legend()
            plt.subplot(2,2,3)
            plt.plot(np.linspace(0,1, diff_pre.shape[0]), diff_pre)
            plt.ylabel("Original - equalized")
            plt.xlabel("relative time")
            plt.subplot(2,2,2)
            plt.plot(max_post_log_likeli_s, color="white", label="POST original")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_post_log_likeli_j_s, color="lightblue", linestyle="--", label="POST equalized", alpha=0.8)
            plt.xticks([])
            plt.legend()
            plt.subplot(2,2,4)
            plt.plot(np.linspace(0,1, diff_post.shape[0]), diff_post)
            plt.ylabel("Original - equalized")
            plt.xlabel("relative time")
            plt.tight_layout()
            plt.show()

        if save_results:
            res_dic = {}
            res_dic["sim_ratio"] = sim_ratio
            res_dic["sim_ratio_j"] = sim_ratio_j


            if pre_model == "pre" and post_model == "post":
                save_path = os.path.dirname(os.path.realpath(__file__)) + "/../temp_data/equalized"
            elif pre_model == "pre_familiar" and post_model == "post_familiar":
                save_path = os.path.dirname(os.path.realpath(__file__)) + "/../temp_data/equalized/familiar_exploration"

            filename = save_path+ "/" + self.session_name

            outfile = open(filename, 'wb')
            pickle.dump(res_dic, outfile)
            outfile.close()
        return sim_ratio, sim_ratio_j

    def memory_drift_combine_sleep_phases_decoded_likelihoods(self, plotting=True, save_fig=False,
                                                              pre_model="pre", post_model="post"):

        event_spike_rasters = []

        for l_s in self.long_sleep:

            # get original bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_ = (
                l_s.get_spike_binned_raster_combined_sleep_phases())

            event_spike_rasters.append(np.hstack(event_spike_rasters_))

        all_spike_rasters = np.hstack(event_spike_rasters)

        # get means of pre and post modes + compression factor
        if pre_model == "pre":
            pre_file_name = self.session_params.default_pre_phmm_model
        if post_model ==  "post":
            post_file_name = self.session_params.default_post_phmm_model

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms


        # do decoding and plot result
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                  event_spike_rasters=all_spike_rasters,
                                                  compression_factor=compression_factor)

        post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                   event_spike_rasters=all_spike_rasters,
                                                   compression_factor=compression_factor)

        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        pre_decoded = np.zeros(max_pre_likeli.shape[0])
        pre_decoded[max_pre_likeli > max_post_likeli] = 1
        pre_decoded = pre_decoded.astype(bool)

        # take the log
        max_pre_log_likeli = np.log(max_pre_likeli)
        max_post_log_likeli = np.log(max_post_likeli)

        # now for each population vector --> generate 100 artificial modes made of pre and post decoded mode
        # --> use as control distribution

        # get decoded modes
        # dec_mode_pre = np.argmax(pre_likeli, axis=1)
        # dec_mode_post = np.argmax(post_likeli, axis=1)
        #
        # n_art_modes = 100
        # max_pre_log_likeli_z_scored = np.zeros(pre_likeli.shape[0])
        # max_post_log_likeli_z_scored = np.zeros(pre_likeli.shape[0])
        #
        # for pop_vec in range(pre_likeli.shape[0]):
        #
        #     art_modes = np.zeros((n_art_modes, pre_mode_means.shape[1]))
        #     # generate artifical modes
        #     for i in range(n_art_modes):
        #         # combine pre and post mode
        #         pre_post_mode = np.vstack((pre_mode_means[dec_mode_pre[pop_vec]], post_mode_means[dec_mode_post[pop_vec]]))
        #         jumbled_ids = np.random.randint(0,2, pre_mode_means.shape[1])
        #         art_modes[i, :] = pre_post_mode[jumbled_ids, np.arange(pre_mode_means.shape[1])]
        #
        #     # decode artificial mode
        #     likeli_control = decode_using_phmm_modes_fast(mode_means=art_modes,
        #                                                   event_spike_rasters=np.expand_dims(all_spike_rasters[:, pop_vec],1),
        #                                                   compression_factor=compression_factor)
        #
        #     likeli_control_log = np.log(likeli_control)
        #     # compute mean and std
        #
        #     max_pre_log_likeli_z_scored[pop_vec] = (max_pre_log_likeli[pop_vec] - np.mean(likeli_control_log))/np.std(likeli_control_log)
        #     max_post_log_likeli_z_scored[pop_vec] = (max_post_log_likeli[pop_vec] - np.mean(likeli_control_log))/np.std(likeli_control_log)

        # compute difference when pre decoded
        diff_decoded_non_decoded = np.zeros(pre_decoded.shape[0])
        diff_decoded_non_decoded[pre_decoded] = max_pre_log_likeli[pre_decoded] - max_post_log_likeli[pre_decoded]
        diff_decoded_non_decoded[~pre_decoded] = max_post_log_likeli[~pre_decoded] - max_pre_log_likeli[~pre_decoded]
        
        # also compute pre/post likelihoods separately
        pre_pre_decoded = max_pre_log_likeli[pre_decoded]
        post_pre_decoded = max_post_log_likeli[pre_decoded]
        pre_post_decoded = max_pre_log_likeli[~pre_decoded]
        post_post_decoded = max_post_log_likeli[~pre_decoded]

        # apply smoothing
        diff_pre_decoded = diff_decoded_non_decoded[pre_decoded]
        diff_post_decoded = diff_decoded_non_decoded[~pre_decoded]

        if plotting or save_fig:

            plt.figure(figsize=(5,4))
            if save_fig:
                plt.style.use('default')
            plt.plot(np.linspace(0,1,diff_decoded_non_decoded_s.shape[0]),
                     diff_decoded_non_decoded_s, label="original", color="grey")
            plt.xlabel("Normalized duration")
            plt.ylabel("Log-likelihood_decoded - \n Log-likelihood_non_decoded")
            plt.legend()
            plt.tight_layout()
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "drift_score_equalized"+self.session_name+".svg"), transparent="True")
            else:
                plt.show()

            plt.figure(figsize=(5,4))
            if save_fig:
                plt.style.use('default')
            plt.plot(np.linspace(0,1,diff_pre_decoded_s.shape[0]),
                     diff_pre_decoded_s, label="Acquisition decoded", color="orange")
            plt.plot(np.linspace(0,1,diff_post_decoded_s.shape[0]),
                     diff_post_decoded_s, label="Recall decoded", color="turquoise")
            plt.xlabel("Normalized duration")
            plt.ylabel("Log-likelihood_decoded - \n Log-likelihood_non_decoded")
            plt.legend()
            plt.tight_layout()
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "drift_score_equalized"+self.session_name+".svg"), transparent="True")
            else:
                plt.show()

        return diff_decoded_non_decoded, diff_pre_decoded, diff_post_decoded, pre_pre_decoded, \
            post_pre_decoded, pre_post_decoded, post_post_decoded

    def memory_drift_combine_sleep_phases_subsets(self, plotting=False, save_fig=False,
                                                                  pre_model="pre", post_model="post"):

        event_spike_rasters = []

        for l_s in self.long_sleep:

            # get original bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_ = (
                l_s.get_spike_binned_raster_combined_sleep_phases())

            event_spike_rasters.append(np.hstack(event_spike_rasters_))

        all_spike_rasters = np.hstack(event_spike_rasters)

        # get means of pre and post modes + compression factor
        if pre_model == "pre":
            pre_file_name = self.session_params.default_pre_phmm_model
        if post_model ==  "post":
            post_file_name = self.session_params.default_post_phmm_model

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

        # get subsets of cells
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle",
                  "rb") as f:
            class_dic = pickle.load(f)

        stable_cells = class_dic["stable_cell_ids"].flatten()
        inc_cells = class_dic["increase_cell_ids"].flatten()
        dec_cells = class_dic["decrease_cell_ids"].flatten()


        # do decoding for all cells
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                  event_spike_rasters=all_spike_rasters,
                                                  compression_factor=compression_factor)

        post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                   event_spike_rasters=all_spike_rasters,
                                                   compression_factor=compression_factor)

        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        sim_ratio_all = (max_post_likeli-max_pre_likeli)/(max_pre_likeli+max_post_likeli)

        # do decoding for dec cells
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_dec = decode_using_phmm_modes_fast(mode_means=pre_mode_means[:, dec_cells],
                                                  event_spike_rasters=all_spike_rasters[dec_cells, :],
                                                  compression_factor=compression_factor)

        post_likeli_dec = decode_using_phmm_modes_fast(mode_means=post_mode_means[:, dec_cells],
                                                   event_spike_rasters=all_spike_rasters[dec_cells, :],
                                                   compression_factor=compression_factor)

        pre_likeli_dec = np.vstack(pre_likeli_dec)
        post_likeli_dec = np.vstack(post_likeli_dec)

        max_pre_likeli_dec = np.max(pre_likeli_dec, axis=1)
        max_post_likeli_dec = np.max(post_likeli_dec, axis=1)

        sim_ratio_dec = (max_post_likeli_dec-max_pre_likeli_dec)/(max_pre_likeli_dec+max_post_likeli_dec)

        # do decoding for inc cells
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_inc = decode_using_phmm_modes_fast(mode_means=pre_mode_means[:, inc_cells],
                                                      event_spike_rasters=all_spike_rasters[inc_cells, :],
                                                      compression_factor=compression_factor)

        post_likeli_inc = decode_using_phmm_modes_fast(mode_means=post_mode_means[:, inc_cells],
                                                       event_spike_rasters=all_spike_rasters[inc_cells, :],
                                                       compression_factor=compression_factor)

        pre_likeli_inc = np.vstack(pre_likeli_inc)
        post_likeli_inc = np.vstack(post_likeli_inc)

        max_pre_likeli_inc = np.max(pre_likeli_inc, axis=1)
        max_post_likeli_inc = np.max(post_likeli_inc, axis=1)

        sim_ratio_inc = (max_post_likeli_inc-max_pre_likeli_inc)/(max_pre_likeli_inc+max_post_likeli_inc)

        # do decoding for stable cells
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_stable = decode_using_phmm_modes_fast(mode_means=pre_mode_means[:, stable_cells],
                                                      event_spike_rasters=all_spike_rasters[stable_cells, :],
                                                      compression_factor=compression_factor)

        post_likeli_stable = decode_using_phmm_modes_fast(mode_means=post_mode_means[:, stable_cells],
                                                       event_spike_rasters=all_spike_rasters[stable_cells, :],
                                                       compression_factor=compression_factor)

        pre_likeli_stable = np.vstack(pre_likeli_stable)
        post_likeli_stable = np.vstack(post_likeli_stable)

        max_pre_likeli_stable = np.max(pre_likeli_stable, axis=1)
        max_post_likeli_stable = np.max(post_likeli_stable, axis=1)

        sim_ratio_stable = (max_post_likeli_stable-max_pre_likeli_stable)/(max_pre_likeli_stable+max_post_likeli_stable)

        if plotting or save_fig:

            print("To implement")

        return sim_ratio_all, sim_ratio_dec, sim_ratio_inc, sim_ratio_stable

    def memory_drift_combine_sleep_phases_decoded_non_decoded_likelihoods_shuffle(self, pre_model="pre",
                                                                                  post_model="post", n_art_modes=100,
                                                                                  save_result=True):

        event_spike_rasters = []

        for l_s in self.long_sleep:

            # get original bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_ = (
                l_s.get_spike_binned_raster_combined_sleep_phases())

            event_spike_rasters.append(np.hstack(event_spike_rasters_))

        all_spike_rasters = np.hstack(event_spike_rasters)

        # get means of pre and post modes + compression factor
        if pre_model == "pre":
            pre_file_name = self.session_params.default_pre_phmm_model
        if post_model ==  "post":
            post_file_name = self.session_params.default_post_phmm_model

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms


        # do decoding and plot result
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                  event_spike_rasters=all_spike_rasters,
                                                  compression_factor=compression_factor)

        post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                   event_spike_rasters=all_spike_rasters,
                                                   compression_factor=compression_factor)

        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        pre_decoded = np.zeros(max_pre_likeli.shape[0])
        pre_decoded[max_pre_likeli > max_post_likeli] = 1
        pre_decoded = pre_decoded.astype(bool)

        # take the log
        max_pre_log_likeli = np.log(max_pre_likeli)
        max_post_log_likeli = np.log(max_post_likeli)

        # compute difference
        diff_decoded_non_decoded = np.zeros(pre_decoded.shape[0])
        diff_decoded_non_decoded[pre_decoded] = max_pre_log_likeli[pre_decoded] - max_post_log_likeli[pre_decoded]
        diff_decoded_non_decoded[~pre_decoded] = max_post_log_likeli[~pre_decoded] - max_pre_log_likeli[~pre_decoded]


        # now for each population vector --> generate 100 artificial modes made of pre and post decoded mode
        # --> use as control distribution

        # get decoded modes
        dec_mode_pre = np.argmax(pre_likeli, axis=1)
        dec_mode_post = np.argmax(post_likeli, axis=1)

        pre_decoded_diff_z_scored = []
        post_decoded_diff_z_scored = []

        pop_vecs = np.arange(pre_likeli.shape[0])

        for i, pop_vec in enumerate(pop_vecs):

            print("Processing "+str((i/pop_vecs.shape[0])*100)+"%")

            pre_art_modes = np.zeros((n_art_modes, pre_mode_means.shape[1]))
            post_art_modes = np.zeros((n_art_modes, pre_mode_means.shape[1]))
            # generate artifical modes
            for i in range(n_art_modes):
                # combine pre and post mode
                pre_post_mode = np.vstack((pre_mode_means[dec_mode_pre[pop_vec]], post_mode_means[dec_mode_post[pop_vec]]))
                jumbled_ids = np.random.randint(0,2, pre_mode_means.shape[1])
                pre_art_modes[i, :] = pre_post_mode[jumbled_ids, np.arange(pre_mode_means.shape[1])]
                post_art_modes[i, :] = pre_post_mode[~jumbled_ids, np.arange(pre_mode_means.shape[1])]


            # decode artificial mode
            pre_likeli_control = decode_using_phmm_modes_fast(mode_means=pre_art_modes,
                                                          event_spike_rasters=np.expand_dims(all_spike_rasters[:, pop_vec],1),
                                                          compression_factor=compression_factor)

            post_likeli_control = decode_using_phmm_modes_fast(mode_means=post_art_modes,
                                                              event_spike_rasters=np.expand_dims(all_spike_rasters[:, pop_vec],1),
                                                              compression_factor=compression_factor)

            pre_likeli_control_log = np.log(pre_likeli_control)
            post_likeli_control_log = np.log(post_likeli_control)

            # compute difference --> still decoded - non_decoded
            if pre_decoded[pop_vec]:
                # if pre was decoded --> z-score difference using control
                diff_control = pre_likeli_control_log - post_likeli_control_log
                pre_decoded_diff_z_scored.append((diff_decoded_non_decoded[pop_vec]-np.mean(diff_control))/np.std(diff_control))
            else:
                diff_control = post_likeli_control_log - pre_likeli_control_log
                post_decoded_diff_z_scored.append((diff_decoded_non_decoded[pop_vec]-np.mean(diff_control))/np.std(diff_control))


        if save_result:
            res_dic = {}
            res_dic["pre_decoded_diff_z_scored"] = pre_decoded_diff_z_scored
            res_dic["post_decoded_diff_z_scored"] = post_decoded_diff_z_scored

            save_path = os.path.dirname(os.path.realpath(__file__)) + "/../temp_data/shuffled_decoded_non_decoded"
            filename = save_path+ "/" + self.session_name

            outfile = open(filename, 'wb')
            pickle.dump(res_dic, outfile)
            outfile.close()

        return pre_decoded_diff_z_scored, post_decoded_diff_z_scored

    def memory_drift_combine_sleep_phases_decoded_non_decoded_cosine_distance_shuffle(self, pre_model="pre",
                                                                                      post_model="post", n_art_modes=100,
                                                                                      save_result=True):

        event_spike_rasters = []

        for l_s in self.long_sleep:

            # get original bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_ = (
                l_s.get_spike_binned_raster_combined_sleep_phases())

            event_spike_rasters.append(np.hstack(event_spike_rasters_))

        all_spike_rasters = np.hstack(event_spike_rasters)

        # get means of pre and post modes + compression factor
        if pre_model == "pre":
            pre_file_name = self.session_params.default_pre_phmm_model
        if post_model ==  "post":
            post_file_name = self.session_params.default_post_phmm_model

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        # computing mode means in 12 spike format --> only needed if similarity with sleep spike bins is computed
        # pre_mode_means_constant_spike = constant_nr_spike_bin_from_mean_firing(pre_mode_means.T, return_mean_vector=True)
        # pre_mode_means_constant_spike = pre_mode_means_constant_spike.T
        # post_mode_means_constant_spike = constant_nr_spike_bin_from_mean_firing(post_mode_means.T, return_mean_vector=True)
        # post_mode_means_constant_spike = post_mode_means_constant_spike.T

        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

        # do decoding and plot result
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                  event_spike_rasters=all_spike_rasters,
                                                  compression_factor=compression_factor)

        post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                   event_spike_rasters=all_spike_rasters,
                                                   compression_factor=compression_factor)

        # check which PRE / POST mode was decoded
        pre_decoded_mode = np.argmax(pre_likeli, axis=1)
        post_decoded_mode = np.argmax(post_likeli, axis=1)

        # compute maximum likelihoods per model
        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        # check when PRE was decoded
        pre_decoded = np.zeros(max_pre_likeli.shape[0])
        pre_decoded[max_pre_likeli > max_post_likeli] = 1
        pre_decoded = pre_decoded.astype(bool)

        # go through all spike bins
        # --------------------------------------------------------------------------------------------------------------
        pre_decoded_diff_z_scored = []
        post_decoded_diff_z_scored = []

        pop_vecs = np.arange(pre_likeli.shape[0])

        for i, pop_vec in enumerate(pop_vecs):

            pre_art_modes = np.zeros((n_art_modes, pre_mode_means.shape[1]))
            post_art_modes = np.zeros((n_art_modes, pre_mode_means.shape[1]))
            # generate artifical modes
            for i in range(n_art_modes):
                # combine pre and post mode
                pre_post_mode = np.vstack((pre_mode_means[pre_decoded_mode[pop_vec]],
                                           post_mode_means[post_decoded_mode[pop_vec]]))
                jumbled_ids = np.random.randint(0,2, pre_mode_means.shape[1])
                pre_art_modes[i, :] = pre_post_mode[jumbled_ids, np.arange(pre_mode_means.shape[1])]
                post_art_modes[i, :] = pre_post_mode[~jumbled_ids, np.arange(pre_mode_means.shape[1])]

            # first compute distance between the actual decoded modes
            cos_dist_decoded_non_decoded = 1-distance.cdist(np.expand_dims(pre_mode_means[pre_decoded_mode[pop_vec]], 0),
                                                            np.expand_dims(post_mode_means[post_decoded_mode[pop_vec]], 0), metric="cosine")

            # compute distances between artifical modes
            cos_dist_shuffle = np.diagonal(1-distance.cdist(pre_art_modes, post_art_modes, metric="cosine"))

            # compute difference --> still decoded - non_decoded
            if pre_decoded[pop_vec]:
                # if pre was decoded --> z-score difference using control
                pre_decoded_diff_z_scored.append((cos_dist_decoded_non_decoded-np.mean(cos_dist_shuffle))/np.std(cos_dist_shuffle))
            else:

                post_decoded_diff_z_scored.append((cos_dist_decoded_non_decoded-np.mean(cos_dist_shuffle))/np.std(cos_dist_shuffle))

        if save_result:
            res_dic = {}
            res_dic["pre_decoded_diff_z_scored"] = pre_decoded_diff_z_scored
            res_dic["post_decoded_diff_z_scored"] = post_decoded_diff_z_scored

            save_path = os.path.dirname(os.path.realpath(__file__)) + "/../temp_data/shuffled_decoded_non_decoded_cosine"
            filename = save_path+ "/" + self.session_name

            outfile = open(filename, 'wb')
            pickle.dump(res_dic, outfile)
            outfile.close()

        return pre_decoded_diff_z_scored, post_decoded_diff_z_scored

    def memory_drift_combine_sleep_phases_decoded_non_decoded_likelihoods_shuffle_plot_results(self):
        infile = open("temp_data/shuffled_decoded_non_decoded/" + self.session_name, 'rb')
        results = pickle.load(infile)
        pre_decoded_diff_z_scored=np.hstack(results["pre_decoded_diff_z_scored"])
        post_decoded_diff_z_scored=np.hstack(results["post_decoded_diff_z_scored"])
        infile.close()
        plt.plot(uniform_filter1d(pre_decoded_diff_z_scored, int(pre_decoded_diff_z_scored.shape[0]/10)))
        plt.title("Acquisition decoded")
        plt.show()

        plt.plot(uniform_filter1d(post_decoded_diff_z_scored, int(post_decoded_diff_z_scored.shape[0]/10)))
        plt.title("Recall decoded")
        plt.show()


    def memory_drift_sleep_phases_decoded_likelihoods(self, sleep_phase="rem",
                                                                  pre_model="pre", post_model="post"):

        event_spike_rasters = []

        for l_s in self.long_sleep:

            # get original bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_ = (
                l_s.get_spike_binned_raster_sleep_phase(sleep_phase=sleep_phase))

            event_spike_rasters.append(np.hstack(event_spike_rasters_))

        all_spike_rasters = np.hstack(event_spike_rasters)

        # get means of pre and post modes + compression factor
        if pre_model == "pre":
            pre_file_name = self.session_params.default_pre_phmm_model
        if post_model ==  "post":
            post_file_name = self.session_params.default_post_phmm_model

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms


        # do decoding and plot result
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                  event_spike_rasters=all_spike_rasters,
                                                  compression_factor=compression_factor)

        post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                   event_spike_rasters=all_spike_rasters,
                                                   compression_factor=compression_factor)

        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        pre_decoded = np.zeros(max_pre_likeli.shape[0])
        pre_decoded[max_pre_likeli > max_post_likeli] = 1
        pre_decoded = pre_decoded.astype(bool)

        # take the log
        max_pre_log_likeli = np.log(max_pre_likeli)
        max_post_log_likeli = np.log(max_post_likeli)

        # compute difference when pre decoded
        diff_decoded_non_decoded = np.zeros(pre_decoded.shape[0])
        diff_decoded_non_decoded[pre_decoded] = max_pre_log_likeli[pre_decoded] - max_post_log_likeli[pre_decoded]
        diff_decoded_non_decoded[~pre_decoded] = max_post_log_likeli[~pre_decoded] - max_pre_log_likeli[~pre_decoded]

        # also compute pre/post likelihoods separately
        pre_pre_decoded = max_pre_log_likeli[pre_decoded]
        post_post_decoded = max_post_log_likeli[~pre_decoded]


        return pre_pre_decoded, post_post_decoded

    def memory_drift_control_jitter_combine_sleep_phases_bayesian(self, n_moving_average_pop_vec=800,
                                                             nr_spikes_per_jitter_window=10000):

        event_spike_rasters = []
        event_spike_rasters_jittered = []

        for l_s in self.long_sleep:

            # get original bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_ = (
                l_s.get_spike_binned_raster_combined_sleep_phases())

            # get jittered bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_jittered_ = (
                l_s.get_spike_binned_raster_combined_sleep_phases_jittered(nr_spikes_per_jitter_window=nr_spikes_per_jitter_window))

            # _, event_spike_rasters_jittered_ = (
            #     l_s.get_spike_binned_raster_combined_sleep_phases_jittered(nr_spikes_per_jitter_window=np.hstack(event_spike_rasters_).shape[1]))

            event_spike_rasters.append(np.hstack(event_spike_rasters_))
            event_spike_rasters_jittered.append(event_spike_rasters_jittered_)


        all_spike_rasters_jittered = np.hstack(event_spike_rasters_jittered)
        all_spike_rasters = np.hstack(event_spike_rasters)

        # delete events which are too short --> TODO
        # --------------------------------------------------------------------------------------------------------------
        # nr_bins = np.array([x.shape[1] for x in merged_events])

        # get means of pre and post modes + compression factor
        pre_file_name = self.session_params.default_pre_phmm_model
        post_file_name = self.session_params.default_post_phmm_model
        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

        # pre and post model for decoding
        pre_file_name = self.session_params.default_pre_ising_model
        post_file_name = self.session_params.default_post_ising_model

        with open(self.params.pre_proc_dir + 'awake_ising_maps/' + pre_file_name + '.pkl',
                  'rb') as f:
            pre_model_dic = pickle.load(f)

        with open(self.params.pre_proc_dir + 'awake_ising_maps/' + post_file_name + '.pkl',
                      'rb') as f:
            post_model_dic = pickle.load(f)

        time_bin_size_encoding = pre_model_dic["time_bin_size"]

        # load correct compression factor (as defined in parameter file of the session)
        if time_bin_size_encoding == 0.01:
            compression_factor = \
                np.round(self.session_params.sleep_compression_factor_12spikes_100ms * 10, 3)
        elif time_bin_size_encoding == 0.1:
            compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms
        else:
            raise Exception("COMPRESSION FACTOR NEITHER PROVIDED NOR FOUND IN PARAMETER FILE")


        # get template map
        pre_template_map = pre_model_dic["res_map"]
        post_template_map = post_model_dic["res_map"]

        # do decoding and plot result
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli = decode_using_ising_map(template_map=pre_template_map,
                                              event_spike_rasters=[all_spike_rasters],
                                              compression_factor=compression_factor,
                                              cell_selection="all")

        post_likeli = decode_using_ising_map(template_map=post_template_map,
                                            event_spike_rasters=[all_spike_rasters],
                                            compression_factor=compression_factor,
                                            cell_selection="all")

        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        sim_ratio = (max_post_likeli - max_pre_likeli) / (max_post_likeli + max_pre_likeli)

        # do decoding and plot result for jittered data
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_j = decode_using_ising_map(template_map=pre_template_map,
                                            event_spike_rasters=[all_spike_rasters_jittered],
                                            compression_factor=compression_factor,
                                            cell_selection="all")

        post_likeli_j = decode_using_ising_map(template_map=post_template_map,
                                             event_spike_rasters=[all_spike_rasters_jittered],
                                             compression_factor=compression_factor,
                                             cell_selection="all")

        pre_likeli_j = np.vstack(pre_likeli_j)
        post_likeli_j = np.vstack(post_likeli_j)

        max_pre_likeli_j = np.max(pre_likeli_j, axis=1)
        max_post_likeli_j = np.max(post_likeli_j, axis=1)

        sim_ratio_j = (max_post_likeli_j - max_pre_likeli_j) / (max_post_likeli_j + max_pre_likeli_j)

        # apply smoothing
        max_pre_likeli_s = moving_average(max_pre_likeli, n_moving_average_pop_vec)
        max_post_likeli_s = moving_average(max_post_likeli, n_moving_average_pop_vec)
        max_pre_likeli_j_s = moving_average(max_pre_likeli_j, n_moving_average_pop_vec)
        max_post_likeli_j_s = moving_average(max_post_likeli_j, n_moving_average_pop_vec)


        plt.plot(max_pre_likeli_s, color="red", label="PRE original")
        # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
        plt.plot(max_pre_likeli_j_s, color="salmon", linestyle="--", label="PRE jittered")
        plt.yscale("log")
        plt.ylabel("Max likelihood")
        plt.legend()
        plt.tight_layout()
        plt.show()

        plt.plot(max_post_likeli_s, color="white", label="POST original")
        # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
        plt.plot(max_post_likeli_j_s, color="lightblue", linestyle="--", label="POST jittered", alpha=0.8)
        plt.yscale("log")
        plt.ylabel("Max likelihood")
        plt.legend()
        plt.tight_layout()
        plt.show()

        plt.plot(moving_average(sim_ratio, n_moving_average_pop_vec), label="original", color="grey")
        plt.plot(moving_average(sim_ratio_j, n_moving_average_pop_vec), label="spike jittered", color="red", alpha=0.5)
        plt.xlabel("Time")
        plt.ylabel("sim_ratio")
        plt.legend()
        plt.tight_layout()
        plt.show()

        # cut to same length for the ratio
        # --------------------------------------------------------------------------------------------------------------
        print("HERE")

        if max_pre_likeli_s.shape[0] > max_pre_likeli_j_s.shape[0]:
            max_pre_likeli_s = max_pre_likeli_s[:max_pre_likeli_j_s.shape[0]]
            max_post_likeli_s = max_post_likeli_s[:max_post_likeli_j_s.shape[0]]
        else:
            max_pre_likeli_j_s = max_pre_likeli_j_s[:max_pre_likeli_s.shape[0]]
            max_post_likeli_j_s = max_post_likeli_j_s[:max_post_likeli_s.shape[0]]

        plt.plot(max_post_likeli_s/max_post_likeli_j_s, color="lightblue")
        plt.yscale("log")
        plt.ylabel("max post orig / max post jittered")
        plt.title("POST: orig / jittered")
        plt.tight_layout()
        plt.show()

        plt.plot(max_pre_likeli_s/max_pre_likeli_j_s, color="red")
        plt.yscale("log")
        plt.ylabel("max pre orig / max pre jittered")
        plt.title("PRE: orig / jittered")
        plt.tight_layout()
        plt.show()

        c = "white"
        res = [max_pre_likeli, max_pre_likeli_j, max_post_likeli, max_post_likeli_j]
        bplot = plt.boxplot(res, positions=[1, 2, 3, 4], patch_artist=True,
                            labels=["Max PRE", "Max PRE Jittered", "Max POST", "Max POST Jittered"],
                            boxprops=dict(color=c),
                            capprops=dict(color=c),
                            whiskerprops=dict(color=c),
                            flierprops=dict(color=c, markeredgecolor=c),
                            medianprops=dict(color=c), showfliers=False
                            )
        colors = ["magenta", 'magenta', "blue", "blue"]
        for patch, color in zip(bplot['boxes'], colors):
            patch.set_facecolor(color)
        plt.ylabel("Max. Likelihood")
        plt.grid(color="grey", axis="y")
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()

        c = "white"
        res = [pre_likeli.flatten(), pre_likeli_j.flatten(), post_likeli.flatten(), post_likeli_j.flatten()]
        bplot = plt.boxplot(res, positions=[1, 2, 3, 4], patch_artist=True,
                            labels=["PRE", "PRE Jittered", "POST", "POST Jittered"],
                            boxprops=dict(color=c),
                            capprops=dict(color=c),
                            whiskerprops=dict(color=c),
                            flierprops=dict(color=c, markeredgecolor=c),
                            medianprops=dict(color=c), showfliers=False
                            )
        colors = ["magenta", 'magenta', "blue", "blue"]
        for patch, color in zip(bplot['boxes'], colors):
            patch.set_facecolor(color)
        plt.ylabel("Likelihood")
        plt.grid(color="grey", axis="y")
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()

    def memory_drift_control_jitter_long_range_old(self, len_chunk_s = 200, n_equalizing=5,
                                        plot_for_control=False, plotting=True, rem_pop_vec_threshold=10,
                                        nrem_pop_vec_threshold=2, n_moving_average_pop_vec=200):

        event_spike_rasters_rem = []
        event_spike_rasters_nrem = []
        event_spike_rasters_jittered = []
        event_times_rem = []
        event_times_nrem = []

        first = 0
        for l_s in self.long_sleep[:1]:
            duration = l_s.get_duration_sec()

            # get jittered bins
            # ----------------------------------------------------------------------------------------------------------

            _, event_spike_rasters_jittered_, _, _ = (
                l_s.get_spike_binned_raster_jittered_long_range())

            # get original bins
            # ----------------------------------------------------------------------------------------------------------

            event_times_nrem_, event_spike_rasters_nrem_, _ = (
                l_s.get_spike_binned_raster_sleep_phase(sleep_phase="nrem"))

            event_times_rem_, event_spike_rasters_rem_, _ = (
                l_s.get_spike_binned_raster_sleep_phase(sleep_phase="rem"))

            event_spike_rasters_rem.extend(event_spike_rasters_rem_)
            event_spike_rasters_nrem.extend(event_spike_rasters_nrem_)
            event_spike_rasters_jittered.extend(event_spike_rasters_jittered_)

            event_times_rem.extend(event_times_rem_ + first)
            event_times_nrem.extend(event_times_nrem_ + first)
            first += duration

        if plot_for_control:
            x_min = 0
            x_max = 20
            all_rem_rasters = np.hstack(event_spike_rasters_rem)
            all_rem_rasters_jittered = np.hstack(event_spike_rasters_jittered)
            plt.subplot(2,1,1)
            plt.imshow(all_rem_rasters, interpolation='nearest', aspect='auto')
            plt.title("REM: Original")
            plt.xlim(x_min, x_max)
            plt.subplot(2,1,2)
            plt.imshow(all_rem_rasters_jittered, interpolation='nearest', aspect='auto')
            plt.xlim(x_min, x_max)
            plt.title("REM: Jittered")
            plt.tight_layout()
            plt.show()

            x_min = 0
            x_max = 20
            all_rem_rasters = np.hstack(event_spike_rasters_nrem)
            all_rem_rasters_jittered = np.hstack(event_spike_rasters_nrem_jittered)
            plt.subplot(2,1,1)
            plt.imshow(all_rem_rasters, interpolation='nearest', aspect='auto')
            plt.title("NREM: Original")
            plt.xlim(x_min, x_max)
            plt.subplot(2,1,2)
            plt.imshow(all_rem_rasters_jittered, interpolation='nearest', aspect='auto')
            plt.xlim(x_min, x_max)
            plt.title("NREM: Jittered")
            plt.tight_layout()
            plt.show()

        # need to sort events according to occurrence time and merge adjacent ones of same time
        labels_events = np.zeros(len(event_spike_rasters_rem)+len(event_spike_rasters_nrem))
        labels_events[:len(event_spike_rasters_rem)] = 1
        all_start_times = np.hstack((np.vstack(event_times_rem)[:, 0], np.vstack(event_times_nrem)[:,0]))
        all_end_times = np.hstack((np.vstack(event_times_rem)[:, 1], np.vstack(event_times_nrem)[:,2]))
        all_spike_rasters = event_spike_rasters_rem + event_spike_rasters_nrem
        all_spike_rasters_jittered = np.hstack(event_spike_rasters_jittered)

        # sort events according to time
        sorted_spike_rasters = [x for _, x in sorted(zip(all_start_times, all_spike_rasters))]
        sorted_labels_events = [x for _, x in sorted(zip(all_start_times, labels_events))]
        sorted_start_times = [x for x in sorted(all_start_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_start_times, all_end_times))]

        # merge events
        # --------------------------------------------------------------------------------------------------------------
        merged_events = []
        merged_events_labels = []
        merged_events_start_times = []
        merged_events_end_times = []

        curr_label = sorted_labels_events[0]
        ev_id = 0
        merged_event = []
        merged_event_jittered = []
        while ev_id < len(sorted_spike_rasters):
            if sorted_labels_events[ev_id] == curr_label:
                merged_event.append(sorted_spike_rasters[ev_id])
            else:
                # need to append the event to merged_events --> use end time
                merged_events.append(np.hstack(merged_event))
                merged_events_labels.append(curr_label)
                merged_events_end_times.append(sorted_end_times[ev_id])
                # initialize new event --> use start time
                merged_event = []
                curr_label = sorted_labels_events[ev_id]
                # append current element
                merged_event.append(sorted_spike_rasters[ev_id])
                merged_events_start_times.append(sorted_start_times[ev_id])
            ev_id += 1

        # delete events which are too short --> TODO
        # --------------------------------------------------------------------------------------------------------------
        # nr_bins = np.array([x.shape[1] for x in merged_events])

        # get means of pre and post modes + compression factor
        pre_file_name = self.session_params.default_pre_phmm_model
        post_file_name = self.session_params.default_post_phmm_model
        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        # do decoding and plot result
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli = decode_using_phmm_modes(mode_means=pre_mode_means,
                                             event_spike_rasters=merged_events,
                                             compression_factor=compression_factor,
                                             cell_selection="all")

        post_likeli = decode_using_phmm_modes(mode_means=post_mode_means,
                                              event_spike_rasters=merged_events,
                                              compression_factor=compression_factor,
                                              cell_selection="all")

        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        sim_ratio = (max_post_likeli - max_pre_likeli) / (max_post_likeli + max_pre_likeli)

        # do decoding and plot result for jittered data
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_j = decode_using_phmm_modes(mode_means=pre_mode_means,
                                               event_spike_rasters=[all_spike_rasters_jittered],
                                               compression_factor=compression_factor,
                                               cell_selection="all")

        post_likeli_j = decode_using_phmm_modes(mode_means=post_mode_means,
                                                event_spike_rasters=[all_spike_rasters_jittered],
                                                compression_factor=compression_factor,
                                                cell_selection="all")

        pre_likeli_j = np.vstack(pre_likeli_j)
        post_likeli_j = np.vstack(post_likeli_j)

        max_pre_likeli_j = np.max(pre_likeli_j, axis=1)
        max_post_likeli_j = np.max(post_likeli_j, axis=1)

        sim_ratio_j = (max_post_likeli_j - max_pre_likeli_j) / (max_post_likeli_j + max_pre_likeli_j)

        plt.plot(max_pre_likeli, label="PRE: original")
        plt.plot(max_pre_likeli_j, label="PRE: jittered")
        plt.legend()
        plt.show()

        plt.plot(max_post_likeli, label="POST: original")
        plt.plot(max_post_likeli_j, label="POST: jittered")
        plt.legend()
        plt.show()

        plt.plot(moving_average(sim_ratio, n_moving_average_pop_vec), label="original")
        plt.plot(moving_average(sim_ratio_j, n_moving_average_pop_vec), label="spike jittered")
        plt.xlabel("Time")
        plt.ylabel("sim_ratio")
        plt.legend()
        plt.tight_layout()
        plt.show()

    def memory_drift_control_whole_sleep_jumbled_modes(self, n_moving_average_pop_vec=800, plot_for_control=False,
                                                             nr_spikes_per_jitter_window=200):

        event_spike_rasters = []

        for l_s in self.long_sleep:

            # get original bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_ = (
                l_s.get_spike_binned_raster_combined_sleep_phases())

            event_spike_rasters.append(np.hstack(event_spike_rasters_))

        all_spike_rasters = np.hstack(event_spike_rasters)

        # delete events which are too short --> TODO
        # --------------------------------------------------------------------------------------------------------------
        # nr_bins = np.array([x.shape[1] for x in merged_events])

        # get means of pre and post modes + compression factor
        pre_file_name = self.session_params.default_pre_phmm_model
        post_file_name = self.session_params.default_post_phmm_model
        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        # do decoding and plot result
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                  event_spike_rasters=all_spike_rasters,
                                                  compression_factor=compression_factor)

        post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                event_spike_rasters=all_spike_rasters,
                                                compression_factor=compression_factor)

        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        sim_ratio = (max_post_likeli - max_pre_likeli) / (max_post_likeli + max_pre_likeli)

        # jumble post
        # --------------------------------------------------------------------------------------------------------------
        post_mode_means_jumbled = np.zeros((post_mode_means.shape[0], post_mode_means.shape[1]))
        for cell_id, cell_lambdas in enumerate(post_mode_means.T):
            post_mode_means_jumbled[:, cell_id] = np.random.permutation(cell_lambdas)

        if plot_for_control:
            plt.subplot(1,2,1)
            plt.imshow(post_mode_means.T, interpolation='nearest', aspect='auto')
            plt.title("POST: original")
            plt.xlabel("Modes")
            plt.ylabel("Cell ID")
            plt.subplot(1,2,2)
            plt.imshow(post_mode_means_jumbled.T, interpolation='nearest', aspect='auto')
            plt.title("POST: jumbled")
            plt.xlabel("Modes")
            plt.tight_layout()
            plt.show()

        post_likeli_jumbled = decode_using_phmm_modes_fast(mode_means=post_mode_means_jumbled,
                                                           event_spike_rasters=all_spike_rasters,
                                                           compression_factor=compression_factor)


        post_likeli_jumbled = np.vstack(post_likeli_jumbled)
        max_post_likeli_j = np.max(post_likeli_jumbled, axis=1)


        # jumble pre
        # --------------------------------------------------------------------------------------------------------------
        pre_mode_means_jumbled = np.zeros((pre_mode_means.shape[0], pre_mode_means.shape[1]))
        for cell_id, cell_lambdas in enumerate(pre_mode_means.T):
            pre_mode_means_jumbled[:, cell_id] = np.random.permutation(cell_lambdas)

        if plot_for_control:
            plt.subplot(1,2,1)
            plt.imshow(pre_mode_means.T, interpolation='nearest', aspect='auto')
            plt.title("PRE: original")
            plt.xlabel("Modes")
            plt.ylabel("Cell ID")
            plt.subplot(1,2,2)
            plt.imshow(pre_mode_means_jumbled.T, interpolation='nearest', aspect='auto')
            plt.title("PRE: jumbled")
            plt.xlabel("Modes")
            plt.tight_layout()
            plt.show()

        pre_likeli_jumbled = decode_using_phmm_modes_fast(mode_means=pre_mode_means_jumbled,
                                                           event_spike_rasters=all_spike_rasters,
                                                           compression_factor=compression_factor)


        pre_likeli_jumbled = np.vstack(pre_likeli_jumbled)
        max_pre_likeli_j = np.max(pre_likeli_jumbled, axis=1)


        sim_ratio_pre_j = (max_post_likeli - max_pre_likeli_j) / (max_post_likeli + max_pre_likeli_j)
        sim_ratio_post_j = (max_post_likeli_j - max_pre_likeli) / (max_post_likeli_j + max_pre_likeli)
        sim_ratio_both_j = (max_post_likeli_j - max_pre_likeli_j) / (max_post_likeli_j + max_pre_likeli_j)

        plt.plot(moving_average(sim_ratio, n_moving_average_pop_vec), label="original", color="grey")
        plt.plot(moving_average(sim_ratio_post_j, n_moving_average_pop_vec), label="post_jumbled", color="red", alpha=0.5)
        plt.xlabel("Time")
        plt.ylabel("sim_ratio")
        plt.legend()
        plt.tight_layout()
        plt.show()

        plt.plot(moving_average(sim_ratio, n_moving_average_pop_vec), label="original", color="grey")
        plt.plot(moving_average(sim_ratio_pre_j, n_moving_average_pop_vec), label="pre_jumbled", color="red", alpha=0.5)
        plt.xlabel("Time")
        plt.ylabel("sim_ratio")
        plt.legend()
        plt.tight_layout()
        plt.show()

        plt.plot(moving_average(sim_ratio, n_moving_average_pop_vec), label="original", color="grey")
        plt.plot(moving_average(sim_ratio_both_j, n_moving_average_pop_vec), label="both_jumbled", color="red", alpha=0.5)
        plt.xlabel("Time")
        plt.ylabel("sim_ratio")
        plt.legend()
        plt.tight_layout()
        plt.show()

        plt.plot(moving_average(max_pre_likeli, n_moving_average_pop_vec), color="grey", label="PRE")
        plt.plot(moving_average(max_post_likeli, n_moving_average_pop_vec), color="red", label="POST")
        plt.plot(moving_average(max_post_likeli_j, n_moving_average_pop_vec), color="salmon", linestyle="--", label="POST jumbled")
        plt.yscale("log")
        plt.ylabel("Max likelihood")
        plt.legend()
        plt.tight_layout()
        plt.show()

    def memory_drift_fam_exploration(self, n_moving_average_pop_vec=800, plotting=True, z_score=False):

        event_spike_rasters = []

        for l_s in self.long_sleep:

            # get original bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_ = (
                l_s.get_spike_binned_raster_combined_sleep_phases())

            event_spike_rasters.append(np.hstack(event_spike_rasters_))

        all_spike_rasters = np.hstack(event_spike_rasters)

        # delete events which are too short --> TODO
        # --------------------------------------------------------------------------------------------------------------
        # nr_bins = np.array([x.shape[1] for x in merged_events])

        # get means of pre and post modes + compression factor
        pre_file_name = self.session_params.default_pre_phmm_model
        post_file_name = self.session_params.default_post_phmm_model
        fam_pre_file_name = self.session_params.default_exp_fam_1_model
        fam_post_file_name = self.session_params.default_exp_fam_2_model

        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + fam_pre_file_name + '.pkl', 'rb') as f:
            fam_pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        fam_pre_mode_means = fam_pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + fam_post_file_name + '.pkl', 'rb') as f:
            fam_post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        fam_post_mode_means = fam_post_model_dic.means_

        # do decoding and plot result
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                  event_spike_rasters=all_spike_rasters,
                                                  compression_factor=compression_factor)

        post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                   event_spike_rasters=all_spike_rasters,
                                                   compression_factor=compression_factor)

        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        sim_ratio = (max_post_likeli - max_pre_likeli) / (max_post_likeli + max_pre_likeli)

        fam_post_likeli = decode_using_phmm_modes_fast(mode_means=fam_post_mode_means,
                                                           event_spike_rasters=all_spike_rasters,
                                                           compression_factor=compression_factor)

        fam_post_likeli = np.vstack(fam_post_likeli)
        max_fam_post_likeli = np.max(fam_post_likeli, axis=1)

        fam_pre_likeli = decode_using_phmm_modes_fast(mode_means=fam_pre_mode_means,
                                                          event_spike_rasters=all_spike_rasters,
                                                          compression_factor=compression_factor)

        fam_pre_likeli = np.vstack(fam_pre_likeli)
        max_fam_pre_likeli = np.max(fam_pre_likeli, axis=1)
        sim_ratio_fam = (max_fam_post_likeli - max_fam_pre_likeli) / (max_fam_post_likeli + max_fam_pre_likeli)

        sim_ratio_pre_fam_post = (max_post_likeli - max_fam_pre_likeli) / (max_post_likeli + max_fam_pre_likeli)
        sim_ratio_pre_post_fam = (max_fam_post_likeli - max_pre_likeli) / (max_fam_post_likeli + max_pre_likeli)

        # take the log
        max_pre_log_likeli = np.log(max_pre_likeli)
        max_fam_pre_log_likeli = np.log(max_fam_pre_likeli)
        max_post_log_likeli = np.log(max_post_likeli)
        max_fam_post_log_likeli = np.log(max_fam_post_likeli)

        # concatenate likelihood vectors to see which of the four models is max
        all_max_likeli = np.vstack((max_pre_log_likeli, max_fam_pre_log_likeli,
                                    max_post_log_likeli, max_fam_post_log_likeli))

        decoded_model = np.argmax(all_max_likeli, axis=0)

        # compute fraction each model was decoded --> 1st and 2nd half
        frac_first_half = np.zeros(4)
        model, count = np.unique(decoded_model[:int(decoded_model.shape[0]/2)], return_counts=True)
        frac_first_half[model] = count/int(decoded_model.shape[0]/2)
        frac_second_half = np.zeros(4)
        model, count = np.unique(decoded_model[-int(decoded_model.shape[0]/2):], return_counts=True)
        frac_second_half[model] = count/int(decoded_model.shape[0]/2)

        # only select pre/post if one of them was selected
        pre_post_decoded = np.logical_or(decoded_model == 0, decoded_model == 2)

        max_pre_log_likeli = max_pre_log_likeli[pre_post_decoded]
        max_post_log_likeli = max_post_log_likeli[pre_post_decoded]
        max_fam_pre_log_likeli = max_fam_pre_log_likeli[~pre_post_decoded]
        max_fam_post_log_likeli = max_fam_post_log_likeli[~pre_post_decoded]

        # window_size = 500
        # nr_windows = int(all_max_likeli.shape[1]/window_size)
        # pre_dec = np.zeros(nr_windows)
        # pre_fam_dec = np.zeros(nr_windows)
        # post_dec = np.zeros(nr_windows)
        # post_fam_dec = np.zeros(nr_windows)
        # for w in range(int(all_max_likeli.shape[1]/window_size)):
        #     pre_dec[w] = np.count_nonzero(decoded_model[w*window_size:(w+1)*window_size]==0)/window_size
        #     pre_fam_dec[w] = np.count_nonzero(decoded_model[w*window_size:(w+1)*window_size]==1)/window_size
        #     post_dec[w] = np.count_nonzero(decoded_model[w*window_size:(w+1)*window_size]==2)/window_size
        #     post_fam_dec[w] = np.count_nonzero(decoded_model[w*window_size:(w+1)*window_size]==3)/window_size
        #
        # # apply smoothing
        # max_pre_log_likeli_s = uniform_filter1d(max_pre_log_likeli, n_moving_average_pop_vec)
        # max_pre_log_likeli_j_s = uniform_filter1d(max_pre_log_likeli_j, n_moving_average_pop_vec)
        # max_post_log_likeli_s = uniform_filter1d(max_post_log_likeli, n_moving_average_pop_vec)
        # max_post_log_likeli_j_s = uniform_filter1d(max_post_log_likeli_j, n_moving_average_pop_vec)

        # n_smoothing = 20
        # plt.figure(figsize=(10,8))
        # plt.plot(uniform_filter1d(pre_dec,n_smoothing), label="Acquisition")
        # plt.plot(uniform_filter1d(pre_fam_dec, n_smoothing), label="Pre familiar expl.")
        # plt.plot(uniform_filter1d(post_dec, n_smoothing), label="Recall")
        # plt.plot(uniform_filter1d(post_fam_dec, n_smoothing), label="Post familiar expl.")
        # plt.legend(ncol=2, loc=3)
        # plt.ylim(0, 0.7)
        # plt.ylabel("Decoding probability \n(smoothed)")
        # plt.xlabel("Time window")
        # plt.show()

        if plotting:
            # compute difference
            diff_pre = max_pre_log_likeli - max_pre_log_likeli_j
            diff_post = max_post_log_likeli - max_post_log_likeli_j

            plt.figure(figsize=(10, 7))
            plt.plot(moving_average(sim_ratio, n_moving_average_pop_vec), label="original", color="grey")
            plt.plot(moving_average(sim_ratio_pre_fam_post, n_moving_average_pop_vec), label="pre_vs_post_fam",
                     color="red", alpha=0.5)
            plt.plot(moving_average(sim_ratio_pre_post_fam, n_moving_average_pop_vec), label="pre_famv_vs_post",
                     color="orange", alpha=0.5)
            plt.plot(moving_average(sim_ratio_fam, n_moving_average_pop_vec), label="fam_exploration", color="white")
            plt.xlabel("Time")
            plt.ylabel("sim_ratio")
            plt.legend()
            plt.tight_layout()
            plt.show()
            plt.figure(figsize=(12,8))
            plt.subplot(2,2,1)
            plt.plot(max_pre_log_likeli_s, color="red", label="PRE original")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_pre_log_likeli_j_s, color="salmon", linestyle="--", label="PRE familiar_exp")
            plt.xticks([])
            plt.ylabel("Max log likelihood")
            plt.legend()
            plt.subplot(2,2,3)
            plt.plot(np.linspace(0,1, diff_pre.shape[0]), diff_pre)
            plt.ylabel("original - fam_exploration")
            plt.xlabel("relative time")
            plt.subplot(2,2,2)
            plt.plot(max_post_log_likeli_s, color="white", label="POST original")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_post_log_likeli_j_s, color="lightblue", linestyle="--", label="POST fam_exploration", alpha=0.8)
            plt.xticks([])
            plt.legend()
            plt.subplot(2,2,4)
            plt.plot(np.linspace(0,1, diff_post.shape[0]), diff_post)
            plt.ylabel("original - fam_exploration")
            plt.xlabel("relative time")
            plt.tight_layout()
            plt.show()

        if z_score:
            # need to z-score to be able to compare between different sessions: PRE likeli
            temp_pre = np.hstack((max_pre_log_likeli, max_fam_pre_log_likeli))
            temp_pre_z = zscore(temp_pre)
            max_pre_log_likeli = temp_pre_z[:max_pre_log_likeli.shape[0]]
            max_pre_log_likeli_j = temp_pre_z[max_pre_log_likeli.shape[0]:]

            # need to z-score to be able to compare between different sessions: POST likeli
            temp_post = np.hstack((max_post_log_likeli, max_fam_post_log_likeli))
            temp_post_z = zscore(temp_post)
            max_post_log_likeli = temp_post_z[:max_post_log_likeli.shape[0]]
            max_post_log_likeli_j = temp_post_z[max_post_log_likeli.shape[0]:]

        return max_pre_log_likeli, max_fam_pre_log_likeli, max_post_log_likeli, max_fam_post_log_likeli, sim_ratio, \
               sim_ratio_fam, frac_first_half, frac_second_half

    def memory_drift_control_null_model(self, n_moving_average_pop_vec=10000, plot_for_control=False,
                                        plotting=False, save_results=False, window_in_min=30, load_from_temp=False):
        if load_from_temp:
            infile = open("temp_data/null_model_raster/" + self.session_name, 'rb')
            event_spike_rasters_generated = pickle.load(infile)
            infile.close()
        else:
            event_spike_rasters_generated = []
        event_spike_rasters = []
        for i_sleep, l_s in enumerate(self.long_sleep):
            if not load_from_temp:
                # get generated data
                # ----------------------------------------------------------------------------------------------------------
                _, event_spike_rasters_generated_ = (
                    l_s.get_event_spike_rasters_artificial_combined_sleep_phases(window_in_min=window_in_min))

                event_spike_rasters_generated.append(np.hstack(event_spike_rasters_generated_))

            # get original data
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_ = (
                l_s.get_spike_binned_raster_slow_combined_sleep_phases())

            event_spike_rasters.append(np.hstack(event_spike_rasters_))

        if not load_from_temp:
            # save rasters
            save_path = os.path.dirname(os.path.realpath(__file__)) + "/../temp_data/null_model_raster"
            filename = save_path+ "/" + self.session_name

            outfile = open(filename, 'wb')
            pickle.dump(event_spike_rasters_generated, outfile)
            outfile.close()

        all_spike_rasters = np.hstack(event_spike_rasters)

        # event_spike_rasters_generated = [x[0] for x in event_spike_rasters_generated]
        all_spike_rasters_generated = np.hstack(event_spike_rasters_generated)
        # delete events which are too short --> TODO
        # --------------------------------------------------------------------------------------------------------------
        # nr_bins = np.array([x.shape[1] for x in merged_events])

        # get means of pre and post modes + compression factor
        pre_file_name = self.session_params.default_pre_phmm_model
        post_file_name = self.session_params.default_post_phmm_model
        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        # do decoding and plot result
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                  event_spike_rasters=all_spike_rasters,
                                                  compression_factor=compression_factor)

        post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                   event_spike_rasters=all_spike_rasters,
                                                   compression_factor=compression_factor)

        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        sim_ratio = (max_post_likeli - max_pre_likeli) / (max_post_likeli + max_pre_likeli)


        post_likeli_generated = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                           event_spike_rasters=all_spike_rasters_generated,
                                                           compression_factor=compression_factor)


        post_likeli_generated = np.vstack(post_likeli_generated)
        max_post_likeli_j = np.max(post_likeli_generated, axis=1)


        pre_likeli_generated = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                          event_spike_rasters=all_spike_rasters_generated,
                                                          compression_factor=compression_factor)

        pre_likeli_generated = np.vstack(pre_likeli_generated)
        max_pre_likeli_j = np.max(pre_likeli_generated, axis=1)

        sim_ratio_j = (max_post_likeli_j - max_pre_likeli_j) / (max_post_likeli_j + max_pre_likeli_j)

        # take the log
        max_pre_log_likeli = np.log(max_pre_likeli)
        max_pre_log_likeli_j = np.log(max_pre_likeli_j)
        max_post_log_likeli = np.log(max_post_likeli)
        max_post_log_likeli_j = np.log(max_post_likeli_j)

        # interpolate missing values
        if max_pre_log_likeli_j.shape[0] < max_pre_log_likeli.shape[0]:
            max_pre_log_likeli_j = np.interp(np.linspace(0,len(max_pre_log_likeli_j),len(max_pre_log_likeli)),
                                             np.arange(len(max_pre_log_likeli_j)),max_pre_log_likeli_j)

            max_post_log_likeli_j = np.interp(np.linspace(0,len(max_post_log_likeli_j),len(max_post_log_likeli)),
                                             np.arange(len(max_post_log_likeli_j)),max_post_log_likeli_j)
        elif max_pre_log_likeli.shape[0] < max_pre_log_likeli_j.shape[0]:
            max_pre_log_likeli = np.interp(np.linspace(0, len(max_pre_log_likeli), len(max_pre_log_likeli_j)),
                                             np.arange(len(max_pre_log_likeli)), max_pre_log_likeli)

            max_post_log_likeli = np.interp(np.linspace(0, len(max_post_log_likeli), len(max_post_log_likeli_j)),
                                              np.arange(len(max_post_log_likeli)), max_post_log_likeli)

        # apply smoothing
        max_pre_log_likeli_s = uniform_filter1d(max_pre_log_likeli, int(max_pre_log_likeli.shape[0]/5))
        max_pre_log_likeli_j_s = uniform_filter1d(max_pre_log_likeli_j, int(max_pre_log_likeli.shape[0]/5))
        max_post_log_likeli_s = uniform_filter1d(max_post_log_likeli, int(max_pre_log_likeli.shape[0]/5))
        max_post_log_likeli_j_s = uniform_filter1d(max_post_log_likeli_j, int(max_pre_log_likeli.shape[0]/5))

        diff_pre = max_pre_log_likeli_s - max_pre_log_likeli_j_s
        diff_post = max_post_log_likeli_s - max_post_log_likeli_j_s

        if plotting:
            plt.plot(moving_average(sim_ratio, n_moving_average_pop_vec), label="original", color="grey")
            plt.plot(moving_average(sim_ratio_j, n_moving_average_pop_vec), label="artificial", color="red", alpha=0.5)
            plt.xlabel("Time")
            plt.ylabel("sim_ratio")
            plt.legend()
            plt.tight_layout()
            plt.show()

            plt.figure(figsize=(12,8))
            plt.subplot(2,2,1)
            plt.plot(max_pre_log_likeli_s, color="red", label="PRE original")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_pre_log_likeli_j_s, color="salmon", linestyle="--", label="PRE generated")
            plt.xticks([])
            plt.ylabel("Max log likelihood")
            plt.legend()
            plt.subplot(2,2,3)
            plt.plot(np.linspace(0,1, diff_pre.shape[0]), diff_pre)
            plt.ylabel("Original - Generated")
            plt.xlabel("relative time")
            plt.subplot(2,2,2)
            plt.plot(max_post_log_likeli_s, color="white", label="POST original")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_post_log_likeli_j_s, color="lightblue", linestyle="--", label="POST generated", alpha=0.8)
            plt.xticks([])
            plt.legend()
            plt.subplot(2,2,4)
            plt.plot(np.linspace(0,1, diff_post.shape[0]), diff_post)
            plt.ylabel("Original - Generated")
            plt.xlabel("relative time")
            plt.tight_layout()
            plt.show()

        if save_results:

            res_dic = {}
            res_dic["sim_ratio"] = sim_ratio
            res_dic["sim_ratio_j"] = sim_ratio_j
            res_dic["diff_pre"] = diff_pre
            res_dic["diff_post"] = diff_post
            res_dic["max_pre"] = max_pre_log_likeli_s
            res_dic["max_pre_jit"] = max_pre_log_likeli_j_s
            res_dic["max_post"] = max_post_log_likeli_s
            res_dic["max_post_jit"] = max_post_log_likeli_j_s

            save_path = os.path.dirname(os.path.realpath(__file__)) + "/../temp_data/null_model"
            filename = save_path+ "/" + self.session_name

            outfile = open(filename, 'wb')
            pickle.dump(res_dic, outfile)
            outfile.close()

        return sim_ratio, sim_ratio_j, diff_pre, diff_post, max_pre_log_likeli_s, \
            max_pre_log_likeli_j_s, max_post_log_likeli_s, max_post_log_likeli_j_s

    # </editor-fold>

    def memory_drift_nrem_outside_swr(self, n_moving_average_pop_vec=10000, plotting=False, outside_swr_min_bin_size=0.5):

        event_spike_rasters_swr = []
        event_spike_rasters_outside_swr = []
        bin_dur =[]

        for l_s in self.long_sleep:
            # get outside SWR bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_nrem_outside_swr_, bin_dur_ = (
                l_s.get_spike_binned_raster_around_swr(return_bin_duration=True))

            # _, event_spike_rasters_nrem_outside_swr_ = (
            #     l_s.get_spike_binned_raster_nrem_outside_swr())

            # get SWR bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_swr_ = (
                l_s.get_spike_binned_raster_sleep_phase(sleep_phase="nrem"))

            event_spike_rasters_swr.append(np.hstack(event_spike_rasters_swr_))
            event_spike_rasters_outside_swr.append(np.hstack(event_spike_rasters_nrem_outside_swr_))

            bin_dur.append(np.hstack(bin_dur_))

        bin_dur =np.hstack(bin_dur)
        all_spike_rasters_outside_swr = np.hstack(event_spike_rasters_outside_swr)
        all_spike_rasters_swr = np.hstack(event_spike_rasters_swr)

        # only use long bins
        orig_nr_bins = all_spike_rasters_outside_swr.shape[1]
        all_spike_rasters_outside_swr = all_spike_rasters_outside_swr[:, bin_dur > outside_swr_min_bin_size]
        print(str(np.round(all_spike_rasters_outside_swr.shape[1]/(orig_nr_bins*0.01),2))+
              "% of bins remained (threshold: "+str(outside_swr_min_bin_size)+"s)")

        # delete events which are too short --> TODO
        # --------------------------------------------------------------------------------------------------------------
        # nr_bins = np.array([x.shape[1] for x in merged_events])

        # get means of pre and post modes + compression factor
        pre_file_name = self.session_params.default_pre_phmm_model
        post_file_name = self.session_params.default_post_phmm_model
        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        # do decoding and plot result
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                  event_spike_rasters=all_spike_rasters_swr,
                                                  compression_factor=compression_factor)

        post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                   event_spike_rasters=all_spike_rasters_swr,
                                                   compression_factor=compression_factor)

        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        sim_ratio = (max_post_likeli - max_pre_likeli) / (max_post_likeli + max_pre_likeli)

        # do decoding and plot result for jittered data
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_j = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                    event_spike_rasters=all_spike_rasters_outside_swr,
                                                    compression_factor=compression_factor)

        post_likeli_j = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                     event_spike_rasters=all_spike_rasters_outside_swr,
                                                     compression_factor=compression_factor)

        pre_likeli_j = np.vstack(pre_likeli_j)
        post_likeli_j = np.vstack(post_likeli_j)

        max_pre_likeli_j = np.max(pre_likeli_j, axis=1)
        max_post_likeli_j = np.max(post_likeli_j, axis=1)

        # look at distribution of decoded PRE modes
        mode_decoded_pre, mode_decoded_pre_counts = np.unique(np.argmax(pre_likeli, axis=1), return_counts=True)
        mode_decoded_pre_outside, mode_decoded_pre_counts_outside = np.unique(np.argmax(pre_likeli_j, axis=1),
                                                                              return_counts=True)
        modes_original_pre =np.zeros(pre_likeli.shape[1])
        modes_original_pre[mode_decoded_pre] = mode_decoded_pre_counts/np.sum(mode_decoded_pre_counts)
        modes_excluded_pre =np.zeros(pre_likeli.shape[1])
        modes_excluded_pre[mode_decoded_pre_outside] = mode_decoded_pre_counts_outside/np.sum(mode_decoded_pre_counts_outside)

        # look at distribution of decoded POST modes
        mode_decoded_post, mode_decoded_post_counts = np.unique(np.argmax(post_likeli, axis=1), return_counts=True)
        mode_decoded_post_outside, mode_decoded_post_counts_outside = np.unique(np.argmax(post_likeli_j, axis=1),
                                                                              return_counts=True)
        modes_original_post =np.zeros(post_likeli.shape[1])
        modes_original_post[mode_decoded_post] = mode_decoded_post_counts/np.sum(mode_decoded_post_counts)
        modes_excluded_post =np.zeros(post_likeli.shape[1])
        modes_excluded_post[mode_decoded_post_outside] = mode_decoded_post_counts_outside/np.sum(mode_decoded_post_counts_outside)


        sim_ratio_j = (max_post_likeli_j - max_pre_likeli_j) / (max_post_likeli_j + max_pre_likeli_j)

        # cut to same length for the ratio
        # --------------------------------------------------------------------------------------------------------------

        # take the log
        max_pre_log_likeli = np.log(max_pre_likeli)
        max_pre_log_likeli_j = np.log(max_pre_likeli_j)
        max_post_log_likeli = np.log(max_post_likeli)
        max_post_log_likeli_j = np.log(max_post_likeli_j)

        # interpolate missing values
        max_pre_log_likeli_j = np.interp(np.linspace(0, len(max_pre_log_likeli_j), len(max_pre_log_likeli)),
                                         np.arange(len(max_pre_log_likeli_j)), max_pre_log_likeli_j)

        max_post_log_likeli_j = np.interp(np.linspace(0, len(max_post_log_likeli_j), len(max_post_log_likeli)),
                                          np.arange(len(max_post_log_likeli_j)), max_post_log_likeli_j)

        # apply smoothing
        max_pre_log_likeli_s = uniform_filter1d(max_pre_log_likeli, n_moving_average_pop_vec)
        max_pre_log_likeli_j_s = uniform_filter1d(max_pre_log_likeli_j, n_moving_average_pop_vec)
        max_post_log_likeli_s = uniform_filter1d(max_post_log_likeli, n_moving_average_pop_vec)
        max_post_log_likeli_j_s = uniform_filter1d(max_post_log_likeli_j, n_moving_average_pop_vec)

        # compute difference
        diff_pre = max_pre_log_likeli_s - max_pre_log_likeli_j_s
        diff_post = max_post_log_likeli_s - max_post_log_likeli_j_s


        if plotting:

            plt.scatter(modes_original_pre, modes_excluded_pre)
            plt.title("PRE mode decoding (fraction)")
            plt.xlabel("NREM SWR")
            plt.ylabel("OUTSIDE")
            plt.tight_layout()
            plt.show()

            plt.scatter(modes_original_post, modes_excluded_post)
            plt.title("POST mode decoding (fraction)")
            plt.xlabel("NREM SWR")
            plt.ylabel("OUTSIDE")
            plt.tight_layout()
            plt.show()

            plt.plot(moving_average(sim_ratio, n_moving_average_pop_vec), label="SWRs", color="grey")
            plt.plot(moving_average(sim_ratio_j, int(sim_ratio_j.shape[0]/10)),
                     label="NREM w/o SWRs", color="red", alpha=0.5)
            plt.xlabel("Time")
            plt.ylabel("sim_ratio")
            plt.legend()
            plt.tight_layout()
            plt.show()

            plt.figure(figsize=(12,8))
            plt.subplot(2,2,1)
            plt.plot(max_pre_log_likeli_s, color="red", label="PRE SWRs")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_pre_log_likeli_j_s, color="salmon", linestyle="--", label="PRE NREM w/o SWRs")
            plt.xticks([])
            plt.ylabel("Max log likelihood")
            plt.legend()
            plt.subplot(2,2,3)
            plt.plot(np.linspace(0,1, diff_pre.shape[0]), diff_pre)
            plt.ylabel("SWRs - NREM w/o SWRs")
            plt.xlabel("relative time")
            plt.subplot(2,2,2)
            plt.plot(max_post_log_likeli_s, color="white", label="POST SWRs")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_post_log_likeli_j_s, color="lightblue", linestyle="--", label="POST NREM w/o SWRs", alpha=0.8)
            plt.xticks([])
            plt.legend()
            plt.subplot(2,2,4)
            plt.plot(np.linspace(0,1, diff_post.shape[0]), diff_post)
            plt.ylabel("SWRs - NREM w/o SWRs")
            plt.xlabel("relative time")
            plt.tight_layout()
            plt.show()

        # need to z-score to be able to compare between different sessions: PRE likeli
        temp_pre = np.hstack((max_pre_log_likeli_s, max_pre_log_likeli_j_s))
        temp_pre_z = zscore(temp_pre)
        max_pre_log_likeli_s = temp_pre_z[:max_pre_log_likeli_s.shape[0]]
        max_pre_log_likeli_j_s = temp_pre_z[max_pre_log_likeli_s.shape[0]:]

        # need to z-score to be able to compare between different sessions: POST likeli
        temp_post = np.hstack((max_post_log_likeli_s, max_post_log_likeli_j_s))
        temp_post_z = zscore(temp_post)
        max_post_log_likeli_s = temp_post_z[:max_post_log_likeli_s.shape[0]]
        max_post_log_likeli_j_s = temp_post_z[max_post_log_likeli_s.shape[0]:]

        return max_pre_log_likeli_s, max_pre_log_likeli_j_s, max_post_log_likeli_s, max_post_log_likeli_j_s

    def memory_drift_nrem_outside_swr_vs_jitter(self, n_moving_average_pop_vec=2000, plotting=False,
                                                nr_spikes_per_jitter_window=2000):

        event_spike_rasters_swr = []
        event_spike_rasters_outside_swr = []
        event_spike_rasters_swr_jittered = []


        for l_s in self.long_sleep:
            # get outside SWR bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_nrem_outside_swr_ = (
                l_s.get_spike_binned_raster_around_swr())

            _, event_spike_rasters_jittered_ = (
                l_s.get_spike_binned_raster_sleep_phase_jittered(sleep_phase="nrem",
                                                                 nr_spikes_per_jitter_window=nr_spikes_per_jitter_window))

            # get SWR bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_swr_ = (
                l_s.get_spike_binned_raster_sleep_phase(sleep_phase="nrem"))

            event_spike_rasters_swr.append(np.hstack(event_spike_rasters_swr_))
            event_spike_rasters_outside_swr.append(np.hstack(event_spike_rasters_nrem_outside_swr_))
            event_spike_rasters_swr_jittered.append(event_spike_rasters_jittered_)

        all_spike_rasters_outside_swr = np.hstack(event_spike_rasters_outside_swr)
        all_spike_rasters_swr = np.hstack(event_spike_rasters_swr)
        all_spike_rasters_swr_jittered = np.hstack(event_spike_rasters_swr_jittered)

        # delete events which are too short --> TODO
        # --------------------------------------------------------------------------------------------------------------
        # nr_bins = np.array([x.shape[1] for x in merged_events])

        # get means of pre and post modes + compression factor
        pre_file_name = self.session_params.default_pre_phmm_model
        post_file_name = self.session_params.default_post_phmm_model
        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        # do decoding and plot result
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                  event_spike_rasters=all_spike_rasters_swr,
                                                  compression_factor=compression_factor)

        post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                   event_spike_rasters=all_spike_rasters_swr,
                                                   compression_factor=compression_factor)

        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        sim_ratio = (max_post_likeli - max_pre_likeli) / (max_post_likeli + max_pre_likeli)


        # do decoding of jittere swr_data
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_jittered = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                  event_spike_rasters=all_spike_rasters_swr_jittered,
                                                  compression_factor=compression_factor)

        post_likeli_jittered = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                   event_spike_rasters=all_spike_rasters_swr_jittered,
                                                   compression_factor=compression_factor)

        pre_likeli_jittered = np.vstack(pre_likeli_jittered)
        post_likeli_jittered = np.vstack(post_likeli_jittered)

        max_pre_likeli_jittered = np.max(pre_likeli_jittered, axis=1)
        max_post_likeli_jittered = np.max(post_likeli_jittered, axis=1)


        # do decoding and plot result for jittered data
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_j = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                    event_spike_rasters=all_spike_rasters_outside_swr,
                                                    compression_factor=compression_factor)

        post_likeli_j = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                     event_spike_rasters=all_spike_rasters_outside_swr,
                                                     compression_factor=compression_factor)

        pre_likeli_j = np.vstack(pre_likeli_j)
        post_likeli_j = np.vstack(post_likeli_j)

        max_pre_likeli_j = np.max(pre_likeli_j, axis=1)
        max_post_likeli_j = np.max(post_likeli_j, axis=1)

        # look at distribution of decoded PRE modes
        mode_decoded_pre, mode_decoded_pre_counts = np.unique(np.argmax(pre_likeli, axis=1), return_counts=True)
        mode_decoded_pre_outside, mode_decoded_pre_counts_outside = np.unique(np.argmax(pre_likeli_j, axis=1),
                                                                              return_counts=True)
        modes_original_pre =np.zeros(pre_likeli.shape[1])
        modes_original_pre[mode_decoded_pre] = mode_decoded_pre_counts/np.sum(mode_decoded_pre_counts)
        modes_excluded_pre =np.zeros(pre_likeli.shape[1])
        modes_excluded_pre[mode_decoded_pre_outside] = mode_decoded_pre_counts_outside/np.sum(mode_decoded_pre_counts_outside)

        # look at distribution of decoded POST modes
        mode_decoded_post, mode_decoded_post_counts = np.unique(np.argmax(post_likeli, axis=1), return_counts=True)
        mode_decoded_post_outside, mode_decoded_post_counts_outside = np.unique(np.argmax(post_likeli_j, axis=1),
                                                                                return_counts=True)
        modes_original_post =np.zeros(post_likeli.shape[1])
        modes_original_post[mode_decoded_post] = mode_decoded_post_counts/np.sum(mode_decoded_post_counts)
        modes_excluded_post =np.zeros(post_likeli.shape[1])
        modes_excluded_post[mode_decoded_post_outside] = mode_decoded_post_counts_outside/np.sum(mode_decoded_post_counts_outside)


        sim_ratio_j = (max_post_likeli_j - max_pre_likeli_j) / (max_post_likeli_j + max_pre_likeli_j)

        # cut to same length for the ratio
        # --------------------------------------------------------------------------------------------------------------

        # take the log
        max_pre_log_likeli = np.log(max_pre_likeli)
        max_pre_log_likeli_j = np.log(max_pre_likeli_j)
        max_pre_log_likeli_jittered = np.log(max_pre_likeli_jittered)
        max_post_log_likeli = np.log(max_post_likeli)
        max_post_log_likeli_j = np.log(max_post_likeli_j)
        max_post_log_likeli_jittered = np.log(max_post_likeli_jittered)

        # interpolate missing values
        max_pre_log_likeli_j = np.interp(np.linspace(0, len(max_pre_log_likeli_j), len(max_pre_log_likeli)),
                                         np.arange(len(max_pre_log_likeli_j)), max_pre_log_likeli_j)

        max_post_log_likeli_j = np.interp(np.linspace(0, len(max_post_log_likeli_j), len(max_post_log_likeli)),
                                          np.arange(len(max_post_log_likeli_j)), max_post_log_likeli_j)

        # apply smoothing
        max_pre_log_likeli_s = uniform_filter1d(max_pre_log_likeli, n_moving_average_pop_vec)
        max_pre_log_likeli_j_s = uniform_filter1d(max_pre_log_likeli_j, n_moving_average_pop_vec)
        max_pre_log_likeli_jittered_s = uniform_filter1d(max_pre_log_likeli_jittered, n_moving_average_pop_vec)
        max_post_log_likeli_s = uniform_filter1d(max_post_log_likeli, n_moving_average_pop_vec)
        max_post_log_likeli_j_s = uniform_filter1d(max_post_log_likeli_j, n_moving_average_pop_vec)
        max_post_log_likeli_jittered_s = uniform_filter1d(max_post_log_likeli_jittered, n_moving_average_pop_vec)

        # compute difference
        diff_pre = max_pre_log_likeli_s - max_pre_log_likeli_j_s
        diff_post = max_post_log_likeli_s - max_post_log_likeli_j_s

        if plotting:

            plt.scatter(modes_original_pre, modes_excluded_pre)
            plt.title("PRE mode decoding (fraction)")
            plt.xlabel("REM&NREM")
            plt.ylabel("Excluded")
            plt.tight_layout()
            plt.show()

            plt.scatter(modes_original_post, modes_excluded_post)
            plt.title("POST mode decoding (fraction)")
            plt.xlabel("REM&NREM")
            plt.ylabel("Excluded")
            plt.tight_layout()
            plt.show()

            plt.plot(moving_average(sim_ratio, n_moving_average_pop_vec), label="SWRs", color="grey")
            plt.plot(moving_average(sim_ratio_j, n_moving_average_pop_vec), label="NREM w/o SWRs", color="red", alpha=0.5)
            plt.xlabel("Time")
            plt.ylabel("sim_ratio")
            plt.legend()
            plt.tight_layout()
            plt.show()

            plt.figure(figsize=(12,8))
            plt.subplot(2,2,1)
            plt.plot(max_pre_log_likeli_s, color="red", label="PRE SWRs")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_pre_log_likeli_j_s, color="salmon", linestyle="--", label="PRE NREM w/o SWRs")
            plt.xticks([])
            plt.ylabel("Max log likelihood")
            plt.legend()
            plt.subplot(2,2,3)
            plt.plot(np.linspace(0,1, diff_pre.shape[0]), diff_pre)
            plt.ylabel("SWRs - NREM w/o SWRs")
            plt.xlabel("relative time")
            plt.subplot(2,2,2)
            plt.plot(max_post_log_likeli_s, color="white", label="POST SWRs")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_post_log_likeli_j_s, color="lightblue", linestyle="--", label="POST NREM w/o SWRs", alpha=0.8)
            plt.xticks([])
            plt.legend()
            plt.subplot(2,2,4)
            plt.plot(np.linspace(0,1, diff_post.shape[0]), diff_post)
            plt.ylabel("SWRs - NREM w/o SWRs")
            plt.xlabel("relative time")
            plt.tight_layout()
            plt.show()


        # need to z-score to be able to compare between different sessions: PRE likeli
        temp_pre = np.hstack((max_pre_log_likeli_s, max_pre_log_likeli_j_s, max_pre_log_likeli_jittered_s))
        temp_pre_z = zscore(temp_pre)
        max_pre_log_likeli_s = temp_pre_z[:max_pre_log_likeli_s.shape[0]]
        max_pre_log_likeli_j_s = temp_pre_z[max_pre_log_likeli_s.shape[0]:(max_pre_log_likeli_s.shape[0]+max_pre_log_likeli_j_s.shape[0])]
        max_pre_log_likeli_jittered_s = temp_pre_z[(max_pre_log_likeli_s.shape[0]+max_pre_log_likeli_j_s.shape[0]):]

        # need to z-score to be able to compare between different sessions: POST likeli
        temp_post = np.hstack((max_post_log_likeli_s, max_post_log_likeli_j_s, max_post_log_likeli_jittered_s))
        temp_post_z = zscore(temp_post)
        max_post_log_likeli_s = temp_post_z[:max_post_log_likeli_s.shape[0]]
        max_post_log_likeli_j_s = temp_post_z[max_post_log_likeli_s.shape[0]:(max_post_log_likeli_s.shape[0]+max_post_log_likeli_j_s.shape[0])]
        max_post_log_likeli_jittered_s = temp_post_z[(max_post_log_likeli_s.shape[0]+max_post_log_likeli_j_s.shape[0]):]


        return max_pre_log_likeli_s, max_pre_log_likeli_j_s, max_pre_log_likeli_jittered_s,\
            max_post_log_likeli_s, max_post_log_likeli_j_s, max_post_log_likeli_jittered_s

    def memory_drift_nrem_outside_swr_vs_jitter_and_rem(self, only_decoded=False, plotting=False, z_scored=False,
                                                    nr_spikes_per_jitter_window=2000,
                                                        outside_swr_min_bin_size_s=0.5, save_results=False):

        event_spike_rasters_swr = []
        event_spike_rasters_outside_swr = []
        event_spike_rasters_outside_swr_jittered = []
        event_spike_rasters_swr_jittered = []
        event_spike_rasters_rem = []
        event_spike_rasters_rem_jittered = []
        bin_dur =[]

        for l_s in self.long_sleep:
            # get SWR bins --> original and jittered
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_swr_ = (
                l_s.get_spike_binned_raster_sleep_phase(sleep_phase="nrem"))

            _, event_spike_rasters_nrem_jittered_ = (
                l_s.get_spike_binned_raster_sleep_phase_jittered(sleep_phase="nrem",
                                                                 nr_spikes_per_jitter_window=nr_spikes_per_jitter_window))

            # get outside SWR bins --> original and jittered
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_nrem_outside_swr_, bin_dur_ = (
                l_s.get_spike_binned_raster_around_swr(return_bin_duration=True))

            _, event_spike_rasters_nrem_outside_swr_jittered_ = (
                l_s.get_spike_binned_raster_around_swr_jittered(nr_spikes_per_jitter_window=nr_spikes_per_jitter_window))

            # get REM bins --> original and jittered
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_rem_ = (
                l_s.get_spike_binned_raster_sleep_phase(sleep_phase="rem"))

            _, event_spike_rasters_rem_jittered_ = (
                l_s.get_spike_binned_raster_sleep_phase_jittered(sleep_phase="rem",
                                                                 nr_spikes_per_jitter_window=nr_spikes_per_jitter_window))

            event_spike_rasters_swr.append(np.hstack(event_spike_rasters_swr_))
            event_spike_rasters_outside_swr.append(np.hstack(event_spike_rasters_nrem_outside_swr_))
            event_spike_rasters_swr_jittered.append(event_spike_rasters_nrem_jittered_)
            event_spike_rasters_outside_swr_jittered.append(event_spike_rasters_nrem_outside_swr_jittered_)
            event_spike_rasters_rem_jittered.append(event_spike_rasters_rem_jittered_)
            bin_dur.append(np.hstack(bin_dur_))
            event_spike_rasters_rem.append(np.hstack(event_spike_rasters_rem_))

        bin_dur =np.hstack(bin_dur)
        all_spike_rasters_outside_swr = np.hstack(event_spike_rasters_outside_swr)
        all_spike_rasters_outside_swr_jittered = np.hstack(event_spike_rasters_outside_swr_jittered)
        all_spike_rasters_swr = np.hstack(event_spike_rasters_swr)
        all_spike_rasters_swr_jittered = np.hstack(event_spike_rasters_swr_jittered)
        all_spike_rasters_rem = np.hstack(event_spike_rasters_rem)
        all_spike_rasters_rem_jittered = np.hstack(event_spike_rasters_rem_jittered)

        # only use long bins
        orig_nr_bins = all_spike_rasters_outside_swr.shape[1]
        all_spike_rasters_outside_swr = all_spike_rasters_outside_swr[:, bin_dur > outside_swr_min_bin_size_s]
        print(str(np.round(all_spike_rasters_outside_swr.shape[1]/(orig_nr_bins*0.01),2))+
              "% of bins remained (threshold: "+str(outside_swr_min_bin_size_s)+"s)")

        # delete events which are too short --> TODO
        # --------------------------------------------------------------------------------------------------------------
        # nr_bins = np.array([x.shape[1] for x in merged_events])

        # get means of pre and post modes + compression factor
        pre_file_name = self.session_params.default_pre_phmm_model
        post_file_name = self.session_params.default_post_phmm_model
        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        # do decoding and plot result
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                  event_spike_rasters=all_spike_rasters_swr,
                                                  compression_factor=compression_factor)

        post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                   event_spike_rasters=all_spike_rasters_swr,
                                                   compression_factor=compression_factor)

        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        sim_ratio = (max_post_likeli - max_pre_likeli) / (max_post_likeli + max_pre_likeli)


        # do decoding of jittere swr_data
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_jittered = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                           event_spike_rasters=all_spike_rasters_swr_jittered,
                                                           compression_factor=compression_factor)

        post_likeli_jittered = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                            event_spike_rasters=all_spike_rasters_swr_jittered,
                                                            compression_factor=compression_factor)

        pre_likeli_jittered = np.vstack(pre_likeli_jittered)
        post_likeli_jittered = np.vstack(post_likeli_jittered)

        max_pre_likeli_jittered = np.max(pre_likeli_jittered, axis=1)
        max_post_likeli_jittered = np.max(post_likeli_jittered, axis=1)

        # do decoding for outside swr
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_out = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                    event_spike_rasters=all_spike_rasters_outside_swr,
                                                    compression_factor=compression_factor)

        post_likeli_out = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                     event_spike_rasters=all_spike_rasters_outside_swr,
                                                     compression_factor=compression_factor)

        pre_likeli_out = np.vstack(pre_likeli_out)
        post_likeli_out = np.vstack(post_likeli_out)

        max_pre_likeli_out = np.max(pre_likeli_out, axis=1)
        max_post_likeli_out = np.max(post_likeli_out, axis=1)


        # do decoding for outside swr --> jittered
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_out_jittered = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                    event_spike_rasters=all_spike_rasters_outside_swr_jittered,
                                                    compression_factor=compression_factor)

        post_likeli_out_jittered = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                     event_spike_rasters=all_spike_rasters_outside_swr_jittered,
                                                     compression_factor=compression_factor)

        pre_likeli_out_jittered = np.vstack(pre_likeli_out_jittered)
        post_likeli_out_jittered = np.vstack(post_likeli_out_jittered)

        max_pre_likeli_out_jittered = np.max(pre_likeli_out_jittered, axis=1)
        max_post_likeli_out_jittered = np.max(post_likeli_out_jittered, axis=1)

        # do decoding for rem
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_rem = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                      event_spike_rasters=all_spike_rasters_rem,
                                                      compression_factor=compression_factor)

        post_likeli_rem = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                       event_spike_rasters=all_spike_rasters_rem,
                                                       compression_factor=compression_factor)

        pre_likeli_rem = np.vstack(pre_likeli_rem)
        post_likeli_rem = np.vstack(post_likeli_rem)

        max_pre_likeli_rem = np.max(pre_likeli_rem, axis=1)
        max_post_likeli_rem = np.max(post_likeli_rem, axis=1)


        # do decoding for rem --> jittered
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_rem_jittered = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                      event_spike_rasters=all_spike_rasters_rem_jittered,
                                                      compression_factor=compression_factor)

        post_likeli_rem_jittered = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                       event_spike_rasters=all_spike_rasters_rem_jittered,
                                                       compression_factor=compression_factor)

        pre_likeli_rem_jittered = np.vstack(pre_likeli_rem_jittered)
        post_likeli_rem_jittered = np.vstack(post_likeli_rem_jittered)

        max_pre_likeli_rem_jittered = np.max(pre_likeli_rem_jittered, axis=1)
        max_post_likeli_rem_jittered = np.max(post_likeli_rem_jittered, axis=1)


        # look at distribution of decoded PRE modes
        mode_decoded_pre, mode_decoded_pre_counts = np.unique(np.argmax(pre_likeli, axis=1), return_counts=True)
        mode_decoded_pre_outside, mode_decoded_pre_counts_outside = np.unique(np.argmax(pre_likeli_out, axis=1),
                                                                              return_counts=True)
        mode_decoded_pre_rem, mode_decoded_pre_counts_rem = np.unique(np.argmax(pre_likeli_rem, axis=1),
                                                                      return_counts=True)

        modes_original_pre =np.zeros(pre_likeli.shape[1])
        modes_original_pre[mode_decoded_pre] = mode_decoded_pre_counts/np.sum(mode_decoded_pre_counts)
        modes_excluded_pre =np.zeros(pre_likeli.shape[1])
        modes_excluded_pre[mode_decoded_pre_outside] = mode_decoded_pre_counts_outside/np.sum(mode_decoded_pre_counts_outside)
        modes_rem_pre =np.zeros(pre_likeli.shape[1])
        modes_rem_pre[mode_decoded_pre_rem] = mode_decoded_pre_counts_rem/np.sum(mode_decoded_pre_counts_rem)

        # look at distribution of decoded POST modes
        mode_decoded_post, mode_decoded_post_counts = np.unique(np.argmax(post_likeli, axis=1), return_counts=True)
        mode_decoded_post_outside, mode_decoded_post_counts_outside = np.unique(np.argmax(post_likeli_out, axis=1),
                                                                                return_counts=True)
        mode_decoded_post_rem, mode_decoded_post_counts_rem = np.unique(np.argmax(post_likeli_rem, axis=1),
                                                                                return_counts=True)

        modes_original_post =np.zeros(post_likeli.shape[1])
        modes_original_post[mode_decoded_post] = mode_decoded_post_counts/np.sum(mode_decoded_post_counts)
        modes_excluded_post =np.zeros(post_likeli.shape[1])
        modes_excluded_post[mode_decoded_post_outside] = mode_decoded_post_counts_outside/np.sum(mode_decoded_post_counts_outside)
        modes_rem_post =np.zeros(post_likeli.shape[1])
        modes_rem_post[mode_decoded_post_rem] = mode_decoded_post_counts_rem/np.sum(mode_decoded_post_counts_rem)

        # take the log, pre:
        # --------------------------------------------------------------------------------------------------------------
        # swr
        max_pre_log_likeli = np.log(max_pre_likeli)
        max_pre_log_likeli_jittered = np.log(max_pre_likeli_jittered)
        # outside swr
        max_pre_log_likeli_out = np.log(max_pre_likeli_out)
        max_pre_log_likeli_out_jittered = np.log(max_pre_likeli_out_jittered)
        # rem
        max_pre_log_likeli_rem = np.log(max_pre_likeli_rem)
        max_pre_log_likeli_rem_jittered = np.log(max_pre_likeli_rem_jittered)
        # post:
        # --------------------------------------------------------------------------------------------------------------
        # swr
        max_post_log_likeli = np.log(max_post_likeli)
        max_post_log_likeli_jittered = np.log(max_post_likeli_jittered)
        # outside swr
        max_post_log_likeli_out = np.log(max_post_likeli_out)
        max_post_log_likeli_out_jittered = np.log(max_post_likeli_out_jittered)
        # rem
        max_post_log_likeli_rem = np.log(max_post_likeli_rem)
        max_post_log_likeli_rem_jittered = np.log(max_post_likeli_rem_jittered)

        # interpolate missing values: swr <-> swr_jittered, out <-> out_jittered, rem <-> rem_jittered
        # --------------------------------------------------------------------------------------------------------------
        # swr
        max_pre_log_likeli_jittered = np.interp(np.linspace(0, len(max_pre_log_likeli_jittered), len(max_pre_log_likeli)),
                                         np.arange(len(max_pre_log_likeli_jittered)), max_pre_log_likeli_jittered)

        max_post_log_likeli_jittered = np.interp(np.linspace(0, len(max_post_log_likeli_jittered), len(max_post_log_likeli)),
                                          np.arange(len(max_post_log_likeli_jittered)), max_post_log_likeli_jittered)
        # outside swr
        max_pre_log_likeli_out_jittered = np.interp(np.linspace(0, len(max_pre_log_likeli_out_jittered), len(max_pre_log_likeli_out)),
                                         np.arange(len(max_pre_log_likeli_out_jittered)), max_pre_log_likeli_out_jittered)

        max_post_log_likeli_out_jittered = np.interp(np.linspace(0, len(max_post_log_likeli_out_jittered), len(max_post_log_likeli_out)),
                                          np.arange(len(max_post_log_likeli_out_jittered)), max_post_log_likeli_out_jittered)

        # rem
        max_pre_log_likeli_rem_jittered = np.interp(np.linspace(0, len(max_pre_log_likeli_rem_jittered), len(max_pre_log_likeli_rem)),
                                         np.arange(len(max_pre_log_likeli_rem_jittered)), max_pre_log_likeli_rem_jittered)

        max_post_log_likeli_rem_jittered = np.interp(np.linspace(0, len(max_post_log_likeli_rem_jittered), len(max_post_log_likeli_rem)),
                                          np.arange(len(max_post_log_likeli_rem_jittered)), max_post_log_likeli_rem_jittered)

        if z_scored:
            # need to z-score to be able to compare between different sessions: PRE likeli
            temp_pre = np.hstack((max_pre_log_likeli, max_pre_log_likeli_out, max_pre_log_likeli_jittered, max_pre_log_likeli_rem))
            temp_pre_z = zscore(temp_pre)
            max_pre_log_likeli = temp_pre_z[:max_pre_log_likeli.shape[0]]
            max_pre_log_likeli_out = temp_pre_z[max_pre_log_likeli.shape[0]:(max_pre_log_likeli.shape[0]+max_pre_log_likeli_out.shape[0])]
            max_pre_log_likeli_jittered= temp_pre_z[(max_pre_log_likeli.shape[0]+
                                                        max_pre_log_likeli_out.shape[0]):(max_pre_log_likeli.shape[0]+
                 max_pre_log_likeli_out.shape[0]+max_pre_log_likeli_jittered.shape[0])]
            max_pre_log_likeli_rem = temp_pre_z[-max_pre_log_likeli_rem.shape[0]:]

            # need to z-score to be able to compare between different sessions: POST likeli
            temp_post = np.hstack((max_post_log_likeli, max_post_log_likeli_out, max_post_log_likeli_jittered, max_post_log_likeli_rem))
            temp_post_z = zscore(temp_post)
            max_post_log_likeli = temp_post_z[:max_post_log_likeli.shape[0]]
            max_post_log_likeli_out = temp_post_z[max_post_log_likeli.shape[0]:(max_post_log_likeli.shape[0]+max_post_log_likeli_out.shape[0])]
            max_post_log_likeli_jittered = temp_post_z[(max_post_log_likeli.shape[0]+
                                                        max_post_log_likeli_out.shape[0]):(max_post_log_likeli.shape[0]+
                                                                                          max_post_log_likeli_out.shape[0]+max_post_log_likeli_jittered.shape[0])]
            max_post_log_likeli_rem = temp_post_z[-max_post_log_likeli_rem.shape[0]:]

        if plotting:
            plt.figure(figsize=(6,6))
            # first and second half for acquisition
            res = [max_pre_log_likeli_jittered, max_pre_log_likeli, max_pre_log_likeli_out,
                   max_pre_log_likeli_rem]
            c="white"
            bplot=plt.boxplot(res, positions=[1, 2, 3, 4], patch_artist=True,
                              labels=["Acquisition\n NREM SWR jittered", "Acquisition \n NREM SWR", "Acquisition\n NREM outside SWR",
                                      "Acquisition\n REM"],
                              boxprops=dict(color=c),
                              capprops=dict(color=c),
                              whiskerprops=dict(color=c),
                              flierprops=dict(color=c, markeredgecolor=c),
                              medianprops=dict(color=c), showfliers=False)
            plt.tick_params(axis='x', labelrotation=45)
            plt.tight_layout()
            plt.show()

            plt.figure(figsize=(6,6))
            # first and second half for acquisition
            res = [max_post_log_likeli_jittered_s, max_post_log_likeli_s, max_post_log_likeli_out_s,
                   max_post_log_likeli_rem_s]

            bplot=plt.boxplot(res, positions=[1, 2, 3, 4], patch_artist=True,
                              labels=["Recall\n NREM SWR jittered", "Recall \n NREM SWR", "Recall\n NREM outside SWR",
                                      "Recall\n REM"],
                              boxprops=dict(color=c),
                              capprops=dict(color=c),
                              whiskerprops=dict(color=c),
                              flierprops=dict(color=c, markeredgecolor=c),
                              medianprops=dict(color=c), showfliers=False)
            plt.tick_params(axis='x', labelrotation=45)
            plt.tight_layout()
            plt.show()

        if save_results:
            res_dic = {}
            res_dic["results"] = [max_pre_log_likeli, max_pre_log_likeli_jittered, max_pre_log_likeli_out,
                max_pre_log_likeli_out_jittered, max_pre_log_likeli_rem, max_pre_log_likeli_rem_jittered,\
                max_post_log_likeli, max_post_log_likeli_jittered, max_post_log_likeli_out,
                max_post_log_likeli_out_jittered, max_post_log_likeli_rem, max_post_log_likeli_rem_jittered, \
                modes_original_pre, modes_excluded_pre, modes_rem_pre, modes_original_post, modes_excluded_post,
                modes_rem_post]

            save_path = os.path.dirname(os.path.realpath(__file__)) + "/../temp_data/outside_rem_swr"
            filename = save_path+ "/" + self.session_name

            outfile = open(filename, 'wb')
            pickle.dump(res_dic, outfile)
            outfile.close()

        return [max_pre_log_likeli, max_pre_log_likeli_jittered, max_pre_log_likeli_out,
                max_pre_log_likeli_out_jittered, max_pre_log_likeli_rem, max_pre_log_likeli_rem_jittered,\
                max_post_log_likeli, max_post_log_likeli_jittered, max_post_log_likeli_out,
                max_post_log_likeli_out_jittered, max_post_log_likeli_rem, max_post_log_likeli_rem_jittered, \
                modes_original_pre, modes_excluded_pre, modes_rem_pre, modes_original_post, modes_excluded_post,
                modes_rem_post]

    def memory_drift_excluded_periods(self, n_moving_average_pop_vec=10000, plotting=False):

        event_spike_rasters_original = []
        event_spike_rasters_excluded = []

        for l_s in self.long_sleep:
            # get outside SWR bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_excluded_ = (
                l_s.get_spike_binned_raster_excluded_periods())

            # get SWR bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_original_ = (
                l_s.get_spike_binned_raster_combined_sleep_phases())

            event_spike_rasters_original.append(np.hstack(event_spike_rasters_original_))
            if not event_spike_rasters_excluded_ is None:
                event_spike_rasters_excluded.append(np.hstack(event_spike_rasters_excluded_))

        all_spike_rasters_excluded = np.hstack(event_spike_rasters_excluded)
        all_spike_rasters_original = np.hstack(event_spike_rasters_original)

        # delete events which are too short --> TODO
        # --------------------------------------------------------------------------------------------------------------
        # nr_bins = np.array([x.shape[1] for x in merged_events])

        # get means of pre and post modes + compression factor
        pre_file_name = self.session_params.default_pre_phmm_model
        post_file_name = self.session_params.default_post_phmm_model
        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        # do decoding and plot result
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                  event_spike_rasters=all_spike_rasters_original,
                                                  compression_factor=compression_factor)

        post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                   event_spike_rasters=all_spike_rasters_original,
                                                   compression_factor=compression_factor)

        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        sim_ratio = (max_post_likeli - max_pre_likeli) / (max_post_likeli + max_pre_likeli)

        # do decoding and plot result for jittered data
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_j = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                    event_spike_rasters=all_spike_rasters_excluded,
                                                    compression_factor=compression_factor)

        post_likeli_j = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                     event_spike_rasters=all_spike_rasters_excluded,
                                                     compression_factor=compression_factor)

        pre_likeli_j = np.vstack(pre_likeli_j)
        post_likeli_j = np.vstack(post_likeli_j)

        max_pre_likeli_j = np.max(pre_likeli_j, axis=1)
        max_post_likeli_j = np.max(post_likeli_j, axis=1)

        sim_ratio_j = (max_post_likeli_j - max_pre_likeli_j) / (max_post_likeli_j + max_pre_likeli_j)

        # look at distribution of decoded PRE modes
        mode_decoded_pre, mode_decoded_pre_counts = np.unique(np.argmax(pre_likeli, axis=1), return_counts=True)
        mode_decoded_pre_outside, mode_decoded_pre_counts_outside = np.unique(np.argmax(pre_likeli_j, axis=1),
                                                                              return_counts=True)
        modes_original_pre =np.zeros(pre_likeli.shape[1])
        modes_original_pre[mode_decoded_pre] = mode_decoded_pre_counts/np.sum(mode_decoded_pre_counts)
        modes_excluded_pre =np.zeros(pre_likeli.shape[1])
        modes_excluded_pre[mode_decoded_pre_outside] = mode_decoded_pre_counts_outside/np.sum(mode_decoded_pre_counts_outside)

        # look at distribution of decoded POST modes
        mode_decoded_post, mode_decoded_post_counts = np.unique(np.argmax(post_likeli, axis=1), return_counts=True)
        mode_decoded_post_outside, mode_decoded_post_counts_outside = np.unique(np.argmax(post_likeli_j, axis=1),
                                                                              return_counts=True)
        modes_original_post =np.zeros(post_likeli.shape[1])
        modes_original_post[mode_decoded_post] = mode_decoded_post_counts/np.sum(mode_decoded_post_counts)
        modes_excluded_post =np.zeros(post_likeli.shape[1])
        modes_excluded_post[mode_decoded_post_outside] = mode_decoded_post_counts_outside/np.sum(mode_decoded_post_counts_outside)

        # take the log
        max_pre_log_likeli = np.log(max_pre_likeli)
        max_pre_log_likeli_j = np.log(max_pre_likeli_j)
        max_post_log_likeli = np.log(max_post_likeli)
        max_post_log_likeli_j = np.log(max_post_likeli_j)

        # interpolate missing values
        max_pre_log_likeli_j = np.interp(np.linspace(0, len(max_pre_log_likeli_j), len(max_pre_log_likeli)),
                                         np.arange(len(max_pre_log_likeli_j)), max_pre_log_likeli_j)

        max_post_log_likeli_j = np.interp(np.linspace(0, len(max_post_log_likeli_j), len(max_post_log_likeli)),
                                          np.arange(len(max_post_log_likeli_j)), max_post_log_likeli_j)

        # apply smoothing
        max_pre_log_likeli_s = uniform_filter1d(max_pre_log_likeli, n_moving_average_pop_vec)
        max_pre_log_likeli_j_s = uniform_filter1d(max_pre_log_likeli_j, n_moving_average_pop_vec)
        max_post_log_likeli_s = uniform_filter1d(max_post_log_likeli, n_moving_average_pop_vec)
        max_post_log_likeli_j_s = uniform_filter1d(max_post_log_likeli_j, n_moving_average_pop_vec)

        # compute difference
        diff_pre = max_pre_log_likeli_s - max_pre_log_likeli_j_s
        diff_post = max_post_log_likeli_s - max_post_log_likeli_j_s

        # interpolate to match sim_ratio length of orignal
        sim_ratio_j = np.interp(np.linspace(0, len(sim_ratio_j), len(sim_ratio)),
                        np.arange(len(sim_ratio_j)), sim_ratio_j)

        if plotting:
            plt.scatter(modes_original_pre, modes_excluded_pre)
            plt.title("PRE mode decoding (fraction)")
            plt.xlabel("REM&NREM")
            plt.ylabel("Excluded")
            plt.tight_layout()
            plt.show()

            plt.scatter(modes_original_post, modes_excluded_post)
            plt.title("POST mode decoding (fraction)")
            plt.xlabel("REM&NREM")
            plt.ylabel("Excluded")
            plt.tight_layout()
            plt.show()

            plt.plot(moving_average(sim_ratio, n_moving_average_pop_vec), label="original", color="grey")
            plt.plot(moving_average(sim_ratio_j, n_moving_average_pop_vec), label="excluded", color="red", alpha=0.5)
            plt.xlabel("Time")
            plt.ylabel("sim_ratio")
            plt.legend()
            plt.tight_layout()
            plt.show()

            plt.figure(figsize=(12,8))
            plt.subplot(2,2,1)
            plt.plot(max_pre_log_likeli_s, color="red", label="original")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_pre_log_likeli_j_s, color="salmon", linestyle="--", label="excluded")
            plt.xticks([])
            plt.ylabel("Max log likelihood")
            plt.legend()
            plt.subplot(2,2,3)
            plt.plot(np.linspace(0,1, diff_pre.shape[0]), diff_pre)
            plt.ylabel("original - excluded")
            plt.xlabel("relative time")
            plt.subplot(2,2,2)
            plt.plot(max_post_log_likeli_s, color="white", label="POST original")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_post_log_likeli_j_s, color="lightblue", linestyle="--", label="POST excluded", alpha=0.8)
            plt.xticks([])
            plt.legend()
            plt.subplot(2,2,4)
            plt.plot(np.linspace(0,1, diff_post.shape[0]), diff_post)
            plt.ylabel("original - excluded")
            plt.xlabel("relative time")
            plt.tight_layout()
            plt.show()

        return sim_ratio, sim_ratio_j, diff_pre, diff_post

    def memory_drift_nrem_swr_vs_swr_excluded_periods(self, n_moving_average_pop_vec=10000, smoothing=False,
                                          plotting=False, z_score=False):

        event_spike_rasters_original = []
        event_spike_rasters_excluded = []

        for l_s in self.long_sleep:
            # get SWR bins from excluded periods
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_excluded_ = (
                l_s.get_spike_binned_raster_swr_during_excluded_periods())

            # get SWR bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_original_ = (
                l_s.get_spike_binned_raster_sleep_phase(sleep_phase="nrem"))

            event_spike_rasters_original.append(np.hstack(event_spike_rasters_original_))
            if not event_spike_rasters_excluded_ is None:
                event_spike_rasters_excluded.append(np.hstack(event_spike_rasters_excluded_))

        all_spike_rasters_excluded = np.hstack(event_spike_rasters_excluded)
        all_spike_rasters_original = np.hstack(event_spike_rasters_original)

        # delete events which are too short --> TODO
        # --------------------------------------------------------------------------------------------------------------
        # nr_bins = np.array([x.shape[1] for x in merged_events])

        # get means of pre and post modes + compression factor
        pre_file_name = self.session_params.default_pre_phmm_model
        post_file_name = self.session_params.default_post_phmm_model
        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        # do decoding and plot result
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                  event_spike_rasters=all_spike_rasters_original,
                                                  compression_factor=compression_factor)

        post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                   event_spike_rasters=all_spike_rasters_original,
                                                   compression_factor=compression_factor)

        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        sim_ratio = (max_post_likeli - max_pre_likeli) / (max_post_likeli + max_pre_likeli)

        # do decoding and plot result for jittered data
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_j = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                    event_spike_rasters=all_spike_rasters_excluded,
                                                    compression_factor=compression_factor)

        post_likeli_j = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                     event_spike_rasters=all_spike_rasters_excluded,
                                                     compression_factor=compression_factor)

        pre_likeli_j = np.vstack(pre_likeli_j)
        post_likeli_j = np.vstack(post_likeli_j)

        max_pre_likeli_j = np.max(pre_likeli_j, axis=1)
        max_post_likeli_j = np.max(post_likeli_j, axis=1)

        sim_ratio_j = (max_post_likeli_j - max_pre_likeli_j) / (max_post_likeli_j + max_pre_likeli_j)

        # look at distribution of decoded PRE modes
        mode_decoded_pre, mode_decoded_pre_counts = np.unique(np.argmax(pre_likeli, axis=1), return_counts=True)
        mode_decoded_pre_outside, mode_decoded_pre_counts_outside = np.unique(np.argmax(pre_likeli_j, axis=1),
                                                                              return_counts=True)
        modes_original_pre =np.zeros(pre_likeli.shape[1])
        modes_original_pre[mode_decoded_pre] = mode_decoded_pre_counts/np.sum(mode_decoded_pre_counts)
        modes_excluded_pre =np.zeros(pre_likeli.shape[1])
        modes_excluded_pre[mode_decoded_pre_outside] = mode_decoded_pre_counts_outside/np.sum(mode_decoded_pre_counts_outside)

        # look at distribution of decoded POST modes
        mode_decoded_post, mode_decoded_post_counts = np.unique(np.argmax(post_likeli, axis=1), return_counts=True)
        mode_decoded_post_outside, mode_decoded_post_counts_outside = np.unique(np.argmax(post_likeli_j, axis=1),
                                                                              return_counts=True)
        modes_original_post =np.zeros(post_likeli.shape[1])
        modes_original_post[mode_decoded_post] = mode_decoded_post_counts/np.sum(mode_decoded_post_counts)
        modes_excluded_post =np.zeros(post_likeli.shape[1])
        modes_excluded_post[mode_decoded_post_outside] = mode_decoded_post_counts_outside/np.sum(mode_decoded_post_counts_outside)

        # take the log
        max_pre_log_likeli = np.log(max_pre_likeli)
        max_pre_log_likeli_j = np.log(max_pre_likeli_j)
        max_post_log_likeli = np.log(max_post_likeli)
        max_post_log_likeli_j = np.log(max_post_likeli_j)

        # only select decoded
        pre_decoded_sleep = max_pre_log_likeli > max_post_log_likeli
        pre_decoded_awake = max_pre_log_likeli_j > max_post_log_likeli_j

        max_pre_log_likeli = max_pre_log_likeli[pre_decoded_sleep]
        max_post_log_likeli = max_post_log_likeli[~pre_decoded_sleep]
        max_pre_log_likeli_j = max_pre_log_likeli_j[pre_decoded_awake]
        max_post_log_likeli_j =max_post_log_likeli_j[~pre_decoded_awake]

        if smoothing:
            # apply smoothing
            max_pre_log_likeli_s = uniform_filter1d(max_pre_log_likeli, n_moving_average_pop_vec)
            max_pre_log_likeli_j_s = uniform_filter1d(max_pre_log_likeli_j, n_moving_average_pop_vec)
            max_post_log_likeli_s = uniform_filter1d(max_post_log_likeli, n_moving_average_pop_vec)
            max_post_log_likeli_j_s = uniform_filter1d(max_post_log_likeli_j, n_moving_average_pop_vec)
        else:
            max_pre_log_likeli_s = max_pre_log_likeli
            max_pre_log_likeli_j_s = max_pre_log_likeli_j
            max_post_log_likeli_s = max_post_log_likeli
            max_post_log_likeli_j_s = max_post_log_likeli_j


        if plotting:
            # interpolate missing values
            max_pre_log_likeli_j = np.interp(np.linspace(0, len(max_pre_log_likeli_j), len(max_pre_log_likeli)),
                                             np.arange(len(max_pre_log_likeli_j)), max_pre_log_likeli_j)

            max_post_log_likeli_j = np.interp(np.linspace(0, len(max_post_log_likeli_j), len(max_post_log_likeli)),
                                              np.arange(len(max_post_log_likeli_j)), max_post_log_likeli_j)
            # compute difference
            diff_pre = max_pre_log_likeli_s - max_pre_log_likeli_j_s
            diff_post = max_post_log_likeli_s - max_post_log_likeli_j_s
            plt.scatter(modes_original_pre, modes_excluded_pre)
            plt.title("PRE mode decoding (fraction)")
            plt.xlabel("REM&NREM")
            plt.ylabel("Excluded")
            plt.tight_layout()
            plt.show()

            plt.scatter(modes_original_post, modes_excluded_post)
            plt.title("POST mode decoding (fraction)")
            plt.xlabel("REM&NREM")
            plt.ylabel("Excluded")
            plt.tight_layout()
            plt.show()

            # interpolate to match sim_ratio length of orignal
            sim_ratio_j = np.interp(np.linspace(0, len(sim_ratio_j), len(sim_ratio)),
                                np.arange(len(sim_ratio_j)), sim_ratio_j)

            plt.plot(moving_average(sim_ratio, n_moving_average_pop_vec), label="original", color="grey")
            plt.plot(moving_average(sim_ratio_j, n_moving_average_pop_vec), label="excluded", color="red", alpha=0.5)
            plt.xlabel("Time")
            plt.ylabel("sim_ratio")
            plt.legend()
            plt.tight_layout()
            plt.show()

            plt.figure(figsize=(12,8))
            plt.subplot(2,2,1)
            plt.plot(max_pre_log_likeli_s, color="red", label="original")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_pre_log_likeli_j_s, color="salmon", linestyle="--", label="excluded")
            plt.xticks([])
            plt.ylabel("Max log likelihood")
            plt.legend()
            plt.subplot(2,2,3)
            plt.plot(np.linspace(0,1, diff_pre.shape[0]), diff_pre)
            plt.ylabel("original - excluded")
            plt.xlabel("relative time")
            plt.subplot(2,2,2)
            plt.plot(max_post_log_likeli_s, color="white", label="POST original")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_post_log_likeli_j_s, color="lightblue", linestyle="--", label="POST excluded", alpha=0.8)
            plt.xticks([])
            plt.legend()
            plt.subplot(2,2,4)
            plt.plot(np.linspace(0,1, diff_post.shape[0]), diff_post)
            plt.ylabel("original - excluded")
            plt.xlabel("relative time")
            plt.tight_layout()
            plt.show()

        if z_score:
            # need to z-score to be able to compare between different sessions: PRE likeli
            temp_pre = np.hstack((max_pre_log_likeli_s, max_pre_log_likeli_j_s))
            temp_pre_z = zscore(temp_pre)
            max_pre_log_likeli_s = temp_pre_z[:max_pre_log_likeli_s.shape[0]]
            max_pre_log_likeli_j_s = temp_pre_z[max_pre_log_likeli_s.shape[0]:]

            # need to z-score to be able to compare between different sessions: POST likeli
            temp_post = np.hstack((max_post_log_likeli_s, max_post_log_likeli_j_s))
            temp_post_z = zscore(temp_post)
            max_post_log_likeli_s = temp_post_z[:max_post_log_likeli_s.shape[0]]
            max_post_log_likeli_j_s = temp_post_z[max_post_log_likeli_s.shape[0]:]

        return max_pre_log_likeli_s, max_pre_log_likeli_j_s, max_post_log_likeli_s, max_post_log_likeli_j_s, \
            sim_ratio, sim_ratio_j

    def memory_drift_vs_duration_excluded_periods(self, window_size_min=5):

        event_spike_rasters_original = []
        duration_excluded = []
        total_dur = []

        for l_s in self.long_sleep:
            # get outside SWR bins
            # ----------------------------------------------------------------------------------------------------------
            duration_excluded_ = (
                l_s.get_duration_excluded_periods_s(window_size_min=window_size_min))

            # get total duration
            total_dur_ = l_s.get_duration_sec()

            # get SWR bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_original_ = (
                l_s.get_spike_binned_raster_combined_sleep_phases())

            event_spike_rasters_original.append(np.hstack(event_spike_rasters_original_))
            duration_excluded.append(duration_excluded_)
            total_dur.append(total_dur_)

        all_spike_rasters_original = np.hstack(event_spike_rasters_original)
        frac_excluded = np.sum(duration_excluded)/np.sum(total_dur)

         # get means of pre and post modes + compression factor
        pre_file_name = self.session_params.default_pre_phmm_model
        post_file_name = self.session_params.default_post_phmm_model
        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        # do decoding and plot result
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                  event_spike_rasters=all_spike_rasters_original,
                                                  compression_factor=compression_factor)

        post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                   event_spike_rasters=all_spike_rasters_original,
                                                   compression_factor=compression_factor)

        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        sim_ratio = (max_post_likeli - max_pre_likeli) / (max_post_likeli + max_pre_likeli)

        return sim_ratio, frac_excluded

    def memory_drift_spike_bin_size(self, n_moving_average_pop_vec=10000, save_fig=False,
                                          plotting=False, nr_spikes_per_bin = 6):

        event_spike_rasters_original = []
        event_spike_rasters_n_spikes = []

        for l_s in self.long_sleep:
            # get outside SWR bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_n_spikes_ = (
                l_s.get_spike_binned_raster_combined_sleep_phases(spikes_per_bin=nr_spikes_per_bin))

            # get SWR bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_original_ = (
                l_s.get_spike_binned_raster_combined_sleep_phases())

            event_spike_rasters_original.append(np.hstack(event_spike_rasters_original_))
            event_spike_rasters_n_spikes.append(np.hstack(event_spike_rasters_n_spikes_))

        all_spike_rasters_n_spikes = np.hstack(event_spike_rasters_n_spikes)
        all_spike_rasters_original = np.hstack(event_spike_rasters_original)

        # delete events which are too short --> TODO
        # --------------------------------------------------------------------------------------------------------------
        # nr_bins = np.array([x.shape[1] for x in merged_events])

        # get means of pre and post modes + compression factor
        pre_file_name = self.session_params.default_pre_phmm_model
        post_file_name = self.session_params.default_post_phmm_model
        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms
        compression_factor_n = compression_factor / (12/nr_spikes_per_bin)

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        # do decoding and plot result
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                  event_spike_rasters=all_spike_rasters_original,
                                                  compression_factor=compression_factor)

        post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                   event_spike_rasters=all_spike_rasters_original,
                                                   compression_factor=compression_factor)

        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        sim_ratio = (max_post_likeli - max_pre_likeli) / (max_post_likeli + max_pre_likeli)

        # do decoding and plot result for jittered data
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_j = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                    event_spike_rasters=all_spike_rasters_n_spikes,
                                                    compression_factor=compression_factor_n)

        post_likeli_j = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                     event_spike_rasters=all_spike_rasters_n_spikes,
                                                     compression_factor=compression_factor_n)

        pre_likeli_j = np.vstack(pre_likeli_j)
        post_likeli_j = np.vstack(post_likeli_j)

        max_pre_likeli_j = np.max(pre_likeli_j, axis=1)
        max_post_likeli_j = np.max(post_likeli_j, axis=1)

        sim_ratio_j = (max_post_likeli_j - max_pre_likeli_j) / (max_post_likeli_j + max_pre_likeli_j)

        # look at distribution of decoded PRE modes
        mode_decoded_pre, mode_decoded_pre_counts = np.unique(np.argmax(pre_likeli, axis=1), return_counts=True)
        mode_decoded_pre_outside, mode_decoded_pre_counts_outside = np.unique(np.argmax(pre_likeli_j, axis=1),
                                                                              return_counts=True)
        modes_original =np.zeros(pre_likeli.shape[1])
        modes_original[mode_decoded_pre] = mode_decoded_pre_counts/np.sum(mode_decoded_pre_counts)
        modes_excluded =np.zeros(pre_likeli.shape[1])
        modes_excluded[mode_decoded_pre_outside] = mode_decoded_pre_counts_outside/np.sum(mode_decoded_pre_counts_outside)

        # look at distribution of decoded POST modes
        mode_decoded_post, mode_decoded_post_counts = np.unique(np.argmax(post_likeli, axis=1), return_counts=True)
        mode_decoded_post_outside, mode_decoded_post_counts_outside = np.unique(np.argmax(post_likeli_j, axis=1),
                                                                              return_counts=True)
        modes_original =np.zeros(post_likeli.shape[1])
        modes_original[mode_decoded_post] = mode_decoded_post_counts/np.sum(mode_decoded_post_counts)
        modes_excluded =np.zeros(post_likeli.shape[1])
        modes_excluded[mode_decoded_post_outside] = mode_decoded_post_counts_outside/np.sum(mode_decoded_post_counts_outside)

        # take the log
        max_pre_log_likeli = np.log(max_pre_likeli)
        max_pre_log_likeli_j = np.log(max_pre_likeli_j)
        max_post_log_likeli = np.log(max_post_likeli)
        max_post_log_likeli_j = np.log(max_post_likeli_j)

        # interpolate missing values
        max_pre_log_likeli_j = np.interp(np.linspace(0, len(max_pre_log_likeli_j), len(max_pre_log_likeli)),
                                         np.arange(len(max_pre_log_likeli_j)), max_pre_log_likeli_j)

        max_post_log_likeli_j = np.interp(np.linspace(0, len(max_post_log_likeli_j), len(max_post_log_likeli)),
                                          np.arange(len(max_post_log_likeli_j)), max_post_log_likeli_j)

        # apply smoothing
        max_pre_log_likeli_s = uniform_filter1d(max_pre_log_likeli, n_moving_average_pop_vec)
        max_pre_log_likeli_j_s = uniform_filter1d(max_pre_log_likeli_j, n_moving_average_pop_vec)
        max_post_log_likeli_s = uniform_filter1d(max_post_log_likeli, n_moving_average_pop_vec)
        max_post_log_likeli_j_s = uniform_filter1d(max_post_log_likeli_j, n_moving_average_pop_vec)

        # compute difference
        diff_pre = max_pre_log_likeli_s - max_pre_log_likeli_j_s
        diff_post = max_post_log_likeli_s - max_post_log_likeli_j_s

        # interpolate to match sim_ratio length of original
        sim_ratio_j = np.interp(np.linspace(0, len(sim_ratio_j), len(sim_ratio)),
                        np.arange(len(sim_ratio_j)), sim_ratio_j)

        if plotting or save_fig:

            plt.scatter(modes_original, modes_excluded)
            plt.title("PRE mode decoding (fraction)")
            plt.xlabel("12 spikes")
            plt.ylabel("6 spikes")
            plt.tight_layout()
            plt.show()

            plt.scatter(modes_original, modes_excluded)
            plt.title("POST mode decoding (fraction)")
            plt.xlabel("12 spikes")
            plt.ylabel("6 spikes")
            plt.tight_layout()
            plt.show()


            plt.figure(figsize=(4,3))
            if save_fig:
                plt.style.use('default')
            plt.plot(np.linspace(0,1,moving_average(sim_ratio, n_moving_average_pop_vec).shape[0]),
                     moving_average(sim_ratio, n_moving_average_pop_vec), label="12 spikes", color="grey")
            plt.plot(np.linspace(0,1,moving_average(sim_ratio_j, n_moving_average_pop_vec).shape[0]),
                     moving_average(sim_ratio_j, n_moving_average_pop_vec), label=str(nr_spikes_per_bin)+" spikes",
                     color="red", alpha=0.5)
            plt.xlabel("Normalized duration")
            plt.ylabel("Drift score")
            plt.legend()
            plt.tight_layout()
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "drift_score_time_bin_size_spikes_"+str(nr_spikes_per_bin)+"_"+
                                         self.session_name+".svg"), transparent="True")
                plt.close()
            else:
                plt.show()


            plt.figure(figsize=(12,8))
            plt.subplot(2,2,1)
            plt.plot(max_pre_log_likeli_s, color="red", label="PRE 12 spikes")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_pre_log_likeli_j_s, color="salmon", linestyle="--", label="PRE "+str(nr_spikes_per_bin)+" spikes")
            plt.xticks([])
            plt.ylabel("Max log likelihood")
            plt.legend()
            plt.subplot(2,2,3)
            plt.plot(np.linspace(0,1, diff_pre.shape[0]), diff_pre)
            plt.ylabel("12 spikes - n spikes")
            plt.xlabel("relative time")
            plt.subplot(2,2,2)
            plt.plot(max_post_log_likeli_s, color="white", label="POST 12 spikes")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_post_log_likeli_j_s, color="lightblue", linestyle="--", label="POST "+  str(nr_spikes_per_bin)
                                                                                       +" spikes", alpha=0.8)
            plt.xticks([])
            plt.legend()
            plt.subplot(2,2,4)
            plt.plot(np.linspace(0,1, diff_post.shape[0]), diff_post)
            plt.ylabel("12 spikes - n spikes")
            plt.xlabel("relative time")
            plt.tight_layout()
            plt.show()

        return sim_ratio, sim_ratio_j

    def memory_drift_subsets(self, n_moving_average_pop_vec=10000, plotting=False, pre_model ="pre",
                             post_model="post", subset="stable"):

        event_spike_rasters_original = []

        for l_s in self.long_sleep:

            # get SWR bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_original_ = (
                l_s.get_spike_binned_raster_combined_sleep_phases())

            event_spike_rasters_original.append(np.hstack(event_spike_rasters_original_))

        all_spike_rasters_original = np.hstack(event_spike_rasters_original)

        # delete events which are too short --> TODO
        # --------------------------------------------------------------------------------------------------------------
        # nr_bins = np.array([x.shape[1] for x in merged_events])

        # get means of pre and post modes + compression factor
        if pre_model == "pre":
            pre_file_name = self.session_params.default_pre_phmm_model
        elif pre_model == "pre_familiar":
            pre_file_name = self.session_params.default_exp_fam_1_model
        if post_model ==  "post":
            post_file_name = self.session_params.default_post_phmm_model
        elif post_model == "post_familiar":
            post_file_name = self.session_params.default_exp_fam_2_model
        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_


        # get cell subsets
        # --------------------------------------------------------------------------------------------------------------
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle",
                  "rb") as f:
            class_dic = pickle.load(f)

        if subset == "stable":
            cell_ids = class_dic["stable_cell_ids"]
        elif subset == "inc":
            cell_ids = class_dic["increase_cell_ids"]
        elif subset == "dec":
            cell_ids = class_dic["decrease_cell_ids"]
        else:
            raise Exception("Cell subset not defined")

        pre_mode_means_subset = pre_mode_means[:, cell_ids]
        post_mode_means_subset = post_mode_means[:, cell_ids]
        all_spike_rasters_subset = all_spike_rasters_original[cell_ids, :]

        # do decoding and plot result
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                  event_spike_rasters=all_spike_rasters_original,
                                                  compression_factor=compression_factor)

        post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                   event_spike_rasters=all_spike_rasters_original,
                                                   compression_factor=compression_factor)

        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        sim_ratio = (max_post_likeli - max_pre_likeli) / (max_post_likeli + max_pre_likeli)

        # do decoding and plot result for jittered data
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_j = decode_using_phmm_modes_fast(mode_means=pre_mode_means_subset,
                                                    event_spike_rasters=all_spike_rasters_subset,
                                                    compression_factor=compression_factor)

        post_likeli_j = decode_using_phmm_modes_fast(mode_means=post_mode_means_subset,
                                                     event_spike_rasters=all_spike_rasters_subset,
                                                     compression_factor=compression_factor)

        pre_likeli_j = np.vstack(pre_likeli_j)
        post_likeli_j = np.vstack(post_likeli_j)

        max_pre_likeli_j = np.max(pre_likeli_j, axis=1)
        max_post_likeli_j = np.max(post_likeli_j, axis=1)

        sim_ratio_j = (max_post_likeli_j - max_pre_likeli_j) / (max_post_likeli_j + max_pre_likeli_j)

        # look at distribution of decoded PRE modes
        mode_decoded_pre, mode_decoded_pre_counts = np.unique(np.argmax(pre_likeli, axis=1), return_counts=True)
        mode_decoded_pre_outside, mode_decoded_pre_counts_outside = np.unique(np.argmax(pre_likeli_j, axis=1),
                                                                              return_counts=True)
        modes_original_pre =np.zeros(pre_likeli.shape[1])
        modes_original_pre[mode_decoded_pre] = mode_decoded_pre_counts/np.sum(mode_decoded_pre_counts)
        modes_excluded_pre =np.zeros(pre_likeli.shape[1])
        modes_excluded_pre[mode_decoded_pre_outside] = mode_decoded_pre_counts_outside/np.sum(mode_decoded_pre_counts_outside)

        # look at distribution of decoded POST modes
        mode_decoded_post, mode_decoded_post_counts = np.unique(np.argmax(post_likeli, axis=1), return_counts=True)
        mode_decoded_post_outside, mode_decoded_post_counts_outside = np.unique(np.argmax(post_likeli_j, axis=1),
                                                                                return_counts=True)
        modes_original_post =np.zeros(post_likeli.shape[1])
        modes_original_post[mode_decoded_post] = mode_decoded_post_counts/np.sum(mode_decoded_post_counts)
        modes_excluded_post =np.zeros(post_likeli.shape[1])
        modes_excluded_post[mode_decoded_post_outside] = mode_decoded_post_counts_outside/np.sum(mode_decoded_post_counts_outside)

        # take the log
        max_pre_log_likeli = np.log(max_pre_likeli)
        max_pre_log_likeli_j = np.log(max_pre_likeli_j)
        max_post_log_likeli = np.log(max_post_likeli)
        max_post_log_likeli_j = np.log(max_post_likeli_j)

        # interpolate missing values
        max_pre_log_likeli_j = np.interp(np.linspace(0, len(max_pre_log_likeli_j), len(max_pre_log_likeli)),
                                         np.arange(len(max_pre_log_likeli_j)), max_pre_log_likeli_j)

        max_post_log_likeli_j = np.interp(np.linspace(0, len(max_post_log_likeli_j), len(max_post_log_likeli)),
                                          np.arange(len(max_post_log_likeli_j)), max_post_log_likeli_j)

        # apply smoothing
        max_pre_log_likeli_s = uniform_filter1d(max_pre_log_likeli, n_moving_average_pop_vec)
        max_pre_log_likeli_j_s = uniform_filter1d(max_pre_log_likeli_j, n_moving_average_pop_vec)
        max_post_log_likeli_s = uniform_filter1d(max_post_log_likeli, n_moving_average_pop_vec)
        max_post_log_likeli_j_s = uniform_filter1d(max_post_log_likeli_j, n_moving_average_pop_vec)

        # compute difference
        diff_pre = max_pre_log_likeli_s - max_pre_log_likeli_j_s
        diff_post = max_post_log_likeli_s - max_post_log_likeli_j_s

        # interpolate to match sim_ratio length of orignal
        sim_ratio_j = np.interp(np.linspace(0, len(sim_ratio_j), len(sim_ratio)),
                                np.arange(len(sim_ratio_j)), sim_ratio_j)

        if plotting:
            plt.scatter(modes_original_pre, modes_excluded_pre)
            plt.title("PRE mode decoding (fraction)")
            plt.xlabel("REM&NREM")
            plt.ylabel("Excluded")
            plt.tight_layout()
            plt.show()

            plt.scatter(modes_original_post, modes_excluded_post)
            plt.title("POST mode decoding (fraction)")
            plt.xlabel("REM&NREM")
            plt.ylabel("Excluded")
            plt.tight_layout()
            plt.show()

            plt.plot(moving_average(sim_ratio, n_moving_average_pop_vec), label="original", color="grey")
            plt.plot(moving_average(sim_ratio_j, n_moving_average_pop_vec), label=subset, color="red", alpha=0.5)
            plt.xlabel("Time")
            plt.ylabel("sim_ratio")
            plt.legend()
            plt.tight_layout()
            plt.show()

            plt.figure(figsize=(12,8))
            plt.subplot(2,2,1)
            plt.plot(max_pre_log_likeli_s, color="red", label="PRE original")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_pre_log_likeli_j_s, color="salmon", linestyle="--", label="PRE "+subset)
            plt.xticks([])
            plt.ylabel("Max log likelihood")
            plt.legend()
            plt.subplot(2,2,3)
            plt.plot(np.linspace(0,1, diff_pre.shape[0]), diff_pre)
            plt.ylabel("original - "+subset)
            plt.xlabel("relative time")
            plt.subplot(2,2,2)
            plt.plot(max_post_log_likeli_s, color="white", label="POST original")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_post_log_likeli_j_s, color="lightblue", linestyle="--", label="POST "+subset, alpha=0.8)
            plt.xticks([])
            plt.legend()
            plt.subplot(2,2,4)
            plt.plot(np.linspace(0,1, diff_post.shape[0]), diff_post)
            plt.ylabel("original - "+subset)
            plt.xlabel("relative time")
            plt.tight_layout()
            plt.show()

        return sim_ratio, sim_ratio_j, diff_pre, diff_post

    def memory_drift_subsets_pre_post_vs_fam(self, n_moving_average_pop_vec=10000, plotting=False, subset="stable"):

        event_spike_rasters_original = []

        for l_s in self.long_sleep:

            # get SWR bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_original_ = (
                l_s.get_spike_binned_raster_combined_sleep_phases())

            event_spike_rasters_original.append(np.hstack(event_spike_rasters_original_))

        all_spike_rasters_original = np.hstack(event_spike_rasters_original)

        # delete events which are too short --> TODO
        # --------------------------------------------------------------------------------------------------------------
        # nr_bins = np.array([x.shape[1] for x in merged_events])

        # get means of pre and post modes + compression factor

        pre_file_name = self.session_params.default_pre_phmm_model
        pre_fam_file_name = self.session_params.default_exp_fam_1_model
        post_file_name = self.session_params.default_post_phmm_model
        post_fam_file_name = self.session_params.default_exp_fam_2_model
        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + pre_fam_file_name + '.pkl', 'rb') as f:
            pre_fam_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_fam_mode_means = pre_fam_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_fam_file_name + '.pkl', 'rb') as f:
            post_fam_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_fam_mode_means = post_fam_model_dic.means_


        # get cell subsets
        # --------------------------------------------------------------------------------------------------------------
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle",
                  "rb") as f:
            class_dic = pickle.load(f)

        if subset == "stable":
            cell_ids = class_dic["stable_cell_ids"]
        elif subset == "inc":
            cell_ids = class_dic["increase_cell_ids"]
        elif subset == "dec":
            cell_ids = class_dic["decrease_cell_ids"]
        else:
            raise Exception("Cell subset not defined")

        pre_mode_means_subset = pre_mode_means[:, cell_ids]
        post_mode_means_subset = post_mode_means[:, cell_ids]
        pre_fam_mode_means_subset = pre_fam_mode_means[:, cell_ids]
        post_fam_mode_means_subset = post_fam_mode_means[:, cell_ids]
        all_spike_rasters_subset = all_spike_rasters_original[cell_ids, :]

        # do decoding and plot result
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means_subset,
                                                  event_spike_rasters=all_spike_rasters_subset,
                                                  compression_factor=compression_factor)

        post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means_subset,
                                                   event_spike_rasters=all_spike_rasters_subset,
                                                   compression_factor=compression_factor)

        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        sim_ratio = (max_post_likeli - max_pre_likeli) / (max_post_likeli + max_pre_likeli)

        # do decoding and plot result for jittered data
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_j = decode_using_phmm_modes_fast(mode_means=pre_fam_mode_means_subset,
                                                    event_spike_rasters=all_spike_rasters_subset,
                                                    compression_factor=compression_factor)

        post_likeli_j = decode_using_phmm_modes_fast(mode_means=post_fam_mode_means_subset,
                                                     event_spike_rasters=all_spike_rasters_subset,
                                                     compression_factor=compression_factor)

        pre_likeli_j = np.vstack(pre_likeli_j)
        post_likeli_j = np.vstack(post_likeli_j)

        max_pre_likeli_j = np.max(pre_likeli_j, axis=1)
        max_post_likeli_j = np.max(post_likeli_j, axis=1)

        sim_ratio_j = (max_post_likeli_j - max_pre_likeli_j) / (max_post_likeli_j + max_pre_likeli_j)

        # take the log
        max_pre_log_likeli = np.log(max_pre_likeli)
        max_pre_log_likeli_j = np.log(max_pre_likeli_j)
        max_post_log_likeli = np.log(max_post_likeli)
        max_post_log_likeli_j = np.log(max_post_likeli_j)

        # interpolate missing values
        max_pre_log_likeli_j = np.interp(np.linspace(0, len(max_pre_log_likeli_j), len(max_pre_log_likeli)),
                                         np.arange(len(max_pre_log_likeli_j)), max_pre_log_likeli_j)

        max_post_log_likeli_j = np.interp(np.linspace(0, len(max_post_log_likeli_j), len(max_post_log_likeli)),
                                          np.arange(len(max_post_log_likeli_j)), max_post_log_likeli_j)

        # apply smoothing
        max_pre_log_likeli_s = uniform_filter1d(max_pre_log_likeli, n_moving_average_pop_vec)
        max_pre_log_likeli_j_s = uniform_filter1d(max_pre_log_likeli_j, n_moving_average_pop_vec)
        max_post_log_likeli_s = uniform_filter1d(max_post_log_likeli, n_moving_average_pop_vec)
        max_post_log_likeli_j_s = uniform_filter1d(max_post_log_likeli_j, n_moving_average_pop_vec)

        # compute difference
        diff_pre = max_pre_log_likeli_s - max_pre_log_likeli_j_s
        diff_post = max_post_log_likeli_s - max_post_log_likeli_j_s

        # interpolate to match sim_ratio length of orignal
        sim_ratio_j = np.interp(np.linspace(0, len(sim_ratio_j), len(sim_ratio)),
                                np.arange(len(sim_ratio_j)), sim_ratio_j)

        if plotting:

            plt.plot(moving_average(sim_ratio, n_moving_average_pop_vec), label="pre_post", color="grey")
            plt.plot(moving_average(sim_ratio_j, n_moving_average_pop_vec), label="pre_fam_post_fam", color="red", alpha=0.5)
            plt.xlabel("Time")
            plt.ylabel("sim_ratio")
            plt.legend()
            plt.title("Using "+subset+" subset")
            plt.tight_layout()
            plt.show()

            plt.figure(figsize=(12,8))
            plt.subplot(2,2,1)
            plt.plot(max_pre_log_likeli_s, color="red", label="PRE subset")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_pre_log_likeli_j_s, color="salmon", linestyle="--", label="PRE_fam subset")
            plt.xticks([])
            plt.ylabel("Max log likelihood")
            plt.legend()
            plt.subplot(2,2,3)
            plt.plot(np.linspace(0,1, diff_pre.shape[0]), diff_pre)
            plt.ylabel("pre - pre_fam")
            plt.xlabel("relative time")
            plt.subplot(2,2,2)
            plt.plot(max_post_log_likeli_s, color="white", label="POST subset")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_post_log_likeli_j_s, color="lightblue", linestyle="--", label="POST_fam subset", alpha=0.8)
            plt.xticks([])
            plt.legend()
            plt.subplot(2,2,4)
            plt.plot(np.linspace(0,1, diff_post.shape[0]), diff_post)
            plt.ylabel("post - post_fam")
            plt.xlabel("relative time")
            aniplt.tight_layout()
            plt.show()

        return sim_ratio, sim_ratio_j


class TwoPopLongSleep:
    def __init__(self, long_sleep_pop_1, long_sleep_pop_2, params, session_params):
        self.params = params
        self.session_params = session_params
        # self.cell_type = sleep_data_obj.get_cell_type()
        self.long_sleep = []
        self.session_name = session_params.session_name
        self.long_sleep_pop_1 = long_sleep_pop_1
        self.long_sleep_pop_2 = long_sleep_pop_2

    def memory_drift_delta_score_interneuron_firing(self, template_type="phmm", rem_pop_vec_threshold=10,
                                                    nrem_pop_vec_threshold=2, n_moving_average_pop_vec=200,
                                                    plotting=False, return_p_values=False,
                                                    use_abs_delta_score=False, interneuron_to_save=None,
                                                    invert_rem_sign=False, time_bin_size_interneuron=0.1):
        """
        Correlating delta score of pHMM decoding with interneurons firing rates

        :param use_abs_delta_score: whether to use absolute (True) or standard delta_score
        :type use_abs_delta_score: bool
        :param return_p_values: whether to return p-values of correlations or not
        :type return_p_values: bool
        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param nrem_pop_vec_threshold: minimum length of NREM epochs (shorter ones are discarded)
        :type nrem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :return: corr_fir_ds_nrem, corr_fir_ds_rem, p_nrem, p_rem --> correlation values and p values for nrem/rem
        :rtype: np.array
        """

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, _, _ = \
            self.long_sleep_pop_1.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=None,
                                                     post_file_name=None, part_to_analyze="rem",
                                                                      pop_vec_threshold=rem_pop_vec_threshold)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, _, _ = \
            self.long_sleep_pop_1.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=None,
                                                     post_file_name=None, part_to_analyze="nrem",
                                                                      pop_vec_threshold=nrem_pop_vec_threshold)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------------------------------

        all_events_ratio = ratio_per_rem_event + ratio_per_nrem_event
        all_events_length = event_lengths_rem + event_lengths_nrem
        all_events_length_s = np.hstack((event_duration_rem_in_s, event_duration_nrem_in_s))
        labels_events = np.zeros(len(ratio_per_rem_event) + len(ratio_per_nrem_event))
        labels_events[:len(ratio_per_rem_event)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:, 0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_ratio_list = [x for _, x in sorted(zip(all_times, all_events_ratio))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_lengths_s = [x for _, x in sorted(zip(all_times, all_events_length_s))]
        sorted_pop_vec_ratio = np.hstack(sorted_events_ratio_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_length_s = []
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat == 1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0] + 1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first + trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat == -1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0] + 1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first + trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))
        merged_events_length_s = np.array(merged_events_length_s)
        merged_events_times = np.vstack(merged_events_times)

        # merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        # merged_nrem_event_times = merged_events_times[merged_events_labels == 0]
        merged_events_length_s = merged_events_times[:, 1] - merged_events_times[:, 0]

        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps == 1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre_post ratio) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_merged_rem_event = []
        ratio_per_merged_nrem_event = []
        ratio_rem_nrem_events = []
        rem_nrem_events_label = []
        ratio_rem_nrem_pop_vec = []
        rem_nrem_pop_vec_label = []

        for event_id, (start_event, end_event) in enumerate(zip(start, end)):
            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                ratio_per_merged_rem_event.append(sorted_pop_vec_ratio[start_event:end_event])
            # nrem event
            else:
                ratio_per_merged_nrem_event.append(sorted_pop_vec_ratio[start_event:end_event])

            ratio_rem_nrem_events.append(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])

            ratio_rem_nrem_pop_vec.extend(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_pop_vec_label.extend(labels_per_pop_vec[start_event:end_event])

        # smooth across population vectors --> that means also across REM/NREM epochs
        # --------------------------------------------------------------------------------------------------------------
        len_new_events = [x.shape[0] for x in ratio_rem_nrem_events]
        ratio_per_pop_vec_new = np.hstack(ratio_rem_nrem_events)
        ratio_per_pop_vec_new_smooth = moving_average(a=np.array(ratio_per_pop_vec_new), n=n_moving_average_pop_vec)

        ratio_rem_nrem_events_smooth = []
        first = 0
        for event_id in range(len(len_new_events)):
            # check if data is still available (due to smoothing --> data gets shorter)
            # if not --> leave loop
            if first >= ratio_per_pop_vec_new_smooth.shape[0]:
                break
            end_event = min(first + len_new_events[event_id], ratio_per_pop_vec_new_smooth.shape[0])
            ratio_rem_nrem_events_smooth.append(ratio_per_pop_vec_new_smooth[first:end_event])
            first += len_new_events[event_id]

        # need to delete indices of events that happend after the last data point after smoothing
        rem_nrem_events_label = rem_nrem_events_label[:len(ratio_rem_nrem_events_smooth)]
        ratio_rem_nrem_events_smooth_arr = np.hstack(ratio_rem_nrem_events_smooth)
        # inter_fir_merged_events = self.long_sleep_pop_2.rasters_for_intervals(intervals_s=merged_events_times, time_bin_size=0.01)
        # inter_fir_merged_events = np.hstack(inter_fir_merged_events)
        #
        # start = 0.0
        # end = 1
        # smooth_fir = moving_average(inter_fir_merged_events[interneuron_to_save, :], n=n_moving_average_pop_vec)
        # plt.subplot(2,1,1)
        # plt.plot(smooth_fir)
        # plt.xlim(start*smooth_fir.shape[0], end*smooth_fir.shape[0])
        # plt.subplot(2, 1, 2)
        # plt.plot(ratio_rem_nrem_events_smooth_arr)
        # plt.xlim(start*ratio_rem_nrem_events_smooth_arr.shape[0], end*ratio_rem_nrem_events_smooth_arr.shape[0])
        # plt.show()

        # get rem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        rem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 1))
        ds_rem = []
        ds_rem_smoothed_within = []
        merged_events_rem_length = []
        ratio_rem_events_smooth = []
        merged_rem_events_time_stamps = []
        for rem_index in rem_events_indices:
            ds_rem.append(ratio_rem_nrem_events_smooth[rem_index][-1] - ratio_rem_nrem_events_smooth[rem_index][0])
            smooth_event = moving_average(a=ratio_rem_nrem_events[rem_index], n=10)
            ds_rem_smoothed_within.append(smooth_event[-1] - smooth_event[0])
            merged_events_rem_length.append(ratio_rem_nrem_events_smooth[rem_index].shape[0])
            ratio_rem_events_smooth.append(ratio_rem_nrem_events_smooth[rem_index])
            merged_rem_events_time_stamps.append(merged_events_times[rem_index])

        # get nrem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        nrem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 0))
        ds_nrem = []
        ds_nrem_smoothed_within = []
        merged_events_nrem_length = []
        merged_nrem_events_time_stamps = []
        ratio_nrem_events_smooth = []
        for nrem_index in nrem_events_indices:
            ds_nrem.append(ratio_rem_nrem_events_smooth[nrem_index][-1] - ratio_rem_nrem_events_smooth[nrem_index][0])
            merged_events_nrem_length.append(ratio_rem_nrem_events_smooth[nrem_index].shape[0])
            ratio_nrem_events_smooth.append(ratio_rem_nrem_events_smooth[nrem_index])
            merged_nrem_events_time_stamps.append(merged_events_times[nrem_index])

        ds_nrem = np.array(ds_nrem)
        ds_rem = np.array(ds_rem)

        # nrem events: get interneuron firing
        merged_nrem_events_time_stamps = np.vstack(merged_nrem_events_time_stamps)
        inter_fir_nrem = self.long_sleep_pop_2.mean_firing_rate_for_interval(intervals_s=merged_nrem_events_time_stamps)
        inter_fir_raster_nrem = self.long_sleep_pop_2.rasters_for_intervals(intervals_s=merged_nrem_events_time_stamps,
                                                                              time_bin_size=time_bin_size_interneuron)

        # dur nrem --> very short intervals lead to large estimates of firing rates
        dur_nrem = merged_nrem_events_time_stamps[:,1] - merged_nrem_events_time_stamps[:,0]
        # plt.scatter(dur_nrem, inter_fir_nrem[5,:])
        # plt.xlim(0,50)
        # plt.xlabel("Interval lenght (s)")
        # plt.ylabel("Estimated firing rate of interneuron")
        # plt.show()

        ds_nrem = np.array(ds_nrem)
        ds_and_fir = np.vstack((ds_nrem, inter_fir_nrem))
        is_nan = np.argwhere(np.isnan(np.sum(ds_and_fir, axis=0)))
        ds_and_fir_nrem = np.delete(ds_and_fir, is_nan.flatten(), axis=1)
        dur_nrem = np.delete(dur_nrem, is_nan.flatten())

        ds_and_fir_nrem_long_periods = ds_and_fir_nrem[:, dur_nrem > 5]

        if use_abs_delta_score:
            # firing rates can only be positive anyways --> can take abs of everything
            corr_fir_ds_nrem = np.corrcoef(np.abs(ds_and_fir_nrem_long_periods))[0, 1:]
        else:
            corr_fir_ds_nrem = np.corrcoef(ds_and_fir_nrem_long_periods)[0, 1:]

        corr_ds_nrem = []
        p_nrem = []
        for i in range(1, ds_and_fir_nrem_long_periods.shape[0]):
            # only select periods where mean firing is < 2std
            mean_selected = ds_and_fir_nrem_long_periods[i, :]
            ds_selected = ds_and_fir_nrem_long_periods[0, :]
            corr_ds_nrem.append(pearsonr(ds_selected, mean_selected)[0])
            if use_abs_delta_score:
                p_nrem.append(pearsonr(np.abs(ds_selected), mean_selected)[1])
            else:
                p_nrem.append(pearsonr(ds_selected, mean_selected)[1])
            # plt.xlabel("Delta_score")
            # plt.ylabel("Mean firing interneuron")

        if interneuron_to_save is not None:
            plt.style.use('default')
            mean_selected = ds_and_fir_nrem_long_periods[interneuron_to_save, :]
            ds_selected = ds_and_fir_nrem_long_periods[0, :]
            if use_abs_delta_score:
                ds_selected = np.abs(ds_selected)
            plt.scatter(ds_selected, mean_selected, color="lightblue")
            plt.text(0, 0.5 * np.max(mean_selected),
                     "R=" + str(np.round(pearsonr(ds_selected, mean_selected)[0], 2)),
                     horizontalalignment='center',
                     verticalalignment='center', color="black")
            if use_abs_delta_score:
                plt.xlabel("abs. Delta score")
            else:
                plt.xlabel("Delta score")
            plt.ylabel("Firing rate (Hz)")
            plt.rcParams['svg.fonttype'] = 'none'
            plt.savefig(os.path.join(save_path, "interneuron_delta_score_example_nrem"+str(interneuron_to_save)+".svg"),
                        transparent="True")
            plt.close()

        if plotting:
            plt.figure(figsize=(20, 10))
            corr_ds_nrem = []
            p_nrem = []
            for i in range(1, ds_and_fir_nrem_long_periods.shape[0]):
                # only select periods where mean firing is < 2std
                mean_selected = ds_and_fir_nrem_long_periods[i, :]
                ds_selected = ds_and_fir_nrem_long_periods[0, :]
                if use_abs_delta_score:
                    ds_selected = np.abs(ds_selected)
                plt.subplot(3, 5, i)
                plt.scatter(ds_selected, mean_selected, color="gray")
                # plt.xlabel("Delta_score")
                # plt.ylabel("Mean firing interneuron")
                plt.text(0, 0.5 * np.max(mean_selected),
                         "R=" + str(np.round(pearsonr(ds_selected, mean_selected)[0], 2)),
                         horizontalalignment='center',
                         verticalalignment='center', color="red")
            plt.suptitle("NREM")
            plt.show()

        # rem events
        merged_rem_events_time_stamps = np.vstack(merged_rem_events_time_stamps)
        inter_fir_rem = self.long_sleep_pop_2.mean_firing_rate_for_interval(intervals_s=merged_rem_events_time_stamps)
        inter_fir_raster_rem = self.long_sleep_pop_2.rasters_for_intervals(intervals_s=merged_events_times,
                                                                              time_bin_size=0.1)

        merged_events_ratio = []
        merged_events_inter_fir = []
        merged_events_times = []
        # merge rem and nrem events for visualization
        if merged_rem_events_time_stamps[0][0] > merged_nrem_events_time_stamps[0][0]:
            # first event is nrem
            for ratio_r, ratio_nr, inter_fir_r, inter_fir_nr, ev_t_rem, ev_t_nrem in zip(ratio_rem_events_smooth, ratio_nrem_events_smooth,
                                                                    inter_fir_raster_rem, inter_fir_raster_nrem,
                                                                                 merged_rem_events_time_stamps, merged_nrem_events_time_stamps):
                merged_events_ratio.append(ratio_nr)
                merged_events_ratio.append(ratio_r)
                merged_events_inter_fir.append(inter_fir_nr)
                merged_events_inter_fir.append(inter_fir_r)
                merged_events_times.append(ev_t_nrem)
                merged_events_times.append(ev_t_rem)
        # merged_events_ratio = np.hstack(merged_events_ratio)
        # merged_events_inter_fir = np.hstack(merged_events_inter_fir)

        if interneuron_to_save is not None:
            # need to smooth interneuron rate and assign it again to events
            len_events_inter_fir = [x.shape[1] for x in merged_events_inter_fir]
            merged_events_inter_fir_arr = np.hstack(merged_events_inter_fir)/time_bin_size_interneuron
            # inter_fir_smooth = moving_average(zscore(merged_events_inter_fir_arr[interneuron_to_save, :]), 1000)
            inter_fir_smooth = moving_average(merged_events_inter_fir_arr[interneuron_to_save, :], 1000)
            # split again into events
            merged_inter_fir_smooth = []
            start = 0
            for ev in len_events_inter_fir:
                if start > inter_fir_smooth.shape[0]:
                    merged_inter_fir_smooth.append(np.ones(10))
                merged_inter_fir_smooth.append(inter_fir_smooth[start:(start+ev)])
                start += ev

            plt.style.use('default')
            fig = plt.figure(figsize=(5,4))
            ax1 = fig.add_subplot(111)
            start = 0
            col = 0
            color_list = ["blue", "red"]
            alpha_list = [1, 0.2]
            ax2 = ax1.twinx()
            for rat, fir, ev_t in zip(merged_events_ratio, merged_inter_fir_smooth, merged_events_times):
                ax1.plot(np.linspace(start, start + (ev_t[1] - ev_t[0]), rat.shape[0]), rat,
                         c=color_list[col], label="sim_ratio")
                ax2.plot(np.linspace(start, start + (ev_t[1] - ev_t[0]), fir.shape[0]), fir, c="black", alpha=0.8,
                         label="Intern. firing rate")
                start += (ev_t[1] - ev_t[0])
                col = np.invert(col)
            # ax1.set_xlim(52800, 55000)
            # ax2.set_xlim(52800, 55000)
            # ax1.set_xlim(47000, 55000)
            # ax2.set_xlim(47000, 55000)
            ax1.set_xlim(69000, 74300)
            ax2.set_xlim(69000, 74300)
            ax1.set_xticks([])
            ax2.set_xticks([69300, 74000], ["19h15m", "20h33m"])
            ax1.set_xlabel("Rest duration")
            ax1.set_ylabel("sim_ratio")
            ax2.set_ylabel("Interneuron firing rate (z-scored)")
            ax2.set_ylim(-30, 120)
            ax1.set_ylim(-1.6, 0.6)
            handles, labels = plt.gca().get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            plt.legend(by_label.values(), by_label.keys(), loc=2)
            plt.tight_layout()
            plt.rcParams['svg.fonttype'] = 'none'
            plt.savefig(
                os.path.join(save_path, "interneuron_delta_score_in_one_plot_" + str(interneuron_to_save) + ".svg"),
                transparent="True")
            plt.close()

        ds_rem = np.array(ds_rem)
        ds_and_mean = np.vstack((ds_rem, inter_fir_rem))
        is_nan = np.argwhere(np.isnan(np.sum(ds_and_mean, axis=0)))
        ds_and_mean_rem = np.delete(ds_and_mean, is_nan.flatten(), axis=1)

        if use_abs_delta_score:
            # firing rates can only be positive anyways --> can take abs of everything
            corr_fir_ds_rem = np.corrcoef(np.abs(ds_and_mean_rem))[0, 1:]
        else:
            corr_fir_ds_rem = np.corrcoef(ds_and_mean_rem)[0, 1:]

        if plotting:
            plt.scatter(corr_fir_ds_rem, corr_fir_ds_nrem)
            plt.xlabel("Corr: interneuron firing vs. delta (REM)")
            plt.ylabel("Corr: interneuron firing vs. delta (NREM)")
            plt.text(0,0, "R="+str(np.round(pearsonr(corr_fir_ds_rem, corr_fir_ds_nrem)[0],2)))
            plt.text(0, -0.02, "p=" + str(pearsonr(corr_fir_ds_rem, corr_fir_ds_nrem)[1]))
            plt.show()

        p_rem = []
        for i in range(1, ds_and_mean_rem.shape[0]):
            if use_abs_delta_score:
                p_rem.append(pearsonr(np.abs(ds_and_mean_rem[0, :]), ds_and_mean_rem[i, :])[1])
            else:
                p_rem.append(pearsonr(ds_and_mean_rem[0, :], ds_and_mean_rem[i, :])[1])

        if interneuron_to_save is not None:
            plt.style.use('default')
            plt.figure(figsize=(4,4))
            if use_abs_delta_score:
                ds = np.abs(ds_and_mean_rem[0,:])
            else:
                ds = ds_and_mean_rem[0,:]
            plt.scatter(ds, ds_and_mean_rem[interneuron_to_save+1,:]/time_bin_size_interneuron, color="salmon")
            plt.text(0, 0.5 * np.max(ds_and_mean_rem[interneuron_to_save+1, :]),
                     "R=" + str(np.round(pearsonr(ds,
                     ds_and_mean_rem[interneuron_to_save+1, :])[0],2)))
            plt.text(0, 0.4 * np.max(ds_and_mean_rem[interneuron_to_save+1, :]),
                     "p=" + str(pearsonr(ds,
                     ds_and_mean_rem[interneuron_to_save+1, :])[1]))
            if use_abs_delta_score:
                plt.xlabel("Abs. Delta score")
            else:
                plt.xlabel("Delta score")
            plt.ylabel("Firing rate (Hz)")
            plt.rcParams['svg.fonttype'] = 'none'
            plt.gca().set_aspect(1 / plt.gca().get_data_ratio())
            plt.savefig(os.path.join(save_path, "interneuron_delta_score_example_rem"+str(interneuron_to_save)+".svg"), transparent="True")
            plt.close()

        if invert_rem_sign:
            ds_rem = -1 * ds_and_mean_rem[0, :]
        else:
            ds_rem = ds_and_mean_rem[0, :]
        ds_nrem = ds_and_fir_nrem_long_periods[0, :]
        delta_combined = np.hstack((ds_rem, ds_nrem))
        if use_abs_delta_score:
            delta_combined = np.abs(delta_combined)
        corr_fir_ds_nrem_rem = []
        p_corr_fir_ds_nrem_rem = []
        for i_interneuron in range(1, ds_and_mean_rem.shape[0]):
            firing_rate_combined = np.hstack(
                (ds_and_mean_rem[i_interneuron, :], ds_and_fir_nrem_long_periods[i_interneuron, :]))
            corr_fir_ds_nrem_rem.append(pearsonr(delta_combined, firing_rate_combined)[0])
            p_corr_fir_ds_nrem_rem.append(pearsonr(delta_combined, firing_rate_combined)[1])
        corr_fir_ds_nrem_rem = np.hstack(corr_fir_ds_nrem_rem)
        p_corr_fir_ds_nrem_rem = np.hstack(p_corr_fir_ds_nrem_rem)

        if interneuron_to_save is not None:
            plt.style.use('default')
            plt.figure(figsize=(4,4))
            firing_rate_combined = np.hstack(
                (ds_and_mean_rem[interneuron_to_save+1, :],
                 ds_and_fir_nrem_long_periods[interneuron_to_save+1, :]))/time_bin_size_interneuron
            plt.scatter(delta_combined, firing_rate_combined, color="grey", edgecolors="white")
            plt.text(0, 0.5 * np.max(firing_rate_combined),
                     "R=" + str(np.round(pearsonr(delta_combined,
                     firing_rate_combined)[0],2)))
            plt.text(0, 0.4 * np.max(firing_rate_combined),
                     "p=" + str(pearsonr(delta_combined,
                     firing_rate_combined)[1]))
            if use_abs_delta_score:
                plt.xlabel("Abs. Delta score")
            else:
                plt.xlabel("Delta score")
            plt.ylim(0, 90)
            plt.ylabel("Firing rate (Hz)")
            plt.rcParams['svg.fonttype'] = 'none'
            plt.gca().set_aspect(1 / plt.gca().get_data_ratio())
            plt.savefig(os.path.join(save_path, "interneuron_delta_score_example_rem_nrem_"+str(interneuron_to_save)+".svg"), transparent="True")
            plt.close()

        if plotting:
            plt.figure(figsize=(20,10))
            for i in range(1, ds_and_mean_rem.shape[0]):
                plt.subplot(3,5,i)
                plt.scatter(ds_and_mean_rem[0,:], ds_and_mean_rem[i,:], color="gray")
                # plt.xlabel("Delta_score")
                # plt.ylabel("Mean firing interneuron")
                plt.text(0, 0.5*np.max(ds_and_mean_rem[i,:]), "R="+str(np.round(pearsonr(ds_and_mean_rem[0,:],
                                                                                         ds_and_mean_rem[i,:])[0], 2)),
                 horizontalalignment='center',
                 verticalalignment='center', color="red")
            plt.suptitle("REM")
            plt.show()

            mean_rem = ds_and_mean_rem[1:,:]
            plt.figure(figsize=(20, 10))
            for i_interneuron in range(1, mean_rem.shape[0]+1):
                # combine REM and NREM
                plt.subplot(3,5,i_interneuron+1)
                firing_rate_combined = np.hstack(
                    (ds_and_mean_rem[i_interneuron, :], ds_and_fir_nrem_long_periods[i_interneuron, :]))
                plt.scatter(delta_combined, firing_rate_combined, color="gray")
                # plt.xlabel("Delta_score")
                # plt.ylabel("Mean firing interneuron")
                plt.text(0, 0.5*np.max(delta_combined), "R="+str(np.round(pearsonr(firing_rate_combined, delta_combined)[0], 2)),
                 horizontalalignment='center',
                 verticalalignment='center', color="red")
            plt.suptitle("REM AND NREM")
            plt.show()

        # plt.scatter(corr_fir_ds_rem, p_rem), plt.hlines(0.05, -0.2, 0.2), plt.show()
        if return_p_values:
            return corr_fir_ds_nrem, corr_fir_ds_rem, corr_fir_ds_nrem_rem, p_nrem, p_rem, p_corr_fir_ds_nrem_rem
        else:
            return corr_fir_ds_nrem, corr_fir_ds_rem, corr_fir_ds_nrem_rem

    def memory_drift_delta_score_delta_interneuron_firing(self, template_type="phmm", rem_pop_vec_threshold=10,
                                                    nrem_pop_vec_threshold=2, n_moving_average_pop_vec=200,
                                                    plotting=False, return_p_values=False,
                                                    use_abs_delta_score=False, interneuron_to_save=None):
        """
        Correlating delta score of pHMM decoding with interneurons firing rates

        :param use_abs_delta_score: whether to use absolute (True) or standard delta_score
        :type use_abs_delta_score: bool
        :param return_p_values: whether to return p-values of correlations or not
        :type return_p_values: bool
        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param nrem_pop_vec_threshold: minimum length of NREM epochs (shorter ones are discarded)
        :type nrem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :return: corr_fir_ds_nrem, corr_fir_ds_rem, p_nrem, p_rem --> correlation values and p values for nrem/rem
        :rtype: np.array
        """

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, _, _ = \
            self.long_sleep_pop_1.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=None,
                                                     post_file_name=None, part_to_analyze="rem",
                                                                      pop_vec_threshold=rem_pop_vec_threshold)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, _, _ = \
            self.long_sleep_pop_1.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=None,
                                                     post_file_name=None, part_to_analyze="nrem",
                                                                      pop_vec_threshold=nrem_pop_vec_threshold)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------------------------------

        all_events_ratio = ratio_per_rem_event + ratio_per_nrem_event
        all_events_length = event_lengths_rem + event_lengths_nrem
        all_events_length_s = np.hstack((event_duration_rem_in_s, event_duration_nrem_in_s))
        labels_events = np.zeros(len(ratio_per_rem_event) + len(ratio_per_nrem_event))
        labels_events[:len(ratio_per_rem_event)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:, 0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_ratio_list = [x for _, x in sorted(zip(all_times, all_events_ratio))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_lengths_s = [x for _, x in sorted(zip(all_times, all_events_length_s))]
        sorted_pop_vec_ratio = np.hstack(sorted_events_ratio_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_length_s = []
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat == 1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0] + 1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first + trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat == -1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0] + 1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first + trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))
        merged_events_length_s = np.array(merged_events_length_s)
        merged_events_times = np.vstack(merged_events_times)

        # merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        # merged_nrem_event_times = merged_events_times[merged_events_labels == 0]
        merged_events_length_s = merged_events_times[:, 1] - merged_events_times[:, 0]

        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps == 1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre_post ratio) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_merged_rem_event = []
        ratio_per_merged_nrem_event = []
        ratio_rem_nrem_events = []
        rem_nrem_events_label = []
        ratio_rem_nrem_pop_vec = []
        rem_nrem_pop_vec_label = []

        for event_id, (start_event, end_event) in enumerate(zip(start, end)):
            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                ratio_per_merged_rem_event.append(sorted_pop_vec_ratio[start_event:end_event])
            # nrem event
            else:
                ratio_per_merged_nrem_event.append(sorted_pop_vec_ratio[start_event:end_event])

            ratio_rem_nrem_events.append(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])

            ratio_rem_nrem_pop_vec.extend(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_pop_vec_label.extend(labels_per_pop_vec[start_event:end_event])

        # smooth across population vectors --> that means also across REM/NREM epochs
        # --------------------------------------------------------------------------------------------------------------
        len_new_events = [x.shape[0] for x in ratio_rem_nrem_events]
        ratio_per_pop_vec_new = np.hstack(ratio_rem_nrem_events)
        ratio_per_pop_vec_new_smooth = moving_average(a=np.array(ratio_per_pop_vec_new), n=n_moving_average_pop_vec)

        ratio_rem_nrem_events_smooth = []
        first = 0
        for event_id in range(len(len_new_events)):
            # check if data is still available (due to smoothing --> data gets shorter)
            # if not --> leave loop
            if first >= ratio_per_pop_vec_new_smooth.shape[0]:
                break
            end_event = min(first + len_new_events[event_id], ratio_per_pop_vec_new_smooth.shape[0])
            ratio_rem_nrem_events_smooth.append(ratio_per_pop_vec_new_smooth[first:end_event])
            first += len_new_events[event_id]

        # need to delete indices of events that happend after the last data point after smoothing
        rem_nrem_events_label = rem_nrem_events_label[:len(ratio_rem_nrem_events_smooth)]

        # get rem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        rem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 1))
        ds_rem = []
        ds_rem_smoothed_within = []
        merged_events_rem_length = []
        ratio_rem_events_smooth = []
        merged_rem_events_time_stamps = []
        for rem_index in rem_events_indices:
            ds_rem.append(ratio_rem_nrem_events_smooth[rem_index][-1] - ratio_rem_nrem_events_smooth[rem_index][0])
            smooth_event = moving_average(a=ratio_rem_nrem_events[rem_index], n=10)
            ds_rem_smoothed_within.append(smooth_event[-1] - smooth_event[0])
            merged_events_rem_length.append(ratio_rem_nrem_events_smooth[rem_index].shape[0])
            ratio_rem_events_smooth.append(ratio_rem_nrem_events_smooth[rem_index])
            merged_rem_events_time_stamps.append(merged_events_times[rem_index])

        # get nrem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        nrem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 0))
        ds_nrem = []
        ds_nrem_smoothed_within = []
        merged_events_nrem_length = []
        merged_nrem_events_time_stamps = []
        ratio_nrem_events_smooth = []
        for nrem_index in nrem_events_indices:
            ds_nrem.append(ratio_rem_nrem_events_smooth[nrem_index][-1] - ratio_rem_nrem_events_smooth[nrem_index][0])
            merged_events_nrem_length.append(ratio_rem_nrem_events_smooth[nrem_index].shape[0])
            ratio_nrem_events_smooth.append(ratio_rem_nrem_events_smooth[nrem_index])
            merged_nrem_events_time_stamps.append(merged_events_times[nrem_index])

        ds_nrem = np.array(ds_nrem)
        ds_rem = np.array(ds_rem)

        # nrem events: get interneuron firing
        merged_nrem_events_time_stamps = np.vstack(merged_nrem_events_time_stamps)
        # inter_fir_nrem = self.long_sleep_pop_2.firing_rate_for_interval(intervals_s=merged_nrem_events_time_stamps)

        inter_delta_fir_nrem =\
            self.long_sleep_pop_2.Delta_firing_for_interval(intervals_s=merged_nrem_events_time_stamps)

        # dur nrem --> very short intervals lead to large estimates of firing rates
        dur_nrem = merged_nrem_events_time_stamps[:,1] - merged_nrem_events_time_stamps[:,0]
        # plt.scatter(dur_nrem, inter_fir_nrem[5,:])
        # plt.xlim(0,50)
        # plt.xlabel("Interval lenght (s)")
        # plt.ylabel("Estimated firing rate of interneuron")
        # plt.show()

        ds_nrem = np.array(ds_nrem)
        ds_and_fir = np.vstack((ds_nrem, inter_delta_fir_nrem))
        is_nan = np.argwhere(np.isnan(np.sum(ds_and_fir, axis=0)))
        ds_and_fir_nrem = np.delete(ds_and_fir, is_nan.flatten(), axis=1)
        dur_nrem = np.delete(dur_nrem, is_nan.flatten())

        ds_and_fir_nrem_long_periods = ds_and_fir_nrem[:, dur_nrem > 5]

        if use_abs_delta_score:
            # firing rates can only be positive anyways --> can take abs of everything
            corr_fir_ds_nrem = np.corrcoef(np.abs(ds_and_fir_nrem_long_periods))[0, 1:]
        else:
            corr_fir_ds_nrem = np.corrcoef(ds_and_fir_nrem_long_periods)[0, 1:]

        corr_ds_nrem = []
        p_nrem = []
        for i in range(1, ds_and_fir_nrem_long_periods.shape[0]):
            # only select periods where mean firing is < 2std
            mean_selected = ds_and_fir_nrem_long_periods[i, :]
            ds_selected = ds_and_fir_nrem_long_periods[0, :]
            corr_ds_nrem.append(pearsonr(ds_selected, mean_selected)[0])
            if use_abs_delta_score:
                p_nrem.append(pearsonr(np.abs(ds_selected), mean_selected)[1])
            else:
                p_nrem.append(pearsonr(ds_selected, mean_selected)[1])
            # plt.xlabel("Delta_score")
            # plt.ylabel("Mean firing interneuron")

        if interneuron_to_save is not None:
            plt.style.use('default')
            mean_selected = ds_and_fir_nrem_long_periods[interneuron_to_save, :]
            ds_selected = ds_and_fir_nrem_long_periods[0, :]
            plt.scatter(ds_selected, mean_selected, color="lightblue")
            plt.text(0, 0.5 * np.max(mean_selected),
                     "R=" + str(np.round(pearsonr(ds_selected, mean_selected)[0], 2)),
                     horizontalalignment='center',
                     verticalalignment='center', color="black")
            plt.xlabel("Delta score")
            plt.ylabel("Firing rate (Hz)")
            plt.tight_layout()
            plt.rcParams['svg.fonttype'] = 'none'
            plt.savefig(os.path.join(save_path, "interneuron_delta_score_example_nrem.svg", transparent="True"))
            plt.close()

        if plotting:
            plt.figure(figsize=(10, 6))
            corr_ds_nrem = []
            p_nrem = []
            for i in range(1, ds_and_fir_nrem_long_periods.shape[0]):
                # only select periods where mean firing is < 2std
                mean_selected = ds_and_fir_nrem_long_periods[i, :]
                ds_selected = ds_and_fir_nrem_long_periods[0, :]
                plt.subplot(3, 5, i)
                plt.scatter(ds_selected, mean_selected, color="gray")
                # plt.xlabel("Delta_score")
                # plt.ylabel("Mean firing interneuron")
                plt.text(0, 0.5 * np.max(mean_selected),
                         "R=" + str(np.round(pearsonr(ds_selected, mean_selected)[0], 2)),
                         horizontalalignment='center',
                         verticalalignment='center', color="red")

            plt.suptitle("NREM")
            plt.show()

        # rem events
        merged_rem_events_time_stamps = np.vstack(merged_rem_events_time_stamps)
        # inter_fir_rem = self.long_sleep_pop_2.firing_rate_for_interval(intervals_s=merged_rem_events_time_stamps)

        inter_delta_fir_nem =\
            self.long_sleep_pop_2.Delta_firing_for_interval(intervals_s=merged_rem_events_time_stamps)

        ds_rem = np.array(ds_rem)
        ds_and_mean = np.vstack((ds_rem, inter_delta_fir_nem))
        is_nan = np.argwhere(np.isnan(np.sum(ds_and_mean, axis=0)))
        ds_and_mean_rem = np.delete(ds_and_mean, is_nan.flatten(), axis=1)

        if use_abs_delta_score:
            # firing rates can only be positive anyways --> can take abs of everything
            corr_fir_ds_rem = np.corrcoef(np.abs(ds_and_mean_rem))[0, 1:]
        else:
            corr_fir_ds_rem = np.corrcoef(ds_and_mean_rem)[0, 1:]

        # combine rem and nrem
        ds_rem_nrem = np.hstack((ds_and_mean_rem, ds_and_fir_nrem_long_periods))
        corr_fir_ds_rem_nrem = np.corrcoef(ds_rem_nrem)[0, 1:]

        if plotting:
            plt.scatter(corr_fir_ds_rem, corr_fir_ds_nrem)
            plt.xlabel("Corr: interneuron firing vs. delta (REM)")
            plt.ylabel("Corr: interneuron firing vs. delta (NREM)")
            plt.text(0,0, "R="+str(np.round(pearsonr(corr_fir_ds_rem, corr_fir_ds_nrem)[0],2)))
            plt.text(0, -0.02, "p=" + str(pearsonr(corr_fir_ds_rem, corr_fir_ds_nrem)[1]))
            plt.tight_layout()
            plt.show()

        p_rem = []
        for i in range(1, ds_and_mean_rem.shape[0]):
            if use_abs_delta_score:
                p_rem.append(pearsonr(ds_and_mean_rem[0, :], np.abs(ds_and_mean_rem[i, :]))[1])
            else:
                p_rem.append(pearsonr(ds_and_mean_rem[0, :], ds_and_mean_rem[i, :])[1])

        p_rem_nrem = []
        for i in range(1, ds_and_mean_rem.shape[0]):
            if use_abs_delta_score:
                p_rem_nrem.append(pearsonr(ds_rem_nrem[0, :], np.abs(ds_rem_nrem[i, :]))[1])
            else:
                p_rem_nrem.append(pearsonr(ds_rem_nrem[0, :], ds_rem_nrem[i, :])[1])

        if interneuron_to_save is not None:
            plt.style.use('default')
            plt.scatter(ds_and_mean_rem[0,:], ds_and_mean_rem[i,:], color="salmon")
            plt.text(0, 0.5 * np.max(ds_and_mean_rem[i, :]),
                     "R=" + str(np.round(pearsonr(ds_and_mean_rem[0, :],
                     ds_and_mean_rem[i, :])[0],2)),
                     horizontalalignment='center',
                     verticalalignment='center', color="black")
            plt.xlabel("Delta score")
            plt.ylabel("Firing rate (Hz)")
            plt.rcParams['svg.fonttype'] = 'none'
            plt.savefig(os.path.join(save_path, "interneuron_delta_score_example_rem.svg", transparent="True"))
            plt.close()

        if plotting:
            plt.figure(figsize=(10,6))
            for i in range(1, ds_and_mean_rem.shape[0]):
                plt.subplot(3,5,i)
                plt.scatter(ds_and_mean_rem[0,:], ds_and_mean_rem[i,:], color="gray")
                # plt.xlabel("Delta_score")
                # plt.ylabel("Mean firing interneuron")
                plt.text(0, 0.5*np.max(ds_and_mean_rem[i,:]), "R="+str(np.round(pearsonr(ds_and_mean_rem[0,:],
                                                                                         ds_and_mean_rem[i,:])[0], 2)),
                 horizontalalignment='center',
                 verticalalignment='center', color="red")
            plt.suptitle("REM")
            plt.show()

            ds_rem = ds_and_mean_rem[0, :]
            ds_nrem = ds_and_fir_nrem_long_periods[0, :]
            delta_combined = np.hstack((ds_rem, ds_nrem))
            mean_rem = ds_and_mean_rem[1:,:]
            plt.figure(figsize=(10,6))
            for i_interneuron in range(1, mean_rem.shape[0]+1):
                # combine REM and NREM
                firing_rate_combined = np.hstack((ds_and_mean_rem[i_interneuron,:],ds_and_fir_nrem_long_periods[i_interneuron, :]))
                plt.subplot(3,5,i_interneuron+1)
                plt.scatter(delta_combined, firing_rate_combined, color="gray")
                # plt.xlabel("Delta_score")
                # plt.ylabel("Mean firing interneuron")
                plt.text(0, 0.5*np.max(delta_combined), "R="+str(np.round(pearsonr(firing_rate_combined, delta_combined)[0], 2)),
                 horizontalalignment='center',
                 verticalalignment='center', color="red")
            plt.suptitle("REM AND NREM")
            plt.show()

        if return_p_values:
            return corr_fir_ds_nrem, corr_fir_ds_rem, corr_fir_ds_rem_nrem, p_nrem, p_rem, p_rem_nrem
        else:
            return corr_fir_ds_nrem, corr_fir_ds_rem, corr_fir_ds_rem_nrem

    def memory_drift_interneuron_vs_firing_prob(self, rem_pop_vec_threshold=10, nrem_pop_vec_threshold=2,
                                                min_length_nrem_period_s=5, plotting=False, partial_corr=False):
        """
        analysis of interneuron firing rate and the correlation with firing prob of inc/dec cells

        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """
        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        merged_rem_intervals, merged_nrem_intervals = \
            self.long_sleep_pop_1.memory_drift_long_sleep_get_merged_interval_times(rem_pop_vec_threshold=rem_pop_vec_threshold,
                                                                                    nrem_pop_vec_threshold=nrem_pop_vec_threshold)

        # nrem events
        inter_fir_nrem = self.long_sleep_pop_2.firing_rate_for_interval(
            intervals_s=np.vstack(merged_nrem_intervals))

        if partial_corr:
            # get stable, decreasing, increasing cells
            with open(self.params.pre_proc_dir + "cell_classification/" +
                      self.session_name + "_" + self.params.stable_cell_method + ".pickle", "rb") as f:
                class_dic = pickle.load(f)

            # stable_ids = class_dic["stable_cell_ids"]
            inc_ids = class_dic["increase_cell_ids"]
            dec_ids = class_dic["decrease_cell_ids"]
            pyr_fir_nrem = self.long_sleep_pop_1.firing_rate_for_interval(
                intervals_s=np.vstack(merged_nrem_intervals))
            dec_fir_nrem = np.mean(pyr_fir_nrem[dec_ids, :], axis=0)
            inc_fir_nrem = np.mean(pyr_fir_nrem[inc_ids, :], axis=0)

        nrem_dec_smooth, rem_dec_smooth, rem_inc_smooth, \
        nrem_inc_smooth, _, _, first_event_label, merged_event_times = \
            self.long_sleep_pop_1.firing_rate_changes(return_p_value=False, use_only_non_stationary_periods=False,
                                                      pop_vec_threshold_rem=rem_pop_vec_threshold,
                                                      pop_vec_threshold_nrem=nrem_pop_vec_threshold,
                                                      filter_initial_period=False, return_merged_times=True)



        # need to match merged event times




        # make sure firing prob. and drift align
        # start with NREM
        if first_event_label == "nrem":
            merged_event_times_nrem = merged_event_times[::2]
            merged_event_times_rem = merged_event_times[1::2]
        else:
            merged_event_times_nrem = merged_event_times[1::2]
            merged_event_times_rem = merged_event_times[::2]

        inter_fir_nrem_clean = []
        dec_fir_nrem_clean = []
        inc_fir_nrem_clean = []
        nrem_dec_clean = []
        nrem_inc_clean = []
        nrem_event_to_delete = []
        merged_event_times_nrem_clean = []
        for event_id, (drift_times, firing_prob_times) in enumerate(zip(merged_nrem_intervals,
                                                                        merged_event_times_nrem)):
            if drift_times[0] == firing_prob_times[0]:
                inter_fir_nrem_clean.append(inter_fir_nrem[:, event_id])
                nrem_dec_clean.append(nrem_dec_smooth[event_id])
                nrem_inc_clean.append(nrem_inc_smooth[event_id])
                if partial_corr:
                    dec_fir_nrem_clean.append(dec_fir_nrem[event_id])
                    inc_fir_nrem_clean.append(inc_fir_nrem[event_id])
                merged_event_times_nrem_clean.append(merged_event_times_nrem[event_id])
            else:
                nrem_event_to_delete.append(event_id)

        if partial_corr:
            dec_fir_nrem_clean = np.array(dec_fir_nrem_clean)
            inc_fir_nrem_clean = np.array(inc_fir_nrem_clean)

        nrem_dec_clean = np.array(nrem_dec_clean)
        nrem_inc_clean = np.array(nrem_inc_clean)
        inter_fir_nrem_clean = np.vstack(inter_fir_nrem_clean).T

        # if events need to be deleted
        if len(nrem_event_to_delete) > 0:
            raise Exception("need to implement deletion")

        merged_event_times_nrem_clean = np.vstack(merged_event_times_nrem_clean)
        dur_nrem = merged_event_times_nrem_clean[:, 1] - merged_event_times_nrem_clean[:, 0]

        # REM
        # --------------------------------------------------------------------------------------------------------------
        inter_fir_rem = self.long_sleep_pop_2.firing_rate_for_interval(
            intervals_s=np.vstack(merged_rem_intervals))

        if partial_corr:
            pyr_fir_rem = self.long_sleep_pop_1.firing_rate_for_interval(
                intervals_s=np.vstack(merged_rem_intervals))
            dec_fir_rem = np.mean(pyr_fir_rem[dec_ids, :], axis=0)
            inc_fir_rem = np.mean(pyr_fir_rem[inc_ids, :], axis=0)

        inter_fir_rem_clean = []
        rem_dec_clean = []
        rem_inc_clean = []
        dec_fir_rem_clean = []
        inc_fir_rem_clean = []
        rem_event_to_delete = []
        merged_event_times_rem_clean = []
        for event_id, (drift_times, firing_prob_times) in enumerate(zip(merged_rem_intervals,
                                                                        merged_event_times_rem)):
            if drift_times[0] == firing_prob_times[0]:
                inter_fir_rem_clean.append(inter_fir_rem[:, event_id])
                rem_dec_clean.append(rem_dec_smooth[event_id])
                rem_inc_clean.append(rem_inc_smooth[event_id])
                merged_event_times_rem_clean.append(merged_event_times_rem[event_id])
                if partial_corr:
                    dec_fir_rem_clean.append(dec_fir_rem[event_id])
                    inc_fir_rem_clean.append(inc_fir_rem[event_id])
            else:
                rem_event_to_delete.append(event_id)

        if partial_corr:
            dec_fir_rem_clean = np.array(dec_fir_rem_clean)
            inc_fir_rem_clean = np.array(inc_fir_rem_clean)

        rem_dec_clean = np.array(rem_dec_clean)
        rem_inc_clean = np.array(rem_inc_clean)
        inter_fir_rem_clean = np.vstack(inter_fir_rem_clean).T

        # if events need to be deleted
        if len(rem_event_to_delete) > 0:
            raise Exception("need to implement deletion")

        merged_rem_events_time_stamps = np.vstack(merged_event_times_rem_clean)
        dur_rem = merged_rem_events_time_stamps[:, 1] - merged_rem_events_time_stamps[:, 0]

        if plotting:
            corr_nrem_inc = []
            plt.figure(figsize=(10,6))
            for i in range(0, inter_fir_nrem.shape[0]):
                # only select periods where mean firing is < 2std
                plt.subplot(3,5,i+1)
                good_bins = np.invert(np.isnan(inter_fir_nrem[i,:]))
                # first delete nans
                nrem_inc_clean_sel = nrem_inc_clean[good_bins]
                inter_fir_mean_nrem_sel = inter_fir_nrem[i,good_bins]
                dur_nrem_sel = dur_nrem[good_bins]
                # now check z-scored data
                good_bins_zscored_inter_fir = dur_nrem_sel > min_length_nrem_period_s
                nrem_inc_clean_sel = nrem_inc_clean_sel[good_bins_zscored_inter_fir]
                inter_fir_mean_nrem_sel = inter_fir_mean_nrem_sel[good_bins_zscored_inter_fir]

                plt.scatter(nrem_inc_clean_sel, inter_fir_mean_nrem_sel, color="gray")
                # plt.xlabel("Delta_score")
                # plt.ylabel("Mean firing interneuron")
                plt.text(0, 0.5*np.max(inter_fir_mean_nrem_sel), "R="+str(np.round(pearsonr(nrem_inc_clean_sel, inter_fir_mean_nrem_sel)[0], 2)),
                 horizontalalignment='center',
                 verticalalignment='center', color="red")
                corr_nrem_inc.append(pearsonr(nrem_inc_clean_sel, inter_fir_mean_nrem_sel)[0])
            plt.suptitle("NREM - INC")
            plt.show()

            corr_nrem_dec = []
            plt.figure(figsize=(10,6))
            for i in range(0, inter_fir_nrem.shape[0]):
                # only select periods where mean firing is < 2std
                plt.subplot(3,5,i+1)
                good_bins = np.invert(np.isnan(inter_fir_nrem[i,:]))
                # first delete nans
                nrem_dec_clean_sel = nrem_dec_clean[good_bins]
                inter_fir_mean_nrem_sel = inter_fir_nrem[i,good_bins]
                dur_nrem_sel = dur_nrem[good_bins]
                # now check z-scored data
                good_bins_zscored_inter_fir = dur_nrem_sel > min_length_nrem_period_s

                nrem_dec_clean_sel = nrem_dec_clean_sel[good_bins_zscored_inter_fir]
                inter_fir_mean_nrem_sel = inter_fir_mean_nrem_sel[good_bins_zscored_inter_fir]

                plt.scatter(nrem_dec_clean_sel, inter_fir_mean_nrem_sel, color="gray")
                # plt.xlabel("Delta_score")
                # plt.ylabel("Mean firing interneuron")
                plt.text(0, 0.5*np.max(inter_fir_mean_nrem_sel), "R="+str(np.round(pearsonr(nrem_dec_clean_sel, inter_fir_mean_nrem_sel)[0], 2)),
                 horizontalalignment='center',
                 verticalalignment='center', color="red")
                corr_nrem_dec.append(pearsonr(nrem_dec_clean_sel, inter_fir_mean_nrem_sel)[0])
            plt.suptitle("NREM - DEC")
            plt.show()

            plt.scatter(corr_nrem_dec, corr_nrem_inc)
            plt.xlabel("Corr: inter firing vs. delta_firing_prob dec")
            plt.ylabel("Corr: inter firing vs. delta_firing_prob inc")
            plt.text(0,0, "R="+str(pearsonr(corr_nrem_dec, corr_nrem_inc)[0]))
            plt.show()

            corr_rem_inc = []

            plt.figure(figsize=(10, 6))
            for i in range(0, inter_fir_rem.shape[0]):
                # only select periods where mean firing is < 2std
                plt.subplot(3, 5, i + 1)
                good_bins = np.invert(np.isnan(inter_fir_rem_clean[i, :]))
                # first delete nans
                rem_inc_clean_sel = rem_inc_clean[good_bins]
                inter_fir_mean_rem_sel = inter_fir_rem_clean[i, good_bins]
                # now check z-scored data
                good_bins_zscored_inter_fir = dur_rem > min_length_nrem_period_s
                rem_inc_clean_sel = rem_inc_clean_sel[good_bins_zscored_inter_fir]
                inter_fir_mean_rem_sel = inter_fir_mean_rem_sel[good_bins_zscored_inter_fir]

                plt.scatter(rem_inc_clean_sel, inter_fir_mean_rem_sel, color="gray")
                # plt.xlabel("Delta_score")
                # plt.ylabel("Mean firing interneuron")
                plt.text(0, 0.5 * np.max(inter_fir_mean_nrem_sel),
                         "R=" + str(np.round(pearsonr(rem_inc_clean_sel, inter_fir_mean_rem_sel)[0], 2)),
                         horizontalalignment='center',
                         verticalalignment='center', color="red")
                corr_rem_inc.append(pearsonr(rem_inc_clean_sel, inter_fir_mean_rem_sel)[0])
            plt.suptitle("REM - INC")
            plt.show()

            corr_rem_dec = []
            plt.figure(figsize=(10, 6))
            for i in range(0, inter_fir_rem.shape[0]):
                # only select periods where mean firing is < 2std
                plt.subplot(3, 5, i + 1)
                good_bins = np.invert(np.isnan(inter_fir_rem_clean[i, :]))
                # first delete nans
                rem_dec_clean_sel = rem_dec_clean[good_bins]
                inter_fir_mean_rem_sel = inter_fir_rem_clean[i, good_bins]
                # now check z-scored data
                good_bins_zscored_inter_fir = dur_rem > min_length_nrem_period_s

                rem_dec_clean_sel = rem_dec_clean_sel[good_bins_zscored_inter_fir]
                inter_fir_mean_rem_sel = inter_fir_mean_rem_sel[good_bins_zscored_inter_fir]

                plt.scatter(rem_dec_clean_sel, inter_fir_mean_rem_sel, color="gray")
                # plt.xlabel("Delta_score")
                # plt.ylabel("Mean firing interneuron")
                plt.text(0, 0.5 * np.max(inter_fir_mean_rem_sel),
                         "R=" + str(np.round(pearsonr(rem_dec_clean_sel, inter_fir_mean_rem_sel)[0], 2)),
                         horizontalalignment='center',
                         verticalalignment='center', color="red")
                corr_rem_dec.append(pearsonr(rem_dec_clean_sel, inter_fir_mean_rem_sel)[0])
            plt.suptitle("REM - DEC")
            plt.show()

            plt.scatter(corr_rem_dec, corr_rem_inc)
            plt.xlabel("Corr: inter firing vs. delta_firing_prob dec")
            plt.ylabel("Corr: inter firing vs. delta_firing_prob inc")
            plt.text(0, 0, "R=" + str(pearsonr(corr_rem_dec, corr_rem_inc)[0]))
            plt.show()

            plt.scatter(corr_rem_inc, corr_nrem_inc)
            plt.xlabel("Corr: inter firing vs. delta_firing_prob inc REM")
            plt.ylabel("Corr: inter firing vs. delta_firing_prob inc NREM")
            plt.text(0, 0, "R=" + str(pearsonr(corr_rem_inc, corr_nrem_inc)[0]))
            plt.show()

            plt.scatter(corr_rem_dec, corr_nrem_dec)
            plt.xlabel("Corr: inter firing vs. delta_firing_prob dec REM")
            plt.ylabel("Corr: inter firing vs. delta_firing_prob dec NREM")
            plt.text(0, 0, "R=" + str(pearsonr(corr_rem_dec, corr_nrem_dec)[0]))
            plt.show()

        else:

            corr_nrem_inc = []
            for i in range(0, inter_fir_nrem_clean.shape[0]):
                # only select periods where mean firing is < 2std
                good_bins = np.invert(np.isnan(inter_fir_nrem_clean[i, :]))
                # first delete nans
                nrem_inc_clean_sel = nrem_inc_clean[good_bins]
                inter_fir_mean_nrem_sel = inter_fir_nrem_clean[i, good_bins]
                if partial_corr:
                    inc_fir_nrem = inc_fir_nrem_clean[good_bins]
                dur_nrem_sel = dur_nrem[good_bins]
                # now check z-scored data
                good_bins_zscored_inter_fir = dur_nrem_sel > min_length_nrem_period_s
                nrem_inc_clean_sel = nrem_inc_clean_sel[good_bins_zscored_inter_fir]
                inter_fir_mean_nrem_sel = inter_fir_mean_nrem_sel[good_bins_zscored_inter_fir]
                if partial_corr:
                    inc_fir_nrem = inc_fir_nrem_clean[good_bins_zscored_inter_fir]
                    corr_nrem_inc.append(partial_correlations(inc_fir_nrem, nrem_inc_clean_sel, inter_fir_mean_nrem_sel))
                else:
                    corr_nrem_inc.append(pearsonr(nrem_inc_clean_sel, inter_fir_mean_nrem_sel)[0])

            corr_nrem_dec = []
            for i in range(0, inter_fir_nrem_clean.shape[0]):
                # only select periods where mean firing is < 2std
                good_bins = np.invert(np.isnan(inter_fir_nrem_clean[i, :]))
                # first delete nans
                nrem_dec_clean_sel = nrem_dec_clean[good_bins]
                inter_fir_mean_nrem_sel = inter_fir_nrem_clean[i, good_bins]
                if partial_corr:
                    dec_fir_nrem = dec_fir_nrem_clean[good_bins]
                dur_nrem_sel = dur_nrem[good_bins]
                # now check z-scored data
                good_bins_zscored_inter_fir = dur_nrem_sel > min_length_nrem_period_s

                nrem_dec_clean_sel = nrem_dec_clean_sel[good_bins_zscored_inter_fir]
                inter_fir_mean_nrem_sel = inter_fir_mean_nrem_sel[good_bins_zscored_inter_fir]
                if partial_corr:
                    dec_fir_nrem = dec_fir_nrem_clean[good_bins_zscored_inter_fir]
                    corr_nrem_dec.append(partial_correlations(dec_fir_nrem, nrem_dec_clean_sel, inter_fir_mean_nrem_sel))
                else:
                    corr_nrem_dec.append(pearsonr(nrem_dec_clean_sel, inter_fir_mean_nrem_sel)[0])

            corr_rem_inc = []
            for i in range(0, inter_fir_rem.shape[0]):
                # only select periods where mean firing is < 2std
                good_bins = np.invert(np.isnan(inter_fir_rem_clean[i, :]))
                # first delete nans
                rem_inc_clean_sel = rem_inc_clean[good_bins]
                if partial_corr:
                    inc_fir_rem = inc_fir_rem_clean[good_bins]
                inter_fir_mean_rem_sel = inter_fir_rem_clean[i, good_bins]
                # now check z-scored data
                good_bins_zscored_inter_fir = dur_rem > min_length_nrem_period_s
                rem_inc_clean_sel = rem_inc_clean_sel[good_bins_zscored_inter_fir]
                inter_fir_mean_rem_sel = inter_fir_mean_rem_sel[good_bins_zscored_inter_fir]
                if partial_corr:
                    inc_fir_rem = inc_fir_rem_clean[good_bins_zscored_inter_fir]
                    corr_rem_inc.append(partial_correlations(inc_fir_rem, rem_inc_clean_sel, inter_fir_mean_rem_sel))
                else:
                    corr_rem_inc.append(pearsonr(rem_inc_clean_sel, inter_fir_mean_rem_sel)[0])

            corr_rem_dec = []
            for i in range(0, inter_fir_rem.shape[0]):
                # only select periods where mean firing is < 2std
                good_bins = np.invert(np.isnan(inter_fir_rem_clean[i, :]))
                # first delete nans
                rem_dec_clean_sel = rem_dec_clean[good_bins]
                if partial_corr:
                    dec_fir_rem = dec_fir_rem_clean[good_bins]
                inter_fir_mean_rem_sel = inter_fir_rem_clean[i, good_bins]
                # now check z-scored data
                good_bins_zscored_inter_fir = dur_rem > min_length_nrem_period_s

                rem_dec_clean_sel = rem_dec_clean_sel[good_bins_zscored_inter_fir]
                inter_fir_mean_rem_sel = inter_fir_mean_rem_sel[good_bins_zscored_inter_fir]
                if partial_corr:
                    dec_fir_rem = dec_fir_rem_clean[good_bins_zscored_inter_fir]
                    corr_rem_dec.append(partial_correlations(dec_fir_rem, rem_dec_clean_sel, inter_fir_mean_rem_sel))
                else:
                    corr_rem_dec.append(pearsonr(rem_dec_clean_sel, inter_fir_mean_rem_sel)[0])

            return corr_rem_dec, corr_rem_inc, corr_nrem_dec, corr_nrem_inc

    def memory_drift_interneuron_vs_firing_rate(self, rem_pop_vec_threshold=10, nrem_pop_vec_threshold=2,
                                                min_length_nrem_period_s=5, plotting=True):
        """
        analysis of interneuron firing rate and the correlation with firing prob of inc/dec cells

        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """
        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        merged_rem_intervals, merged_nrem_intervals = \
            self.long_sleep_pop_1.memory_drift_long_sleep_get_merged_interval_times(rem_pop_vec_threshold=rem_pop_vec_threshold,
                                                                                    nrem_pop_vec_threshold=nrem_pop_vec_threshold)

        # nrem events
        inter_fir_nrem = self.long_sleep_pop_2.firing_rate_for_interval(
            intervals_s=np.vstack(merged_nrem_intervals))

        pyr_fir_nrem = self.long_sleep_pop_1.firing_rate_for_interval(
            intervals_s=np.vstack(merged_nrem_intervals))

        # rem events
        inter_fir_rem = self.long_sleep_pop_2.firing_rate_for_interval(
            intervals_s=np.vstack(merged_rem_intervals))

        pyr_fir_rem = self.long_sleep_pop_1.firing_rate_for_interval(
            intervals_s=np.vstack(merged_rem_intervals))

        # get stable, decreasing, increasing cells
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_"+self.params.stable_cell_method+".pickle", "rb") as f:
            class_dic = pickle.load(f)

        # stable_ids = class_dic["stable_cell_ids"]
        # inc_ids = class_dic["increase_cell_ids"]
        dec_ids = class_dic["decrease_cell_ids"]

        dur_nrem_intervals = merged_nrem_intervals[:, 1] - merged_nrem_intervals[:, 0]

        pyr_fir_rem_dec = pyr_fir_rem[dec_ids, :]
        pyr_fir_nrem_dec = pyr_fir_nrem[dec_ids, :]
        pyr_fir_rem_dec_mean = np.mean(pyr_fir_rem_dec, axis=0)

        # only select nrem periods that are at least 5 sec long (otherwise funny estimation of firing rate)
        pyr_fir_nrem_dec = pyr_fir_nrem_dec[:, dur_nrem_intervals > min_length_nrem_period_s]
        inter_fir_nrem = inter_fir_nrem[:, dur_nrem_intervals > min_length_nrem_period_s]
        pyr_fir_nrem_dec_mean = np.mean(pyr_fir_nrem_dec, axis=0)

        # now compute correlations between interneuron firing and decreasing cell firing during NREM
        # go through all interneurons
        corr_nrem_inter_dec = np.zeros(inter_fir_nrem.shape[0])
        for interneuron_id, inter_fir_nrem_ in enumerate(inter_fir_nrem):
            # go through
            corr_nrem_inter_dec[interneuron_id] = pearsonr(inter_fir_nrem_, pyr_fir_nrem_dec_mean)[0]

        # now compute correlations between interneuron firing and decreasing cell firing during REM
        # go through all interneurons
        corr_rem_inter_dec = np.zeros(inter_fir_rem.shape[0])
        for interneuron_id, inter_fir_rem_ in enumerate(inter_fir_rem):
            # go through
            corr_rem_inter_dec[interneuron_id] = pearsonr(inter_fir_rem_, pyr_fir_rem_dec_mean)[0]

        if plotting:
            plt.scatter(corr_rem_inter_dec, corr_nrem_inter_dec)
            plt.xlabel("Corr. interneuron firing - mean firing dec. cells REM")
            plt.ylabel("Corr. interneuron firing - mean firing dec. cells NREM")
            plt.text(0.1,0.1,"R="+str(np.round(pearsonr(corr_rem_inter_dec, corr_nrem_inter_dec)[0],2)))
            plt.show()
        else:
            return corr_rem_inter_dec, corr_nrem_inter_dec


class PrePostCheeseboard:
    """Class to compare PRE and POST"""

    def __init__(self, pre, post, params, session_params=None):
        self.params = params
        self.session_params = session_params
        self.cell_type = self.params.cell_type
        self.session_name = session_params.session_name

        # initialize each phase
        self.pre = pre
        self.post = post

        # get default models for pre and post
        self.default_pre_phmm_model = self.session_params.default_pre_phmm_model
        self.default_post_phmm_model = self.session_params.default_post_phmm_model

    # <editor-fold desc="Cell classification">

    def classify_cells_firing_rate_distribution(self, alpha=0.01, test="mwu", plot_for_control=True):
        # check if cells have similar firing rates looking at before and after sleep sessions

        lcb_1_raster = self.pre.get_raster()
        lcb_2_raster = self.post.get_raster()

        stable_cell_ids = []
        increase_cell_ids = []
        decrease_cell_ids = []
        for cell_id, (cell_fir_bef, cell_fir_aft) in enumerate(zip(lcb_1_raster, lcb_2_raster)):
            if test == "ks":
                if ks_2samp(data1=cell_fir_aft, data2=cell_fir_bef, alternative="less")[1] < alpha:
                    increase_cell_ids.append(cell_id)
                elif ks_2samp(data1=cell_fir_aft, data2=cell_fir_bef, alternative="greater")[1] < alpha:
                    decrease_cell_ids.append(cell_id)
                else:
                    stable_cell_ids.append(cell_id)
            elif test == "mwu":
                if plot_for_control:
                    p_bef = 1. * np.arange(cell_fir_bef.shape[0]) / (cell_fir_bef.shape[0] - 1)
                    p_aft = 1. * np.arange(cell_fir_aft.shape[0]) / (cell_fir_aft.shape[0] - 1)
                    plt.plot(np.sort(cell_fir_bef), p_bef, label="PRE")
                    plt.plot(np.sort(cell_fir_aft), p_aft, label="POST")
                if mannwhitneyu(x=cell_fir_aft, y=cell_fir_bef, alternative="less")[1] < alpha:
                    decrease_cell_ids.append(cell_id)
                    if plot_for_control:
                        plt.title("Decreasing")
                elif mannwhitneyu(x=cell_fir_aft, y=cell_fir_bef, alternative="greater")[1] < alpha:
                    increase_cell_ids.append(cell_id)
                    if plot_for_control:
                        plt.title("Increasing")
                else:
                    stable_cell_ids.append(cell_id)
                    if plot_for_control:
                        plt.title("stable")
                if plot_for_control:
                    plt.legend()
                    x_min, x_max = plt.gca().get_xlim()
                    plt.text(x_max/2, 0.5, "mean pre="+str(np.round(np.mean(cell_fir_bef),2)))
                    plt.text(x_max/2, 0.4, "mean post="+str(np.round(np.mean(cell_fir_aft),2)))
                    plt.show()

        print("#stable: " + str(len(stable_cell_ids)) + " ,#inc: " +
              str(len(increase_cell_ids)) + " ,#dec:" + str(len(decrease_cell_ids)))

        cell_class_dic = {
            "stable_cell_ids": np.array(stable_cell_ids),
            "decrease_cell_ids": np.array(decrease_cell_ids),
            "increase_cell_ids": np.array(increase_cell_ids)
        }

        with open(self.params.pre_proc_dir + "cell_classification/" + self.session_name + "_" + test +
                  "_awake.pickle", "wb") as f:
            pickle.dump(cell_class_dic, f, pickle.HIGHEST_PROTOCOL)

    def classify_cells_firing_rate_average_rates(self, plotting=False):
        """
        classifies cells into 3 groups (stable, increasing, decreasing) based on the change in their average firing
        rates from PRE to POST

        """

        pre_raster_mean = np.mean(self.pre.get_raster(), axis=1)
        post_raster_mean = np.mean(self.post.get_raster(), axis=1)

        norm_diff = (post_raster_mean-pre_raster_mean)/(post_raster_mean+pre_raster_mean)

        plt.hist(norm_diff, bins=30, density=True)
        plt.xlabel("PRE-POST DIFF. NORMALIZED")
        plt.ylabel("DENSITY")
        plt.show()

        stable_cell_ids = np.argwhere((norm_diff<0.33) & (-0.33<norm_diff))
        decrease_cell_ids = np.argwhere(norm_diff < -0.33)
        increase_cell_ids = np.argwhere(norm_diff > 0.33)

        if plotting:
            plt.subplot(1,2,1)
            plt.imshow(np.expand_dims(pre_raster_mean,1), interpolation='nearest', aspect='auto')
            plt.subplot(1,2,2)
            plt.imshow(np.expand_dims(post_raster_mean,1), interpolation='nearest', aspect='auto')
            plt.show()

        else:
            # create dictionary with labels
            cell_class_dic = {
                "stable_cell_ids": stable_cell_ids,
                "decrease_cell_ids": decrease_cell_ids,
                "increase_cell_ids": increase_cell_ids
            }

            with open(self.params.pre_proc_dir+"cell_classification/"+
                      self.params.session_name+"_mean_firing_awake.pickle", "wb") as f:
                pickle.dump(cell_class_dic, f, pickle.HIGHEST_PROTOCOL)

    def classify_cells_remapping(self, plotting=False, spatial_resolution=5):
        """
        classifies cells into 3 groups (stable, increasing, decreasing) based on the change in remapping properties

        """
        # get rate map from PRE
        rate_maps_pre = self.pre.get_rate_maps(spatial_resolution=spatial_resolution)
        env_dim_pre = self.pre.get_env_dim()
        occ_map_pre = self.pre.get_occ_map(spatial_resolution=spatial_resolution)
        # need to adjust dimensions of post rate map to compute overlap
        rate_maps_post = self.post.get_rate_maps(spatial_resolution=spatial_resolution, env_dim=env_dim_pre)
        occ_map_post = self.post.get_occ_map(spatial_resolution=spatial_resolution, env_dim=env_dim_pre)
        # --------------------------------------------------------------------------------------------------------------
        # compute remapping (correlation of PRE and POST rate maps)
        remapping = []

        for pre, post in zip(rate_maps_pre.T, rate_maps_post.T):
            if np.count_nonzero(pre) > 0 and np.count_nonzero(post) > 0:
                remapping.append(pearsonr(pre.flatten(), post.flatten())[0])
            else:
                remapping.append(0)

        remapping = np.array(remapping)

        # plt.hist(remapping, bins=30, density=True)
        # plt.xlabel("PRE-POST DIFF. NORMALIZED")
        # plt.ylabel("DENSITY")
        # plt.show()

        # stable cells: everything above median
        stable_cell_ids = np.argwhere(remapping > np.median(remapping)).flatten()

        # check if decreasing or increasing
        pre_raster_mean = np.mean(self.pre.get_raster(), axis=1)
        post_raster_mean = np.mean(self.post.get_raster(), axis=1)

        norm_diff = (post_raster_mean-pre_raster_mean)/(post_raster_mean+pre_raster_mean)

        decrease_cell_ids = np.argwhere((remapping < np.median(remapping)) & (norm_diff < 0)).flatten()
        increase_cell_ids = np.argwhere((remapping < np.median(remapping)) & (norm_diff > 0)).flatten()

        # create dictionary with labels
        cell_class_dic = {
            "stable_cell_ids": stable_cell_ids,
            "decrease_cell_ids": decrease_cell_ids,
            "increase_cell_ids": increase_cell_ids
        }

        with open(self.params.pre_proc_dir+"cell_classification/"+
                  self.session_name+"_remapping.pickle", "wb") as f:
            pickle.dump(cell_class_dic, f, pickle.HIGHEST_PROTOCOL)

    def firing_rate_ratios_pre_post_all_cells(self, measure="mean", plotting=False,
                                              use_chunks = True, chunks_in_min=5):

        # get rasters from exploration before/after & sleep
        raster_pre = self.pre.get_raster()
        raster_post = self.post.get_raster()

        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_cells = class_dic["stable_cell_ids"].flatten()
        inc_cells = class_dic["increase_cell_ids"].flatten()
        dec_cells = class_dic["decrease_cell_ids"].flatten()

        # for cell_id in dec_cells:
        #     pre_dec = raster_pre[cell_id, :]
        #     post_dec = raster_post[cell_id, :]
        #
        #     ratio_pre_post = \
        #         np.mean(post_dec) - np.mean(pre_dec) / (np.mean(post_dec) + np.mean(pre_dec))
        #
        #     if ratio_pre_post > 0:
        #         p_bef = 1. * np.arange(pre_dec.shape[0]) / (pre_dec.shape[0] - 1)
        #         p_aft = 1. * np.arange(post_dec.shape[0]) / (post_dec.shape[0] - 1)
        #         plt.plot(np.sort(pre_dec), p_bef, label="PRE")
        #         plt.plot(np.sort(post_dec), p_aft, label="POST")
        #         plt.legend()
        #         x_min, x_max = plt.gca().get_xlim()
        #         plt.text(x_max / 2, 0.5, "mean pre=" + str(np.round(np.mean(pre_dec), 2)))
        #         plt.text(x_max / 2, 0.4, "mean post=" + str(np.round(np.mean(post_dec), 2)))
        #         plt.show()

        if use_chunks:
            chunk_size = int(chunks_in_min/self.params.time_bin_size)

            # go through PRE first
            nr_chunks_pre = int(raster_pre.shape[1]/chunk_size)
            firing_pre = np.zeros((raster_pre.shape[0], nr_chunks_pre))
            for chunk in range(nr_chunks_pre):
                if measure == "mean":
                    firing_pre[:, chunk] = np.mean(
                        raster_pre[:, chunk*chunk_size:(chunk+1)*chunk_size], axis=1)/self.params.time_bin_size
                elif measure == "max":
                    firing_pre[:, chunk] = np.max(
                        raster_pre[:, chunk * chunk_size:(chunk + 1) * chunk_size], axis=1) / self.params.time_bin_size

            # go through POST
            nr_chunks_post = int(raster_post.shape[1]/chunk_size)
            firing_post = np.zeros((raster_post.shape[0], nr_chunks_post))
            for chunk in range(nr_chunks_post):
                if measure == "mean":
                    firing_post[:, chunk] = np.mean(
                        raster_post[:, chunk*chunk_size:(chunk+1)*chunk_size], axis=1)/self.params.time_bin_size
                elif measure == "max":
                    firing_post[:, chunk] = np.max(
                        raster_post[:, chunk * chunk_size:(chunk + 1) * chunk_size], axis=1) / self.params.time_bin_size

            diff = np.mean(firing_post, axis=1)-np.mean(firing_pre, axis=1)
            sum = np.mean(firing_post, axis=1)+np.mean(firing_pre, axis=1)

            ratio_pre_post = diff/sum

        else:
            diff = np.mean(raster_post, axis=1)-np.mean(raster_pre, axis=1)
            sum = np.mean(raster_post, axis=1)+np.mean(raster_pre, axis=1)

            ratio_pre_post = diff/sum

        ratio_stable = ratio_pre_post[stable_cells]
        ratio_dec = ratio_pre_post[dec_cells]
        ratio_inc = ratio_pre_post[inc_cells]

        if plotting:
            plt.hist(ratio_dec, bins=50)
            plt.ylabel("#cells")
            plt.xlabel("Mean_post - Mean_pre /\n (Mean_post + Mean_pre)")
            plt.title("dec")
            plt.tight_layout()
            plt.xlim(-1,1)
            plt.show()

            plt.hist(ratio_inc, bins=50)
            plt.ylabel("#cells")
            plt.xlabel("Mean_post - Mean_pre /\n (Mean_post + Mean_pre)")
            plt.title("inc")
            plt.tight_layout()
            plt.xlim(-1,1)
            plt.show()

            plt.hist(ratio_stable, bins=50)
            plt.ylabel("#cells")
            plt.xlabel("Mean_post - Mean_pre /\n (Mean_post + Mean_pre)")
            plt.title("stable")
            plt.tight_layout()
            plt.xlim(-1,1)
            plt.show()

        return ratio_stable[~np.isnan(ratio_stable)], ratio_dec[~np.isnan(ratio_dec)], ratio_inc[~np.isnan(ratio_inc)]

    # </editor-fold>

    # <editor-fold desc="Remapping, goal coding & firing rate changes">

    def remapping_pre_post_stable(self, spatial_resolution=5, nr_shuffles=500, plot_results=True,
                                  return_distribution=True, nr_trials_to_use=None):
        """
        Estimates remapping between PRE/POST using correlation of spatial
        maps: per population vector (for each spatial bin) and per cell (for entire rate map)

        :param spatial_resolution: resolution of spatial bins in cm
        :type spatial_resolution: int
        :param nr_shuffles: how many shuffles to use for the control
        :type nr_shuffles: in
        :param plot_results: whether to plot the results
        :type plot_results: bool
        :return: percent_stable_place, p --> how many cells have a stable place field (>2std above mean of shuffle),
                 p-value of Kolmogorov-Smirnov test (data vs. shuffle)
        :rtype: float, float
        """
        if nr_trials_to_use is None:
            # get rate map from PRE
            rate_maps_pre = self.pre.get_rate_maps(spatial_resolution=spatial_resolution)
            env_dim_pre = self.pre.get_env_dim()
            occ_map_pre = self.pre.get_occ_map(spatial_resolution=spatial_resolution)
            # need to adjust dimensions of post rate map to compute overlap
            rate_maps_post = self.post.get_rate_maps(spatial_resolution=spatial_resolution, env_dim=env_dim_pre)
            occ_map_post = self.post.get_occ_map(spatial_resolution=spatial_resolution, env_dim=env_dim_pre)
        else:
            nr_trials_pre = self.pre.get_nr_of_trials()
            rate_maps_pre = self.pre.get_rate_maps(spatial_resolution=spatial_resolution,
                                                   trials_to_use=range(nr_trials_pre - nr_trials_to_use, nr_trials_pre))
            env_dim_pre = self.pre.get_env_dim()
            occ_map_pre = self.pre.get_occ_map(spatial_resolution=spatial_resolution,
                                               trials_to_use=range(nr_trials_pre - nr_trials_to_use, nr_trials_pre))

            # need to adjust dimensions of post rate map to compute overlap
            rate_maps_post = self.post.get_rate_maps(spatial_resolution=spatial_resolution, env_dim=env_dim_pre,
                                                     trials_to_use=range(0, nr_trials_to_use))
            occ_map_post = self.post.get_occ_map(spatial_resolution=spatial_resolution, env_dim=env_dim_pre,
                                                 trials_to_use=range(0, nr_trials_to_use))

        # load cell labels
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_"+self.params.stable_cell_method+".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_cells = class_dic["stable_cell_ids"].flatten()
        nr_stable_cells = stable_cells.shape[0]

        # compute remapping based on population vectors per bin
        # --------------------------------------------------------------------------------------------------------------
        remapping_pv = []

        pop_vec_pre = np.reshape(rate_maps_pre, (rate_maps_pre.shape[0]*rate_maps_pre.shape[1], rate_maps_pre.shape[2]))
        pop_vec_post = np.reshape(rate_maps_post, (rate_maps_post.shape[0] * rate_maps_post.shape[1],
                                                  rate_maps_post.shape[2]))

        # only select spatial bins that were visited in PRE and POST
        comb_occ_map = np.logical_and(occ_map_pre.flatten() > 0, occ_map_post.flatten() > 0)
        pop_vec_pre = pop_vec_pre[comb_occ_map, :]
        pop_vec_post = pop_vec_post[comb_occ_map, :]

        pop_vec_pre_stable = pop_vec_pre[:, stable_cells]
        pop_vec_post_stable = pop_vec_post[:, stable_cells]

        for pre, post in zip(pop_vec_pre_stable, pop_vec_post_stable):
            remapping_pv.append(pearsonr(pre.flatten(), post.flatten())[0])

        remapping_pv = np.nan_to_num(np.array(remapping_pv))

        remapping_pv_shuffle = []
        for i in range(nr_shuffles):
            shuffle_res = []
            per_ind = np.random.permutation(np.arange(pop_vec_post_stable.shape[0]))
            shuffled_pop_vec_post = pop_vec_post_stable[per_ind,:]
            for pre, post in zip(pop_vec_pre_stable, shuffled_pop_vec_post):
                shuffle_res.append(pearsonr(pre.flatten(), post.flatten())[0])
            remapping_pv_shuffle.append(shuffle_res)

        remapping_pv_shuffle = np.array(remapping_pv_shuffle)
        remapping_pv_shuffle_flat = remapping_pv_shuffle.flatten()

        data_sorted = np.sort(remapping_pv)
        shuffle_sorted = np.sort(remapping_pv_shuffle_flat)

        # compute statistics
        _, p = ks_2samp(remapping_pv, remapping_pv_shuffle_flat)

        # --------------------------------------------------------------------------------------------------------------
        # compute remapping (correlation of PRE and POST rate maps)
        remapping = []

        for pre, post in zip(rate_maps_pre.T, rate_maps_post.T):
            remapping.append(pearsonr(pre.flatten(), post.flatten())[0])

        remapping = np.array(remapping)
        # compute shuffled data
        remapping_shuffle = []
        for pre, post in zip(rate_maps_pre.T, rate_maps_post.T):
            shuffle_list = []
            post_flat = post.flatten()
            for i in range(nr_shuffles):
                np.random.shuffle(post_flat)
                shuffle_list.append(pearsonr(pre.flatten(), post_flat)[0])
            remapping_shuffle.append(shuffle_list)
        remapping_shuffle = np.vstack(remapping_shuffle)

        remapping_stable = remapping[stable_cells]
        remapping_shuffle_stable = remapping_shuffle[stable_cells,:]

        # check how many cells did not remapped
        const = 0
        for data, control in zip(remapping_stable, remapping_shuffle_stable):
            # if data is 2 std above the mean of control --> no significant remapping
            if data > np.mean(control)+2*np.std(control):
                const += 1

        percent_stable_place = np.round(const / nr_stable_cells * 100, 2)

        binwidth = 0.05
        min_val = min(np.hstack((remapping_shuffle[stable_cells,:].flatten(), remapping[stable_cells].flatten())))
        max_val = max(np.hstack((remapping_shuffle[stable_cells, :].flatten(), remapping[stable_cells].flatten())))

        if plot_results:
            plt.hist(remapping_shuffle[stable_cells,:].flatten(), color="gray", density=True,
                     label="SHUFFLE (n="+str(nr_shuffles)+")", bins=np.arange(min_val, max_val + binwidth, binwidth))
            plt.hist(remapping[stable_cells], label="STABLE CELLS", density=True,
                     bins=np.arange(min_val, max_val + binwidth, binwidth), color="#ffdba1", alpha=0.8)
            plt.legend()
            plt.xlabel("PEARSON R: RATE MAP PRE - RATE MAP POST")
            plt.ylabel("DENSITY")
            plt.title("% CELLS WITH CONSTANT PLACE FIELDS: "+str(percent_stable_place))
            plt.show()

            # calculate the proportional values of samples
            p_data = 1. * np.arange(data_sorted.shape[0]) / (data_sorted.shape[0] - 1)
            p_shuffle = 1. * np.arange(shuffle_sorted.shape[0]) / (shuffle_sorted.shape[0] - 1)

            plt.plot(data_sorted, p_data, label="DATA")
            plt.plot(shuffle_sorted, p_shuffle, label="SHUFFLE")
            plt.legend()
            plt.ylabel("CDF")
            plt.xlabel("PEARSON R")
            plt.title("KS, p-value = " + str(np.round(p, 0)))
            plt.show()

        else:
            if return_distribution:
                return remapping_stable, remapping_shuffle_stable, remapping_pv, remapping_pv_shuffle_flat
            else:
                return percent_stable_place, p

    def remapping_pre_post_all_subsets(self, spatial_resolution=5, nr_shuffles=500, plotting=True,
                                       nr_trials_to_use=None, normalized=False, min_mean_firing_rate=None):
        """
        Estimates remapping between PRE/POST using correlation of spatial
        maps: per population vector (for each spatial bin) and per cell (for entire rate map)

        :param spatial_resolution: resolution of spatial bins in cm
        :type spatial_resolution: int
        :param nr_shuffles: how many shuffles to use for the control
        :type nr_shuffles: in
        :param plot_results: whether to plot the results
        :type plot_results: bool
        :return: percent_stable_place, p --> how many cells have a stable place field (>2std above mean of shuffle),
                 p-value of Kolmogorov-Smirnov test (data vs. shuffle)
        :rtype: float, float
        """
        if nr_trials_to_use is None:
            # get rate map from PRE
            rate_maps_pre = self.pre.get_rate_maps(spatial_resolution=spatial_resolution)
            env_dim_pre = self.pre.get_env_dim()
            occ_map_pre = self.pre.get_occ_map(spatial_resolution=spatial_resolution)
            # need to adjust dimensions of post rate map to compute overlap
            rate_maps_post = self.post.get_rate_maps(spatial_resolution=spatial_resolution, env_dim=env_dim_pre)
            occ_map_post = self.post.get_occ_map(spatial_resolution=spatial_resolution, env_dim=env_dim_pre)
        else:
            nr_trials_pre = self.pre.get_nr_of_trials()
            rate_maps_pre = self.pre.get_rate_maps(spatial_resolution=spatial_resolution,
                                                   trials_to_use=range(nr_trials_pre - nr_trials_to_use, nr_trials_pre))
            env_dim_pre = self.pre.get_env_dim()
            occ_map_pre = self.pre.get_occ_map(spatial_resolution=spatial_resolution,
                                               trials_to_use=range(nr_trials_pre - nr_trials_to_use, nr_trials_pre))

            # need to adjust dimensions of post rate map to compute overlap
            rate_maps_post = self.post.get_rate_maps(spatial_resolution=spatial_resolution, env_dim=env_dim_pre,
                                                     trials_to_use=range(0, nr_trials_to_use))
            occ_map_post = self.post.get_occ_map(spatial_resolution=spatial_resolution, env_dim=env_dim_pre,
                                                 trials_to_use=range(0, nr_trials_to_use))

        # load cell labels
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_"+self.params.stable_cell_method+".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_cells = class_dic["stable_cell_ids"].flatten()
        dec_cells = class_dic["decrease_cell_ids"].flatten()
        inc_cells = class_dic["increase_cell_ids"].flatten()
        nr_stable_cells = stable_cells.shape[0]
        nr_dec_cells = dec_cells.shape[0]
        nr_inc_cells = inc_cells.shape[0]

        # compute remapping based on population vectors per bin
        # --------------------------------------------------------------------------------------------------------------

        pop_vec_pre = np.reshape(rate_maps_pre, (rate_maps_pre.shape[0]*rate_maps_pre.shape[1], rate_maps_pre.shape[2]))
        pop_vec_post = np.reshape(rate_maps_post, (rate_maps_post.shape[0] * rate_maps_post.shape[1],
                                                  rate_maps_post.shape[2]))

        # only select spatial bins that were visited in PRE and POST
        comb_occ_map = np.logical_and(occ_map_pre.flatten() > 0, occ_map_post.flatten() > 0)
        pop_vec_pre = pop_vec_pre[comb_occ_map, :]
        pop_vec_post = pop_vec_post[comb_occ_map, :]

        # select subsets: stable
        pop_vec_pre_stable = pop_vec_pre[:, stable_cells]
        pop_vec_post_stable = pop_vec_post[:, stable_cells]
        # decreasing
        pop_vec_pre_dec = pop_vec_pre[:, dec_cells]
        pop_vec_post_dec = pop_vec_post[:, dec_cells]
        # increasing
        pop_vec_pre_inc = pop_vec_pre[:, inc_cells]
        pop_vec_post_inc = pop_vec_post[:, inc_cells]

        if normalized:
            # use normalized population vectors --> z-score firing rate for each cell across bins
            good_cells_stable = np.logical_and(np.sum(pop_vec_pre_stable, axis=0)>0, np.sum(pop_vec_post_stable, axis=0)>0)
            if not min_mean_firing_rate is None:
                stable_cells_above_min = np.logical_and(np.mean(pop_vec_pre_stable, axis=0)>min_mean_firing_rate,
                                                        np.mean(pop_vec_post_stable, axis=0)>min_mean_firing_rate)
                good_cells_stable = np.logical_and(good_cells_stable, stable_cells_above_min)

            # only select good cells
            pop_vec_pre_stable = pop_vec_pre_stable[:, good_cells_stable]
            pop_vec_post_stable = pop_vec_post_stable[:, good_cells_stable]

            # normalize to 1
            pop_vec_pre_stable = normalize(data=pop_vec_pre_stable, axis=0)
            pop_vec_post_stable = normalize(data=pop_vec_post_stable, axis=0)

            # z-score:
            # pop_vec_pre_stable = zscore(pop_vec_pre_stable, axis=0)[:, good_cells_stable]
            # pop_vec_post_stable = zscore(pop_vec_post_stable, axis=0)[:, good_cells_stable]
            # inc
            good_cells_inc = np.logical_and(np.sum(pop_vec_pre_inc, axis=0)>0, np.sum(pop_vec_post_inc, axis=0)>0)
            if not min_mean_firing_rate is None:
                inc_cells_above_min = np.logical_and(np.mean(pop_vec_pre_inc, axis=0)>min_mean_firing_rate,
                                                        np.mean(pop_vec_post_inc, axis=0)>min_mean_firing_rate)
                good_cells_inc = np.logical_and(good_cells_inc, inc_cells_above_min)

            # only select good cells
            pop_vec_pre_inc = pop_vec_pre_inc[:, good_cells_inc]
            pop_vec_post_inc = pop_vec_post_inc[:, good_cells_inc]

            pop_vec_pre_inc = normalize(data=pop_vec_pre_inc, axis=0)
            pop_vec_post_inc = normalize(data=pop_vec_post_inc, axis=0)
            # z-score
            # pop_vec_pre_inc = zscore(pop_vec_pre_inc, axis=0)[:, good_cells_inc]
            # pop_vec_post_inc = zscore(pop_vec_post_inc, axis=0)[:, good_cells_inc]
            # dec
            good_cells_dec = np.logical_and(np.sum(pop_vec_pre_dec, axis=0)>0, np.sum(pop_vec_post_dec, axis=0)>0)
            if not min_mean_firing_rate is None:
                dec_cells_above_min = np.logical_and(np.mean(pop_vec_pre_dec, axis=0)>min_mean_firing_rate,
                                                     np.mean(pop_vec_post_dec, axis=0)>min_mean_firing_rate)
                good_cells_dec = np.logical_and(good_cells_dec, dec_cells_above_min)

            # only select good cells
            pop_vec_pre_dec = pop_vec_pre_dec[:, good_cells_dec]
            pop_vec_post_dec = pop_vec_post_dec[:, good_cells_dec]

            pop_vec_pre_dec = normalize(data=pop_vec_pre_dec, axis=0)
            pop_vec_post_dec = normalize(data=pop_vec_post_dec, axis=0)
            # z-score
            # pop_vec_pre_dec = zscore(pop_vec_pre_dec, axis=0)[:, good_cells_dec]
            # pop_vec_post_dec = zscore(pop_vec_post_dec, axis=0)[:, good_cells_dec]

        # compute pv correlations pre-post for stable cells
        remapping_pv_stable = []
        for pre, post in zip(pop_vec_pre_stable, pop_vec_post_stable):
            remapping_pv_stable.append(pearsonr(pre.flatten(), post.flatten())[0])
        remapping_pv_stable = np.nan_to_num(np.array(remapping_pv_stable))

        # compute pv correlations pre-post for dec cells
        remapping_pv_dec = []
        for pre, post in zip(pop_vec_pre_dec, pop_vec_post_dec):
            remapping_pv_dec.append(pearsonr(pre.flatten(), post.flatten())[0])
        remapping_pv_dec = np.nan_to_num(np.array(remapping_pv_dec))

        # compute pv correlations pre-post for inc cells
        remapping_pv_inc = []
        for pre, post in zip(pop_vec_pre_inc, pop_vec_post_inc):
            remapping_pv_inc.append(pearsonr(pre.flatten(), post.flatten())[0])
        remapping_pv_inc = np.nan_to_num(np.array(remapping_pv_inc))

        # compute rate map correlations pre-post
        # --------------------------------------------------------------------------------------------------------------
        remapping = []

        for pre, post in zip(rate_maps_pre.T, rate_maps_post.T):
            remapping.append(pearsonr(pre.flatten(), post.flatten())[0])

        remapping = np.hstack(remapping)

        remapping_stable = remapping[stable_cells]
        remapping_dec = remapping[dec_cells]
        remapping_inc = remapping[inc_cells]

        # filter nans
        remapping_stable = remapping_stable[~np.isnan(remapping_stable)]
        remapping_dec = remapping_dec[~np.isnan(remapping_dec)]
        remapping_inc = remapping_inc[~np.isnan(remapping_inc)]

        # compute shuffle for rate map correlations pre-post
        # --------------------------------------------------------------------------------------------------------------
        remapping_shuffle = []
        for pre, post in zip(rate_maps_pre.T, rate_maps_post.T):
            shuffle_list = []
            post_flat = post.flatten()
            for i in range(nr_shuffles):
                np.random.shuffle(post_flat)
                shuffle_list.append(pearsonr(pre.flatten(), post_flat)[0])
            remapping_shuffle.append(shuffle_list)
        remapping_shuffle = np.vstack(remapping_shuffle)
        remapping_shuffle_stable = remapping_shuffle[stable_cells, :]
        remapping_shuffle_dec = remapping_shuffle[dec_cells, :]
        remapping_shuffle_inc = remapping_shuffle[inc_cells, :]

        # check how many cells did not remap: stable
        # --------------------------------------------------------------------------------------------------------------
        const = 0
        for data, control in zip(remapping_stable, remapping_shuffle_stable):
            # if data is 2 std above the mean of control --> no significant remapping
            if data > np.mean(control)+2*np.std(control):
                const += 1
        stable_rate_map_stable = np.round(const / nr_stable_cells * 100, 2)

        # check how many cells did not remap: dec
        # --------------------------------------------------------------------------------------------------------------
        const = 0
        for data, control in zip(remapping_dec, remapping_shuffle_dec):
            # if data is 2 std above the mean of control --> no significant remapping
            if data > np.mean(control)+2*np.std(control):
                const += 1
        stable_rate_map_dec = np.round(const / nr_dec_cells * 100, 2)

        # check how many cells did not remap: inc
        # --------------------------------------------------------------------------------------------------------------
        const = 0
        for data, control in zip(remapping_inc, remapping_shuffle_inc):
            # if data is 2 std above the mean of control --> no significant remapping
            if data > np.mean(control)+2*np.std(control):
                const += 1
        stable_rate_map_inc = np.round(const / nr_inc_cells * 100, 2)


        if plotting:

            c = "white"
            plt.figure(figsize=(3, 4))
            bplot = plt.boxplot([remapping_pv_stable, remapping_pv_dec, remapping_pv_inc],
                                positions=[1, 2, 3], patch_artist=True,
                                labels=["stable", "dec", "inc"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c),
                                )
            colors = ["violet", 'turquoise', "orange"]
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            plt.ylabel("PV correlation: \n Acquisition - Recall")
            plt.grid(color="grey", axis="y")
            plt.ylim(-1, 1)
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.show()

            plt.figure(figsize=(3, 4))
            bplot = plt.boxplot([remapping_stable, remapping_dec, remapping_inc],
                                positions=[1, 2, 3], patch_artist=True,
                                labels=["stable", "dec", "inc"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c),
                                )
            colors = ["violet", 'turquoise', "orange"]
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            plt.ylabel("Rate map correlation: \n Acquisition - Recall")
            plt.grid(color="grey", axis="y")
            plt.xticks(rotation=45)
            plt.ylim(-1, 1)
            plt.tight_layout()
            plt.show()

        print("Stable Place field, stable cells:")
        print(str(stable_rate_map_stable)+"%")
        print("Stable Place field, dec cells:")
        print(str(stable_rate_map_dec)+"%")
        print("Stable Place field, inc cells:")
        print(str(stable_rate_map_inc)+"%")

        return remapping_pv_stable, remapping_pv_dec, remapping_pv_inc, remapping_stable, remapping_dec, remapping_inc, \
            stable_rate_map_stable, stable_rate_map_dec, stable_rate_map_inc



    def remapping_learning_vs_drift(self, spatial_resolution=5, plotting=True, nr_trials_to_use=5):
        """
        Estimates remapping between PRE/POST and between first trials/last trials in PRE using correlation of spatial
        maps: per population vector (for each spatial bin) and per cell (for entire rate map)

        :param spatial_resolution: resolution of spatial bins in cm
        :type spatial_resolution: int
        :param plotting: whether to plot the results
        :type plotting: bool
        :param nr_trials_to_use: trials for analysis (first n and last n trials in PRE)
        :type nr_trials_to_use: int
        :return: remapping_pv_learning, remapping_pv_drift, remapping_rm_learning, remapping_rm_drift --> correlation
                 values for population vectors and rate maps
        :rtype: arrays
        """

        # get rate maps from PRE: beginning and end
        nr_trials_pre = self.pre.get_nr_of_trials()
        rate_maps_pre_start = self.pre.get_rate_maps(spatial_resolution=spatial_resolution,
                                               trials_to_use=range(nr_trials_to_use))
        rate_maps_pre_end = self.pre.get_rate_maps(spatial_resolution=spatial_resolution,
                                               trials_to_use=range(nr_trials_pre - nr_trials_to_use, nr_trials_pre))
        env_dim_pre = self.pre.get_env_dim()
        occ_map_pre_start = self.pre.get_occ_map(spatial_resolution=spatial_resolution,
                                           trials_to_use=range(nr_trials_to_use))
        occ_map_pre_end = self.pre.get_occ_map(spatial_resolution=spatial_resolution,
                                           trials_to_use=range(nr_trials_pre - nr_trials_to_use, nr_trials_pre))

        # get rate maps from POST: need to adjust dimensions of post rate map to compute overlap
        rate_maps_post = self.post.get_rate_maps(spatial_resolution=spatial_resolution, env_dim=env_dim_pre,
                                                 trials_to_use=range(0, nr_trials_to_use))
        occ_map_post = self.post.get_occ_map(spatial_resolution=spatial_resolution, env_dim=env_dim_pre,
                                             trials_to_use=range(0, nr_trials_to_use))

        # compute remapping based on population vectors per bin
        # --------------------------------------------------------------------------------------------------------------

        pop_vec_pre_start = np.reshape(rate_maps_pre_start, (rate_maps_pre_start.shape[0]*rate_maps_pre_start.shape[1],
                                                     rate_maps_pre_start.shape[2]))
        pop_vec_pre_end = np.reshape(rate_maps_pre_end, (rate_maps_pre_end.shape[0]*rate_maps_pre_end.shape[1],
                                                     rate_maps_pre_end.shape[2]))
        pop_vec_post = np.reshape(rate_maps_post, (rate_maps_post.shape[0] * rate_maps_post.shape[1],
                                                  rate_maps_post.shape[2]))

        # only select spatial bins that were visited in PRE and POST
        comb_occ_map = np.logical_and(np.logical_and(occ_map_pre_start.flatten() > 0, occ_map_pre_end.flatten() > 0),
                                      occ_map_post.flatten() > 0)
        pop_vec_pre_start = pop_vec_pre_start[comb_occ_map, :]
        pop_vec_pre_end = pop_vec_pre_end[comb_occ_map, :]
        pop_vec_post = pop_vec_post[comb_occ_map, :]

        # compute remapping during learning
        remapping_pv_learning = []
        for start, end in zip(pop_vec_pre_start, pop_vec_pre_end):
            remapping_pv_learning.append(pearsonr(start.flatten(), end.flatten())[0])

        remapping_pv_learning = np.nan_to_num(np.array(remapping_pv_learning))

        # compute remapping due to drift (PRE-POST)
        remapping_pv_drift = []
        for pre, post in zip(pop_vec_pre_end, pop_vec_post):
            remapping_pv_drift.append(pearsonr(pre.flatten(), post.flatten())[0])

        remapping_pv_drift = np.nan_to_num(np.array(remapping_pv_drift))

        # compute remapping based on correlation of rate maps per cell
        # --------------------------------------------------------------------------------------------------------------

        # learning
        remapping_rm_learning = []
        for start, end in zip(rate_maps_pre_start.T, rate_maps_pre_end.T):
            if np.count_nonzero(start) and np.count_nonzero(end):
                remapping_rm_learning.append(pearsonr(start.flatten(), end.flatten())[0])

        remapping_rm_learning = np.array(remapping_rm_learning)

        # PRE-POST
        remapping_rm_drift = []
        for pre, post in zip(rate_maps_pre_end.T, rate_maps_post.T):
            if np.count_nonzero(pre) and np.count_nonzero(post):
                remapping_rm_drift.append(pearsonr(pre.flatten(), post.flatten())[0])

        remapping_rm_drift = np.array(remapping_rm_drift)

        if plotting:
            c = "white"
            bplot = plt.boxplot([remapping_pv_learning, remapping_pv_drift], positions=[1, 2], patch_artist=True,
                                labels=["Learning", "PRE-POST"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c),
                                )
            colors = ["yellow", 'blue']
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            plt.ylabel("Pop. vec. correlations (Pearson R)")
            plt.grid(color="grey", axis="y")
            plt.ylim(0,1)
            plt.show()


            c = "white"
            bplot = plt.boxplot([remapping_rm_learning, remapping_rm_drift], positions=[1, 2], patch_artist=True,
                                labels=["Learning", "PRE-POST"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c),
                                )
            colors = ["yellow", 'blue']
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            plt.ylabel("Rate map correlations (Pearson R)")
            plt.grid(color="grey", axis="y")
            plt.ylim(0,1)
            plt.show()

        else:
            return remapping_pv_learning, remapping_pv_drift, remapping_rm_learning, remapping_rm_drift

    def remapping_correlates(self, spatial_resolution=5, nr_trials_to_use=None):
        """
        Check which feature correlates with remapping from PRE to POST

        :param spatial_resolution: resolution of spatial bins in cm
        :type spatial_resolution: int
        """
        if nr_trials_to_use is None:
            # get rate map from PRE
            rate_maps_pre = self.pre.get_rate_maps(spatial_resolution=spatial_resolution)
            env_dim_pre = self.pre.get_env_dim()
            # need to adjust dimensions of post rate map to compute overlap
            rate_maps_post = self.post.get_rate_maps(spatial_resolution=spatial_resolution, env_dim=env_dim_pre)
            raster_pre = self.pre.get_raster()
            raster_post = self.post.get_raster()
        else:
            nr_trials_pre = self.pre.get_nr_of_trials()
            rate_maps_pre = self.pre.get_rate_maps(spatial_resolution=spatial_resolution,
                                                   trials_to_use=range(nr_trials_pre - nr_trials_to_use, nr_trials_pre))
            env_dim_pre = self.pre.get_env_dim()

            # need to adjust dimensions of post rate map to compute overlap
            rate_maps_post = self.post.get_rate_maps(spatial_resolution=spatial_resolution, env_dim=env_dim_pre,
                                                     trials_to_use=range(0, nr_trials_to_use))
            raster_pre = self.pre.get_raster(trials_to_use=range(nr_trials_pre - nr_trials_to_use, nr_trials_pre))
            raster_post = self.post.get_raster(trials_to_use=range(0, nr_trials_to_use))

        # --------------------------------------------------------------------------------------------------------------
        # compute remapping (correlation of PRE and POST rate maps)
        remapping = []

        for pre, post in zip(rate_maps_pre.T, rate_maps_post.T):
            if np.count_nonzero(pre) > 0 and np.count_nonzero(post) > 0:
                remapping.append(pearsonr(pre.flatten(), post.flatten())[0])
            else:
                remapping.append(np.nan)

        remapping = np.array(remapping)

        p_value_mwu = []

        for pre, post in zip(raster_pre, raster_post):
            p_value_mwu.append(mannwhitneyu(pre, post)[1])
            # p_value_mwu.append((np.mean(post)-np.mean(pre))/(np.mean(post)+np.mean(pre)))
        p_value_mwu = np.array(p_value_mwu)
        
        p_value_mwu =p_value_mwu[~np.isnan(remapping)]
        remapping = remapping[~np.isnan(remapping)]
        # plt.figure(figsize=(5,1))
        plt.scatter(p_value_mwu, remapping)
        plt.xlabel("p-value MWU")
        plt.ylabel("remapping (R rate map)")
        plt.xscale("log")
        plt.xlim(10e-75, 1)
        plt.title(pearsonr(p_value_mwu, remapping))
        plt.show()

    def firing_rate_changes(self, plotting=True, mean_or_max="mean"):
        """
        Compares average firing rates in PRE and post
        """

        # get raster
        lcb_1_raster = self.pre.get_raster()
        lcb_2_raster = self.post.get_raster()

        # load cell labels
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_"+self.params.stable_cell_method+".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_cells = class_dic["stable_cell_ids"].flatten()
        inc_cells = class_dic["increase_cell_ids"].flatten()
        dec_cells = class_dic["decrease_cell_ids"].flatten()

        if mean_or_max == "mean":
            # compute mean firing rates
            mean_pre = np.mean(lcb_1_raster, axis=1)
            mean_post = np.mean(lcb_2_raster, axis=1)
        elif mean_or_max == "max":
            mean_pre = np.max(lcb_1_raster, axis=1)
            mean_post = np.max(lcb_2_raster, axis=1)


        # plot results
        # --------------------------------------------------------------------------------------------------------------
        if plotting:
            plt.scatter(mean_pre[stable_cells], mean_post[stable_cells], c="#ffdba1", label="STABLE")
            plt.scatter(mean_pre[inc_cells], mean_post[inc_cells], c="#f7959c", label="INCREASING")
            plt.scatter(mean_pre[dec_cells], mean_post[dec_cells], c="#a0c4e4", label="DECREASING")
            plt.xlabel("MEAN FIRING PRE")
            plt.ylabel("MEAN FIRING POST")
            plt.legend()
            plt.show()

            plt.hist((mean_post[dec_cells] - mean_pre[dec_cells]) / (mean_post[dec_cells] + mean_pre[dec_cells]),
                     color="#a0c4e4", label="DECREASING", density=True, edgecolor='blue')
            plt.hist((mean_post[inc_cells] - mean_pre[inc_cells]) / (mean_post[inc_cells] + mean_pre[inc_cells]),
                     color="#f7959c", label="INCREASING", density=True, alpha=0.8, edgecolor='red')
            plt.hist((mean_post[stable_cells] - mean_pre[stable_cells]) / (mean_post[stable_cells] +
                                                                           mean_pre[stable_cells]), color="#ffdba1",
                     label="STABLE", density=True, alpha=0.7, edgecolor='yellow')

            matplotlib.rcParams['mathtext.default'] = 'regular'

            matplotlib.rcParams['text.usetex'] = True
            plt.xlabel(r'$\frac{\overline{firing\_rate}_{POST} - '
                       r'\overline{firing\_rate}_{PRE}}{\overline{firing\_rate}_{POST} + \overline{firing\_rate}_{PRE}}$')
            plt.ylabel("DENSITY")
            plt.ylim(0, 1.4)
            plt.legend(loc="upper left")
            plt.show()
        else:
            return mean_pre[stable_cells], mean_pre[dec_cells], mean_post[stable_cells], mean_post[inc_cells]

    def firing_rates(self, plotting=True, mean_or_max="mean", cells_to_use="all"):
        """
        Comutes mean or max firing rates in pre and post
        """
        # load cell labels
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle", "rb") as f:
            class_dic = pickle.load(f)

        # get raster
        pre_raster = self.pre.get_raster()
        post_raster = self.post.get_raster()

        if mean_or_max == "mean":
            firing_pre = np.mean(pre_raster, axis=1)/self.params.time_bin_size
            firing_post = np.mean(post_raster, axis=1)/self.params.time_bin_size
        elif mean_or_max == "max":
            firing_pre = np.max(pre_raster, axis=1)/self.params.time_bin_size
            firing_post = np.max(post_raster, axis=1)/self.params.time_bin_size

        if cells_to_use == "stable":
            cell_ids = class_dic["stable_cell_ids"].flatten()
        elif cells_to_use == "increasing":
            cell_ids = class_dic["increase_cell_ids"].flatten()
        elif cells_to_use == "decreasing":
            cell_ids = class_dic["decrease_cell_ids"].flatten()

        if cells_to_use is not None:
            firing_pre = firing_pre[cell_ids]
            firing_post = firing_post[cell_ids]

        return np.nan_to_num(firing_pre), np.nan_to_num(firing_post)

    def nr_goals_coded(self, spatial_resolution=1, plotting=True, radius=20, single_cells=True, mean_firing_thresh=None):

        # load cell labels (stable, increasing, decreasing)
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_"+self.params.stable_cell_method+".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_cells = class_dic["stable_cell_ids"].flatten()
        inc_cells = class_dic["increase_cell_ids"].flatten()
        dec_cells = class_dic["decrease_cell_ids"].flatten()

        # get goal locations
        g_l = self.pre.get_goal_locations()

        # --------------------------------------------------------------------------------------------------------------
        # PRE
        # --------------------------------------------------------------------------------------------------------------

        # get rate maps and occupancy maps at specified spatial resolution
        rate_maps_pre = self.pre.get_rate_maps(spatial_resolution=spatial_resolution)
        occ_map_pre = self.pre.get_occ_map(spatial_resolution=spatial_resolution)
        x_min,_, y_min,_= self.pre.get_env_dim()
        _,loc,_ = self.pre.get_raster_location_speed()

        # look at cells separately
        if single_cells:
            # look at stable cells first
            rate_maps_pre_stable = rate_maps_pre[:,:,stable_cells]
            per_cell_nr_goals_coded_pre_stable = nr_goals_coded_per_cell(rate_maps=rate_maps_pre_stable, occ_map=occ_map_pre,
                                                                     goal_locations=g_l, env_x_min=x_min, env_y_min=y_min,
                                                                     radius=radius, plotting=False)

            # look at decreasing cells
            rate_maps_pre_dec = rate_maps_pre[:, :, dec_cells]
            per_cell_nr_goals_coded_pre_dec = nr_goals_coded_per_cell(rate_maps=rate_maps_pre_dec, occ_map=occ_map_pre,
                                                                     goal_locations=g_l, env_x_min=x_min, env_y_min=y_min,
                                                                     radius=radius, plotting=False)

            if mean_firing_thresh is not None:
                raster_pre = self.pre.get_raster()
                mean_fir = np.mean(raster_pre, axis=1)/self.params.time_bin_size

                mean_fir_stable = mean_fir[stable_cells]
                mean_fir_dec = mean_fir[dec_cells]

                per_cell_nr_goals_coded_pre_stable = per_cell_nr_goals_coded_pre_stable[mean_fir_stable > mean_firing_thresh]
                per_cell_nr_goals_coded_pre_dec = per_cell_nr_goals_coded_pre_dec[
                    mean_fir_dec > mean_firing_thresh]



            if plotting:
                plt.hist(per_cell_nr_goals_coded_pre_stable, density=True, color="#ffdba1", label="STABLE")
                plt.hist(per_cell_nr_goals_coded_pre_dec, density=True, color="#a0c4e4", label="DECREASING", alpha=0.6)
                plt.title("GOAL CODING: PRE")
                plt.legend()
                plt.show()

            # --------------------------------------------------------------------------------------------------------------
            # POST
            # --------------------------------------------------------------------------------------------------------------
            rate_maps_post = self.post.get_rate_maps(spatial_resolution=spatial_resolution)
            occ_map_post = self.post.get_occ_map(spatial_resolution=spatial_resolution)
            x_min,_, y_min,_= self.post.get_env_dim()
            _,loc,_ = self.post.get_raster_location_speed()

            # look at stable cells first
            rate_maps_post_stable = rate_maps_post[:,:,stable_cells]
            per_cell_nr_goals_coded_post_stable = nr_goals_coded_per_cell(rate_maps=rate_maps_post_stable, occ_map=occ_map_post,
                                                                     goal_locations=g_l, env_x_min=x_min, env_y_min=y_min,
                                                                     radius=radius, plotting=False)

            # look at inc cells next
            rate_maps_post_inc = rate_maps_post[:, :, inc_cells]
            per_cell_nr_goals_coded_post_inc = nr_goals_coded_per_cell(rate_maps=rate_maps_post_inc, occ_map=occ_map_post,
                                                                     goal_locations=g_l, env_x_min=x_min, env_y_min=y_min,
                                                                     radius=radius, plotting=False)

            if mean_firing_thresh is not None:
                raster_post = self.post.get_raster()
                mean_fir = np.mean(raster_post, axis=1) / self.params.time_bin_size

                mean_fir_stable = mean_fir[stable_cells]
                mean_fir_inc = mean_fir[inc_cells]

                per_cell_nr_goals_coded_post_stable = per_cell_nr_goals_coded_post_stable[
                    mean_fir_stable > mean_firing_thresh]
                per_cell_nr_goals_coded_post_dec = per_cell_nr_goals_coded_post_inc[
                    mean_fir_inc > mean_firing_thresh]

            if plotting:
                plt.hist(per_cell_nr_goals_coded_post_stable, density=True, color="#ffdba1", label="STABLE")
                plt.hist(per_cell_nr_goals_coded_post_inc, density=True, color="#f7959c", label="INCREASING", alpha=0.6)
                plt.title("GOAL CODING: POST")
                plt.legend()
                plt.show()

            return np.median(per_cell_nr_goals_coded_pre_stable), np.median(per_cell_nr_goals_coded_pre_dec), \
                   np.median(per_cell_nr_goals_coded_post_stable), np.median(per_cell_nr_goals_coded_post_inc)

        # look at summary rate map (mean)
        else:
            # ----------------------------------------------------------------------------------------------------------
            # PRE
            # ----------------------------------------------------------------------------------------------------------
            rate_maps_pre_stable = rate_maps_pre[:,:,stable_cells]
            nr_goals_coded_pre_stable = nr_goals_coded_subset_of_cells(rate_maps=rate_maps_pre_stable,
                                                                         occ_map=occ_map_pre,
                                                                         goal_locations=g_l, env_x_min=x_min,
                                                                         env_y_min=y_min,
                                                                         radius=radius, plotting=False)

            # look at decreasing cells
            rate_maps_pre_dec = rate_maps_pre[:, :, dec_cells]
            nr_goals_coded_pre_dec = nr_goals_coded_subset_of_cells(rate_maps=rate_maps_pre_dec, occ_map=occ_map_pre,
                                                                      goal_locations=g_l, env_x_min=x_min,
                                                                      env_y_min=y_min,
                                                                      radius=radius, plotting=False)

            # ----------------------------------------------------------------------------------------------------------
            # POST
            # ----------------------------------------------------------------------------------------------------------
            rate_maps_post = self.post.get_rate_maps(spatial_resolution=spatial_resolution)
            occ_map_post = self.post.get_occ_map(spatial_resolution=spatial_resolution)
            x_min, _, y_min, _ = self.post.get_env_dim()
            _, loc, _ = self.post.get_raster_location_speed()

            # look at stable cells first
            rate_maps_post_stable = rate_maps_post[:, :, stable_cells]
            nr_goals_coded_post_stable = nr_goals_coded_subset_of_cells(rate_maps=rate_maps_post_stable,
                                                                          occ_map=occ_map_post,
                                                                          goal_locations=g_l, env_x_min=x_min,
                                                                          env_y_min=y_min,
                                                                          radius=radius, plotting=False)

            # look at inc cells next
            rate_maps_post_inc = rate_maps_post[:, :, inc_cells]
            nr_goals_coded_post_inc = nr_goals_coded_subset_of_cells(rate_maps=rate_maps_post_inc,
                                                                       occ_map=occ_map_post,
                                                                       goal_locations=g_l, env_x_min=x_min,
                                                                       env_y_min=y_min,
                                                                       radius=radius, plotting=False)

            return nr_goals_coded_pre_stable, nr_goals_coded_pre_dec, \
                   nr_goals_coded_post_stable, nr_goals_coded_post_inc

    def goal_coding(self, spatial_resolution=1, plotting=True, r=20):

        # load cell labels (stable, increasing, decreasing)
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_"+self.params.stable_cell_method+".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_cells = class_dic["stable_cell_ids"].flatten()
        inc_cells = class_dic["increase_cell_ids"].flatten()
        dec_cells = class_dic["decrease_cell_ids"].flatten()

        # get goal locations
        g_l = self.session_params.goal_locations

        # --------------------------------------------------------------------------------------------------------------
        # PRE
        # --------------------------------------------------------------------------------------------------------------

        # get rate maps and occupancy maps at specified spatial resolution
        rate_maps_pre = self.pre.get_rate_maps(spatial_resolution=spatial_resolution)
        occ_map_pre = self.pre.get_occ_map(spatial_resolution=spatial_resolution)
        x_min,_, y_min,_= self.pre.get_env_dim()
        _,loc,_ = self.pre.get_raster_location_speed()

        # look at stable cells first
        rate_maps_pre_stable = rate_maps_pre[:,:,stable_cells]

        per_cell_goal_coding_pre_stable=goal_coding_per_cell(rate_maps=rate_maps_pre_stable, occ_map=occ_map_pre,
                                                              goal_locations=g_l, env_x_min=x_min, env_y_min=y_min,
                                                              radius=r, plotting=False)

            # look at stable cells first
        rate_maps_pre_dec = rate_maps_pre[:, :, dec_cells]
        per_cell_goal_coding_pre_dec = goal_coding_per_cell(rate_maps=rate_maps_pre_dec, occ_map=occ_map_pre,
                                                              goal_locations=g_l, env_x_min=x_min, env_y_min=y_min,
                                                              radius=r, plotting=False)

        if plotting:

            gc_pre_stable = np.sort(per_cell_goal_coding_pre_stable)
            gc_pre_dec = np.sort(per_cell_goal_coding_pre_dec)
            # calculate the proportional values of samples
            p_gc_pre_stable = 1. * np.arange(gc_pre_stable.shape[0]) / (gc_pre_stable.shape[0] - 1)
            p_gc_pre_dec = 1. * np.arange(gc_pre_dec.shape[0]) / (gc_pre_dec.shape[0] - 1)
            p_mwu = mannwhitneyu(gc_pre_stable, gc_pre_dec)[1]

            plt.plot(gc_pre_stable, p_gc_pre_stable, color="#ffdba1", label="STABLE")
            plt.plot(gc_pre_dec, p_gc_pre_dec, color="#a0c4e4", label="DECREASING")
            plt.gca().set_xscale("log")
            plt.ylabel("CDF")
            plt.xlabel("GOAL CODING")
            plt.title("PRE: STABLE CELLS vs. INC. CELLS \n" + "MWU , p-value = " + str(np.round(p_mwu, 5)))
            plt.legend()
            plt.show()


        # --------------------------------------------------------------------------------------------------------------
        # PRE
        # --------------------------------------------------------------------------------------------------------------

        rate_maps_post = self.post.get_rate_maps(spatial_resolution=spatial_resolution)
        occ_map_post = self.post.get_occ_map(spatial_resolution=spatial_resolution)
        x_min,_, y_min,_= self.post.get_env_dim()
        _,loc,_ = self.post.get_raster_location_speed()

        # look at stable cells first
        rate_maps_post_stable = rate_maps_post[:,:,stable_cells]
        per_cell_goal_coding_post_stable = goal_coding_per_cell(rate_maps=rate_maps_post_stable, occ_map=occ_map_post,
                                                              goal_locations=g_l, env_x_min=x_min, env_y_min=y_min,
                                                              radius=20, plotting=False)
        # look at inc cells
        rate_maps_post_inc = rate_maps_post[:,:,inc_cells]
        per_cell_goal_coding_post_inc = goal_coding_per_cell(rate_maps=rate_maps_post_inc, occ_map=occ_map_post,
                                                                goal_locations=g_l, env_x_min=x_min, env_y_min=y_min,
                                                                radius=20, plotting=False)

        if plotting:


            gc_post_stable = np.sort(per_cell_goal_coding_post_stable)
            gc_post_inc = np.sort(per_cell_goal_coding_post_inc)
            # calculate the proportional values of samples
            p_gc_post_stable = 1. * np.arange(gc_post_stable.shape[0]) / (gc_post_stable.shape[0] - 1)
            p_gc_post_inc = 1. * np.arange(gc_post_inc.shape[0]) / (gc_post_inc.shape[0] - 1)
            p_mwu = mannwhitneyu(gc_post_stable, gc_post_inc)[1]

            plt.plot(gc_post_stable, p_gc_post_stable, color="#ffdba1", label="STABLE")
            plt.plot(gc_post_inc, p_gc_post_inc, color="#f7959c", label="INCREASING")
            plt.gca().set_xscale("log")
            plt.ylabel("CDF")
            plt.xlabel("GOAL CODING")
            plt.title("POST: STABLE CELLS vs. INC. CELLS \n" + "MWU , p-value = " + str(np.round(p_mwu, 5)))
            plt.legend()
            plt.show()


            gc_post = np.sort(per_cell_goal_coding_post_stable)
            gc_pre = np.sort(per_cell_goal_coding_pre_stable)
            # calculate the proportional values of samples
            p_gc_post = 1. * np.arange(gc_post.shape[0]) / (gc_post.shape[0] - 1)
            p_gc_pre = 1. * np.arange(gc_pre.shape[0]) / (gc_pre.shape[0] - 1)
            p_mwu = mannwhitneyu(gc_post, gc_pre)[1]

            plt.plot(gc_post, p_gc_post, label="POST", color="r")
            plt.plot(gc_pre, p_gc_pre, label="PRE", color="b")
            plt.ylabel("CDF")
            plt.xlabel("GOAL CODING")
            plt.gca().set_xscale("log")
            plt.title("STABLE CELLS \n" + "MWU , p-value = " + str(np.round(p_mwu, 5)))
            plt.legend()
            plt.show()

        return np.median(per_cell_goal_coding_pre_stable), np.median(per_cell_goal_coding_post_stable), \
                np.median(per_cell_goal_coding_pre_dec), np.median(per_cell_goal_coding_post_inc)

    def distance_peak_firing_closest_goal(self, plotting=True):
        # load all rate maps
        rate_maps_pre = self.pre.get_rate_maps(spatial_resolution=1)
        rate_maps_post = self.post.get_rate_maps(spatial_resolution=1)

        # get goal locations
        g_l = self.pre.get_goal_locations()

        x_min_pre,_, y_min_pre,_= self.pre.get_env_dim()
        x_min_post, _, y_min_post, _ = self.post.get_env_dim()

        # load cell labels
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle",
                  "rb") as f:
            class_dic = pickle.load(f)

        stable_cell_ids = class_dic["stable_cell_ids"].flatten()

        inc_cell_ids = class_dic["increase_cell_ids"].flatten()

        dec_cell_ids = class_dic["decrease_cell_ids"].flatten()

        rate_maps_stable_pre = rate_maps_pre[:,:,stable_cell_ids]
        rate_maps_dec_pre = rate_maps_pre[:,:,dec_cell_ids]

        rate_maps_stable_post = rate_maps_post[:,:,stable_cell_ids]
        rate_maps_inc_post = rate_maps_post[:,:, inc_cell_ids]

        min_distances_stable_pre = distance_peak_firing_to_closest_goal(rate_maps=rate_maps_stable_pre,
                                                                    goal_locations=g_l,
                                                                    env_x_min=x_min_pre, env_y_min=y_min_pre)

        min_distances_dec_pre = distance_peak_firing_to_closest_goal(rate_maps=rate_maps_dec_pre,
                                                                    goal_locations=g_l,
                                                                    env_x_min=x_min_pre, env_y_min=y_min_pre)

        min_distances_stable_post = distance_peak_firing_to_closest_goal(rate_maps=rate_maps_stable_post,
                                                                    goal_locations=g_l,
                                                                    env_x_min=x_min_post, env_y_min=y_min_post)

        min_distances_inc_post = distance_peak_firing_to_closest_goal(rate_maps=rate_maps_inc_post,
                                                                    goal_locations=g_l,
                                                                    env_x_min=x_min_post, env_y_min=y_min_post)


        if plotting:
            stable_sorted_pre = np.sort(min_distances_stable_pre)
            dec_sorted_pre = np.sort(min_distances_dec_pre)
            inc_sorted_post = np.sort(min_distances_inc_post)
            stable_sorted_post = np.sort(min_distances_stable_post)

            p_stable_pre = 1. * np.arange(stable_sorted_pre.shape[0]) / (stable_sorted_pre.shape[0] - 1)
            p_dec_pre = 1. * np.arange(dec_sorted_pre.shape[0]) / (dec_sorted_pre.shape[0] - 1)
            p_inc_post = 1. * np.arange(inc_sorted_post.shape[0]) / (inc_sorted_post.shape[0] - 1)
            p_stable_post = 1. * np.arange(stable_sorted_post.shape[0]) / (stable_sorted_post.shape[0] - 1)

            plt.plot(stable_sorted_pre, p_stable_pre, color="#ffdba1", label="STABLE")
            plt.plot(dec_sorted_pre, p_dec_pre, color="#a0c4e4", label="DECREASING")

            plt.legend()
            plt.xlabel("Min. distance: peak firing loc. to closest goal / cm")
            plt.ylabel("CDF")
            plt.title("PRE")
            plt.show()

            plt.plot(stable_sorted_post, p_stable_post, color="#ffdba1", label="STABLE")
            plt.plot(inc_sorted_post, p_inc_post, color="red", label="INCREASING")

            plt.legend()
            plt.xlabel("Min. distance: peak firing loc. to closest goal / cm")
            plt.ylabel("CDF")
            plt.title("POST")
            plt.show()

            plt.plot(dec_sorted_pre, p_dec_pre, color="#a0c4e4", label="DECREASING")
            plt.plot(inc_sorted_post, p_inc_post, color="red", label="INCREASING")

            plt.legend()
            plt.xlabel("Min. distance: peak firing loc. to closest goal / cm")
            plt.ylabel("CDF")
            plt.title("PRE-POST")
            plt.show()

            plt.plot(stable_sorted_pre, p_stable_pre, color="b", label="PRE")
            plt.plot(stable_sorted_post, p_stable_post, color="r", label="POST")

            plt.legend()
            plt.xlabel("Min. distance: peak firing loc. to closest goal / cm")
            plt.ylabel("CDF")
            plt.title("STABLE CELLS: PRE-POST")
            plt.show()

        else:
            return min_distances_stable_pre, min_distances_dec_pre, min_distances_stable_post, min_distances_inc_post

    # </editor-fold>

    # <editor-fold desc="Spatial coding">

    def spatial_information_per_cell(self, spatial_resolution=1, only_visited=True, plotting=True,
                                     info_measure="sparsity", return_p_values=False, min_firing_rate=0.1):
        """
        computes spatial information per cell using Skaggs information or sparsity

        @param spatial_resolution: spatial resolution of rate maps to be used to compute spatial information
        @type spatial_resolution: int
        @param only_visited: whether to only use visited spatial bins
        @type only_visited: bool
        """

        # load cell labels (stable, increasing, decreasing)
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_"+self.params.stable_cell_method+".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_cells = class_dic["stable_cell_ids"].flatten()
        inc_cells = class_dic["increase_cell_ids"].flatten()
        dec_cells = class_dic["decrease_cell_ids"].flatten()

        # get raster
        raster_pre = self.pre.get_raster()
        raster_post = self.post.get_raster()
        # compute mean firing
        mean_fir = np.mean(np.hstack((raster_pre, raster_post)), axis=1)/self.params.time_bin_size

        # filter low firing cells
        if min_firing_rate > 0:
            dec_cells = dec_cells[mean_fir[dec_cells] > min_firing_rate]
            stable_cells = stable_cells[mean_fir[stable_cells] > min_firing_rate]
            inc_cells = inc_cells[mean_fir[inc_cells] > min_firing_rate]

        # get rate maps and occupancy maps at specified spatial resolution
        rate_maps_pre = self.pre.get_rate_maps(spatial_resolution=spatial_resolution)
        rate_maps_post = self.post.get_rate_maps(spatial_resolution=spatial_resolution)
        occ_map_pre = self.pre.get_occ_map(spatial_resolution=spatial_resolution)
        occ_map_post = self.post.get_occ_map(spatial_resolution=spatial_resolution)

        # reshape --> 2D bins to vector
        rate_maps_pre = np.reshape(rate_maps_pre,
                                   (rate_maps_pre.shape[0] * rate_maps_pre.shape[1], rate_maps_pre.shape[2]))
        occ_map_pre = np.reshape(occ_map_pre, (occ_map_pre.shape[0] * occ_map_pre.shape[1]))
        rate_maps_post = np.reshape(rate_maps_post,
                                   (rate_maps_post.shape[0] * rate_maps_post.shape[1], rate_maps_post.shape[2]))
        occ_map_post = np.reshape(occ_map_post, (occ_map_post.shape[0] * occ_map_post.shape[1]))

        # compute occupancy probabilities
        prob_occ_pre = occ_map_pre / occ_map_pre.sum()
        prob_occ_post = occ_map_post / occ_map_post.sum()

        if only_visited:
            # only use bins that were visited
            prob_occ_pre = occ_map_pre[occ_map_pre>0]/occ_map_pre[occ_map_pre>0].sum()
            rate_maps_pre = rate_maps_pre[occ_map_pre>0,:]

            prob_occ_post = occ_map_post[occ_map_post>0]/occ_map_post[occ_map_post>0].sum()
            rate_maps_post = rate_maps_post[occ_map_post>0,:]

        # initialize sparsity and skaggs info list
        sparsity_pre = []
        skaggs_info_pre_per_sec = []
        skaggs_info_pre_per_spike = []

        bad_cells_pre = []

        # compute for PRE
        # --------------------------------------------------------------------------------------------------------------
        for cell_id, rate_map_pre in enumerate(rate_maps_pre.T):
            if np.count_nonzero(rate_map_pre) == 0:
                bad_cells_pre.append(cell_id)
                sparse_cell = np.nan
                skaggs_info_per_sec = np.nan
                skaggs_info_per_spike = np.nan
            else:
                # compute sparsity
                # sparse_cell = np.round(np.mean(rate_map_pre) ** 2 / np.mean(np.square(rate_map_pre)), 4)

                # using uniform probability
                # prob_occ_pre[:] = 1/prob_occ_pre.shape[0]
                
                sparse_cell = (np.sum(prob_occ_pre * rate_map_pre) ** 2) / np.sum(prob_occ_pre * (rate_map_pre ** 2))

                # find good bins so that there is no problem with the log
                good_bins = (rate_map_pre / rate_map_pre.mean() > 0.0000001)
                mean_rate = np.sum(rate_map_pre[good_bins]*prob_occ_pre[good_bins])
                skaggs_info_per_sec = np.sum(rate_map_pre[good_bins]*prob_occ_pre[good_bins]*
                                             np.log(rate_map_pre[good_bins]/mean_rate))
                skaggs_info_per_spike = np.sum(rate_map_pre[good_bins]/mean_rate*prob_occ_pre[good_bins]*
                                             np.log(rate_map_pre[good_bins]/mean_rate))

            sparsity_pre.append(sparse_cell)
            skaggs_info_pre_per_sec.append(skaggs_info_per_sec)
            skaggs_info_pre_per_spike.append(skaggs_info_per_spike)

        # compute for POST
        # --------------------------------------------------------------------------------------------------------------

        sparsity_post = []
        skaggs_info_post_per_sec = []
        skaggs_info_post_per_spike = []
        bad_cells_post = []

        for cell_id, rate_map_post in enumerate(rate_maps_post.T):
            if np.count_nonzero(rate_map_post) == 0:
                bad_cells_post.append(cell_id)
                sparse_cell = np.nan
                skaggs_info_per_sec = np.nan
                skaggs_info_per_spike = np.nan
            else:
                # compute sparsity
                # sparse_cell = np.round(np.mean(rate_map_post.flatten()) ** 2 /
                #                        np.mean(np.square(rate_map_post.flatten())), 4)

                # using uniform probability
                # prob_occ_post[:] = 1/prob_occ_post.shape[0]

                sparse_cell = (np.sum(prob_occ_post * rate_map_post) ** 2) / np.sum(prob_occ_post * (rate_map_post ** 2))

                good_bins = (rate_map_post / rate_map_post.mean() > 0.0000001)
                mean_rate = np.sum(rate_map_post[good_bins]*prob_occ_post[good_bins])
                skaggs_info_per_sec = np.sum(rate_map_post[good_bins]*prob_occ_post[good_bins]*
                                         np.log(rate_map_post[good_bins]/mean_rate))

                skaggs_info_per_spike = np.sum(rate_map_post[good_bins]/mean_rate*prob_occ_post[good_bins]*
                                         np.log(rate_map_post[good_bins]/mean_rate))

                # plt.imshow(np.expand_dims(rate_map_post,1), interpolation='nearest', aspect='auto')
                # plt.title(str(skaggs_info_sec)+"\n"+str(sparse_cell))
                # plt.show()

            sparsity_post.append(sparse_cell)
            skaggs_info_post_per_sec.append(skaggs_info_per_sec)
            skaggs_info_post_per_spike.append(skaggs_info_per_spike)

        if info_measure == "skaggs_spike":
            info_post = np.array(skaggs_info_post_per_spike)
            info_pre = np.array(skaggs_info_pre_per_spike)
        elif info_measure == "skaggs_second":
            info_post = np.array(skaggs_info_post_per_sec)
            info_pre = np.array(skaggs_info_pre_per_sec)
        elif info_measure == "sparsity":
            info_post = np.array(sparsity_post)
            info_pre = np.array(sparsity_pre)

        if len(bad_cells_post) > 0 or len(bad_cells_pre) > 0:
            if len(bad_cells_post) == 0:
                bad_cells = np.hstack(bad_cells_pre)
            elif len(bad_cells_pre) == 0:
                bad_cells = np.hstack(bad_cells_post)
            else:
                bad_cells = np.unique(np.hstack((np.hstack(bad_cells_pre), np.hstack(bad_cells_post))))
            # stable cells
            bad_stable_cells_indices = np.where(np.in1d(stable_cells, bad_cells))[0]
            stable_cells = np.delete(stable_cells, bad_stable_cells_indices)
            # decreasing cells
            bad_dec_cells_indices = np.where(np.in1d(dec_cells, bad_cells))[0]
            dec_cells = np.delete(dec_cells, bad_dec_cells_indices)
            # increasing cells
            bad_inc_cells_indices = np.where(np.in1d(inc_cells, bad_cells))[0]
            inc_cells = np.delete(inc_cells, bad_inc_cells_indices)
        else:
            bad_stable_cells_indices = None
            bad_dec_cells_indices = None
            bad_inc_cells_indices = None

        # compute for PRE
        pre_stable = info_pre[stable_cells]
        pre_inc = info_pre[inc_cells]
        pre_dec = info_pre[dec_cells]

        # remove nans
        pre_stable = pre_stable[~np.isnan(pre_stable)]
        pre_inc = pre_inc[~np.isnan(pre_inc)]
        pre_dec = pre_dec[~np.isnan(pre_dec)]

        # spatial information POST
        post_stable = info_post[stable_cells]
        post_inc = info_post[inc_cells]
        post_dec = info_post[dec_cells]

        # remove nans
        post_stable = post_stable[~np.isnan(post_stable)]
        post_inc = post_inc[~np.isnan(post_inc)]
        post_dec = post_dec[~np.isnan(post_dec)]

        # plotting
        # --------------------------------------------------------------------------------------------------------------
        if plotting:
            # sort for CDF
            pre_stable_sorted = np.sort(pre_stable)
            pre_dec_sorted = np.sort(pre_dec)

            p_pre_mwu = mannwhitneyu(np.nan_to_num(pre_dec_sorted), np.nan_to_num(pre_stable_sorted))[1]
            p_pre_mwu_one_sided = mannwhitneyu(pre_dec, pre_stable, alternative="greater")[1]

            post_stable_sorted = np.sort(post_stable)
            post_inc_sorted = np.sort(post_inc)

            p_post_mwu = mannwhitneyu(np.nan_to_num(post_inc_sorted), np.nan_to_num(post_stable_sorted))[1]
            p_post_mwu_one_sided = mannwhitneyu(post_inc, post_stable, alternative="greater")[1]

            # compute for stable cells only
            p_stable_mwu = mannwhitneyu(np.nan_to_num(post_stable_sorted), np.nan_to_num(pre_stable_sorted))[1]

            # compute combined spatial information (pre and post)
            all_stable = np.hstack((post_stable, pre_stable))
            all_non_stable = np.hstack((pre_dec, post_inc))

            all_stable_sorted = np.sort(all_stable)
            all_non_stable_sorted = np.sort(all_non_stable)
            p_all_mwu_one_sided = mannwhitneyu(np.nan_to_num(all_non_stable_sorted), np.nan_to_num(pre_stable_sorted),
                                                alternative="greater")[1]

            # PRE
            # calculate the proportional values of samples
            p_pre_stable = 1. * np.arange(pre_stable_sorted.shape[0]) / (pre_stable_sorted.shape[0] - 1)
            p_pre_dec = 1. * np.arange(pre_dec_sorted.shape[0]) / (pre_dec_sorted.shape[0] - 1)
            plt.plot(pre_stable_sorted, p_pre_stable, color="#ffdba1", label="STABLE")
            plt.plot(pre_dec_sorted, p_pre_dec, color="#a0c4e4", label="DECREASING")
            # plt.gca().set_xscale("log")
            plt.ylabel("CDF")
            plt.xlabel(info_measure)
            plt.title("PRE: STABLE CELLS vs. DEC. CELLS \n" + "MWU , p-value = " + str(np.round(p_pre_mwu_one_sided, 5)))
            plt.legend()
            plt.show()
    
            # POST
            # calculate the proportional values of samples
            p_post_stable = 1. * np.arange(post_stable_sorted.shape[0]) / (post_stable_sorted.shape[0] - 1)
            p_post_inc = 1. * np.arange(post_inc_sorted.shape[0]) / (post_inc_sorted.shape[0] - 1)
            plt.plot(post_stable_sorted, p_post_stable, color="#ffdba1", label="STABLE")
            plt.plot(post_inc_sorted, p_post_inc, color="#f7959c", label="INCREASING")
            # plt.gca().set_xscale("log")
            plt.ylabel("CDF")
            plt.xlabel(info_measure)
            plt.title("POST: STABLE CELLS vs. INC. CELLS \n" + "MWU , p-value = " + str(np.round(p_post_mwu_one_sided, 5)))
            plt.legend()
            plt.show()

            # spatial information PRE - POST for stable cells
            plt.plot(pre_stable_sorted, p_pre_stable, color="red", label="PRE")
            plt.plot(post_stable_sorted, p_post_stable, color="blue", label="POST")
            # plt.gca().set_xscale("log")
            plt.ylabel("CDF")
            plt.xlabel(info_measure)
            plt.title("PRE-POST, STABLE CELLS \n" + "MWU , p-value = " + str(np.round(p_stable_mwu, 5)))
            plt.legend()
            plt.show()
            
            # all cells 
            # calculate the proportional values of samples
            # p_all_stable = 1. * np.arange(all_stable_sorted.shape[0]) / (all_stable_sorted.shape[0] - 1)
            p_all_non_stable = 1. * np.arange(all_non_stable_sorted.shape[0]) / (all_non_stable_sorted.shape[0] - 1)
            plt.plot(pre_stable_sorted, p_pre_stable, color="#ffdba1", label="stable")
            plt.plot(all_non_stable_sorted, p_all_non_stable, color="green", label="non-stable")
            # plt.gca().set_xscale("log")
            plt.ylabel("CDF")
            plt.xlabel(info_measure)
            plt.title("POST: STABLE CELLS vs. NON-STABLE CELLS \n" + "MWU , p-value = " + str(np.round(p_all_mwu_one_sided, 5)))
            plt.legend()
            plt.show()

        return pre_stable, post_stable, pre_dec, post_inc, post_dec, pre_inc, bad_stable_cells_indices, \
            bad_dec_cells_indices, bad_inc_cells_indices

    def location_decoding_subset_of_cells(self, test_perc=0.5, plotting=True, cells_to_use="stable",
                                          use_first_25_percent_of_post=False):

        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name +"_"+self.params.stable_cell_method + ".pickle", "rb") as f:
            class_dic = pickle.load(f)

        if cells_to_use == "stable":
            cell_ids = class_dic["stable_cell_ids"].flatten()
        elif cells_to_use == "decreasing":
            cell_ids = class_dic["decrease_cell_ids"].flatten()
        elif cells_to_use == "increasing":
            cell_ids = class_dic["increase_cell_ids"].flatten()
        else:
            raise Exception("Subset not known!")

        trials_to_use = self.pre.default_trials

        first_trial = trials_to_use[0]
        last_trial = trials_to_use[-1]
        nr_test_trials = int((last_trial - first_trial) * test_perc)

        shuff_trials_to_use = np.array(np.copy(trials_to_use))
        np.random.shuffle(shuff_trials_to_use)

        test_trials = shuff_trials_to_use[:nr_test_trials]
        train_trials = shuff_trials_to_use[nr_test_trials:]

        # get rate map --> need to add x_min, y_min of environment to have proper location info
        rate_maps = self.pre.get_rate_maps(spatial_resolution=1, trials_to_use=train_trials)

        # flatten rate maps
        rate_maps_flat = np.reshape(rate_maps, (rate_maps.shape[0] * rate_maps.shape[1], rate_maps.shape[2]))

        test_raster_orig, test_loc_orig, _ = self.pre.get_raster_location_speed(trials_to_use=test_trials)

        # test_raster = test_raster_orig[:,:50]
        # test_loc = test_loc_orig[:50, :]
        test_raster = test_raster_orig
        test_loc = test_loc_orig

        test_raster_stable = test_raster[cell_ids, :]
        rate_maps_flat_stable = rate_maps_flat[:, cell_ids]
        error_stable = []
        for pop_vec, loc in zip(test_raster_stable.T, test_loc):
            if np.count_nonzero(pop_vec) == 0:
                continue
            bl = bayes_likelihood(frm=rate_maps_flat_stable.T, pop_vec=pop_vec, log_likeli=False)
            bl_area = np.reshape(bl, (rate_maps.shape[0], rate_maps.shape[1]))
            pred_bin = np.unravel_index(bl_area.argmax(), bl_area.shape)
            pred_x = pred_bin[0] + self.pre.x_min
            pred_y = pred_bin[1] + self.pre.y_min

            # plt.scatter(pred_x, pred_y, color="red")
            # plt.scatter(loc[0], loc[1], color="gray")
            error_stable.append(np.sqrt((pred_x - loc[0]) ** 2 + (pred_y - loc[1]) ** 2))

        if use_first_25_percent_of_post:
            nr_trials_post = self.post.nr_trials
            # trials_to_use = np.arange(int(nr_trials_post*0.25))
            trials_to_use = np.arange(1)
        else:
            trials_to_use = None

        # test in post
        test_raster_orig, test_loc_orig, _ = self.post.get_raster_location_speed(trials_to_use=trials_to_use)
        test_raster = test_raster_orig
        test_loc = test_loc_orig

        test_raster_stable = test_raster[cell_ids, :]
        pred_loc_stable = []
        error_stable_post = []
        error_stable_post_shuffle = []
        for pop_vec, loc in zip(test_raster_stable.T, test_loc):
            if np.count_nonzero(pop_vec) == 0:
                continue
            bl = bayes_likelihood(frm=rate_maps_flat_stable.T, pop_vec=pop_vec, log_likeli=False)
            bl_area = np.reshape(bl, (rate_maps.shape[0], rate_maps.shape[1]))
            pred_bin = np.unravel_index(bl_area.argmax(), bl_area.shape)
            pred_x = pred_bin[0] + self.pre.x_min
            pred_y = pred_bin[1] + self.pre.y_min
            pred_loc_stable.append([pred_x, pred_y])

            # plt.scatter(pred_x, pred_y, color="red")
            # plt.scatter(loc[0], loc[1], color="gray")
            error_stable_post.append(np.sqrt((pred_x - loc[0]) ** 2 + (pred_y - loc[1]) ** 2))

            # compute control
            bl_c = bayes_likelihood(frm=rate_maps_flat_stable.T, pop_vec=np.random.permutation(pop_vec), log_likeli=False)
            bl_area_c = np.reshape(bl_c, (rate_maps.shape[0], rate_maps.shape[1]))
            pred_bin_c = np.unravel_index(bl_area_c.argmax(), bl_area_c.shape)
            pred_x_c = pred_bin_c[0] + self.pre.x_min
            pred_y_c = pred_bin_c[1] + self.pre.y_min

            error_stable_post_shuffle.append(np.sqrt((pred_x_c - loc[0]) ** 2 + (pred_y_c - loc[1]) ** 2))
        # plt.show()
        error_stable_post = np.array(error_stable_post)
        error_stable_post_shuffle = np.array(error_stable_post_shuffle)

        error_pre_stable_sorted = np.sort(error_stable)
        error_post_stable_sorted = np.sort(error_stable_post)
        error_post_stable_shuffle_sorted = np.sort(error_stable_post_shuffle)

        p_error_pre_stable = 1. * np.arange(error_pre_stable_sorted.shape[0]) / (error_pre_stable_sorted.shape[0] - 1)
        p_error_post_stable = 1. * np.arange(error_post_stable_sorted.shape[0]) / (error_post_stable_sorted.shape[0] - 1)
        p_error_post_stable_shuffle = 1. * np.arange(error_post_stable_shuffle_sorted.shape[0]) / (
                    error_post_stable_shuffle_sorted.shape[0] - 1)

        p_mwu = mannwhitneyu(error_stable, error_stable_post)[1]

        if plotting:
            plt.plot(error_pre_stable_sorted, p_error_pre_stable, color="#ffdba1", label="PRE")
            plt.plot(error_post_stable_sorted, p_error_post_stable, color="#a0c4e4", label="POST")
            plt.plot(error_post_stable_shuffle_sorted, p_error_post_stable_shuffle, color="gray", label="Control POST")
            # plt.gca().set_xscale("log")
            plt.ylabel("CDF")
            plt.xlabel("Error (cm)")
            plt.title("Stable cells: PRE - POST decoding \n" + "MWU , p-value = " + str(np.round(p_mwu, 5)))
            plt.legend()
            plt.show()
        else:
            return error_pre_stable_sorted, error_post_stable_sorted, error_post_stable_shuffle_sorted

    def location_decoding_learning_vs_drift(self):
            error_stable_learning = self.pre.decode_location_beginning_end_learning(cells_to_use="stable", plotting=False)
            p_error_stable = 1. * np.arange(error_stable_learning.shape[0]) / (error_stable_learning.shape[0] - 1)
            error_dec_learning = self.pre.decode_location_beginning_end_learning(cells_to_use="decreasing", plotting=False)
            p_error_dec = 1. * np.arange(error_dec_learning.shape[0]) / (error_dec_learning.shape[0] - 1)

            # compute error for drift

            with open(self.params.pre_proc_dir + "cell_classification/" +
                      self.session_name + "_" + self.params.stable_cell_method + ".pickle", "rb") as f:
                class_dic = pickle.load(f)

            stable_cells = class_dic["stable_cell_ids"].flatten()
            # use last 5 trials of pre
            train_trials = range(len(self.pre.trial_loc_list)-5, len(self.pre.trial_loc_list))

            rate_maps = self.pre.get_rate_maps(spatial_resolution=1, trials_to_use=train_trials)

            rate_maps_flat = np.reshape(rate_maps, (rate_maps.shape[0] * rate_maps.shape[1], rate_maps.shape[2]))

            rate_maps_flat_stable = rate_maps_flat[:, stable_cells]

            # test in post
            test_raster, test_loc, _ = self.post.get_raster_location_speed(trials_to_use=range(5))

            test_raster_stable = test_raster[stable_cells, :]

            error_stable_drift = []
            for pop_vec, loc in zip(test_raster_stable.T, test_loc):
                if np.count_nonzero(pop_vec) == 0:
                    continue
                bl = bayes_likelihood(frm=rate_maps_flat_stable.T, pop_vec=pop_vec, log_likeli=False)
                bl_area = np.reshape(bl, (rate_maps.shape[0], rate_maps.shape[1]))
                pred_bin = np.unravel_index(bl_area.argmax(), bl_area.shape)
                pred_x = pred_bin[0] + self.pre.x_min
                pred_y = pred_bin[1] + self.pre.y_min

                # plt.scatter(pred_x, pred_y, color="red")
                # plt.scatter(loc[0], loc[1], color="gray")
                error_stable_drift.append(np.sqrt((pred_x - loc[0]) ** 2 + (pred_y - loc[1]) ** 2))

            error_stable_drift = np.array(error_stable_drift)
            error_stable_drift_sorted = np.sort(error_stable_drift)
            p_error_stable_drift = 1. * np.arange(error_stable_drift.shape[0]) / (error_stable_drift.shape[0] - 1)

            plt.plot(error_stable_drift_sorted, p_error_stable_drift, label="stable drift")
            plt.plot(error_stable_learning, p_error_stable, label="stable")
            plt.plot(error_dec_learning, p_error_dec, label="dec")
            plt.legend()
            plt.show()

    # </editor-fold>

    # <editor-fold desc="pHMM">

    def cross_val_poisson_hmm_cb_pre_post(self, cl_ar=np.arange(1, 100, 10), cell_selection="all"):
        # --------------------------------------------------------------------------------------------------------------
        # cross validation of poisson hmm fits to data
        #
        # args:     - cl_ar, range object: #clusters to fit to data
        # --------------------------------------------------------------------------------------------------------------

        print(" - CROSS-VALIDATING POISSON HMM ON CHEESEBOARD: PRE AND POST --> OPTIMAL #MODES ...")
        print("  ... using " + str(self.params.cross_val_splits) + "\n")
        # get pre and post raster data
        pre_raster, pre_times = self.pre.get_raster_and_trial_times()
        post_raster, post_times = self.post.get_raster_and_trial_times()

        raster = np.hstack((pre_raster, post_raster))
        trial_lengths = pre_times + post_times

        if self.params.cross_val_splits == "trial_splitting":
            trial_end = np.cumsum(np.array(trial_lengths))
            trial_start = np.concatenate([[0], trial_end[:-1]])
            # test_range = np.vstack((trial_start, trial_end))
            test_range_per_fold = []
            for lo, hi in zip(trial_start, trial_end):
                test_range_per_fold.append(np.array(list(range(lo, hi))))

        elif self.params.cross_val_splits == "custom_splits":

            # number of folds
            nr_folds = 10
            # how many times to fit for each split
            max_nr_fitting = 5
            # how many chunks (for pre-computed splits)
            nr_chunks = 10

            unobserved_lo_array = pickle.load(
                open("temp_data/unobserved_lo_cv" + str(nr_folds) + "_" + str(nr_chunks) + "_chunks", "rb"))
            unobserved_hi_array = pickle.load(
                open("temp_data/unobserved_hi_cv" + str(nr_folds) + "_" + str(nr_chunks) + "_chunks", "rb"))

            # set number of time bins
            bin_num = raster.shape[1]
            bins = np.arange(bin_num + 1)

            # length of one chunk
            n_chunks = int(bin_num / nr_chunks)
            test_range_per_fold = []
            for fold in range(nr_folds):

                # unobserved_lo: start bins (in spike data resolution) for all test data chunks
                unobserved_lo = []
                unobserved_hi = []
                for lo, hi in zip(unobserved_lo_array[fold], unobserved_hi_array[fold]):
                    unobserved_lo.append(bins[lo * n_chunks])
                    unobserved_hi.append(bins[hi * n_chunks])

                unobserved_lo = np.array(unobserved_lo)
                unobserved_hi = np.array(unobserved_hi)

                test_range = []
                for lo, hi in zip(unobserved_lo, unobserved_hi):
                    test_range += (list(range(lo, hi)))
                test_range_per_fold.append(np.array(test_range))

        nr_cores = 12

        folder_name = self.params.session_name + "_pre_post_cb"+ self.cell_type
        new_ml = MlMethodsOnePopulation(params=self.params)
        new_ml.parallelize_cross_val_model(nr_cluster_array=cl_ar, nr_cores=nr_cores, model_type="POISSON_HMM",
                                           raster_data=raster, folder_name=folder_name, splits=test_range_per_fold)
        new_ml.cross_val_view_results(folder_name=folder_name)

        if cell_selection == "stable":
            # load only stable cells
            with open(
                    self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                    "rb") as f:
                class_dic = pickle.load(f)

            stable_ids = class_dic["stable_cell_ids"]

            raster = raster[stable_ids, :]

            folder_name = self.params.session_name + "_pre_post_cb_stable" + self.cell_type

        elif cell_selection == "non_stable":

            # load only stable cells
            with open(
                    self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                    "rb") as f:
                class_dic = pickle.load(f)

            stable_ids = class_dic["stable_cell_ids"]

            raster = np.delete(raster, stable_ids, axis=0)

            folder_name = self.params.session_name + "_pre_post_cb_non_stable" + self.cell_type

        elif cell_selection == "all":

            folder_name = self.params.session_name + "_pre_post_cb_" + self.cell_type

        nr_cores = 12

        new_ml = MlMethodsOnePopulation(params=self.params)
        new_ml.parallelize_cross_val_model(nr_cluster_array=cl_ar, nr_cores=nr_cores, model_type="POISSON_HMM",
                                           raster_data=raster, folder_name=folder_name)
        new_ml.cross_val_view_results(folder_name=folder_name)

    def view_cross_val_results_cb_pre_post(self, range_to_plot=None, cell_selection="all"):
        # --------------------------------------------------------------------------------------------------------------
        # views cross validation results
        #
        # args:     - model_type, string: which type of model ("POISSON_HMM")
        #           - custom_splits, bool: whether custom splits were used for cross validation
        # --------------------------------------------------------------------------------------------------------------
        if cell_selection == "stable":
            folder_name = self.session_name +"_pre_post_cb_stable"+self.cell_type
        elif cell_selection == "non_stable":
            folder_name = self.session_name +"_pre_post_cb_non_stable"+self.cell_type
        elif cell_selection == "all":
            folder_name = self.session_name +"_pre_post_cb_"+self.cell_type

        new_ml = MlMethodsOnePopulation(params=self.params)
        new_ml.cross_val_view_results(folder_name=folder_name, range_to_plot=range_to_plot)

    def fit_poisson_hmm_cb_pre_post(self, nr_modes, cell_selection="all"):
        # --------------------------------------------------------------------------------------------------------------
        # fits poisson hmm to data
        #
        # args:     - nr_modes, int: #clusters to fit to data
        #           - file_identifier, string: string that is added at the end of file for identification
        # --------------------------------------------------------------------------------------------------------------

        print(" - FITTING POISSON HMM WITH "+str(nr_modes)+" MODES ...\n")

        # get pre and post raster data
        pre_raster = self.pre.get_raster()
        post_raster = self.post.get_raster()

        raster = np.hstack((pre_raster, post_raster))

        if cell_selection == "stable":
            # load only stable cells
            with open(
                    self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                    "rb") as f:
                class_dic = pickle.load(f)

            stable_ids = class_dic["stable_cell_ids"]

            raster = raster[stable_ids, :]
            file_name = self.params.session_name + "_pre_post_cb_stable_" + \
                        self.cell_type + "_" + str(nr_modes) + "_modes"

        elif cell_selection == "non_stable":
            # load only stable cells
            with open(
                    self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                    "rb") as f:
                class_dic = pickle.load(f)

            stable_ids = class_dic["stable_cell_ids"]

            raster = np.delete(raster, stable_ids, axis=0)
            file_name = self.params.session_name + "_pre_post_cb_non_stable_" + \
                        self.cell_type + "_" + str(nr_modes) + "_modes"

        elif cell_selection == "all":
            file_name = self.params.session_name + "_pre_post_cb_" + self.cell_type + "_" + str(nr_modes) + "_modes"

        model = PoissonHMM(n_components=nr_modes)
        model.fit(raster.T)
        model.set_time_bin_size(time_bin_size=self.params.time_bin_size)

        with open(self.params.pre_proc_dir+"phmm/"+file_name+".pkl", "wb") as file: pickle.dump(model, file)

        print("  - ... DONE!\n")

    def one_phmm_model_pre_post_evaluate(self, nr_modes):
        # --------------------------------------------------------------------------------------------------------------
        # fits poisson hmm to data and evaluates the goodness of the model by comparing basic statistics (avg. firing
        # rate, correlation values, k-statistics) between real data and data sampled from the model
        #
        # args:     - nr_modes, int: #clusters to fit to data
        #           - load_from_file, bool: whether to load model from file or to fit model again
        # --------------------------------------------------------------------------------------------------------------

        print(" - EVALUATING POISSON HMM FIT (BASIC STATISTICS) ...")

        # get pre and post raster data
        pre_raster = self.pre.get_raster()
        post_raster = self.post.get_raster()

        raster = np.hstack((pre_raster, post_raster))

        nr_time_bins = raster.shape[1]
        # X = X[:, :1000]

        file_name = self.params.session_name + "_pre_post_cb_" + self.cell_type+"_"+str(nr_modes)+"_modes"

        # check if model file exists already --> otherwise fit model again
        if os.path.isfile(self.params.pre_proc_dir+"phmm/" + file_name + ".pkl"):
            print("- LOADING PHMM MODEL FROM FILE\n")
            with open(self.params.pre_proc_dir+"phmm/" + file_name + ".pkl", "rb") as file:
                model = pickle.load(file)
        else:
            print("- PHMM MODEL FILE NOT FOUND --> FITTING PHMM TO DATA\n")
            model = PoissonHMM(n_components=nr_modes)
            model.fit(raster.T)

        samples, sequence = model.sample(nr_time_bins)
        samples = samples.T

        evaluate_clustering_fit(real_data=raster, samples=samples, binning="TEMPORAL_SPIKE",
                                   time_bin_size=0.1, plotting=True)

    def one_phmm_model_pre_post_cb_analysis(self, nr_modes, cell_selection="all"):
        # --------------------------------------------------------------------------------------------------------------
        # fits poisson hmm to data and evaluates the goodness of the model by comparing basic statistics (avg. firing
        # rate, correlation values, k-statistics) between real data and data sampled from the model
        #
        # args:     - nr_modes, int: #clusters to fit to data
        #           - load_from_file, bool: whether to load model from file or to fit model again
        # --------------------------------------------------------------------------------------------------------------

        print(" - EVALUATING POISSON HMM FIT (BASIC STATISTICS) ...")

        # get pre and post raster data
        pre_raster = self.pre.get_raster()
        post_raster = self.post.get_raster()

        if cell_selection == "stable":
            # load only stable cells
            # load cell labels
            with open(self.params.pre_proc_dir + "cell_classification/" +
                      self.session_name + "_" + self.params.stable_cell_method + ".pickle",
                      "rb") as f:
                class_dic = pickle.load(f)

            stable_ids = class_dic["stable_cell_ids"]

            pre_raster = pre_raster[stable_ids, :]
            post_raster = post_raster[stable_ids, :]

            file_name = self.params.session_name + "_pre_post_cb_stable_" + self.cell_type+"_"+str(nr_modes)+"_modes"

        elif cell_selection == "non_stable":
            # load only stable cells
            # load cell labels
            with open(self.params.pre_proc_dir + "cell_classification/" +
                      self.session_name + "_" + self.params.stable_cell_method + ".pickle",
                      "rb") as f:
                class_dic = pickle.load(f)

            stable_ids = class_dic["stable_cell_ids"]

            pre_raster = np.delete(pre_raster, stable_ids, axis=0)
            post_raster = np.delete(post_raster, stable_ids, axis=0)

            file_name = self.params.session_name + "_pre_post_cb_non_stable_" + self.cell_type+"_"+str(nr_modes)+"_modes"

        elif cell_selection == "all":
            file_name = self.params.session_name + "_pre_post_cb_" + self.cell_type+"_"+str(nr_modes)+"_modes"

        raster = np.hstack((pre_raster, post_raster))

        # check if model file exists already --> otherwise fit model again
        if os.path.isfile(self.params.pre_proc_dir+"phmm/" + file_name + ".pkl"):
            print("- LOADING PHMM MODEL FROM FILE\n")
            with open(self.params.pre_proc_dir+"phmm/" + file_name + ".pkl", "rb") as file:
                model = pickle.load(file)
        else:
            print("- PHMM MODEL FILE NOT FOUND --> FITTING PHMM TO DATA\n")
            model = PoissonHMM(n_components=nr_modes)
            model.fit(raster.T)

        # get #occurences modes for pre and post
        pre_modes, pre_counts = np.unique(model.predict(pre_raster.T), return_counts=True)
        pre_counts = (pre_counts / np.sum(pre_counts)) * 100

        # get #occurences modes for pre and post
        post_modes, post_counts = np.unique(model.predict(post_raster.T), return_counts=True)
        post_counts = (post_counts / np.sum(post_counts)) * 100

        # # get lambdas and look at their similarity
        #
        # lambdas = model.means_
        #
        # D = pairwise_distances(lambdas, metric="euclidean")
        # plt.imshow(D)
        # plt.colorbar()
        # plt.show()
        #
        # plt.hist(D.flatten())
        # plt.show()
        # thresh = 0.2
        #
        # lambdas_bin = lambdas.copy()
        # lambdas_bin[lambdas_bin >= thresh] = 1
        # lambdas_bin[lambdas_bin < thresh] = 0
        # lambdas_bin=lambdas_bin.astype(bool)
        # plt.imshow(lambdas_bin)
        # plt.show()
        # D = pairwise_distances(lambdas_bin, metric="jaccard")
        # plt.imshow(D)
        # plt.colorbar()
        # plt.show()
        #
        #
        # exit()

        lines = []
        for i in range(pre_counts.shape[0]):
            pair = [(pre_modes[i], 0), (pre_modes[i], pre_counts[i])]
            lines.append(pair)

        linecoll = matcoll.LineCollection(lines, colors="violet")
        fig, ax = plt.subplots()

        ax.add_collection(linecoll)

        lines = []
        for i in range(post_counts.shape[0]):
            pair = [(post_modes[i], 0), (post_modes[i], post_counts[i])]
            lines.append(pair)

        linecoll = matcoll.LineCollection(lines, colors="lightskyblue")

        ax.add_collection(linecoll)

        plt.scatter(pre_modes, pre_counts, c="violet", label="PRE")
        plt.scatter(post_modes, post_counts, c="lightskyblue", alpha=0.8, label="POST")
        plt.legend()
        plt.xlabel("MODE ID")
        plt.ylabel("MODE OCCURENCE (%)")
        plt.show()

        print("HERE")

    def pre_post_models_analysis(self, gc_threshold=0.7):

        pre_raster, _, _ = self.pre.get_raster_location_speed()
        post_raster, _, _ = self.post.get_raster_location_speed()
        # check how many cells
        nr_cells = pre_raster.shape[0]

        # compute mean firing in PRE
        mean_firing_pre = np.mean(pre_raster, axis=1)
        # compute mean firing in POST
        mean_firing_post = np.mean(post_raster, axis=1)

        if self.params.stable_cell_method == "k_means":
            # load only stable cells
            with open(self.params.pre_proc_dir + "cell_classification/" +
                      self.params.session_name + "_k_means.pickle", "rb") as f:
                class_dic = pickle.load(f)

        elif self.params.stable_cell_method == "mean_firing_awake":
            # load only stable cells
            with open(self.params.pre_proc_dir + "cell_classification/" +
                      self.params.session_name + "_mean_firing_awake.pickle", "rb") as f:
                class_dic = pickle.load(f)

        stable_cells = class_dic["stable_cell_ids"]
        inc_cells = class_dic["increase_cell_ids"]
        dec_cells = class_dic["decrease_cell_ids"]

        # get PRE model
        with open(self.params.pre_proc_dir + "phmm/" + self.default_pre_phmm_model + ".pkl", "rb") as file:
            pre_model = pickle.load(file)

        pre_model_means = pre_model.means_
        # z-score for visualization
        pre_model_means_z = zscore(pre_model_means, axis=1)

        # get POST model
        with open(self.params.pre_proc_dir + "phmm/" + self.default_post_phmm_model + ".pkl", "rb") as file:
            post_model = pickle.load(file)

        post_model_means = post_model.means_
        # z-score for visualization
        post_model_means_z = zscore(post_model_means, axis=1)
        # find goal coding modes

        gc_pre = []
        for mode in range(pre_model_means.shape[0]):
            gc_pre.append(self.pre.analyze_modes_goal_coding(file_name=self.default_pre_phmm_model, mode_ids=mode))
        gc_pre = np.array(gc_pre)
        gc_pre_sorted = gc_pre.argsort()

        gc_post = []
        for mode in range(post_model_means.shape[0]):
            gc_post.append(self.pre.analyze_modes_goal_coding(file_name=self.default_pre_phmm_model, mode_ids=mode))
        gc_post = np.array(gc_post)
        gc_post_sorted = gc_post.argsort()

        # compute cell labels
        dec_stable_inc = np.hstack((dec_cells.flatten(), stable_cells.flatten(), inc_cells.flatten()))
        cell_labels = np.hstack((np.zeros(dec_cells.flatten().shape[0]), np.ones(stable_cells.flatten().shape[0]),
                                 2* np.ones(inc_cells.flatten().shape[0])))

        # order pre modes
        pre_model_means_sorted = pre_model_means_z[:, dec_stable_inc]
        pre_model_means_sorted = pre_model_means_sorted[gc_pre_sorted, :]

        # order post modes
        post_model_means_sorted = post_model_means_z[:, dec_stable_inc]
        post_model_means_sorted = post_model_means_sorted[gc_post_sorted, :]

        fig = plt.figure(figsize=(8,7))
        gs = fig.add_gridspec(13, 6)

        ax1 = fig.add_subplot(gs[0, :])
        ax1.set_title("CELL CLASSIFICATION (b:dec, r:inc, w:stable)")
        ax1.imshow(np.expand_dims(cell_labels, 0), cmap="bwr")
        ax1.get_xaxis().set_ticks([])
        ax1.get_yaxis().set_ticks([])
        ax2 = fig.add_subplot(gs[1:6, :])
        ax2.imshow(pre_model_means_sorted)
        ax2.get_xaxis().set_ticks([])
        ax2.set_ylabel("PRE MODES ORDERED")
        ax3 = fig.add_subplot(gs[6:, :])
        ax3.imshow(post_model_means_sorted)
        ax3.set_ylabel("POST MODES ORDERED")
        ax3.set_xlabel("CELLS ORDERED")
        plt.show()

        # compute entropy per cell (selective participation) PRE
        entropy_per_cell_pre = []
        for cell_means in pre_model_means.T:
            entropy_per_cell_pre.append(entropy(cell_means))
        entropy_per_cell_pre = np.nan_to_num(np.array(entropy_per_cell_pre))

        # compute entropy per cell (selective participation) POST
        entropy_per_cell_post = []
        for cell_means in post_model_means.T:
            entropy_per_cell_post.append(entropy(cell_means))
        entropy_per_cell_post = np.nan_to_num(np.array(entropy_per_cell_post))

        plt.subplot(2,1,1)
        plt.hist(entropy_per_cell_pre[stable_cells], color="#ffdba1", label="STABLE CELLS", density=True, bins=20)
        plt.hist(entropy_per_cell_pre[dec_cells], color="#a0c4e4", label="DECREASING CELLS", alpha=0.7, density=True, bins=20)
        _, y_max = plt.gca().get_ylim()
        plt.vlines(np.median(entropy_per_cell_pre[stable_cells]),0,y_max, colors="y")
        plt.vlines(np.median(entropy_per_cell_pre[dec_cells]),0,y_max, colors="b")
        plt.legend()
        plt.ylabel("DENSITY")
        plt.title("PRE - ALL MODES")
        plt.subplot(2,1,2)
        plt.hist(entropy_per_cell_post[stable_cells], color="#ffdba1", label="STABLE CELLS", density=True, bins=20)
        plt.hist(entropy_per_cell_post[inc_cells], color="#f7959c", label="INCREASING CELLS", alpha=0.7, density=True, bins=20)
        _, y_max = plt.gca().get_ylim()
        plt.vlines(np.median(entropy_per_cell_post[stable_cells]),0,y_max, colors="y")
        plt.vlines(np.median(entropy_per_cell_post[inc_cells]),0,y_max, colors="r")
        plt.legend()
        plt.ylabel("DENSITY")
        plt.xlabel("ENTROPY LAMBDA VALUES PER CELL")
        plt.title("POST - ALL MODES")
        plt.show()

        # only look at goal coding modes

        gc_pre_modes = np.argwhere(gc_pre > gc_threshold).flatten()
        gc_post_modes = np.argwhere(gc_post > gc_threshold).flatten()

        # compute entropy per cell (selective participation) PRE
        entropy_per_cell_pre = []
        for cell_means in pre_model_means[gc_pre_modes,:].T:
            entropy_per_cell_pre.append(entropy(cell_means))
        entropy_per_cell_pre = np.nan_to_num(np.array(entropy_per_cell_pre))

        # compute entropy per cell (selective participation) POST
        entropy_per_cell_post = []
        for cell_means in post_model_means[gc_post_modes,:].T:
            entropy_per_cell_post.append(entropy(cell_means))
        entropy_per_cell_post = np.nan_to_num(np.array(entropy_per_cell_post))


        plt.subplot(2,1,1)
        plt.hist(entropy_per_cell_pre[stable_cells], color="#ffdba1", label="STABLE CELLS", density=True, bins=20)
        plt.hist(entropy_per_cell_pre[dec_cells], color="#a0c4e4", label="DECREASING CELLS", alpha=0.7, density=True, bins=20)
        _, y_max = plt.gca().get_ylim()
        plt.vlines(np.median(entropy_per_cell_pre[stable_cells]),0,y_max, colors="y")
        plt.vlines(np.median(entropy_per_cell_pre[dec_cells]),0,y_max, colors="b")
        plt.legend()
        plt.ylabel("DENSITY")
        plt.title("PRE - GC MODES")
        plt.subplot(2,1,2)
        plt.hist(entropy_per_cell_post[stable_cells], color="#ffdba1", label="STABLE CELLS", density=True, bins=20)
        plt.hist(entropy_per_cell_post[inc_cells], color="#f7959c", label="INCREASING CELLS", alpha=0.7, density=True, bins=20)
        _, y_max = plt.gca().get_ylim()
        plt.vlines(np.median(entropy_per_cell_post[stable_cells]),0,y_max, colors="y")
        plt.vlines(np.median(entropy_per_cell_post[inc_cells]),0,y_max, colors="r")
        plt.legend()
        plt.ylabel("DENSITY")
        plt.xlabel("ENTROPY LAMBDA VALUES PER CELL")
        plt.title("POST - GC MODES")
        plt.show()

        # participation in goal coding PRE
        pre_means_gc = pre_model_means[gc_pre_modes,:]
        pre_mean_non_gc = np.delete(pre_model_means, gc_pre_modes, axis=0)

        goal_coding_modes_means_overall_modes_pre = np.mean(pre_means_gc, axis=0)/mean_firing_pre
        non_goal_coding_modes_means_overall_modes_pre = np.mean(pre_mean_non_gc, axis=0) / mean_firing_pre
        # plt.figure(figsize=(5,8))
        plt.scatter(goal_coding_modes_means_overall_modes_pre[stable_cells],
                    non_goal_coding_modes_means_overall_modes_pre[stable_cells], color="#ffdba1", label="STABLE CELLS")
        plt.scatter(goal_coding_modes_means_overall_modes_pre[dec_cells],
                    non_goal_coding_modes_means_overall_modes_pre[dec_cells], color="#a0c4e4", label="DECREASING CELLS")
        plt.legend(loc="upper left")
        plt.xlabel("PARTICIPATION GOAL CODING")
        plt.ylabel("PARTICIPATION NON GOAL CODING")
        plt.gca().set_aspect("equal")
        _,x_max = plt.gca().get_xlim()
        plt.hlines(1,0,x_max, color="gray", zorder=-1000)
        _,y_max = plt.gca().get_ylim()
        plt.vlines(1,0,y_max, color="gray", zorder=-1000)
        plt.title("CELL PARTICIPATION PRE")
        plt.show()

        # participation in goal coding POST
        post_means_gc = post_model_means[gc_post_modes, :]
        post_mean_non_gc = np.delete(post_model_means, gc_post_modes, axis=0)

        goal_coding_modes_means_overall_modes_post = np.mean(post_means_gc, axis=0) / mean_firing_post
        non_goal_coding_modes_means_overall_modes_post = np.mean(post_mean_non_gc, axis=0) / mean_firing_post
        # plt.figure(figsize=(5,8))
        plt.scatter(goal_coding_modes_means_overall_modes_post[stable_cells],
                    non_goal_coding_modes_means_overall_modes_post[stable_cells], color="#ffdba1", label="STABLE CELLS")
        plt.scatter(goal_coding_modes_means_overall_modes_post[inc_cells],
                    non_goal_coding_modes_means_overall_modes_post[inc_cells], color="#f7959c", label="INCREASING CELLS")
        plt.legend(loc="upper left")
        plt.xlabel("PARTICIPATION GOAL CODING")
        plt.ylabel("PARTICIPATION NON GOAL CODING")
        plt.gca().set_aspect("equal")
        _, x_max = plt.gca().get_xlim()
        plt.hlines(1, 0, x_max, color="gray", zorder=-1000)
        _, y_max = plt.gca().get_ylim()
        plt.vlines(1, 0, y_max, color="gray", zorder=-1000)
        plt.title("CELL PARTICIPATION POST")
        plt.show()

        plt.scatter(goal_coding_modes_means_overall_modes_pre[stable_cells],
                    non_goal_coding_modes_means_overall_modes_pre[stable_cells], color="grey", label="PRE")
        plt.scatter(goal_coding_modes_means_overall_modes_post[stable_cells],
                    non_goal_coding_modes_means_overall_modes_post[stable_cells], color="red", alpha=0.7,
                    label="POST")
        plt.legend(loc="upper left")
        plt.xlabel("PARTICIPATION GOAL CODING")
        plt.ylabel("PARTICIPATION NON GOAL CODING")
        plt.gca().set_aspect("equal")
        _, x_max = plt.gca().get_xlim()
        plt.hlines(1, 0, x_max, color="gray", zorder=-1000)
        _, y_max = plt.gca().get_ylim()
        plt.vlines(1, 0, y_max, color="gray", zorder=-1000)
        plt.title("GOAL CODING STABLE CELLS PRE - POST")
        plt.show()


        diff = np.mean(pre_means_gc, axis=0) - np.mean(pre_mean_non_gc, axis=0)/\
               (np.mean(pre_means_gc, axis=0) + np.mean(pre_mean_non_gc, axis=0))

        plt.hist(diff[stable_cells], color="#ffdba1", density=True, label="STABLE CELLS")
        plt.hist(diff[dec_cells], color="#a0c4e4", alpha=0.8, density=True, label="DECREASING CELLS")
        plt.xlabel("DELTA IN LAMBDAS: GOAL CODING MODES - NON GOAL CODING MODES")
        plt.ylabel("DENSITY")
        plt.title("PRE")
        plt.legend()
        plt.show()

        diff_post = np.mean(post_means_gc, axis=0) - np.mean(post_mean_non_gc, axis=0)/\
               (np.mean(post_means_gc, axis=0) + np.mean(post_mean_non_gc, axis=0))

        plt.hist(diff_post[stable_cells], color="#ffdba1", density=True, label="STABLE CELLS")
        plt.hist(diff_post[inc_cells], color="#f7959c", alpha=0.8, density=True, label="INCREASING CELLS")
        plt.xlabel("DELTA IN LAMBDAS: GOAL CODING MODES - NON GOAL CODING MODES")
        plt.ylabel("DENSITY")
        plt.title("POST")
        plt.legend()

        plt.show()

    def pre_post_models_goal_coding(self, gc_threshold=0.9, plotting=False):

        pre_raster, _, _ = self.pre.get_raster_location_speed()
        post_raster, _, _ = self.post.get_raster_location_speed()
        # check how many cells
        nr_cells = pre_raster.shape[0]

        # compute mean firing in PRE
        mean_firing_pre = np.mean(pre_raster, axis=1)
        # compute mean firing in POST
        mean_firing_post = np.mean(post_raster, axis=1)

        # select only cells that fire in PRE and POST
        cell_sel = np.logical_and(mean_firing_pre>0, mean_firing_post>0)
        to_delete = np.argwhere(~cell_sel).flatten()

        if self.params.stable_cell_method == "k_means":
            # load only stable cells
            with open(self.params.pre_proc_dir + "cell_classification/" +
                      self.session_name + "_k_means.pickle", "rb") as f:
                class_dic = pickle.load(f)

        elif self.params.stable_cell_method == "mean_firing_awake":
            # load only stable cells
            with open(self.params.pre_proc_dir + "cell_classification/" +
                      self.session_name + "_mean_firing_awake.pickle", "rb") as f:
                class_dic = pickle.load(f)

        stable_cells = class_dic["stable_cell_ids"].flatten()
        # delete stable cells which dont fire in pre and post
        for del_cell in to_delete:
            stable_cells = stable_cells[stable_cells != del_cell]

        # get PRE model
        with open(self.params.pre_proc_dir + "phmm/" + self.default_pre_phmm_model + ".pkl", "rb") as file:
            pre_model = pickle.load(file)
        pre_model_means = pre_model.means_

        # get POST model
        with open(self.params.pre_proc_dir + "phmm/" + self.default_post_phmm_model + ".pkl", "rb") as file:
            post_model = pickle.load(file)

        post_model_means = post_model.means_

        gc_pre = []
        for mode in range(pre_model_means.shape[0]):
            gc_pre.append(self.pre.analyze_modes_goal_coding(file_name=self.default_pre_phmm_model, mode_ids=mode))
        gc_pre = np.array(gc_pre)

        gc_post = []
        for mode in range(post_model_means.shape[0]):
            gc_post.append(self.pre.analyze_modes_goal_coding(file_name=self.default_pre_phmm_model, mode_ids=mode))
        gc_post = np.array(gc_post)

        # only look at goal coding modes
        gc_pre_modes = np.argwhere(gc_pre > gc_threshold).flatten()
        gc_post_modes = np.argwhere(gc_post > gc_threshold).flatten()

        # participation in goal coding PRE
        pre_means_gc = pre_model_means[gc_pre_modes,:]
        pre_mean_non_gc = np.delete(pre_model_means, gc_pre_modes, axis=0)

        goal_coding_modes_means_overall_modes_pre = np.mean(pre_means_gc, axis=0)/mean_firing_pre
        non_goal_coding_modes_means_overall_modes_pre = np.mean(pre_mean_non_gc, axis=0) / mean_firing_pre

        # participation in goal coding POST
        post_means_gc = post_model_means[gc_post_modes, :]
        post_mean_non_gc = np.delete(post_model_means, gc_post_modes, axis=0)

        goal_coding_modes_means_overall_modes_post = np.mean(post_means_gc, axis=0) / mean_firing_post
        non_goal_coding_modes_means_overall_modes_post = np.mean(post_mean_non_gc, axis=0) / mean_firing_post

        # compute gain in goal coding
        gain_gc = goal_coding_modes_means_overall_modes_post[stable_cells] - goal_coding_modes_means_overall_modes_pre[stable_cells]

        if plotting:
            plt.hist(gain_gc, bins=20)
            plt.xlabel("GAIN GC")
            _, y_max = plt.gca().get_ylim()
            plt.vlines(np.median(gain_gc), 0, y_max, color="gray")
            plt.show()

            plt.scatter(goal_coding_modes_means_overall_modes_pre[stable_cells],
                        non_goal_coding_modes_means_overall_modes_pre[stable_cells], color="grey", label="PRE")
            plt.scatter(goal_coding_modes_means_overall_modes_post[stable_cells],
                        non_goal_coding_modes_means_overall_modes_post[stable_cells], color="red", alpha=0.7,
                        label="POST")
            plt.legend(loc="upper left")
            plt.xlabel("PARTICIPATION GOAL CODING")
            plt.ylabel("PARTICIPATION NON GOAL CODING")
            plt.gca().set_aspect("equal")
            _, x_max = plt.gca().get_xlim()
            plt.hlines(1, 0, x_max, color="gray", zorder=-1000)
            _, y_max = plt.gca().get_ylim()
            plt.vlines(1, 0, y_max, color="gray", zorder=-1000)
            plt.title("GOAL CODING STABLE CELLS PRE - POST")
            plt.show()

        return np.median(gain_gc)

    def phmm_modes_likelihoods(self, cells_to_use="stable", decoded_mode=True, log_likeli=True, plotting=True):
        # get pre phmm name
        if cells_to_use == "stable":
            model_name = self.pre.session_params.default_pre_phmm_model_stable
        elif cells_to_use == "decreasing":
            model_name = self.pre.session_params.default_pre_phmm_model_dec
        elif cells_to_use == "increasing":
            model_name = self.pre.session_params.default_pre_phmm_model_inc
        likeli_pre = self.pre.decode_awake_activity_time_binning(trials_to_use="all", model_name=model_name, cells_to_use=cells_to_use)[0]
        if log_likeli:
            likeli_pre = np.log(likeli_pre)
        if decoded_mode:
            likeli_pre_decoded = np.max(likeli_pre, axis=1)
        else:
            likeli_pre_decoded = likeli_pre.flatten()
        likeli_post = self.post.decode_awake_activity_time_binning(model_name=model_name, cells_to_use=cells_to_use)[0]
        if log_likeli:
            likeli_post = np.log(likeli_post)
        if decoded_mode:
            likeli_post_decoded = np.max(likeli_post, axis=1)
        else:
            likeli_post_decoded = likeli_post.flatten()

        print("PRE < POST")
        print(mannwhitneyu(likeli_post_decoded, likeli_pre_decoded, alternative="greater"))
        print("PRE > POST")
        print(mannwhitneyu(likeli_post_decoded, likeli_pre_decoded, alternative="less"))

        if plotting:
            res = [likeli_pre_decoded, likeli_post_decoded]
            labels=["pre", "post"]
            c = "white"
            bplot = plt.boxplot(res, positions=[1, 2], patch_artist=True,
                                labels=labels,
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            if log_likeli:
                plt.ylabel("log-likelihood")
            else:
                plt.ylabel("likelihood")
            plt.show()

        else:
            return np.nanmedian(likeli_post_decoded), np.nanmedian(likeli_pre_decoded)

    def get_optimal_number_states(self):
        # --------------------------------------------------------------------------------------------------------------
        # cross validation of poisson hmm fits to data
        #
        # args:     - cl_ar, range object: #clusters to fit to data
        # --------------------------------------------------------------------------------------------------------------

        optimal_number_pre = self.pre.get_optimal_number_states()
        optimal_number_post = self.post.get_optimal_number_states()

        return optimal_number_pre, optimal_number_post

    def pre_post_modes_cell_contribution(self, plotting=False):

        # load only stable cells
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_cells = class_dic["stable_cell_ids"].flatten()
        inc_cells = class_dic["increase_cell_ids"].flatten()
        dec_cells = class_dic["decrease_cell_ids"].flatten()

        # get PRE model
        with open(self.params.pre_proc_dir + "phmm/" + self.default_pre_phmm_model + ".pkl", "rb") as file:
            pre_model = pickle.load(file)

        pre_model_means = pre_model.means_
        # z-score for visualization
        pre_model_means_z = zscore(pre_model_means, axis=1)

        # get POST model
        with open(self.params.pre_proc_dir + "phmm/" + self.default_post_phmm_model + ".pkl", "rb") as file:
            post_model = pickle.load(file)

        post_model_means = post_model.means_
        # z-score for visualization
        post_model_means_z = zscore(post_model_means, axis=1)

        post_modes_mean_per_cell = np.mean(post_model_means, axis=0)
        pre_modes_mean_per_cell = np.mean(pre_model_means, axis=0)

        # normalized ratio
        nom = post_modes_mean_per_cell - pre_modes_mean_per_cell
        denom = post_modes_mean_per_cell + pre_modes_mean_per_cell

        norm_ratio = nom/denom

        res = [norm_ratio[stable_cells], norm_ratio[dec_cells], norm_ratio[inc_cells]]

        if plotting:
            plt.figure(figsize=(4, 8))
            c = "white"
            bplot = plt.boxplot(res, positions=[1, 2, 3], patch_artist=True,
                                labels=["persistent", "decreasing", "increasing"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            plt.ylabel("Mean rate Recall states - Mean rate Acquisition states\n"
                       "Mean rate Recall states + Mean rate Acquisition states")
            colors = ["magenta", 'turquoise', "orange"]
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.show()

        return res

    def pre_post_model_decoding(self, plotting=True, model="phmm", save_fig=False,
                                n_moving_average_pop_vec=10, smoothing=False, z_score=False):

        # get spike rasters: PRE
        spike_rasters_pre = self.pre.get_spike_bin_raster(return_estimated_times=False)
        spike_rasters_pre = np.hstack(spike_rasters_pre)

        # filter empty bins
        spike_rasters_pre = spike_rasters_pre[:, np.sum(spike_rasters_pre, axis=0)>11]

        # get spike rasters: POST
        spike_rasters_post = self.post.get_spike_bin_raster(return_estimated_times=False)
        spike_rasters_post = np.hstack(spike_rasters_post)
        
        # filter empty bins
        spike_rasters_post = spike_rasters_post[:, np.sum(spike_rasters_post, axis=0)>11]


        # delete events which are too short --> TODO
        # --------------------------------------------------------------------------------------------------------------
        # nr_bins = np.array([x.shape[1] for x in merged_events])

        if model == "phmm":
            # get means of pre and post modes + compression factor
            pre_file_name = self.session_params.default_pre_phmm_model
            post_file_name = self.session_params.default_post_phmm_model
            compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

            # get pre and post model to do decoding
            with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
                pre_model_dic = pickle.load(f)
            # get means of model (lambdas) for decoding
            pre_mode_means = pre_model_dic.means_

            with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
                post_model_dic = pickle.load(f)
            # get means of model (lambdas) for decoding
            post_mode_means = post_model_dic.means_

            # do decoding and plot result
            # ----------------------------------------------------------------------------------------------------------
            # do the decoding using pre and post model
            pre_model_pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                      event_spike_rasters=spike_rasters_pre,
                                                      compression_factor=compression_factor)

            post_model_pre_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                       event_spike_rasters=spike_rasters_pre,
                                                       compression_factor=compression_factor)

            # do decoding and plot result for pre sleep
            # ----------------------------------------------------------------------------------------------------------
            # do the decoding using pre and post model
            pre_model_post_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                        event_spike_rasters=spike_rasters_post,
                                                        compression_factor=compression_factor)

            post_model_post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                         event_spike_rasters=spike_rasters_post,
                                                         compression_factor=compression_factor)

        elif model == "ising":

            pre_file_name = self.session_params.default_pre_ising_model
            post_file_name = self.session_params.default_post_ising_model

            with open(self.params.pre_proc_dir + 'awake_ising_maps/' + pre_file_name + '.pkl',
                      'rb') as f:
                pre_model_dic = pickle.load(f)
            pre_template_map = pre_model_dic["res_map"]

            with open(self.params.pre_proc_dir + 'awake_ising_maps/' + post_file_name + '.pkl',
                      'rb') as f:
                post_model_dic = pickle.load(f)
            post_template_map = post_model_dic["res_map"]


            # get time_bin_size of encoding
            time_bin_size_encoding = pre_model_dic["time_bin_size"]

            # load correct compression factor (as defined in parameter file of the session)
            if time_bin_size_encoding == 0.01:
                compression_factor = \
                    np.round(self.session_params.sleep_compression_factor_12spikes_100ms * 10, 3)
            elif time_bin_size_encoding == 0.1:
                compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

            pre_model_pre_likeli = decode_using_ising_map_fast(mode_means=pre_template_map,
                                                      event_spike_rasters=spike_rasters_pre,
                                                      compression_factor=compression_factor)

            post_model_pre_likeli = decode_using_ising_map_fast(mode_means=post_template_map,
                                                       event_spike_rasters=spike_rasters_pre,
                                                       compression_factor=compression_factor)

            # do decoding and plot result for pre sleep
            # ----------------------------------------------------------------------------------------------------------
            # do the decoding using pre and post model
            pre_model_post_likeli = decode_using_ising_map_fast(mode_means=pre_template_map,
                                                        event_spike_rasters=spike_rasters_post,
                                                        compression_factor=compression_factor)

            post_model_post_likeli = decode_using_ising_map_fast(mode_means=post_template_map,
                                                         event_spike_rasters=spike_rasters_post,
                                                         compression_factor=compression_factor)

        max_pre_model_pre_likeli = np.max(pre_model_pre_likeli, axis=1)
        max_post_model_pre_likeli = np.max(post_model_pre_likeli, axis=1)

        sim_ratio_pre = (max_post_model_pre_likeli - max_pre_model_pre_likeli) / (max_post_model_pre_likeli + max_pre_model_pre_likeli)

        max_pre_model_post_likeli = np.max(pre_model_post_likeli, axis=1)
        max_post_model_post_likeli = np.max(post_model_post_likeli, axis=1)

        sim_ratio_post = (max_post_model_post_likeli - max_pre_model_post_likeli) / (max_post_model_post_likeli + max_pre_model_post_likeli)

        # cut to same length for the ratio
        # --------------------------------------------------------------------------------------------------------------

        # take the log
        max_pre_log_likeli_pre = np.log(max_pre_model_pre_likeli)
        max_pre_log_likeli_post = np.log(max_pre_model_post_likeli)
        max_post_log_likeli_pre = np.log(max_post_model_pre_likeli)
        max_post_log_likeli_post = np.log(max_post_model_post_likeli)

        # interpolate values --> likelihoods
        # --------------------------------------------------------------------------------------------------------------
        # max_pre_log_likeli_post = np.interp(np.linspace(0, len(max_pre_log_likeli_post), len(max_pre_log_likeli_pre)),
        #                                  np.arange(len(max_pre_log_likeli_post)), max_pre_log_likeli_post)
        #
        # max_post_log_likeli_post = np.interp(np.linspace(0, len(max_post_log_likeli_post), len(max_post_log_likeli_pre)),
        #                                   np.arange(len(max_post_log_likeli_post)), max_post_log_likeli_post)


        # interpolate values --> sim_ratio
        # --------------------------------------------------------------------------------------------------------------
        # sim_ratio_post = np.interp(np.linspace(0, len(sim_ratio_post), len(sim_ratio_pre)),
        #                         np.arange(len(sim_ratio_post)), sim_ratio_post)

        if smoothing:
            # apply smoothing
            max_pre_log_likeli_pre   = uniform_filter1d(max_pre_log_likeli_pre, n_moving_average_pop_vec)
            max_pre_log_likeli_post = uniform_filter1d(max_pre_log_likeli_post, n_moving_average_pop_vec)
            max_post_log_likeli_pre = uniform_filter1d(max_post_log_likeli_pre, n_moving_average_pop_vec)
            max_post_log_likeli_post = uniform_filter1d(max_post_log_likeli_post, n_moving_average_pop_vec)


        if plotting or save_fig:

            c = "white"
            res = [max_pre_log_likeli_s, max_pre_log_likeli_j_s, max_post_log_likeli_s, max_post_log_likeli_j_s]
            bplot = plt.boxplot(res, positions=[1, 2, 3, 4], patch_artist=True,
                                labels=["PRE model: PRE", "PRE model: POST", "POST model: PRE", "POST model: POST"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            colors = ["magenta", 'magenta', "blue", "blue"]
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            plt.ylabel("Max log-likelihood")
            plt.grid(color="grey", axis="y")
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.show()

        # if z_score:
        #     # zscore to be able to combine results for all sessions
        #     temp_pre = np.hstack((max_pre_log_likeli_s, max_pre_log_likeli_j_s))
        #     temp_pre_z = zscore(temp_pre)
        #     max_pre_log_likeli_pre = temp_pre_z[:max_pre_log_likeli_s.shape[0]]
        #     max_pre_log_likeli_post = temp_pre_z[max_pre_log_likeli_s.shape[0]:]
        #
        #     # zscore to be able to combine results for all sessions: POST model
        #     temp_post = np.hstack((max_post_log_likeli_s, max_post_log_likeli_j_s))
        #     temp_post_z = zscore(temp_post)
        #     max_post_log_likeli_pre = temp_post_z[:max_post_log_likeli_s.shape[0]]
        #     max_post_log_likeli_post = temp_post_z[max_post_log_likeli_s.shape[0]:]


        return max_pre_log_likeli_pre, max_pre_log_likeli_post, max_post_log_likeli_pre, max_post_log_likeli_post

    # </editor-fold>

    # <editor-fold desc="Others">

    def sparsity_and_distance_of_modes(self, plotting=True, use_spike_bins=False, n_spikes_per_bin=12, metric="cosine"):

        sparsity_pre, distance_pre = self.pre.sparsity_and_distance_of_modes(use_spike_bins=use_spike_bins,
                                                                             metric=metric,
                                                                             n_spikes_per_bin=n_spikes_per_bin)

        sparsity_post, distance_post = self.post.sparsity_and_distance_of_modes(use_spike_bins=use_spike_bins,
                                                                             metric=metric,
                                                                             n_spikes_per_bin=n_spikes_per_bin)

        if plotting:
            c = "white"
            res = [sparsity_pre, sparsity_post]
            bplot = plt.boxplot(res, positions=[1, 2], patch_artist=True,
                                labels=["PRE modes", "POST modes"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            plt.ylabel("Sparsity")
            plt.title(mannwhitneyu(sparsity_pre, sparsity_post))
            plt.show()


            c = "white"
            res = [distance_pre, distance_post]
            bplot = plt.boxplot(res, positions=[1, 2], patch_artist=True,
                                labels=["PRE modes", "POST modes"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            plt.ylabel("Distance between modes (cosine)")
            plt.title(mannwhitneyu(distance_pre, distance_post))
            plt.show()

        return sparsity_pre, sparsity_post, distance_pre, distance_post

    def cell_type_correlations(self, awake_smoothing = 3):

        # load cell labels
        # --------------------------------------------------------------------------------------------------------------
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.params.session_name + "_"+self.params.stable_cell_method +".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_cells = class_dic["stable_cell_ids"].flatten()
        inc_cells = class_dic["increase_cell_ids"].flatten()
        dec_cells = class_dic["decrease_cell_ids"].flatten()

        n_stable_cells = len(stable_cells)
        n_inc_cells = len(inc_cells)
        n_dec_cells = len(dec_cells)

        # PRE DATA
        # --------------------------------------------------------------------------------------------------------------
        raster_pre = self.pre.get_raster()
        # apply smoothing
        raster_pre = uniform_filter1d(raster_pre, size=awake_smoothing, axis=1)

        # select stable and dec for pre
        raster_stable_dec_pre = zscore(np.vstack((raster_pre[stable_cells, :], raster_pre[dec_cells, :])), axis=1)

        # compute all correlations
        all_corr_stable_dec_pre = np.corrcoef(raster_stable_dec_pre)

        # select all within stable correlations
        corr_stable_pre = upper_tri_without_diag(all_corr_stable_dec_pre[:n_stable_cells,:n_stable_cells])

        # select within dec correlations
        # select all within stable correlations
        corr_dec_pre = upper_tri_without_diag(all_corr_stable_dec_pre[n_stable_cells+1:,n_stable_cells+1:])

        # select all stable to dec correlations
        corr_stable_dec_pre = all_corr_stable_dec_pre[:n_stable_cells, n_stable_cells+1:].flatten()
        plt.subplot(2,1,1)
        plt.hist(corr_stable_pre, np.arange(np.min(np.hstack((corr_stable_pre,corr_stable_dec_pre))),
                 np.max(np.hstack((corr_stable_pre,corr_stable_dec_pre))),0.02), density=True, color="#ffdba1",label="STABLE")
        plt.hist(corr_stable_dec_pre,np.arange(np.min(np.hstack((corr_stable_pre,corr_stable_dec_pre))),
                 np.max(np.hstack((corr_stable_pre,corr_stable_dec_pre))),0.02), density=True, color="blue", alpha=0.5, label="STABLE-DEC")
        plt.title("CORRELATIONS - PRE")
        plt.grid(color="grey")
        plt.ylabel("DENSITY")
        plt.legend()
        plt.subplot(2,1,2)
        plt.hist(corr_stable_pre, np.arange(np.min(np.hstack((corr_stable_pre,corr_dec_pre))),
                 np.max(np.hstack((corr_stable_pre,corr_dec_pre))),0.02), density=True, color="#ffdba1",label="STABLE")
        plt.hist(corr_dec_pre, np.arange(np.min(np.hstack((corr_stable_pre,corr_dec_pre))),
                 np.max(np.hstack((corr_stable_pre,corr_dec_pre))),0.02), density=True, color="#a0c4e4", label="DEC", alpha=0.5)
        plt.grid(color="grey")
        plt.ylabel("DENSITY")
        plt.legend()
        plt.xlabel("CORRELATIONS")
        plt.show()

        # POST DATA
        # --------------------------------------------------------------------------------------------------------------
        raster_post = self.post.get_raster()
        # apply smoothing
        raster_post = uniform_filter1d(raster_post, size=awake_smoothing, axis=1)

        # select stable and dec for pre
        raster_stable_inc_post = zscore(np.vstack((raster_post[stable_cells, :], raster_post[inc_cells, :])), axis=1)

        # compute all correlations
        all_corr_stable_inc_post = np.corrcoef(raster_stable_inc_post)

        # select all within stable correlations
        corr_stable_post = upper_tri_without_diag(all_corr_stable_inc_post[:n_stable_cells, :n_stable_cells])

        # select within dec correlations
        # select all within stable correlations
        corr_inc_post = upper_tri_without_diag(all_corr_stable_inc_post[n_stable_cells + 1:, n_stable_cells + 1:])

        # select all stable to inc correlations
        corr_stable_inc_post = all_corr_stable_inc_post[:n_stable_cells, n_stable_cells + 1:].flatten()


        plt.subplot(2, 1, 1)
        plt.hist(corr_stable_post, np.arange(np.min(np.hstack((corr_stable_post, corr_stable_inc_post))),
                                            np.max(np.hstack((corr_stable_post, corr_stable_inc_post))), 0.02),
                 density=True, color="#ffdba1", label="STABLE")
        plt.hist(corr_stable_dec_pre, np.arange(np.min(np.hstack((corr_stable_post, corr_stable_inc_post))),
                                                np.max(np.hstack((corr_stable_post, corr_stable_inc_post))), 0.02),
                 density=True, color="red", alpha=0.5, label="STABLE-INC")
        plt.title("CORRELATIONS - POST")
        plt.grid(color="grey")
        plt.ylabel("DENSITY")
        plt.legend()
        plt.subplot(2, 1, 2)
        plt.hist(corr_stable_post, np.arange(np.min(np.hstack((corr_stable_post, corr_inc_post))),
                                            np.max(np.hstack((corr_stable_post, corr_inc_post))), 0.02), density=True,
                 color="#ffdba1", label="STABLE")
        plt.hist(corr_inc_post, np.arange(np.min(np.hstack((corr_stable_post, corr_inc_post))),
                                         np.max(np.hstack((corr_stable_post, corr_inc_post))), 0.02), density=True,
                 color="#f7959c", label="INC", alpha=0.5)
        plt.grid(color="grey")
        plt.ylabel("DENSITY")
        plt.legend()
        plt.xlabel("CORRELATIONS")
        plt.show()

        # corr_test = upper_tri_without_diag(np.corrcoef(raster_stable_pre_z))

    def distinguishing_goals(self, plotting=True):

        stable, dec, inc, all = self.pre.distinguishing_goals(plotting=False)

        stable = stable[~np.isnan(stable)]
        dec = dec[~np.isnan(dec)]
        inc = inc[~np.isnan(inc)]
        all = all[~np.isnan(all)]

        if plotting:
            c="white"
            res = [stable, dec, inc, all]
            bplot = plt.boxplot(res, positions=[1,2,3,4], patch_artist=True,
                                labels=["Stable", "Dec", "Inc", "All"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c),showfliers=False)
            plt.ylabel("Mean accuracy SVM")
            plt.title("PRE")
            plt.show()

        stable, dec, inc, all_cells = self.post.distinguishing_goals(plotting=False)

        stable = stable[~np.isnan(stable)]
        dec = dec[~np.isnan(dec)]
        inc = inc[~np.isnan(inc)]
        all_cells = all[~np.isnan(all_cells)]

        if plotting:
            c="white"
            res = [stable, dec, inc, all_cells]
            bplot = plt.boxplot(res, positions=[1,2,3,4], patch_artist=True,
                                labels=["Stable", "Dec", "Inc", "All"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c),showfliers=False)
            plt.ylabel("Mean accuracy SVM")
            plt.title("POST")
            plt.show()

    def identify_single_goals(self, plotting=True):

        stable, dec, inc, all = self.pre.identify_single_goal(plotting=False)

        stable = stable[~np.isnan(stable)]
        dec = dec[~np.isnan(dec)]
        inc = inc[~np.isnan(inc)]
        all = all[~np.isnan(all)]

        if plotting:
            c="white"
            res = [stable, dec, inc, all]
            bplot = plt.boxplot(res, positions=[1,2,3,4], patch_artist=True,
                                labels=["Stable", "Dec", "Inc", "All"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c),showfliers=False)
            plt.ylabel("Mean accuracy SVM - Multiclass")
            plt.title("PRE")
            plt.show()

        stable, dec, inc, all_cells = self.post.identify_single_goal(plotting=False)

        stable = stable[~np.isnan(stable)]
        dec = dec[~np.isnan(dec)]
        inc = inc[~np.isnan(inc)]
        all_cells = all[~np.isnan(all_cells)]

        if plotting:
            c="white"
            res = [stable, dec, inc, all_cells]
            bplot = plt.boxplot(res, positions=[1,2,3,4], patch_artist=True,
                                labels=["Stable", "Dec", "Inc", "All"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c),showfliers=False)
            plt.ylabel("Mean accuracy SVM - Multiclass")
            plt.title("POST")
            plt.show()
    
    def plot_rate_maps_stable(self, spatial_resolution=2, gaussian_std=2, save_fig=False):
        pre_rm = self.pre.get_rate_maps(spatial_resolution=spatial_resolution,gaussian_std=gaussian_std)
        pre_occ = self.pre.get_occ_map(spatial_resolution=spatial_resolution)
        env_dim_pre = self.pre.get_env_dim()
        post_rm = self.post.get_rate_maps(spatial_resolution=spatial_resolution,gaussian_std=gaussian_std, env_dim=env_dim_pre)
        # goal locations
        gl = self.pre.session_params.goal_locations

        # offset for plotting:
        if self.session_name == "mjc163R4R_0114":
            x_off = 40
            y_off = 20
            stable_cells_to_plot = np.array([3, 7, 12])
        elif self.session_name == "mjc163R2R_0114":
            x_off = 45
            y_off = 5
            stable_cells_to_plot = np.array([6, 15, 12])
        elif self.session_name == "mjc169R1R_0114":
            x_off = 40
            y_off = 15
            stable_cells_to_plot = np.array([2, 10, 12])
        elif self.session_name == "mjc169R4R_0114":
            x_off = 40
            y_off = 10
            stable_cells_to_plot = np.array([4, 11, 12])
        elif self.session_name == "mjc163R1L_0114":
            x_off = 50
            y_off = 10
            stable_cells_to_plot = np.array([7, 11, 12])
        elif self.session_name == "mjc148R4R_0113":
            x_off = 32
            y_off = 5
            stable_cells_to_plot = np.array([0, 2, 3])
        elif self.session_name == "mjc163R3L_0114":
            x_off = 40
            y_off = 25
            stable_cells_to_plot = np.array([7, 2, 3])

        plt.imshow(pre_occ.T)
        for g_l in gl:
            plt.scatter((g_l[0] - self.pre.x_min - x_off) / (spatial_resolution/self.pre.spatial_factor),
                        (g_l[1] - self.pre.y_min - y_off) / (spatial_resolution/self.pre.spatial_factor),
                        label="goal locations", marker="x", color="black", s=30, zorder=1000000)
        plt.show()

        # get cell classification
        with open(self.params.pre_proc_dir + "cell_classification/" +
                      self.session_name + "_" + self.params.stable_cell_method + ".pickle","rb") as f:
            class_dic = pickle.load(f)


        stable_ids = class_dic["stable_cell_ids"].flatten()

        pre_rm_stable = pre_rm[:, :, stable_ids]
        post_rm_stable = post_rm[:, :, stable_ids]
        cmap0 = "viridis"
        # for i, (pre, post) in enumerate(zip(pre_rm_stable.T, post_rm_stable.T)):
        #     # max_val = np.max([np.max(np.max(pre)), np.max(np.max(post))])
        #     plt.subplot(1,2,1)
        #     plt.imshow(pre, norm=NonLinearNormalize(vmin=0.0, vmax=np.max(np.max(pre)), a1=8, a2=0.6))
        #     plt.text(10,10, str(np.round(np.max(np.max(pre)),2)))
        #     plt.subplot(1,2,2)
        #     plt.imshow(post, norm=NonLinearNormalize(vmin=0.0, vmax=np.max(np.max(post)), a1=8, a2=0.6))
        #     plt.text(10,10, str(np.round(np.max(np.max(post)),2)))
        #     plt.title(i)
        #     plt.show()
        # mjc163R4R_0114
        # --------------------------------------------------------------------------------------------------------------
        # first 3 rows, first column
        # stable_cells_to_plot = np.array([21, 3, 36])
        # first 3 rows, second column
        # stable_cells_to_plot = np.array([7, 8, 9])
        # second 3 rows, first column
        # stable_cells_to_plot = np.array([12, 19, 28])
        # second 3 rows, second column
        # stable_cells_to_plot = np.array([29, 30, 32])

        # mjc169R1R_0114
        # --------------------------------------------------------------------------------------------------------------
        # first 3 rows, first column
        # stable_cells_to_plot = np.array([2, 10, 12])

        # x = np.linspace(0,1, 100)
        # # y = 1./(1+np.exp(-0.1*(x-2)))
        # a1 = 8
        # a2 = 0.6
        # y = 1./(1+np.exp(-a1*(x-a2)))
        # plt.plot(x,y)
        # plt.show()

        import matplotlib as mpl

        # max_val = np.max(np.max(pre_rm_stable[:,:,2]))
        # stable_cells_to_plot = np.array([2, 10, 12])
        # cmap0 = "viridis"
        # plt.subplot(1,2,1)
        # plt.imshow(pre_rm_stable[:, :, 2].T, cmap=cmap0)
        # plt.colorbar()
        # plt.subplot(1,2,2)
        # fig, ax = plt.subplots(1,1)
        # plt.imshow(pre_rm_stable[:, :, 2].T, cmap=cmap0, norm=NonLinearNormalize(vmin=0.0, vmax=max_val, a1=10, a2=0.8))
        # plt.imshow(pre_rm_stable[:, :, 2].T, cmap=cmap0, norm=PowerNorm(0.02, vmin=0, vmax=max_val))
        # bounds = np.arange(0,max_val)
        # stretched_bounds = np.interp(np.linspace(0, 1, 257), np.linspace(0, 1, len(bounds)), bounds)
        # norm = mpl.colors.BoundaryNorm(stretched_bounds, ncolors=256)
        # # create the colorbar
        # cb = mpl.colorbar.ColorbarBase(ax2, cmap=cmap0, norm=norm, ticks=bounds)
        # plt.colorbar()
        # plt.show()

        pre_rm_stable = pre_rm_stable[:,:, stable_cells_to_plot]
        post_rm_stable =post_rm_stable[:,:, stable_cells_to_plot]

        plt.style.use('default')
        from matplotlib.colors import LinearSegmentedColormap
        # cmap0 = LinearSegmentedColormap.from_list('', ['white', 'red'])
        cmap0 = "viridis"
        fig = plt.figure(figsize=(8, 12))
        # fig = plt.figure(figsize=(8, 12))
        gs = fig.add_gridspec(26, 20)
        ax1 = fig.add_subplot(gs[:8, :10])
        ax2 = fig.add_subplot(gs[:8, 10:])
        max_val = np.max(np.max(pre_rm_stable[:,:,0]))
        ax1.imshow(pre_rm_stable[:,:,0].T, cmap=cmap0, norm=NonLinearNormalize(vmin=0.0, vmax=max_val, a1=8, a2=0.6))
        ax1.text(20,3, np.round(max_val,2), color="red")
        for g_l in gl:
            ax1.scatter((g_l[0] - self.pre.x_min - x_off) / (spatial_resolution/self.pre.spatial_factor),
                        (g_l[1] - self.pre.y_min - y_off) / (spatial_resolution/self.pre.spatial_factor),
                        label="goal locations", marker="x", color="black", s=30, zorder=1000000)
        ax1.set_yticks([])
        ax1.set_xticks([])
        ax1.axis("off")
        ax1.set_title("Acquisition")
        max_val = np.max(np.max(post_rm_stable[:, :, 0]))
        ax2.imshow(post_rm_stable[:,:,0].T, cmap=cmap0, norm=NonLinearNormalize(vmin=0.0, vmax=max_val, a1=8, a2=0.6))
        ax2.text(20,3, np.round(max_val,2), color="red")
        for g_l in gl:
            ax2.scatter((g_l[0] - self.pre.x_min - x_off) / (spatial_resolution/self.pre.spatial_factor),
                        (g_l[1] - self.pre.y_min - y_off) / (spatial_resolution/self.pre.spatial_factor),
                        label="goal locations", marker="x", color="black", s=30, zorder=1000000)

        ax2.set_yticks([])
        ax2.set_xticks([])
        ax2.set_title("Recall")
        ax2.axis("off")
        # second cell
        ax4 = fig.add_subplot(gs[9:17, :10])
        ax5 = fig.add_subplot(gs[9:17, 10:])
        max_val = np.max(np.max(pre_rm_stable[:,:,1]))
        ax4.imshow(pre_rm_stable[:,:,1].T, cmap=cmap0, norm=NonLinearNormalize(vmin=0.0, vmax=max_val, a1=8, a2=0.6))
        ax4.text(20,3, np.round(max_val,2), color="red")
        for g_l in gl:
            ax4.scatter((g_l[0] - self.pre.x_min - x_off) / (spatial_resolution/self.pre.spatial_factor),
                        (g_l[1] - self.pre.y_min - y_off) / (spatial_resolution/self.pre.spatial_factor),
                        label="goal locations", marker="x", color="black", s=30, zorder=1000000)
        ax4.set_yticks([])
        ax4.set_xticks([])
        ax4.axis("off")
        max_val = np.max(np.max(post_rm_stable[:,:,1]))
        ax5.imshow(post_rm_stable[:,:,1].T, cmap=cmap0, norm=NonLinearNormalize(vmin=0.0, vmax=max_val, a1=8, a2=0.6))
        ax5.text(20,3, np.round(max_val,2), color="red")
        for g_l in gl:
            ax5.scatter((g_l[0] - self.pre.x_min - x_off) / (spatial_resolution/self.pre.spatial_factor),
                        (g_l[1] - self.pre.y_min - y_off) / (spatial_resolution/self.pre.spatial_factor),
                        label="goal locations", marker="x", color="black", s=30, zorder=1000000)

        ax5.set_yticks([])
        ax5.set_xticks([])
        ax5.axis("off")

        # third cell
        ax7 = fig.add_subplot(gs[18:26, :10])
        ax8 = fig.add_subplot(gs[18:26, 10:])
        max_val = np.max(np.max(pre_rm_stable[:,:,2]))
        ax7.imshow(pre_rm_stable[:,:,2].T, cmap=cmap0, norm=NonLinearNormalize(vmin=0.0, vmax=max_val, a1=8, a2=0.6))
        ax7.text(20,3, np.round(max_val,2), color="red")
        for g_l in gl:
            ax7.scatter((g_l[0] - self.pre.x_min - x_off) / (spatial_resolution/self.pre.spatial_factor),
                        (g_l[1] - self.pre.y_min - y_off) / (spatial_resolution/self.pre.spatial_factor),
                        label="goal locations", marker="x", color="black", s=30, zorder=1000000)
        ax7.set_yticks([])
        ax7.set_xticks([])
        ax7.axis("off")
        max_val = np.max(np.max(post_rm_stable[:,:,2]))
        ax8.imshow(post_rm_stable[:,:,2].T, cmap=cmap0, norm=NonLinearNormalize(vmin=0.0, vmax=max_val, a1=8, a2=0.6))
        ax8.text(20,3, np.round(max_val,2), color="red")
        for g_l in gl:
            ax8.scatter((g_l[0] - self.pre.x_min - x_off) / (spatial_resolution/self.pre.spatial_factor),
                        (g_l[1] - self.pre.y_min - y_off) / (spatial_resolution/self.pre.spatial_factor),
                        label="goal locations", marker="x", color="black", s=30, zorder=1000000)

        ax8.set_yticks([])
        ax8.set_xticks([])
        ax8.axis("off")
        plt.tight_layout()
        if save_fig:
            plt.rcParams['svg.fonttype'] = 'none'
            plt.savefig(os.path.join(save_path, "rate_maps_stable_cells.svg"), transparent="True")
        else:
            plt.show()

    def plot_rate_maps_dec(self, spatial_resolution=2, gaussian_std=2, save_fig=False):
        pre_rm = self.pre.get_rate_maps(spatial_resolution=spatial_resolution,gaussian_std=gaussian_std)
        env_dim_pre = self.pre.get_env_dim()
        post_rm = self.post.get_rate_maps(spatial_resolution=spatial_resolution,gaussian_std=gaussian_std, env_dim=env_dim_pre)
        # goal locations
        gl = self.pre.session_params.goal_locations

        # offset for plotting:
        if self.session_name == "mjc163R4R_0114":
            x_off = 40
            y_off = 20
            # selected cells
            # ----------------------------------------------------------------------------------------------------------
            dec_cells_to_plot = np.array([7, 44, 27])
        elif self.session_name == "mjc163R2R_0114":
            x_off = 45
            y_off = 5
            dec_cells_to_plot = np.array([20, 24, 28])
        elif self.session_name == "mjc169R1R_0114":
            x_off = 40
            y_off = 15
        elif self.session_name == "mjc169R4R_0114":
            x_off = 40
            y_off = 10
        elif self.session_name == "mjc163R1L_0114":
            x_off = 50
            y_off = 10
        elif self.session_name == "mjc148R4R_0113":
            x_off = 32
            y_off = 5
        elif self.session_name == "mjc163R3L_0114":
            x_off = 40
            y_off = 25

        # get cell classification


        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle","rb") as f:
            class_dic = pickle.load(f)

        dec_ids = class_dic["decrease_cell_ids"].flatten()

        pre_rm_dec = pre_rm[:, :, dec_ids]
        post_rm_dec = post_rm[:, :, dec_ids]

        # cmap0 = "viridis"
        # for i, (pre, post) in enumerate(zip(pre_rm_dec.T, post_rm_dec.T)):
        #     # max_val = np.max([np.max(np.max(pre)), np.max(np.max(post))])
        #     plt.subplot(1,2,1)
        #     plt.imshow(pre, norm=NonLinearNormalize(vmin=0.0, vmax=np.max(np.max(pre)), a1=8, a2=0.6))
        #     plt.text(10,10, str(np.round(np.max(np.max(pre)),2)))
        #     plt.subplot(1,2,2)
        #     plt.imshow(post, norm=NonLinearNormalize(vmin=0.0, vmax=np.max(np.max(post)), a1=8, a2=0.6))
        #     plt.text(10,10, str(np.round(np.max(np.max(post)),2)))
        #     plt.title(i)
        #     plt.show()

        pre_rm_dec = pre_rm_dec[:,:, dec_cells_to_plot]
        post_rm_dec =post_rm_dec[:,:, dec_cells_to_plot]

        plt.style.use('default')
        from matplotlib.colors import LinearSegmentedColormap
        # cmap0 = LinearSegmentedColormap.from_list('', ['white', 'red'])
        cmap0 = "viridis"
        fig = plt.figure(figsize=(8, 12))
        # fig = plt.figure(figsize=(8, 12))
        gs = fig.add_gridspec(26, 20)
        ax1 = fig.add_subplot(gs[:8, :10])
        ax2 = fig.add_subplot(gs[:8, 10:])
        max_val = np.max(np.max(pre_rm_dec[:,:,0]))
        ax1.imshow(pre_rm_dec[:,:,0].T, cmap=cmap0, norm=NonLinearNormalize(vmin=0.0, vmax=max_val, a1=8, a2=0.6))
        ax1.text(20,3, np.round(max_val,2), color="red")
        for g_l in gl:
            ax1.scatter((g_l[0] - self.pre.x_min - x_off) / (spatial_resolution/self.pre.spatial_factor),
                        (g_l[1] - self.pre.y_min - y_off) / (spatial_resolution/self.pre.spatial_factor),
                        label="goal locations", marker="x", color="black", s=30, zorder=1000000)
        ax1.set_yticks([])
        ax1.set_xticks([])
        ax1.axis("off")
        ax1.set_title("Acquisition")
        max_val = np.max(np.max(post_rm_dec[:,:,0]))
        ax2.imshow(post_rm_dec[:,:,0].T, cmap=cmap0, norm=NonLinearNormalize(vmin=0.0, vmax=max_val, a1=8, a2=0.6))
        ax2.text(20,3, np.round(max_val,2), color="red")
        for g_l in gl:
            ax2.scatter((g_l[0] - self.pre.x_min - x_off) / (spatial_resolution/self.pre.spatial_factor),
                        (g_l[1] - self.pre.y_min - y_off) / (spatial_resolution/self.pre.spatial_factor),
                        label="goal locations", marker="x", color="black", s=30, zorder=1000000)

        ax2.set_yticks([])
        ax2.set_xticks([])
        ax2.set_title("Recall")
        ax2.axis("off")
        # second cell
        ax4 = fig.add_subplot(gs[9:17, :10])
        ax5 = fig.add_subplot(gs[9:17, 10:])
        max_val = np.max(np.max(pre_rm_dec[:,:,1]))
        ax4.imshow(pre_rm_dec[:,:,1].T, cmap=cmap0, norm=NonLinearNormalize(vmin=0.0, vmax=max_val, a1=8, a2=0.6))
        ax4.text(20,3, np.round(max_val,2), color="red")
        for g_l in gl:
            ax4.scatter((g_l[0] - self.pre.x_min - x_off) / (spatial_resolution/self.pre.spatial_factor),
                        (g_l[1] - self.pre.y_min - y_off) / (spatial_resolution/self.pre.spatial_factor),
                        label="goal locations", marker="x", color="black", s=30, zorder=1000000)
        ax4.set_yticks([])
        ax4.set_xticks([])
        ax4.axis("off")
        max_val = np.max(np.max(post_rm_dec[:,:,1]))
        ax5.imshow(post_rm_dec[:,:,1].T, cmap=cmap0, norm=NonLinearNormalize(vmin=0.0, vmax=max_val, a1=8, a2=0.6))
        ax5.text(20,3, np.round(max_val,2), color="red")
        for g_l in gl:
            ax5.scatter((g_l[0] - self.pre.x_min - x_off) / (spatial_resolution/self.pre.spatial_factor),
                        (g_l[1] - self.pre.y_min - y_off) / (spatial_resolution/self.pre.spatial_factor),
                        label="goal locations", marker="x", color="black", s=30, zorder=1000000)

        ax5.set_yticks([])
        ax5.set_xticks([])
        ax5.axis("off")

        # third cell
        ax7 = fig.add_subplot(gs[18:26, :10])
        ax8 = fig.add_subplot(gs[18:26, 10:])
        max_val = np.max(np.max(pre_rm_dec[:,:,2]))
        ax7.imshow(pre_rm_dec[:,:,2].T, cmap=cmap0, norm=NonLinearNormalize(vmin=0.0, vmax=max_val, a1=8, a2=0.6))
        ax7.text(20,3, np.round(max_val,2), color="red")
        for g_l in gl:
            ax7.scatter((g_l[0] - self.pre.x_min - x_off) / (spatial_resolution/self.pre.spatial_factor),
                        (g_l[1] - self.pre.y_min - y_off) / (spatial_resolution/self.pre.spatial_factor),
                        label="goal locations", marker="x", color="black", s=30, zorder=1000000)
        ax7.set_yticks([])
        ax7.set_xticks([])
        ax7.axis("off")
        max_val = np.max(np.max(post_rm_dec[:,:,2]))
        ax8.imshow(post_rm_dec[:,:,2].T, cmap=cmap0, norm=NonLinearNormalize(vmin=0.0, vmax=max_val, a1=8, a2=0.6))
        ax8.text(20,3, np.round(max_val,2), color="red")
        for g_l in gl:
            ax8.scatter((g_l[0] - self.pre.x_min - x_off) / (spatial_resolution/self.pre.spatial_factor),
                        (g_l[1] - self.pre.y_min - y_off) / (spatial_resolution/self.pre.spatial_factor),
                        label="goal locations", marker="x", color="black", s=30, zorder=1000000)

        ax8.set_yticks([])
        ax8.set_xticks([])
        ax8.axis("off")
        plt.tight_layout()
        if save_fig:
            plt.rcParams['svg.fonttype'] = 'none'
            plt.savefig(os.path.join(save_path, "rate_maps_dec_cells.svg"), transparent="True")
        else:
            plt.show()

    def plot_rate_maps_inc(self, spatial_resolution=2, gaussian_std=2, save_fig=False):
        pre_rm = self.pre.get_rate_maps(spatial_resolution=spatial_resolution,gaussian_std=gaussian_std)
        env_dim_pre = self.pre.get_env_dim()
        post_rm = self.post.get_rate_maps(spatial_resolution=spatial_resolution,gaussian_std=gaussian_std, env_dim=env_dim_pre)
        # goal locations
        gl = self.pre.session_params.goal_locations

        # offset for plotting:
        if self.session_name == "mjc163R4R_0114":
            x_off = 40
            y_off = 20
            # selected cells
            # ----------------------------------------------------------------------------------------------------------
            # inc_cells_to_plot = np.array([2, 5, 22])
            inc_cells_to_plot = np.array([19, 22, 27])
        elif self.session_name == "mjc163R2R_0114":
            x_off = 45
            y_off = 5
            inc_cells_to_plot = np.array([6, 15, 12])
        elif self.session_name == "mjc169R1R_0114":
            x_off = 40
            y_off = 15
            inc_cells_to_plot = np.array([2, 10, 12])
        elif self.session_name == "mjc169R4R_0114":
            x_off = 40
            y_off = 10
            inc_cells_to_plot = np.array([4, 11, 12])
        elif self.session_name == "mjc163R1L_0114":
            x_off = 50
            y_off = 10
            inc_cells_to_plot = np.array([7, 11, 12])
        elif self.session_name == "mjc148R4R_0113":
            x_off = 32
            y_off = 5
            inc_cells_to_plot = np.array([0, 2, 3])
        elif self.session_name == "mjc163R3L_0114":
            x_off = 40
            y_off = 25
            inc_cells_to_plot = np.array([7, 2, 3])

        # get cell classification
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle","rb") as f:
            class_dic = pickle.load(f)

        inc_ids = class_dic["increase_cell_ids"].flatten()

        pre_rm_inc = pre_rm[:, :, inc_ids]
        post_rm_inc = post_rm[:, :, inc_ids]

        # for i, (pre, post) in enumerate(zip(pre_rm_inc.T, post_rm_inc.T)):
        #     max_val = np.max([np.max(np.max(pre)), np.max(np.max(post))])
        #     plt.subplot(1,2,1)
        #     plt.imshow(pre, vmin=0, vmax=max_val)
        #     plt.subplot(1,2,2)
        #     plt.imshow(post, vmin=0, vmax=max_val)
        #     plt.colorbar()
        #     plt.title(i)
        #     plt.show()
        #
        pre_rm_inc = pre_rm_inc[:,:, inc_cells_to_plot]
        post_rm_inc =post_rm_inc[:,:, inc_cells_to_plot]

        plt.style.use('default')
        from matplotlib.colors import LinearSegmentedColormap
        # cmap0 = LinearSegmentedColormap.from_list('', ['white', 'red'])
        cmap0 = "viridis"
        fig = plt.figure(figsize=(8, 12))
        # fig = plt.figure(figsize=(8, 12))
        gs = fig.add_gridspec(26, 20)
        ax1 = fig.add_subplot(gs[:8, :10])
        ax2 = fig.add_subplot(gs[:8, 10:])
        max_val = np.max(np.max(pre_rm_inc[:,:,0]))
        ax1.imshow(pre_rm_inc[:,:,0].T, cmap=cmap0, norm=NonLinearNormalize(vmin=0.0, vmax=max_val, a1=8, a2=0.6))
        ax1.text(20,3, np.round(max_val,2), color="red")
        for g_l in gl:
            ax1.scatter((g_l[0] - self.pre.x_min - x_off) / (spatial_resolution/self.pre.spatial_factor),
                        (g_l[1] - self.pre.y_min - y_off) / (spatial_resolution/self.pre.spatial_factor),
                        label="goal locations", marker="x", color="black", s=30, zorder=1000000)
        ax1.set_yticks([])
        ax1.set_xticks([])
        ax1.axis("off")
        ax1.set_title("Acquisition")
        max_val = np.max(np.max(post_rm_inc[:,:,0]))
        ax2.imshow(post_rm_inc[:,:,0].T, cmap=cmap0, norm=NonLinearNormalize(vmin=0.0, vmax=max_val, a1=8, a2=0.6))
        ax2.text(20,3, np.round(max_val,2), color="red")
        for g_l in gl:
            ax2.scatter((g_l[0] - self.pre.x_min - x_off) / (spatial_resolution/self.pre.spatial_factor),
                        (g_l[1] - self.pre.y_min - y_off) / (spatial_resolution/self.pre.spatial_factor),
                        label="goal locations", marker="x", color="black", s=30, zorder=1000000)

        ax2.set_yticks([])
        ax2.set_xticks([])
        ax2.set_title("Recall")
        ax2.axis("off")
        # second cell
        ax4 = fig.add_subplot(gs[9:17, :10])
        ax5 = fig.add_subplot(gs[9:17, 10:])
        max_val = np.max(np.max(pre_rm_inc[:,:,1]))
        ax4.imshow(pre_rm_inc[:,:,1].T, cmap=cmap0, norm=NonLinearNormalize(vmin=0.0, vmax=max_val, a1=8, a2=0.6))
        ax4.text(20,3, np.round(max_val,2), color="red")
        for g_l in gl:
            ax4.scatter((g_l[0] - self.pre.x_min - x_off) / (spatial_resolution/self.pre.spatial_factor),
                        (g_l[1] - self.pre.y_min - y_off) / (spatial_resolution/self.pre.spatial_factor),
                        label="goal locations", marker="x", color="black", s=30, zorder=1000000)
        ax4.set_yticks([])
        ax4.set_xticks([])
        ax4.axis("off")
        max_val = np.max(np.max(post_rm_inc[:,:,1]))
        ax5.imshow(post_rm_inc[:,:,1].T, cmap=cmap0, norm=NonLinearNormalize(vmin=0.0, vmax=max_val, a1=8, a2=0.6))
        ax5.text(20,3, np.round(max_val,2), color="red")
        for g_l in gl:
            ax5.scatter((g_l[0] - self.pre.x_min - x_off) / (spatial_resolution/self.pre.spatial_factor),
                        (g_l[1] - self.pre.y_min - y_off) / (spatial_resolution/self.pre.spatial_factor),
                        label="goal locations", marker="x", color="black", s=30, zorder=1000000)

        ax5.set_yticks([])
        ax5.set_xticks([])
        ax5.axis("off")

        # third cell
        ax7 = fig.add_subplot(gs[18:26, :10])
        ax8 = fig.add_subplot(gs[18:26, 10:])
        max_val = np.max(np.max(pre_rm_inc[:,:,2]))
        ax7.imshow(pre_rm_inc[:,:,2].T, cmap=cmap0, norm=NonLinearNormalize(vmin=0.0, vmax=max_val, a1=8, a2=0.6))
        ax7.text(20,3, np.round(max_val,2), color="red")
        for g_l in gl:
            ax7.scatter((g_l[0] - self.pre.x_min - x_off) / (spatial_resolution/self.pre.spatial_factor),
                        (g_l[1] - self.pre.y_min - y_off) / (spatial_resolution/self.pre.spatial_factor),
                        label="goal locations", marker="x", color="black", s=30, zorder=1000000)
        ax7.set_yticks([])
        ax7.set_xticks([])
        ax7.axis("off")
        max_val = np.max(np.max(post_rm_inc[:,:,2]))
        ax8.imshow(post_rm_inc[:,:,2].T, cmap=cmap0, norm=NonLinearNormalize(vmin=0.0, vmax=max_val, a1=8, a2=0.6))
        ax8.text(20,3, np.round(max_val,2), color="red")
        for g_l in gl:
            ax8.scatter((g_l[0] - self.pre.x_min - x_off) / (spatial_resolution/self.pre.spatial_factor),
                        (g_l[1] - self.pre.y_min - y_off) / (spatial_resolution/self.pre.spatial_factor),
                        label="goal locations", marker="x", color="black", s=30, zorder=1000000)

        ax8.set_yticks([])
        ax8.set_xticks([])
        ax8.axis("off")
        plt.tight_layout()
        if save_fig:
            plt.rcParams['svg.fonttype'] = 'none'
            plt.savefig(os.path.join(save_path, "rate_maps_inc_cells.svg"), transparent="True")
        else:
            plt.show()

    # </editor-fold>


class ExplorationExplorationCheeseboard:
    """Class to compare PRE and POST"""

    def __init__(self, exp1, exp2, cb, params, session_params=None):
        self.params = params
        self.session_params = session_params
        self.cell_type = self.params.cell_type
        self.session_name = session_params.session_name

        # initialize each phase
        self.exp1 = exp1
        self.exp2 = exp2
        self.cb = cb

        # get default models for pre and post
        self.default_pre_phmm_model = self.session_params.default_pre_phmm_model
        self.default_post_phmm_model = self.session_params.default_post_phmm_model

    def pre_model_decoding(self, plotting=True, model="phmm", save_fig=False, z_score=False):

        # get spike rasters: PRE
        spike_rasters_cb = self.cb.get_spike_bin_raster(return_estimated_times=False)
        spike_rasters_cb = np.hstack(spike_rasters_cb)

        # filter empty bins
        spike_rasters_cb = spike_rasters_cb[:, np.sum(spike_rasters_cb, axis=0)>11]

        # get spike rasters: exploration familiar
        spike_rasters_fam = self.exp1.get_spike_bin_raster(return_estimated_times=False)

        # filter empty bins
        spike_rasters_fam = spike_rasters_fam[:, np.sum(spike_rasters_fam, axis=0)>11]

        # get spike binned rasters from exploration cheeseboard
        spike_rasters_exp = self.exp2.get_spike_bin_raster(return_estimated_times=False)

        # filter empty bins
        spike_rasters_exp = spike_rasters_exp[:, np.sum(spike_rasters_exp, axis=0)>11]

        # delete events which are too short --> TODO
        # --------------------------------------------------------------------------------------------------------------
        # nr_bins = np.array([x.shape[1] for x in merged_events])

        if model == "phmm":
            # get means of pre and post modes + compression factor
            pre_file_name = self.session_params.default_pre_phmm_model
            post_file_name = self.session_params.default_post_phmm_model
            compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

            # get pre model to do decoding
            with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
                pre_model_dic = pickle.load(f)
            # get means of model (lambdas) for decoding
            pre_mode_means = pre_model_dic.means_

            # do decoding and plot result
            # ----------------------------------------------------------------------------------------------------------
            # do the decoding using pre and post model
            pre_model_cb_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                                event_spike_rasters=spike_rasters_cb,
                                                                compression_factor=compression_factor)

            # do decoding and plot result for pre sleep
            # ----------------------------------------------------------------------------------------------------------
            # do the decoding using pre and post model
            pre_model_fam_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                                 event_spike_rasters=spike_rasters_fam,
                                                                 compression_factor=compression_factor)

            pre_model_exp_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                                 event_spike_rasters=spike_rasters_exp,
                                                                 compression_factor=compression_factor)

        elif model == "ising":

            pre_file_name = self.session_params.default_pre_ising_model
            post_file_name = self.session_params.default_post_ising_model

            with open(self.params.pre_proc_dir + 'awake_ising_maps/' + pre_file_name + '.pkl',
                      'rb') as f:
                pre_model_dic = pickle.load(f)
            pre_template_map = pre_model_dic["res_map"]

            with open(self.params.pre_proc_dir + 'awake_ising_maps/' + post_file_name + '.pkl',
                      'rb') as f:
                post_model_dic = pickle.load(f)
            post_template_map = post_model_dic["res_map"]


            # get time_bin_size of encoding
            time_bin_size_encoding = pre_model_dic["time_bin_size"]

            # load correct compression factor (as defined in parameter file of the session)
            if time_bin_size_encoding == 0.01:
                compression_factor = \
                    np.round(self.session_params.sleep_compression_factor_12spikes_100ms * 10, 3)
            elif time_bin_size_encoding == 0.1:
                compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

            pre_model_pre_likeli = decode_using_ising_map_fast(mode_means=pre_template_map,
                                                               event_spike_rasters=spike_rasters_pre,
                                                               compression_factor=compression_factor)

            post_model_pre_likeli = decode_using_ising_map_fast(mode_means=post_template_map,
                                                                event_spike_rasters=spike_rasters_pre,
                                                                compression_factor=compression_factor)

            # do decoding and plot result for pre sleep
            # ----------------------------------------------------------------------------------------------------------
            # do the decoding using pre and post model
            pre_model_post_likeli = decode_using_ising_map_fast(mode_means=pre_template_map,
                                                                event_spike_rasters=spike_rasters_post,
                                                                compression_factor=compression_factor)

            post_model_post_likeli = decode_using_ising_map_fast(mode_means=post_template_map,
                                                                 event_spike_rasters=spike_rasters_post,
                                                                 compression_factor=compression_factor)

        max_pre_model_cb_likeli = np.max(pre_model_cb_likeli, axis=1)

        max_pre_model_fam_likeli = np.max(pre_model_fam_likeli, axis=1)
        max_post_model_exp_likeli = np.max(pre_model_exp_likeli, axis=1)

        # take the log
        max_pre_log_likeli_cb = np.log(max_pre_model_cb_likeli)
        max_pre_log_likeli_fam = np.log(max_pre_model_fam_likeli)
        max_pre_log_likeli_exp = np.log(max_post_model_exp_likeli)


        if plotting or save_fig:

            c = "white"
            res = [max_pre_log_likeli_cb, max_pre_log_likeli_fam, max_pre_log_likeli_exp]
            bplot = plt.boxplot(res, positions=[1, 2, 3], patch_artist=True,
                                labels=["PRE model: PRE", "PRE model: expl. fam", "PRE model: expl. cb"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            plt.ylabel("Max log-likelihood")
            plt.grid(color="grey", axis="y")
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.show()

        if z_score:
            # zscore to be able to combine results for all sessions
            temp_pre = np.hstack((max_pre_log_likeli_cb, max_pre_log_likeli_fam, max_pre_log_likeli_exp))
            temp_pre_z = zscore(temp_pre)
            max_pre_log_likeli_cb = temp_pre_z[:max_pre_log_likeli_cb.shape[0]]
            max_pre_log_likeli_fam = temp_pre_z[max_pre_log_likeli_cb.shape[0]:(max_pre_log_likeli_cb.shape[0]+
                                                                                max_pre_log_likeli_fam.shape[0])]
            max_pre_log_likeli_exp = temp_pre_z[(max_pre_log_likeli_cb.shape[0]+
                                                                                max_pre_log_likeli_fam.shape[0]):]

        return max_pre_log_likeli_cb, max_pre_log_likeli_fam, max_pre_log_likeli_exp


class ExplorationExploration:
    """Class to compare PRE and POST"""

    def __init__(self, exp1, exp2, params, session_params=None):
        self.params = params
        self.session_params = session_params
        self.cell_type = self.params.cell_type
        self.session_name = session_params.session_name

        # initialize each phase
        self.exp1 = exp1
        self.exp2 = exp2

    def classify_cells_firing_rate_distribution(self, alpha=0.01, test="mwu", plot_for_control=True):
        # check if cells have similar firing rates looking at before and after sleep sessions

        lcb_1_raster = self.exp1.get_raster()
        lcb_2_raster = self.exp2.get_raster()

        stable_cell_ids = []
        increase_cell_ids = []
        decrease_cell_ids = []
        for cell_id, (cell_fir_bef, cell_fir_aft) in enumerate(zip(lcb_1_raster, lcb_2_raster)):
            if test == "ks":
                if ks_2samp(data1=cell_fir_aft, data2=cell_fir_bef, alternative="less")[1] < alpha:
                    increase_cell_ids.append(cell_id)
                elif ks_2samp(data1=cell_fir_aft, data2=cell_fir_bef, alternative="greater")[1] < alpha:
                    decrease_cell_ids.append(cell_id)
                else:
                    stable_cell_ids.append(cell_id)
            elif test == "mwu":
                if plot_for_control:
                    p_bef = 1. * np.arange(cell_fir_bef.shape[0]) / (cell_fir_bef.shape[0] - 1)
                    p_aft = 1. * np.arange(cell_fir_aft.shape[0]) / (cell_fir_aft.shape[0] - 1)
                    plt.plot(np.sort(cell_fir_bef), p_bef, label="PRE")
                    plt.plot(np.sort(cell_fir_aft), p_aft, label="POST")
                if mannwhitneyu(x=cell_fir_aft, y=cell_fir_bef, alternative="less")[1] < alpha:
                    decrease_cell_ids.append(cell_id)
                    if plot_for_control:
                        plt.title("Decreasing")
                elif mannwhitneyu(x=cell_fir_aft, y=cell_fir_bef, alternative="greater")[1] < alpha:
                    increase_cell_ids.append(cell_id)
                    if plot_for_control:
                        plt.title("Increasing")
                else:
                    stable_cell_ids.append(cell_id)
                    if plot_for_control:
                        plt.title("stable")
                if plot_for_control:
                    plt.legend()
                    x_min, x_max = plt.gca().get_xlim()
                    plt.text(x_max/2, 0.5, "mean pre="+str(np.round(np.mean(cell_fir_bef),2)))
                    plt.text(x_max/2, 0.4, "mean post="+str(np.round(np.mean(cell_fir_aft),2)))
                    plt.show()

        print("#stable: " + str(len(stable_cell_ids)) + " ,#inc: " +
              str(len(increase_cell_ids)) + " ,#dec:" + str(len(decrease_cell_ids)))

        cell_class_dic = {
            "stable_cell_ids": np.array(stable_cell_ids),
            "decrease_cell_ids": np.array(decrease_cell_ids),
            "increase_cell_ids": np.array(increase_cell_ids)
        }

        with open(self.params.pre_proc_dir + "cell_classification_exp_fam/" + self.session_name + "_" + test +
                  "_awake.pickle", "wb") as f:
            pickle.dump(cell_class_dic, f, pickle.HIGHEST_PROTOCOL)


class PreLongSleepPost(LongSleep, PrePostCheeseboard):
    """Class for long sleep and PRE and POST cheeseboard task"""

    def __init__(self, sleep_data_obj, pre, post, params, session_params=None):

        # initialize parent classes
        LongSleep.__init__(self, sleep_data_obj=sleep_data_obj, params=params, session_params=session_params)

        # TODO: check which class attributes are overwritten by PrePostCheeseboard

        PrePostCheeseboard.__init__(self, pre=pre, post=post, params=params, session_params=session_params)

        self.cell_type = sleep_data_obj.get_cell_type()

    # <editor-fold desc="Firing rate analysis">

    def firing_rate_distributions(self, cells_to_use="stable", plotting=True, separate_sleep_phases=True):
        print("Processing " +self.session_name)
        # get rasters from exploration before/after
        raster_pre = self.pre.get_raster()
        raster_post = self.post.get_raster()

        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle", "rb") as f:
            class_dic = pickle.load(f)

        if cells_to_use == "stable":
            cell_ids = class_dic["stable_cell_ids"].flatten()
        elif cells_to_use =="increasing":
            cell_ids = class_dic["increase_cell_ids"].flatten()
        elif cells_to_use == "decreasing":
            cell_ids = class_dic["decrease_cell_ids"].flatten()

        if separate_sleep_phases:
            raster_sleep_nrem = self.get_sleep_phase_raster(sleep_phase="nrem")
            raster_sleep_rem = self.get_sleep_phase_raster(sleep_phase="rem")

            firing_pre_mean = np.mean(raster_pre[cell_ids, :], axis=1)
            firing_sleep_nrem_mean = np.mean(raster_sleep_nrem[cell_ids, :], axis=1)
            firing_sleep_rem_mean = np.mean(raster_sleep_rem[cell_ids, :], axis=1)
            firing_post_mean = np.mean(raster_post[cell_ids, :], axis=1)

            # normalize mean firing rates
            max_mean_fir = np.max(np.vstack((firing_pre_mean, firing_sleep_nrem_mean, firing_sleep_rem_mean,
                                             firing_post_mean)), axis=0)

            firing_pre_norm = firing_pre_mean/max_mean_fir
            firing_sleep_nrem_norm = firing_sleep_nrem_mean / max_mean_fir
            firing_sleep_rem_norm = firing_sleep_rem_mean / max_mean_fir
            firing_post_norm = firing_post_mean / max_mean_fir

            if plotting:
                p_pre_norm = 1. * np.arange(firing_pre_norm.shape[0]) / (firing_pre_norm.shape[0] - 1)
                p_sleep_nrem_norm = 1. * np.arange(firing_sleep_nrem_norm.shape[0]) / (firing_sleep_nrem_norm.shape[0] - 1)
                p_sleep_rem_norm = 1. * np.arange(firing_sleep_rem_norm.shape[0]) / (firing_sleep_rem_norm.shape[0] - 1)
                p_post_norm = 1. * np.arange(firing_post_norm.shape[0]) / (firing_post_norm.shape[0] - 1)

                plt.plot(np.sort(firing_pre_norm), p_pre_norm, label="PRE")
                plt.plot(np.sort(firing_sleep_rem_norm), p_sleep_rem_norm, label="REM")
                plt.plot(np.sort(firing_sleep_nrem_norm), p_sleep_nrem_norm, label="NREM")
                plt.plot(np.sort(firing_post_norm), p_post_norm, label="POST")
                plt.title(cells_to_use)
                plt.xlabel("Mean firing rate / normalized")
                plt.ylabel("CDF")
                plt.legend()
                plt.show()
            else:
                return firing_pre_norm, firing_sleep_rem_norm, firing_sleep_nrem_norm, firing_post_norm

        else:
            raster_sleep = self.get_sleep_raster()
            firing_pre_mean = np.mean(raster_pre[cell_ids, :], axis=1)
            firing_sleep_mean = np.mean(raster_sleep[cell_ids, :], axis=1)
            firing_post_mean = np.mean(raster_post[cell_ids, :], axis=1)

            # normalize mean firing rates
            max_mean_fir = np.max(np.vstack((firing_pre_mean, firing_sleep_mean, firing_post_mean)), axis=0)

            firing_pre_norm = firing_pre_mean/max_mean_fir
            firing_sleep_norm = firing_sleep_mean / max_mean_fir
            firing_post_norm = firing_post_mean / max_mean_fir

            if plotting:

                p_pre_norm = 1. * np.arange(firing_pre_norm.shape[0]) / (firing_pre_norm.shape[0] - 1)
                p_sleep_norm = 1. * np.arange(firing_sleep_norm.shape[0]) / (firing_sleep_norm.shape[0] - 1)
                p_post_norm = 1. * np.arange(firing_post_norm.shape[0]) / (firing_post_norm.shape[0] - 1)

                plt.plot(np.sort(firing_pre_norm), p_pre_norm, label="PRE")
                plt.plot(np.sort(firing_sleep_norm), p_sleep_norm, label="Sleep")
                plt.plot(np.sort(firing_post_norm), p_post_norm, label="POST")
                plt.title(cells_to_use)
                plt.xlabel("Mean firing rate / normalized")
                plt.ylabel("CDF")
                plt.legend()
                plt.show()
            else:
                return firing_pre_norm, firing_sleep_norm, firing_post_norm

    def firing_rate_distributions_all_cells(self, plotting=True, measure="mean", chunks_in_min=5, z_score=False):

        # get rasters from exploration before/after & sleep
        raster_pre = self.pre.get_raster()
        raster_post = self.post.get_raster()
        raster_sleep = self.get_sleep_raster()

        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_cells = class_dic["stable_cell_ids"].flatten()
        inc_cells = class_dic["increase_cell_ids"].flatten()
        dec_cells = class_dic["decrease_cell_ids"].flatten()

        chunk_size = int(chunks_in_min/self.params.time_bin_size)

        # go through PRE first
        nr_chunks_pre = int(raster_pre.shape[1]/chunk_size)
        firing_pre = np.zeros((raster_pre.shape[0], nr_chunks_pre))
        for chunk in range(nr_chunks_pre):
            if measure == "mean":
                firing_pre[:, chunk] = np.mean(
                    raster_pre[:, chunk*chunk_size:(chunk+1)*chunk_size], axis=1)/self.params.time_bin_size
            elif measure == "max":
                firing_pre[:, chunk] = np.max(
                    raster_pre[:, chunk * chunk_size:(chunk + 1) * chunk_size], axis=1) / self.params.time_bin_size

        # go through sleep
        nr_chunks_sleep = int(raster_sleep.shape[1]/chunk_size)
        firing_sleep = np.zeros((raster_sleep.shape[0], nr_chunks_sleep))
        for chunk in range(nr_chunks_sleep):
            if measure == "mean":
                firing_sleep[:, chunk] = np.mean(
                    raster_sleep[:, chunk*chunk_size:(chunk+1)*chunk_size], axis=1)/self.params.time_bin_size
            elif measure == "max":
                firing_sleep[:, chunk] = np.max(
                    raster_sleep[:, chunk * chunk_size:(chunk + 1) * chunk_size], axis=1) / self.params.time_bin_size

        # go through POST
        nr_chunks_post = int(raster_post.shape[1]/chunk_size)
        firing_post = np.zeros((raster_post.shape[0], nr_chunks_post))
        for chunk in range(nr_chunks_post):
            if measure == "mean":
                firing_post[:, chunk] = np.mean(
                    raster_post[:, chunk*chunk_size:(chunk+1)*chunk_size], axis=1)/self.params.time_bin_size
            elif measure == "max":
                firing_post[:, chunk] = np.max(
                    raster_post[:, chunk * chunk_size:(chunk + 1) * chunk_size], axis=1) / self.params.time_bin_size

        # compute mean per cell
        firing_pre = np.mean(firing_pre, axis=1)
        firing_post = np.mean(firing_post, axis=1)
        firing_sleep = np.mean(firing_sleep, axis=1)

        if z_score:
            # combine to z-score for each cell
            all_firing = np.vstack((firing_pre, firing_sleep, firing_post))
            all_firing_z = zscore(all_firing, axis=1)

            # split again into pre, sleep, post
            firing_pre_z = all_firing_z[:, 0]
            firing_sleep_z = all_firing_z[:, 1]
            firing_post_z = all_firing_z[:, 2]

            # split into subsets
            firing_pre_z_stable = firing_pre_z[stable_cells, :].flatten()
            firing_pre_z_dec = firing_pre_z[dec_cells, :].flatten()
            firing_pre_z_inc = firing_pre_z[inc_cells, :].flatten()

            firing_sleep_z_stable = firing_sleep_z[stable_cells, :].flatten()
            firing_sleep_z_dec = firing_sleep_z[dec_cells, :].flatten()
            firing_sleep_z_inc = firing_sleep_z[inc_cells, :].flatten()

            firing_post_z_stable = firing_post_z[stable_cells, :].flatten()
            firing_post_z_dec = firing_post_z[dec_cells, :].flatten()
            firing_post_z_inc = firing_post_z[inc_cells, :].flatten()

        else:
            firing_pre_stable = firing_pre[stable_cells]
            firing_post_stable = firing_post[stable_cells]
            firing_sleep_stable = firing_sleep[stable_cells]
            firing_pre_dec = firing_pre[dec_cells]
            firing_post_dec = firing_post[dec_cells]
            firing_sleep_dec = firing_sleep[dec_cells]
            firing_pre_inc = firing_pre[inc_cells]
            firing_post_inc = firing_post[inc_cells]
            firing_sleep_inc = firing_sleep[inc_cells]

        if plotting:

            p_pre_stable = 1. * np.arange(firing_pre_z_stable.shape[0]) / (firing_pre_z_stable.shape[0] - 1)
            p_sleep_stable = 1. * np.arange(firing_sleep_z_stable.shape[0]) / (firing_sleep_z_stable.shape[0] - 1)
            p_post_stable = 1. * np.arange(firing_post_z_stable.shape[0]) / (firing_post_z_stable.shape[0] - 1)

            p_pre_dec = 1. * np.arange(firing_pre_z_dec.shape[0]) / (firing_pre_z_dec.shape[0] - 1)
            p_sleep_dec = 1. * np.arange(firing_sleep_z_dec.shape[0]) / (firing_sleep_z_dec.shape[0] - 1)
            p_post_dec = 1. * np.arange(firing_post_z_dec.shape[0]) / (firing_post_z_dec.shape[0] - 1)

            p_pre_inc = 1. * np.arange(firing_pre_z_inc.shape[0]) / (firing_pre_z_inc.shape[0] - 1)
            p_sleep_inc = 1. * np.arange(firing_sleep_z_inc.shape[0]) / (firing_sleep_z_inc.shape[0] - 1)
            p_post_inc = 1. * np.arange(firing_post_z_inc.shape[0]) / (firing_post_z_inc.shape[0] - 1)

            plt.plot(np.sort(firing_pre_z_stable), p_pre_stable, color="magenta", label="stable")
            plt.plot(np.sort(firing_pre_z_inc), p_pre_inc, color="orange", label="inc")
            plt.plot(np.sort(firing_pre_z_dec), p_pre_dec, color="turquoise", label="dec")
            plt.xlabel(measure + " (Hz)")
            plt.ylabel("CDF")
            plt.legend()
            plt.title("PRE")
            plt.show()

            plt.plot(np.sort(firing_sleep_z_stable), p_sleep_stable, color="magenta", label="stable")
            plt.plot(np.sort(firing_sleep_z_inc), p_sleep_inc, color="orange",label="inc")
            plt.plot(np.sort(firing_sleep_z_dec), p_sleep_dec, color="turquoise",label="dec")
            plt.xlabel(measure + " (Hz)")
            plt.ylabel("CDF")
            plt.legend()
            plt.title("Sleep")
            plt.show()

            plt.plot(np.sort(firing_post_z_stable), p_post_stable, color="magenta",label="stable")
            plt.plot(np.sort(firing_post_z_inc), p_post_inc, color="orange",label="inc")
            plt.plot(np.sort(firing_post_z_dec), p_post_dec, color="turquoise",label="dec")
            plt.xlabel(measure + " (Hz)")
            plt.ylabel("CDF")
            plt.legend()
            plt.title("Post")
            plt.show()

        if z_score:
            return firing_pre_z_stable, firing_pre_z_dec, firing_pre_z_inc, firing_sleep_z_stable, firing_sleep_z_dec, \
                   firing_sleep_z_inc, firing_post_z_stable, firing_post_z_dec, firing_post_z_inc
        else:
            return firing_pre_stable, firing_pre_dec, firing_pre_inc, firing_sleep_stable, firing_sleep_dec, \
                firing_sleep_inc, firing_post_stable, firing_post_dec, firing_post_inc

    def firing_rate_ratios_all_cells(self, plotting=True, measure="mean", chunks_in_min=5):

        # get rasters from exploration before/after & sleep
        raster_pre = self.pre.get_raster()
        raster_post = self.post.get_raster()
        raster_sleep = self.get_sleep_raster()

        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_cells = class_dic["stable_cell_ids"].flatten()
        inc_cells = class_dic["increase_cell_ids"].flatten()
        dec_cells = class_dic["decrease_cell_ids"].flatten()

        chunk_size = int(chunks_in_min/self.params.time_bin_size)

        # go through PRE first
        nr_chunks_pre = int(raster_pre.shape[1]/chunk_size)
        firing_pre = np.zeros((raster_pre.shape[0], nr_chunks_pre))
        for chunk in range(nr_chunks_pre):
            if measure == "mean":
                firing_pre[:, chunk] = np.mean(
                    raster_pre[:, chunk*chunk_size:(chunk+1)*chunk_size], axis=1)/self.params.time_bin_size
            elif measure == "max":
                firing_pre[:, chunk] = np.max(
                    raster_pre[:, chunk * chunk_size:(chunk + 1) * chunk_size], axis=1) / self.params.time_bin_size

        # go through sleep
        nr_chunks_sleep = int(raster_sleep.shape[1]/chunk_size)
        firing_sleep = np.zeros((raster_sleep.shape[0], nr_chunks_sleep))
        for chunk in range(nr_chunks_sleep):
            if measure == "mean":
                firing_sleep[:, chunk] = np.mean(
                    raster_sleep[:, chunk*chunk_size:(chunk+1)*chunk_size], axis=1)/self.params.time_bin_size
            elif measure == "max":
                firing_sleep[:, chunk] = np.max(
                    raster_sleep[:, chunk * chunk_size:(chunk + 1) * chunk_size], axis=1) / self.params.time_bin_size

        # go through POST
        nr_chunks_post = int(raster_post.shape[1]/chunk_size)
        firing_post = np.zeros((raster_post.shape[0], nr_chunks_post))
        for chunk in range(nr_chunks_post):
            if measure == "mean":
                firing_post[:, chunk] = np.mean(
                    raster_post[:, chunk*chunk_size:(chunk+1)*chunk_size], axis=1)/self.params.time_bin_size
            elif measure == "max":
                firing_post[:, chunk] = np.max(
                    raster_post[:, chunk * chunk_size:(chunk + 1) * chunk_size], axis=1) / self.params.time_bin_size

        # combine to z-score for each cell
        all_firing = np.hstack((firing_pre, firing_sleep, firing_post))

        nom_pre = (np.mean(firing_sleep, axis=1)-np.mean(firing_pre, axis=1))
        denom_pre = (np.mean(firing_sleep, axis=1)+np.mean(firing_pre, axis=1))
        ratio_pre_sleep = nom_pre / denom_pre

        nom_post = (np.mean(firing_sleep, axis=1)-np.mean(firing_post, axis=1))
        denom_post = (np.mean(firing_sleep, axis=1)+np.mean(firing_post, axis=1))
        ratio_post_sleep = nom_post / denom_post

        ratio_pre_sleep[ratio_pre_sleep==np.inf] = np.nan
        ratio_post_sleep[ratio_post_sleep==np.inf] = np.nan

        ratio_pre_sleep_stable = ratio_pre_sleep[stable_cells]
        ratio_post_sleep_stable = ratio_post_sleep[stable_cells]

        ratio_pre_sleep_dec = ratio_pre_sleep[dec_cells]
        ratio_post_sleep_dec = ratio_post_sleep[dec_cells]

        ratio_pre_sleep_inc = ratio_pre_sleep[inc_cells]
        ratio_post_sleep_inc = ratio_post_sleep[inc_cells]

        if plotting:
            c = "white"
            y_dat = [ratio_pre_sleep_stable[~np.isnan(ratio_pre_sleep_stable)], ratio_post_sleep_stable[~np.isnan(ratio_post_sleep_stable)]]
            plt.figure(figsize=(2, 4))
            bplot = plt.boxplot(y_dat, positions=[1, 2], patch_artist=True,
                                labels=["sleep/PRE", "sleep/POST"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False)
            # whis = [0.01, 99.99]
            plt.tight_layout()
            plt.title("Stable")
            plt.show()

            c = "white"
            y_dat = [ratio_pre_sleep_dec[~np.isnan(ratio_pre_sleep_dec)], ratio_post_sleep_dec[~np.isnan(ratio_post_sleep_dec)]]
            plt.figure(figsize=(2, 4))
            bplot = plt.boxplot(y_dat, positions=[1, 2], patch_artist=True,
                                labels=["sleep/PRE", "sleep/POST"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False)
            # whis = [0.01, 99.99]
            plt.title("Dec")
            plt.show()

            c = "white"
            y_dat = [ratio_pre_sleep_inc[~np.isnan(ratio_pre_sleep_inc)], ratio_post_sleep_inc[~np.isnan(ratio_post_sleep_inc)]]
            plt.figure(figsize=(2, 4))
            bplot = plt.boxplot(y_dat, positions=[1, 2], patch_artist=True,
                                labels=["sleep/PRE", "sleep/POST"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False)
            # whis = [0.01, 99.99]
            plt.title("Inc")
            plt.show()

        else:
            return ratio_pre_sleep_stable[~np.isnan(ratio_pre_sleep_stable)], \
                   ratio_post_sleep_stable[~np.isnan(ratio_post_sleep_stable)], \
                   ratio_pre_sleep_dec[~np.isnan(ratio_pre_sleep_dec)], \
                   ratio_post_sleep_dec[~np.isnan(ratio_post_sleep_dec)], \
                   ratio_pre_sleep_inc[~np.isnan(ratio_pre_sleep_inc)], \
                   ratio_post_sleep_inc[~np.isnan(ratio_post_sleep_inc)]

    # </editor-fold>

    # <editor-fold desc="Overexpressed pHMM modes">

    def over_expressed_modes_goal_coding(self, template_type, pre_file_name=None, post_file_name=None,
                                         rem_pop_vec_threshold=10, cells_to_compare="stable", perc_inc_thresh=20,
                                         post_or_pre="post", plotting=False, shuffling=False):
        """
        checks which modes in sleep are over expressed when only stable cells are used. Then, goal coding properties
        of these modes are analyzed

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param pre_file_name: name of PRE phmm model, if None --> uses default
        :type pre_file_name: str or None
        :param post_file_name: name of POST phmm model, if None --> uses default
        :type post_file_name: str or None
        :param rem_pop_vec_threshold: min. number of rem vectors per epoch (shorter epochs are discarded)
        :type rem_pop_vec_threshold: int
        :param cells_to_compare: compare with "stable" or "increasing" cells
        :type cells_to_compare: str
        :param perc_inc_thresh: defines threshold for over expression (e.g. 20 --> if 20% increase of occurence when
                                only using stable to base line [all cells] --> mode is over expressed)
        :type perc_inc_thresh: int
        :param post_or_pre: checking goal coding in pre ("pre") or post ("post")
        :type post_or_pre: str
        :param plotting: whether to plot results
        :type plotting: bool
        :return: frac_goal_coding_increase_stable_rem, frac_goal_coding_stable_rem,
                 frac_goal_coding_increase_stable_nrem, frac_goal_coding_stable_nrem
        :rtype: float, float, float, float
        """
        print("ANALYZING GOAL CODING OF STABLE CELLS \n")

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem= self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, cells_to_use="all")


        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_rem, post_prob_rem= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold,
                                                     cells_to_use="all")



        _, _, _, _, pre_prob_rem_stable, post_prob_rem_stable= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold,
                                                     cells_to_use=cells_to_compare, shuffling=shuffling)


        _, _, _, _, pre_prob_nrem_stable, post_prob_nrem_stable= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=rem_pop_vec_threshold,
                                                     cells_to_use=cells_to_compare, shuffling=shuffling)

        if post_or_pre == "pre":
            nr_modes = pre_prob_rem.shape[1]
            # check reactivated modes
            # ----------------------------------------------------------------------------------------------------------
            # rem with all cells
            modes_rem = np.argmax(pre_prob_rem, axis=1)
            # rem with only stable cells
            modes_rem_stable = np.argmax(pre_prob_rem_stable, axis=1)
            # nrem with all cells
            modes_nrem = np.argmax(pre_prob_nrem, axis=1)
            # nrem with stable cells
            modes_nrem_stable = np.argmax(pre_prob_nrem_stable, axis=1)

        elif post_or_pre == "post":
            nr_modes = post_prob_rem.shape[1]
            # check reactivated modes
            # ----------------------------------------------------------------------------------------------------------
            # rem with all cells
            modes_rem = np.argmax(post_prob_rem, axis=1)
            # rem with only stable cells
            modes_rem_stable = np.argmax(post_prob_rem_stable, axis=1)
            # nrem with all cells
            modes_nrem = np.argmax(post_prob_nrem, axis=1)
            # nrem with stable cells
            modes_nrem_stable = np.argmax(post_prob_nrem_stable, axis=1)

        mode_ids = np.arange(nr_modes)

        modes_rem, counts = np.unique(modes_rem, return_counts=True)
        counts_rem = np.zeros(nr_modes)
        counts_rem[modes_rem] = counts

        modes_rem_stable, counts = np.unique(modes_rem_stable, return_counts=True)
        counts_rem_stable = np.zeros(nr_modes)
        counts_rem_stable[modes_rem_stable] = counts

        modes_nrem, counts = np.unique(modes_nrem, return_counts=True)
        counts_nrem = np.zeros(nr_modes)
        counts_nrem[modes_nrem] = counts

        modes_nrem_stable, counts = np.unique(modes_nrem_stable, return_counts=True)
        counts_nrem_stable = np.zeros(nr_modes)
        counts_nrem_stable[modes_nrem_stable] = counts

        if plotting:

            lines = []
            for i in range(mode_ids.shape[0]):
                pair = [(mode_ids[i], 0), (mode_ids[i], counts_rem[i])]
                lines.append(pair)

            linecoll = matcoll.LineCollection(lines)
            fig, ax = plt.subplots()
            ax.add_collection(linecoll)
            plt.scatter(mode_ids, counts_rem, label="ALL CELLS")
            plt.scatter(mode_ids, counts_rem_stable, label="ONLY "+cells_to_compare)
            plt.xlabel("MODE ID")
            plt.ylabel("COUNTS")
            plt.title("REM")
            plt.legend()
            plt.show()


            lines = []
            for i in range(mode_ids.shape[0]):
                pair = [(mode_ids[i], 0), (mode_ids[i], counts_nrem[i])]
                lines.append(pair)

            linecoll = matcoll.LineCollection(lines)
            fig, ax = plt.subplots()
            ax.add_collection(linecoll)
            plt.scatter(mode_ids, counts_nrem, label="ALL CELLS")
            plt.scatter(mode_ids, counts_nrem_stable, label="ONLY "+cells_to_compare)
            plt.xlabel("MODE ID")
            plt.ylabel("COUNTS")
            plt.title("NREM")
            plt.legend()
            plt.show()

        counts_nrem[counts_nrem==0] = np.nan
        perc_change_nrem = np.nan_to_num(counts_nrem_stable/(counts_nrem/100)-100)
        modes_increase_stable_nrem = perc_change_nrem > perc_inc_thresh

        counts_nrem[counts_rem==0] = np.nan
        perc_change_rem = np.nan_to_num(counts_rem_stable/(counts_rem/100)-100)
        modes_increase_stable_rem = perc_change_rem > perc_inc_thresh

        ################################################################################################################
        # Awake data
        ################################################################################################################

        if post_or_pre == "pre":
            awake = self.pre
            def_model = self.default_pre_phmm_model
        elif post_or_pre == "post":
            awake = self.post
            def_model = self.default_post_phmm_model

        # rem
        # --------------------------------------------------------------------------------------------------------------
        frac_goal_coding_increase_stable_rem = awake.analyze_modes_goal_coding(file_name=def_model,
                                                              mode_ids=mode_ids[modes_increase_stable_rem],
                                                              plotting=plotting)

        frac_goal_coding_stable_rem = awake.analyze_modes_goal_coding(file_name=def_model,
                                                              mode_ids=mode_ids[~modes_increase_stable_rem],
                                                              plotting=plotting)
        # nrem
        # --------------------------------------------------------------------------------------------------------------

        frac_goal_coding_increase_stable_nrem = awake.analyze_modes_goal_coding(file_name=def_model,
                                                                                  mode_ids=mode_ids[
                                                                                      modes_increase_stable_nrem],
                                                                                  plotting=plotting)

        frac_goal_coding_stable_nrem = awake.analyze_modes_goal_coding(file_name=def_model,
                                                                         mode_ids=mode_ids[~modes_increase_stable_nrem],
                                                                         plotting=plotting)

        return frac_goal_coding_increase_stable_rem, frac_goal_coding_stable_rem, frac_goal_coding_increase_stable_nrem, \
               frac_goal_coding_stable_nrem

    def over_expressed_modes_spatial_information(self, template_type, pre_file_name=None, post_file_name=None,
                                                 rem_pop_vec_threshold=10, cells_to_compare="stable",
                                                 perc_inc_thresh=20, post_or_pre="post", plotting=False):
        """
        checks which modes in sleep are over expressed when only stable cells are used. Then, goal coding properties
        of these modes are analyzed

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param pre_file_name: name of PRE phmm model, if None --> uses default
        :type pre_file_name: str or None
        :param post_file_name: name of POST phmm model, if None --> uses default
        :type post_file_name: str or None
        :param rem_pop_vec_threshold: min. number of rem vectors per epoch (shorter epochs are discarded)
        :type rem_pop_vec_threshold: int
        :param cells_to_compare: compare with "stable" or "increasing" cells
        :type cells_to_compare: str
        :param perc_inc_thresh: defines threshold for over expression (e.g. 20 --> if 20% increase of occurence when
                                only using stable to base line [all cells] --> mode is over expressed)
        :type perc_inc_thresh: int
        :param post_or_pre: checking goal coding in pre ("pre") or post ("post")
        :type post_or_pre: str
        :param plotting: whether to plot results
        :type plotting: bool
        :return: frac_goal_coding_increase_stable_rem, frac_goal_coding_stable_rem,
                 frac_goal_coding_increase_stable_nrem, frac_goal_coding_stable_nrem
        :rtype: float, float, float, float
        """
        print("ANALYZING GOAL CODING OF STABLE CELLS \n")

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem= self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, cells_to_use="all")


        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_rem, post_prob_rem= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold,
                                                     cells_to_use="all")



        _, _, _, _, pre_prob_rem_stable, post_prob_rem_stable= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold,
                                                     cells_to_use=cells_to_compare)


        _, _, _, _, pre_prob_nrem_stable, post_prob_nrem_stable= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=rem_pop_vec_threshold,
                                                     cells_to_use=cells_to_compare)

        if post_or_pre == "pre":
            nr_modes = pre_prob_rem.shape[1]
            # check reactivated modes
            # ----------------------------------------------------------------------------------------------------------
            # rem with all cells
            modes_rem = np.argmax(pre_prob_rem, axis=1)
            # rem with only stable cells
            modes_rem_stable = np.argmax(pre_prob_rem_stable, axis=1)
            # nrem with all cells
            modes_nrem = np.argmax(pre_prob_nrem, axis=1)
            # nrem with stable cells
            modes_nrem_stable = np.argmax(pre_prob_nrem_stable, axis=1)

        elif post_or_pre == "post":
            nr_modes = post_prob_rem.shape[1]
            # check reactivated modes
            # ----------------------------------------------------------------------------------------------------------
            # rem with all cells
            modes_rem = np.argmax(post_prob_rem, axis=1)
            # rem with only stable cells
            modes_rem_stable = np.argmax(post_prob_rem_stable, axis=1)
            # nrem with all cells
            modes_nrem = np.argmax(post_prob_nrem, axis=1)
            # nrem with stable cells
            modes_nrem_stable = np.argmax(post_prob_nrem_stable, axis=1)

        mode_ids = np.arange(nr_modes)

        modes_rem, counts = np.unique(modes_rem, return_counts=True)
        counts_rem = np.zeros(nr_modes)
        counts_rem[modes_rem] = counts

        modes_rem_stable, counts = np.unique(modes_rem_stable, return_counts=True)
        counts_rem_stable = np.zeros(nr_modes)
        counts_rem_stable[modes_rem_stable] = counts

        modes_nrem, counts = np.unique(modes_nrem, return_counts=True)
        counts_nrem = np.zeros(nr_modes)
        counts_nrem[modes_nrem] = counts

        modes_nrem_stable, counts = np.unique(modes_nrem_stable, return_counts=True)
        counts_nrem_stable = np.zeros(nr_modes)
        counts_nrem_stable[modes_nrem_stable] = counts

        if plotting:

            lines = []
            for i in range(mode_ids.shape[0]):
                pair = [(mode_ids[i], 0), (mode_ids[i], counts_rem[i])]
                lines.append(pair)

            linecoll = matcoll.LineCollection(lines)
            fig, ax = plt.subplots()
            ax.add_collection(linecoll)
            plt.scatter(mode_ids, counts_rem, label="ALL CELLS")
            plt.scatter(mode_ids, counts_rem_stable, label="ONLY "+cells_to_compare)
            plt.xlabel("MODE ID")
            plt.ylabel("COUNTS")
            plt.title("REM")
            plt.legend()
            plt.show()


            lines = []
            for i in range(mode_ids.shape[0]):
                pair = [(mode_ids[i], 0), (mode_ids[i], counts_nrem[i])]
                lines.append(pair)

            linecoll = matcoll.LineCollection(lines)
            fig, ax = plt.subplots()
            ax.add_collection(linecoll)
            plt.scatter(mode_ids, counts_nrem, label="ALL CELLS")
            plt.scatter(mode_ids, counts_nrem_stable, label="ONLY "+cells_to_compare)
            plt.xlabel("MODE ID")
            plt.ylabel("COUNTS")
            plt.title("NREM")
            plt.legend()
            plt.show()

        counts_nrem[counts_nrem==0] = np.nan
        perc_change_nrem = np.nan_to_num(counts_nrem_stable/(counts_nrem/100)-100)
        modes_increase_stable_nrem = perc_change_nrem > perc_inc_thresh

        counts_nrem[counts_rem==0] = np.nan
        perc_change_rem = np.nan_to_num(counts_rem_stable/(counts_rem/100)-100)
        modes_increase_stable_rem = perc_change_rem > perc_inc_thresh

        ################################################################################################################
        # Awake data
        ################################################################################################################

        if post_or_pre == "pre":
            awake = self.pre
            def_model = self.default_pre_phmm_model
        elif post_or_pre == "post":
            awake = self.post
            def_model = self.default_post_phmm_model

        # rem
        # --------------------------------------------------------------------------------------------------------------
        spatial_info_increase_stable_rem = awake.analyze_modes_spatial_information(file_name=def_model,
                                                              mode_ids=mode_ids[modes_increase_stable_rem],
                                                              plotting=False)

        spatial_info_stable_rem = awake.analyze_modes_spatial_information(file_name=def_model,
                                                              mode_ids=mode_ids[~modes_increase_stable_rem],
                                                              plotting=False)
        # nrem
        # --------------------------------------------------------------------------------------------------------------

        spatial_info_increase_stable_nrem = awake.analyze_modes_spatial_information(file_name=def_model,
                                                                                  mode_ids=mode_ids[
                                                                                      modes_increase_stable_nrem],
                                                                                  plotting=False)

        spatial_info_stable_nrem = awake.analyze_modes_spatial_information(file_name=def_model,
                                                                         mode_ids=mode_ids[~modes_increase_stable_nrem],
                                                                         plotting=False)

        return spatial_info_increase_stable_rem, spatial_info_stable_rem, spatial_info_increase_stable_nrem, \
               spatial_info_stable_nrem

    # </editor-fold>

    # <editor-fold desc="Memory drift analysis">
    def memory_drift_around_goals_and_away(self, measure="normalized_ratio", pre_file_name=None, post_file_name=None,
                                           n_moving_average_pop_vec=200, rem_pop_vec_threshold=10,
                                           nrem_pop_vec_threshold=2,
                                           plotting=False, cells_to_use="all", sleep_classification_method="std",
                                           shuffling=False, max_dist=5):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param cells_to_use: which cells to use ("all", "stable", "inc", "dec")
        :type cells_to_use: str
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """
        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_per_rem_event, _ = \
            self.memory_drift_long_sleep_get_results(template_type="ising", pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use, sleep_classification_method=
                                                     sleep_classification_method, shuffling=shuffling,
                                                     return_pre_prob_list=True)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_per_nrem_event, _ = \
            self.memory_drift_long_sleep_get_results(template_type="ising", pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=nrem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use, sleep_classification_method=
                                                     sleep_classification_method, shuffling=shuffling,
                                                     return_pre_prob_list=True)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------_decoding_similarity_temporal------------------------
        all_events_pre_prob = pre_prob_per_rem_event + pre_prob_per_nrem_event
        all_events_ratio = ratio_per_rem_event + ratio_per_nrem_event
        all_events_length = event_lengths_rem + event_lengths_nrem
        all_events_length_s = np.hstack((event_duration_rem_in_s, event_duration_nrem_in_s))
        labels_events = np.zeros(len(ratio_per_rem_event) + len(ratio_per_nrem_event))
        labels_events[:len(ratio_per_rem_event)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:, 0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_pre_prob_list = [x for _, x in sorted(zip(all_times, all_events_pre_prob))]
        sorted_pop_vec_pre_prob = np.vstack(sorted_events_pre_prob_list)
        sorted_events_ratio_list = [x for _, x in sorted(zip(all_times, all_events_ratio))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_lengths_s = [x for _, x in sorted(zip(all_times, all_events_length_s))]
        sorted_pop_vec_ratio = np.hstack(sorted_events_ratio_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_length_s = []
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat == 1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0] + 1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first + trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat == -1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0] + 1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first + trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))
        merged_events_length_s = np.array(merged_events_length_s)
        merged_events_times = np.vstack(merged_events_times)

        # merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        # merged_nrem_event_times = merged_events_times[merged_events_labels == 0]
        merged_events_length_s = merged_events_times[:, 1] - merged_events_times[:, 0]

        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps == 1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre_post ratio) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_rem_nrem_pop_vec_around_goals = []
        pre_prob_rem_nrem_pop_vec_away_from_goals = []
        ratio_per_merged_rem_event_around_goals = []
        ratio_per_merged_nrem_event_around_goals = []
        ratio_rem_nrem_events_around_goals = []
        ratio_per_merged_rem_event_away_from_goals = []
        ratio_per_merged_nrem_event_away_from_goals = []
        ratio_rem_nrem_events_away_from_goals = []
        rem_nrem_events_label = []
        ratio_rem_nrem_pop_vec_away_from_goals = []
        ratio_rem_nrem_pop_vec_around_goals = []
        rem_nrem_pop_vec_label_around_goals = []
        rem_nrem_pop_vec_label_away_from_goals = []

        # need to get model to convert array to 2D matrix for locations
        template_file_name = self.long_sleep[0].session_params.default_pre_ising_model

        with open(self.params.pre_proc_dir + 'awake_ising_maps/' + template_file_name + '.pkl', 'rb') as f:
            model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        res_2d = model_dic["occ_map"].shape

        # factor a.u. to cm
        # scaling_fac = self.session_params.data_params_dictionary["spatial_factor"]

        # ising maps are decoded using 5 a.u.
        # spatial_bin_size = 5 * scaling_fac

        # goal locations
        gl = self.long_sleep[0].session_params.goal_locations

        # offset for plotting:
        if self.session_name == "mjc163R4R_0114":
            x_off = 0
            y_off = 20
        elif self.session_name == "mjc163R2R_0114":
            x_off = 0
            y_off = 0
        elif self.session_name == "mjc169R1R_0114":
            x_off = -10
            y_off = 5
        elif self.session_name == "mjc169R4R_0114":
            x_off = 25
            y_off = 15
        elif self.session_name == "mjc163R1L_0114":
            x_off = 38
            y_off = 5
        elif self.session_name == "mjc148R4R_0113":
            x_off = 32
            y_off = 5
        elif self.session_name == "mjc163R3L_0114":
            x_off = 25
            y_off = 20

        if plotting:
            plt.imshow(model_dic["occ_map"].T)
            for g_l in gl:
                plt.scatter((g_l[0] - self.pre.x_min - x_off) / 5,
                            (g_l[1] - self.pre.y_min - y_off) / 5,
                            label="goal locations", marker="x", color="red", s=30, zorder=1000000)
            plt.show()

        goal_locations = []
        for g in gl:
            goal_locations.append(np.array([(g[0] - self.pre.x_min - x_off) / 5,
                                            (g[1] - self.pre.y_min - y_off) / 5]))
        goal_locations = np.vstack(goal_locations)

        with open(self.session_name+"_goal_locations_2_25cm_res", "wb") as fp:  # Pickling
            pickle.dump(goal_locations, fp)


        for start_event, end_event in zip(start, end):
            # need to filter here reactivations of goals and away from the goals for each event
            pre_prob_current_event = sorted_pop_vec_pre_prob[start_event:end_event]
            around_goals = np.zeros(pre_prob_current_event.shape[0])
            indices = np.arange(start_event, end_event)
            for nr_bin, bin in enumerate(pre_prob_current_event):
                b = np.reshape(bin, (res_2d[0], res_2d[1]))
                # find maximum
                pos = np.asarray(np.unravel_index(b.argmax(), b.shape))
                # check distance to all goals
                within_radius = np.linalg.norm(goal_locations - pos, axis=1) < max_dist
                around_goals[nr_bin] = np.count_nonzero(within_radius) > 0

            around_goals = around_goals.astype(bool)
            away_from_goals = np.invert(around_goals)

            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                ratio_per_merged_rem_event_away_from_goals.append(sorted_pop_vec_ratio[indices[away_from_goals]])
                ratio_per_merged_rem_event_around_goals.append(sorted_pop_vec_ratio[indices[around_goals]])
            # nrem event
            else:
                ratio_per_merged_nrem_event_away_from_goals.append(sorted_pop_vec_ratio[indices[away_from_goals]])
                ratio_per_merged_nrem_event_around_goals.append(sorted_pop_vec_ratio[indices[around_goals]])

            ratio_rem_nrem_events_around_goals.append(sorted_pop_vec_ratio[indices[around_goals]])
            ratio_rem_nrem_events_away_from_goals.append(sorted_pop_vec_ratio[indices[away_from_goals]])
            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])

            pre_prob_rem_nrem_pop_vec_around_goals.extend(sorted_pop_vec_pre_prob[indices[around_goals]])
            ratio_rem_nrem_pop_vec_around_goals.extend(sorted_pop_vec_ratio[indices[around_goals]])
            rem_nrem_pop_vec_label_around_goals.extend(labels_per_pop_vec[indices[around_goals]])
            pre_prob_rem_nrem_pop_vec_away_from_goals.extend(sorted_pop_vec_pre_prob[indices[away_from_goals]])
            ratio_rem_nrem_pop_vec_away_from_goals.extend(sorted_pop_vec_ratio[indices[away_from_goals]])
            rem_nrem_pop_vec_label_away_from_goals.extend(labels_per_pop_vec[indices[away_from_goals]])

        # smooth across population vectors --> that means also across REM/NREM epochs
        # --------------------------------------------------------------------------------------------------------------
        len_new_events_away_from_goals = [x.shape[0] for x in ratio_rem_nrem_events_away_from_goals]
        len_new_events_around_goals = [x.shape[0] for x in ratio_rem_nrem_events_around_goals]
        ratio_per_pop_vec_new_around_goals = np.hstack(ratio_rem_nrem_events_around_goals)
        ratio_per_pop_vec_new_away_from_goals = np.hstack(ratio_rem_nrem_events_away_from_goals)
        ratio_per_pop_vec_new_smooth_around_goals = moving_average(a=np.array(ratio_per_pop_vec_new_around_goals),
                                                                   n=n_moving_average_pop_vec)
        ratio_per_pop_vec_new_smooth_away_from_goals = moving_average(a=np.array(ratio_per_pop_vec_new_away_from_goals),
                                                                   n=n_moving_average_pop_vec)

        ratio_rem_nrem_events_smooth_away_from_goals = []
        first = 0
        for event_id in range(len(len_new_events_away_from_goals)):
            # check if data is still available (due to smoothing --> data gets shorter)
            # if not --> leave loop
            if first >= ratio_per_pop_vec_new_smooth_away_from_goals.shape[0]:
                break
            end_event = min(first + len_new_events_away_from_goals[event_id], ratio_per_pop_vec_new_smooth_away_from_goals.shape[0])
            ratio_rem_nrem_events_smooth_away_from_goals.append(ratio_per_pop_vec_new_smooth_away_from_goals[first:end_event])
            first += len_new_events_away_from_goals[event_id]

        # need to delete indices of events that happend after the last data point after smoothing
        rem_nrem_events_label_away_from_goals = rem_nrem_events_label[:len(ratio_rem_nrem_events_smooth_away_from_goals)]

        ratio_rem_nrem_events_smooth_around_goals = []
        first = 0
        for event_id in range(len(len_new_events_around_goals)):
            # check if data is still available (due to smoothing --> data gets shorter)
            # if not --> leave loop
            if first >= ratio_per_pop_vec_new_smooth_around_goals.shape[0]:
                break
            end_event = min(first + len_new_events_around_goals[event_id], ratio_per_pop_vec_new_smooth_around_goals.shape[0])
            ratio_rem_nrem_events_smooth_around_goals.append(ratio_per_pop_vec_new_smooth_around_goals[first:end_event])
            first += len_new_events_around_goals[event_id]

        # need to delete indices of events that happend after the last data point after smoothing
        rem_nrem_events_label_around_goals = rem_nrem_events_label[:len(ratio_rem_nrem_events_smooth_around_goals)]


        # get rem data of merged events after smoothing: around goals
        # --------------------------------------------------------------------------------------------------------------
        rem_events_indices_around_goals = np.squeeze(np.argwhere(np.array(rem_nrem_events_label_around_goals) == 1))
        ds_rem_around_goals = []
        ds_rem_smoothed_within_around_goals = []
        merged_events_rem_length_around_goals = []
        ratio_rem_events_smooth_around_goals = []
        event_lengths_rem_around_goals = []
        for rem_index in rem_events_indices_around_goals:
            ds_rem_around_goals.append(ratio_rem_nrem_events_smooth_around_goals[rem_index][-1] - ratio_rem_nrem_events_smooth_around_goals[rem_index][0])
            smooth_event = moving_average(a=ratio_rem_nrem_events_around_goals[rem_index], n=10)
            event_lengths_rem_around_goals.append(ratio_rem_nrem_events_around_goals[rem_index].shape[0])
            if smooth_event.shape[0] > 0:
                ds_rem_smoothed_within_around_goals.append(smooth_event[-1] - smooth_event[0])
            merged_events_rem_length_around_goals.append(ratio_rem_nrem_events_smooth_around_goals[rem_index].shape[0])
            ratio_rem_events_smooth_around_goals.append(ratio_rem_nrem_events_smooth_around_goals[rem_index])
        # get rem data of merged events after smoothing: away from goals
        # --------------------------------------------------------------------------------------------------------------
        rem_events_indices_away_from_goals = np.squeeze(np.argwhere(np.array(rem_nrem_events_label_away_from_goals) == 1))
        ds_rem_away_from_goals = []
        ds_rem_smoothed_within_away_from_goals = []
        merged_events_rem_length_away_from_goals = []
        ratio_rem_events_smooth_away_from_goals = []
        event_lengths_rem_away_from_goals = []
        for rem_index in rem_events_indices_away_from_goals:
            ds_rem_away_from_goals.append(ratio_rem_nrem_events_smooth_away_from_goals[rem_index][-1] - ratio_rem_nrem_events_smooth_away_from_goals[rem_index][0])
            smooth_event = moving_average(a=ratio_rem_nrem_events_away_from_goals[rem_index], n=10)
            event_lengths_rem_away_from_goals.append(ratio_rem_nrem_events_away_from_goals[rem_index].shape[0])
            if smooth_event.shape[0] > 0:
                ds_rem_smoothed_within_away_from_goals.append(smooth_event[-1] - smooth_event[0])
            merged_events_rem_length_away_from_goals.append(ratio_rem_nrem_events_smooth_away_from_goals[rem_index].shape[0])
            ratio_rem_events_smooth_away_from_goals.append(ratio_rem_nrem_events_smooth_away_from_goals[rem_index])


        # get nrem data of merged events after smoothing: around goals
        # --------------------------------------------------------------------------------------------------------------
        nrem_events_indices_around_goals = np.squeeze(np.argwhere(np.array(rem_nrem_events_label_around_goals) == 0))
        ds_nrem_around_goals = []
        ds_nrem_smoothed_within_around_goals = []
        merged_events_nrem_length_around_goals = []
        ratio_nrem_events_smooth_around_goals = []
        event_lengths_nrem_around_goals = []
        for nrem_index in nrem_events_indices_around_goals:
            if ((nrem_index < len(ratio_rem_nrem_events_smooth_around_goals)) and (ratio_rem_nrem_events_smooth_around_goals[nrem_index].shape[0] > 0)):
                ds_nrem_around_goals.append(ratio_rem_nrem_events_smooth_around_goals[nrem_index][-1] - ratio_rem_nrem_events_smooth_around_goals[nrem_index][0])
                smooth_event = moving_average(a=ratio_rem_nrem_events_around_goals[nrem_index], n=10)
                event_lengths_nrem_around_goals.append(ratio_rem_nrem_events_around_goals[nrem_index].shape[0])
                if smooth_event.shape[0] > 0:
                    ds_nrem_smoothed_within_around_goals.append(smooth_event[-1] - smooth_event[0])
                merged_events_nrem_length_around_goals.append(ratio_rem_nrem_events_smooth_around_goals[nrem_index].shape[0])
                ratio_nrem_events_smooth_around_goals.append(ratio_rem_nrem_events_smooth_around_goals[nrem_index])

        # get nrem data of merged events after smoothing: away from goals
        # --------------------------------------------------------------------------------------------------------------
        nrem_events_indices_away_from_goals = np.squeeze(np.argwhere(np.array(rem_nrem_events_label_away_from_goals) == 0))
        ds_nrem_away_from_goals = []
        ds_nrem_smoothed_within_away_from_goals = []
        merged_events_nrem_length_away_from_goals = []
        ratio_nrem_events_smooth_away_from_goals = []
        event_lengths_nrem_away_from_goals=[]
        for nrem_index in nrem_events_indices_away_from_goals:
            if ((nrem_index < len(ratio_rem_nrem_events_smooth_away_from_goals)) and (ratio_rem_nrem_events_smooth_away_from_goals[nrem_index].shape[0] > 0)):
                ds_nrem_away_from_goals.append(ratio_rem_nrem_events_smooth_away_from_goals[nrem_index][-1] - ratio_rem_nrem_events_smooth_away_from_goals[nrem_index][0])
                smooth_event = moving_average(a=ratio_rem_nrem_events_away_from_goals[nrem_index], n=10)
                event_lengths_nrem_away_from_goals.append(ratio_rem_nrem_events_away_from_goals[nrem_index].shape[0])
                if smooth_event.shape[0] > 0:
                    ds_nrem_smoothed_within_away_from_goals.append(smooth_event[-1] - smooth_event[0])
                merged_events_nrem_length_away_from_goals.append(ratio_rem_nrem_events_smooth_away_from_goals[nrem_index].shape[0])
                ratio_nrem_events_smooth_away_from_goals.append(ratio_rem_nrem_events_smooth_away_from_goals[nrem_index])


        # compute delta score sum & cum sum
        # --------------------------------------------------------------------------------------------------------------
        ds_nrem_sum_smoothed_within_around_goals = np.sum(np.array(ds_nrem_smoothed_within_around_goals))
        ds_rem_sum_smoothed_within_around_goals = np.sum(np.array(ds_rem_smoothed_within_around_goals))
        ds_nrem_sum_around_goals = np.sum(np.array(ds_nrem_around_goals))
        ds_rem_sum_around_goals = np.sum(np.array(ds_rem_around_goals))
        ds_nrem_cum_around_goals = np.cumsum(np.array(ds_nrem_around_goals))
        ds_rem_cum_around_goals = np.cumsum(np.array(ds_rem_around_goals))
        ds_nrem_sum_smoothed_within_away_from_goals = np.sum(np.array(ds_nrem_smoothed_within_away_from_goals))
        ds_rem_sum_smoothed_within_away_from_goals = np.sum(np.array(ds_rem_smoothed_within_away_from_goals))
        ds_nrem_sum_away_from_goals = np.sum(np.array(ds_nrem_away_from_goals))
        ds_rem_sum_away_from_goals = np.sum(np.array(ds_rem_away_from_goals))
        ds_nrem_cum_away_from_goals = np.cumsum(np.array(ds_nrem_away_from_goals))
        ds_rem_cum_away_from_goals = np.cumsum(np.array(ds_rem_away_from_goals))

        # compute cross correlation of delta scores NREM/REM: away from goals
        # --------------------------------------------------------------------------------------------------------------
        min_len_away_from_goals = min(len(ds_rem_away_from_goals), len(ds_nrem_away_from_goals))
        ds_rem_arr_away_from_goals = np.array(ds_rem_away_from_goals)[:min_len_away_from_goals]
        ds_nrem_arr_away_from_goals = np.array(ds_nrem_away_from_goals)[:min_len_away_from_goals]
        corr_list_pos_away_from_goals = []
        corr_list_neg_away_from_goals = []
        shift_array_cross_corr = [0, 1, 2, 3, 4, 5, 6]
        for shift in shift_array_cross_corr:
            corr_list_pos_away_from_goals.append(
                np.round(pearsonr(ds_rem_arr_away_from_goals[shift:], ds_nrem_arr_away_from_goals[:ds_nrem_arr_away_from_goals.shape[0] - shift])[0], 2))
            corr_list_neg_away_from_goals.append(
                np.round(pearsonr(ds_nrem_arr_away_from_goals[shift:], ds_rem_arr_away_from_goals[:ds_rem_arr_away_from_goals.shape[0] - shift])[0], 2))
        corr_list_away_from_goals = np.hstack((np.flip(np.array(corr_list_neg_away_from_goals)), np.array(corr_list_pos_away_from_goals)))

        # compute cross correlation of delta scores NREM/REM: around goals
        # --------------------------------------------------------------------------------------------------------------
        min_len_around_goals = min(len(ds_rem_around_goals), len(ds_nrem_around_goals))
        ds_rem_arr_around_goals = np.array(ds_rem_around_goals)[:min_len_around_goals]
        ds_nrem_arr_around_goals = np.array(ds_nrem_around_goals)[:min_len_around_goals]
        corr_list_pos_around_goals = []
        corr_list_neg_around_goals = []
        shift_array_cross_corr = [0, 1, 2, 3, 4, 5, 6]
        for shift in shift_array_cross_corr:
            corr_list_pos_around_goals.append(
                np.round(pearsonr(ds_rem_arr_around_goals[shift:], ds_nrem_arr_around_goals[:ds_nrem_arr_around_goals.shape[0] - shift])[0], 2))
            corr_list_neg_away_from_goals.append(
                np.round(pearsonr(ds_nrem_arr_around_goals[shift:], ds_rem_arr_around_goals[:ds_rem_arr_around_goals.shape[0] - shift])[0], 2))
        corr_list_around_goals = np.hstack((np.flip(np.array(corr_list_neg_around_goals)), np.array(corr_list_pos_around_goals)))


        # get mean score per event
        # --------------------------------------------------------------------------------------------------------------
        mean_nrem_around_goals = []
        for i, event in enumerate(ratio_per_merged_nrem_event_around_goals):
            mean_nrem_around_goals.append(np.mean(event))
        mean_rem_around_goals = []
        for i, event in enumerate(ratio_per_merged_rem_event_around_goals):
            mean_rem_around_goals.append(np.mean(event))

        mean_nrem_away_from_goals = []
        for i, event in enumerate(ratio_per_merged_nrem_event_away_from_goals):
            mean_nrem_away_from_goals.append(np.mean(event))
        mean_rem_away_from_goals = []
        for i, event in enumerate(ratio_per_merged_rem_event_away_from_goals):
            mean_rem_away_from_goals.append(np.mean(event))

        if plotting:

            c = "white"
            res = [ds_rem_around_goals, ds_rem_away_from_goals, ds_nrem_around_goals, ds_nrem_away_from_goals]
            bplot = plt.boxplot(res, positions=[1, 2, 3, 4], patch_artist=True,
                                labels=["REM around", "REM away", "NREM around", "NREM away"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            colors = ["magenta", 'magenta', "blue", "blue"]
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            plt.title("Delta score: around & away from goals")
            plt.ylabel("Delta score")
            plt.grid(color="grey", axis="y")
            plt.show()
            print("REM")
            print(mannwhitneyu(ds_rem_around_goals, ds_rem_away_from_goals))
            print("NREM")
            print(mannwhitneyu(ds_nrem_around_goals, ds_nrem_away_from_goals))


            c = "white"
            res = [event_lengths_nrem_around_goals, event_lengths_nrem_away_from_goals,
                   event_lengths_rem_around_goals, event_lengths_rem_away_from_goals]
            bplot = plt.boxplot(res, positions=[1, 2, 3, 4], patch_artist=True,
                                labels=["NREM around goals", "NREM away from goals", "REM around goals", "REM away from goals"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            colors = ["magenta", 'magenta', "blue", "blue"]
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            plt.title("Event length (#pop. vecs)")
            plt.ylabel("Event length (#pop. vecs)")
            plt.grid(color="grey", axis="y")
            plt.show()


            # plotting
            fig = plt.figure()
            ax = fig.add_subplot()
            start = 0
            for event, label in zip(ratio_rem_nrem_events_smooth_away_from_goals, rem_nrem_events_label):
                event_length = event.shape[0]
                if label:
                    ax.plot(np.arange(start, start + event_length), event, c="r", label="REM", linewidth=0.4)
                else:
                    ax.plot(np.arange(start, start + event_length), event, c="b", label="NREM")
                start += event_length
            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())
            plt.grid()
            plt.title("Away from goals, SMOOTHING: n=" + str(n_moving_average_pop_vec))
            plt.xlabel("POPULATION VECTOR ID")
            if measure == "normalized_ratio":
                plt.ylim(-1, 1)
                plt.ylabel("PRE_POST RATIO")
            elif measure == "log_like_ratio":
                plt.ylabel("LOG-LIKELIHOOD RATIO")
                y_min, y_max = ax.get_ylim()
                plt.ylim(y_min, -1 * y_min)

            elif measure == "model_evidence":
                plt.ylabel("MODEL EVIDENCE")
                y_min, y_max = ax.get_ylim()
                plt.ylim(y_min, -1 * y_min)
            plt.show()

            # plotting
            fig = plt.figure()
            ax = fig.add_subplot()
            start = 0
            for event, label in zip(ratio_rem_nrem_events_smooth_around_goals, rem_nrem_events_label):
                event_length = event.shape[0]
                if label:
                    ax.plot(np.arange(start, start + event_length), event, c="r", label="REM", linewidth=0.4)
                else:
                    ax.plot(np.arange(start, start + event_length), event, c="b", label="NREM")
                start += event_length
            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())
            plt.grid()
            plt.title("Around goals, SMOOTHING: n=" + str(n_moving_average_pop_vec))
            plt.xlabel("POPULATION VECTOR ID")
            if measure == "normalized_ratio":
                plt.ylim(-1, 1)
                plt.ylabel("PRE_POST RATIO")
            elif measure == "log_like_ratio":
                plt.ylabel("LOG-LIKELIHOOD RATIO")
                y_min, y_max = ax.get_ylim()
                plt.ylim(y_min, -1 * y_min)

            elif measure == "model_evidence":
                plt.ylabel("MODEL EVIDENCE")
                y_min, y_max = ax.get_ylim()
                plt.ylim(y_min, -1 * y_min)
            plt.show()


            def make_square_axes(ax):
                """Make an axes square in screen units.

                Should be called after plotting.
                """
                ax.set_aspect(1 / ax.get_data_ratio())

            plt.scatter(ds_rem_arr_around_goals, ds_nrem_arr_around_goals)
            plt.title("Around goals, NEIGHBOURING PERIODS, R=" + str(np.round(pearsonr(ds_rem_arr_around_goals, ds_nrem_arr_around_goals)[0], 2)))
            plt.xlabel("DELTA SCORE REM")
            plt.ylabel("DELTA SCORE NREM")
            make_square_axes(plt.gca())
            plt.show()

            plt.scatter(ds_rem_arr_away_from_goals, ds_nrem_arr_away_from_goals)
            plt.title("Away from goals, NEIGHBOURING PERIODS, R=" + str(np.round(pearsonr(ds_rem_arr_away_from_goals, ds_nrem_arr_away_from_goals)[0], 2)))
            plt.xlabel("DELTA SCORE REM")
            plt.ylabel("DELTA SCORE NREM")
            make_square_axes(plt.gca())
            plt.show()

            # x_axis_cross_corr = -1 * np.flip(np.array(shift_array_cross_corr) + 1)
            # x_axis_cross_corr = np.hstack((x_axis_cross_corr, np.array(shift_array_cross_corr) + 1))
            # plt.plot(x_axis_cross_corr, corr_list_away_from_goals, marker=".")
            # plt.xlabel("REM OFFSET (+X MEANS REM BEHIND NREM BY X)")
            # plt.ylabel("CORRELATION DELTA SCORE REM VS. NREM")
            # plt.title("CROSS CORRELATION OF DELTA SCORES REM VS. NREM\n Away from goals")
            # plt.show()
            #
            # x_axis_cross_corr = -1 * np.flip(np.array(shift_array_cross_corr) + 1)
            # x_axis_cross_corr = np.hstack((x_axis_cross_corr, np.array(shift_array_cross_corr) + 1))
            # plt.plot(x_axis_cross_corr, corr_list_around_goals, marker=".")
            # plt.xlabel("REM OFFSET (+X MEANS REM BEHIND NREM BY X)")
            # plt.ylabel("CORRELATION DELTA SCORE REM VS. NREM")
            # plt.title("CROSS CORRELATION OF DELTA SCORES REM VS. NREM\n Around goals")
            # plt.show()

            #
            # plt.plot(ds_rem, color="gray", linewidth=0.8)
            # plt.scatter(range(len(ds_rem)), ds_rem, c=merged_events_length_s[merged_events_labels == 1], cmap=cm.Reds)
            # a = plt.colorbar()
            # a.set_label("DURATION / s")
            # plt.hlines(0, 0, len(ds_rem), color="w", linewidth=0.5, zorder=-1000)
            # plt.ylabel("DELTA SCORE")
            # plt.xlabel("EVENT ID")
            # plt.title("REM: PER EVENT DELTA SCORE & DURATION")
            # plt.show()
            #
            # plt.plot(ds_nrem, color="gray", linewidth=0.8)
            # plt.scatter(range(len(ds_nrem)), ds_nrem, c=merged_events_length_s[merged_events_labels == 0], cmap=cm.Reds)
            # a = plt.colorbar()
            # a.set_label("DURATION / s")
            # plt.hlines(0, 0, len(ds_nrem), color="w", linewidth=0.5, zorder=-1000)
            # plt.ylabel("DELTA SCORE")
            # plt.xlabel("EVENT ID")
            # plt.title("NREM: PER EVENT DELTA SCORE & DURATION")
            # plt.show()

            # plt.plot(merged_events_length_s[merged_events_labels == 1], color="gray", linewidth=0.8)
            # plt.scatter(range(len(merged_events_length_s[merged_events_labels == 1])),
            #             merged_events_length_s[merged_events_labels == 1], c=ds_rem, cmap=cm.coolwarm)
            # a = plt.colorbar()
            # a.set_label("DELTA SCORE")
            # plt.ylabel("DURATION EVENT")
            # plt.xlabel("EVENT ID")
            # plt.title("REM: PER EVENT DELTA SCORE & DURATION")
            # plt.show()
            #
            # plt.plot(merged_events_length_s[merged_events_labels == 0], color="gray", linewidth=0.8)
            # plt.scatter(range(len(merged_events_length_s[merged_events_labels == 0])),
            #             merged_events_length_s[merged_events_labels == 0], c=ds_nrem, cmap=cm.coolwarm)
            # a = plt.colorbar()
            # a.set_label("DELTA SCORE")
            # plt.ylabel("DURATION EVENT")
            # plt.xlabel("EVENT ID")
            # plt.title("NREM: PER EVENT DELTA SCORE & DURATION")
            # plt.show()

            # check which phase came first
            if rem_nrem_events_label[0] == 1:
                # first event was a rem event
                rem_x_axis = np.arange(0, 2 * len(ds_rem_away_from_goals), 2)
                nrem_x_axis = np.arange(1, 2 * len(ds_nrem_away_from_goals) + 1, 2)
            elif rem_nrem_events_label[0] == 0:
                # first event was a nrem event
                nrem_x_axis = np.arange(0, 2 * len(ds_nrem_away_from_goals), 2)
                rem_x_axis = np.arange(1, 2 * len(ds_rem_away_from_goals) + 1, 2)

            plt.plot(rem_x_axis, ds_rem_away_from_goals, marker=".", label="REM", color="r")
            plt.plot(nrem_x_axis, ds_nrem_away_from_goals, marker=".", label="NREM", color="b")
            plt.legend()
            plt.hlines(0, 0, rem_x_axis.shape[0] + nrem_x_axis.shape[0], color="gray")
            plt.ylabel("DELTA SCORE")
            plt.xlabel("EVENT ID")
            plt.title("PER EVENT DELTA SCORE: away from goals")
            plt.show()

            if rem_nrem_events_label[0] == 1:
                # first event was a rem event
                rem_x_axis = np.arange(0, 2 * len(ds_rem_around_goals), 2)
                nrem_x_axis = np.arange(1, 2 * len(ds_nrem_around_goals) + 1, 2)
            elif rem_nrem_events_label[0] == 0:
                # first event was a nrem event
                nrem_x_axis = np.arange(0, 2 * len(ds_nrem_around_goals), 2)
                rem_x_axis = np.arange(1, 2 * len(ds_rem_around_goals) + 1, 2)

            plt.plot(rem_x_axis, ds_rem_around_goals, marker=".", label="REM", color="r")
            plt.plot(nrem_x_axis, ds_nrem_around_goals, marker=".", label="NREM", color="b")
            plt.legend()
            plt.hlines(0, 0, rem_x_axis.shape[0] + nrem_x_axis.shape[0], color="gray")
            plt.ylabel("DELTA SCORE")
            plt.xlabel("EVENT ID")
            plt.title("PER EVENT DELTA SCORE: around goals")
            plt.show()


            plt.hist(merged_events_length_s[merged_events_labels == 1], bins=10, label="REM", density=True, color="r")
            plt.hist(merged_events_length_s[merged_events_labels == 0], bins=10, label="NREM", density=True, color="b")
            plt.title("EVENT DURATION (REM.POP.VEC.THRS.=" + str(rem_pop_vec_threshold) + ")")
            plt.xlabel("DURATION OF EVENT / s")
            plt.ylabel("DENSITY")
            plt.legend()
            plt.show()


            plt.scatter(1, ds_nrem_sum_around_goals, color="b", label="NREM", zorder=1000)
            # plt.scatter(1, ds_nrem_cum_smoothed_within, color="lightskyblue", label="NREM - WITHIN", zorder=1000)
            plt.scatter(1, ds_rem_sum_around_goals, color="r", label="REM", zorder=1000)
            # plt.scatter(1, ds_rem_cum_smoothed_within, color="lightcoral", label="REM - WITHIN", zorder=1000)
            plt.legend()
            plt.ylabel("CUMULATIVE DELTA SCORE")
            plt.title("SMOOTHING: n=" + str(n_moving_average_pop_vec)+"\n AROUND GOALS")
            plt.grid()
            plt.show()

            plt.scatter(1, ds_nrem_sum_away_from_goals, color="b", label="NREM", zorder=1000)
            # plt.scatter(1, ds_nrem_cum_smoothed_within, color="lightskyblue", label="NREM - WITHIN", zorder=1000)
            plt.scatter(1, ds_rem_sum_away_from_goals, color="r", label="REM", zorder=1000)
            # plt.scatter(1, ds_rem_cum_smoothed_within, color="lightcoral", label="REM - WITHIN", zorder=1000)
            plt.legend()
            plt.ylabel("CUMULATIVE DELTA SCORE")
            plt.title("SMOOTHING: n=" + str(n_moving_average_pop_vec)+"\n AWAY FROM GOALS")
            plt.grid()
            plt.show()


            plt.title("SMOOTHING n=" + str(n_moving_average_pop_vec) + ", POP.THR.REM=" + str(rem_pop_vec_threshold)+"\n Around goals")
            plt.plot(ds_nrem_cum_around_goals, color="b", label="NREM")
            plt.plot(ds_rem_cum_around_goals, color="r", label="REM")
            plt.grid()
            plt.xlabel("EVENT ID")
            plt.legend()
            plt.ylabel("CUM SUM")
            plt.show()

            plt.title("SMOOTHING n=" + str(n_moving_average_pop_vec) + ", POP.THR.REM=" + str(rem_pop_vec_threshold)+"\n Away from goals")
            plt.plot(ds_nrem_cum_away_from_goals, color="b", label="NREM")
            plt.plot(ds_rem_cum_away_from_goals, color="r", label="REM")
            plt.grid()
            plt.xlabel("EVENT ID")
            plt.legend()
            plt.ylabel("CUM SUM")
            plt.show()




        else:
            return ds_rem_around_goals, ds_rem_away_from_goals, ds_nrem_around_goals, ds_nrem_away_from_goals

    def memory_drift_rem_nrem_decoding_similarity_plot_decoded_map(self, pre_file_name=None, post_file_name=None,
                                                                   rem_pop_vec_threshold=100, cells_to_use="all",
                                                                   save_fig=False, pre_or_post="pre"):

        _, _, default_ising_pre, default_ising_post = self.long_sleep[0].get_pre_post_templates()

        if pre_or_post == "pre":
            ising_map = default_ising_pre
        elif pre_or_post == "post":
            ising_map = default_ising_post

        with open(self.params.pre_proc_dir + 'awake_ising_maps/' + ising_map + '.pkl',
                  'rb') as f:
            model_dic = pickle.load(f)

        # factor a.u. to cm
        scaling_fac = self.session_params.data_params_dictionary["spatial_factor"]

        # ising maps are decoded using 5 a.u.
        spatial_bin_size = 5 * scaling_fac

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        _, _, _, _, pre_prob_rem, post_prob_rem = \
            self.memory_drift_long_sleep_get_results(template_type="ising", pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold,
                                                     cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        _, _, _, _, pre_prob_nrem, \
        post_prob_nrem = self.memory_drift_long_sleep_get_results(template_type="ising", pre_file_name=pre_file_name,
                                                                  post_file_name=post_file_name, part_to_analyze="nrem",
                                                                  pop_vec_threshold=2, cells_to_use=cells_to_use)

        if pre_or_post == "pre":
            # rem pre map results and normalize
            rem_mode_freq = np.zeros(pre_prob_rem.shape[1])
            map_result_rem = np.argmax(pre_prob_rem, axis=1)

        elif pre_or_post == "post":
            # rem post map results and normalize
            rem_mode_freq = np.zeros(post_prob_rem.shape[1])
            map_result_rem = np.argmax(post_prob_rem, axis=1)

        rem_mode_id, rem_mode_count = np.unique(map_result_rem, return_counts=True)
        rem_mode_freq[rem_mode_id] = rem_mode_count
        rem_mode_freq_norm = rem_mode_freq / np.sum(rem_mode_freq)

        if pre_or_post == "pre":
            # get nrem pre map results and normalize
            nrem_mode_freq = np.zeros(pre_prob_nrem.shape[1])
            map_result_nrem = np.argmax(pre_prob_nrem, axis=1)
        elif pre_or_post == "post":
            # get nrem pre map results and normalize
            nrem_mode_freq = np.zeros(post_prob_nrem.shape[1])
            map_result_nrem = np.argmax(post_prob_nrem, axis=1)

        nrem_mode_id, nrem_mode_count = np.unique(map_result_nrem, return_counts=True)
        nrem_mode_freq[nrem_mode_id] = nrem_mode_count
        nrem_mode_freq_norm = nrem_mode_freq / np.sum(nrem_mode_freq)

        print("Correlation, R = "+str(pearsonr(nrem_mode_freq_norm, rem_mode_freq_norm)))

        nrem_mode_freq_spatial = np.reshape(nrem_mode_freq_norm,
                                                (model_dic["res_map"].shape[1], model_dic["res_map"].shape[2]))
        rem_mode_freq_spatial = np.reshape(rem_mode_freq_norm,
                                               (model_dic["res_map"].shape[1], model_dic["res_map"].shape[2]))

        nrem_mode_freq_spatial[model_dic["occ_map"] == 0] = np.nan
        rem_mode_freq_spatial[model_dic["occ_map"] == 0] = np.nan

        plt.style.use('default')

        # plt.imshow(model_dic["occ_map"].T, origin="lower")
        # for g_l in self.pre.goal_locations:
        #     plt.scatter((g_l[0] - self.pre.x_min+8) / spatial_bin_size, (g_l[1] - self.pre.y_min+1) / spatial_bin_size)
        # plt.show()

        max_val = np.nanmax(np.hstack((nrem_mode_freq_spatial.flatten(),rem_mode_freq_spatial.flatten())))

        plt.imshow(nrem_mode_freq_spatial.T, origin='lower', cmap="Reds", vmin=0, vmax=max_val)
        a = plt.colorbar()
        a.set_label("Prob. of decoding bin")
        plt.xticks([0, 16, 32, 48], np.array([0, 16, 32, 48]) * spatial_bin_size)
        plt.yticks([0, 16, 32, 48], np.array([0, 16, 32, 48]) * spatial_bin_size)
        plt.xlabel("x (cm)")
        plt.ylabel("y (cm)")
        plt.title("NREM")
        plt.xlim(-2,50)
        plt.ylim(-2,50)
        # plt.gca().yaxis.set_major_locator(MaxNLocator(integer=True))
        # corrected goal locations based on occupancy map/tracking data ... there was an offset
        for g_l in self.pre.goal_locations:
            plt.scatter((g_l[0] - self.pre.x_min + 8) / spatial_bin_size,
                        (g_l[1] - self.pre.y_min + 1) / spatial_bin_size,
                        label="goal locations", color="black", s=30)
        handles, labels = plt.gca().get_legend_handles_labels()
        by_label = OrderedDict(zip(labels, handles))
        plt.gca().legend(by_label.values(), by_label.keys())
        if save_fig:
            plt.rcParams['svg.fonttype'] = 'none'
            plt.savefig(os.path.join(save_path, "decoding_ising_nrem.svg"), transparent="True")
            plt.close()
        else:
            plt.show()

        plt.imshow(rem_mode_freq_spatial.T, origin='lower', cmap="Reds", vmin=0, vmax=max_val)
        a = plt.colorbar()
        a.set_label("Prob. of decoding bin")
        plt.xticks([0, 16, 32, 48], np.array([0, 16, 32, 48]) * spatial_bin_size)
        plt.yticks([0, 16, 32, 48], np.array([0, 16, 32, 48]) * spatial_bin_size)
        plt.title("REM")
        plt.xlabel("x (cm)")
        plt.ylabel("y (cm)")
        plt.xlim(-2,50)
        plt.ylim(-2,50)
        # plt.gca().yaxis.set_major_locator(MaxNLocator(integer=True))
        # corrected goal locations based on occupancy map/tracking data ... there was an offset
        for g_l in self.pre.goal_locations:
            plt.scatter((g_l[0] - self.pre.x_min + 8) / spatial_bin_size,
                        (g_l[1] - self.pre.y_min + 1) / spatial_bin_size,
                        label="goal locations", color="black", s=30)
        handles, labels = plt.gca().get_legend_handles_labels()
        by_label = OrderedDict(zip(labels, handles))
        plt.gca().legend(by_label.values(), by_label.keys())
        if save_fig:
            plt.rcParams['svg.fonttype'] = 'none'
            plt.savefig(os.path.join(save_path, "decoding_ising_rem.svg"), transparent="True")
            plt.close()
        else:
            plt.show()

    def goal_reactivations_occupancy_post(self, pre_file_name=None, post_file_name=None,
                          pop_vec_threshold=1, cells_to_use="all", radius_around_goal=10,
                          max_dist=5, sleep_phase="rem", first_half_of_post=False,
                                          plotting=False, third_of_sleep=1, z_score_occ=True):
        """
        goal reactivation analysis


        :return: #times goal was decoded and occupancy in post
        :rtype:
        """
        # get length of sleep in seconds

        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        if sleep_phase == "rem":
            # get rem data first
            # --------------------------------------------------------------------------------------------------------------
            pre_prob, _, event_times = \
                self.memory_drift_long_sleep_get_raw_results(template_type="ising", pre_file_name=pre_file_name,
                                                         post_file_name=post_file_name, part_to_analyze="rem",
                                                         pop_vec_threshold=pop_vec_threshold, cells_to_use=cells_to_use)
        elif sleep_phase == "nrem":
            # get nrem data
            # ----------------------------------------------------------------------------------------------------------
            pre_prob, _, event_times = \
                self.memory_drift_long_sleep_get_raw_results(template_type="ising", pre_file_name=pre_file_name,
                                                         post_file_name=post_file_name, part_to_analyze="nrem",
                                                         pop_vec_threshold=2, cells_to_use=cells_to_use)

        # check if only a subset of sleep is supposed to be used
        if not third_of_sleep is None:
            if third_of_sleep == 1:
                # first third of sleep
                sleep_chunk_interval = [0, 1/3*len_sleep]
            elif third_of_sleep == 2:
                # second third of sleep
                sleep_chunk_interval = [1/3*len_sleep, 2/3*len_sleep]
            elif third_of_sleep == 3:
                # last third of sleep
                sleep_chunk_interval = [2/3*len_sleep, len_sleep]

            # only select REM or NREM phases that are within the interval
            pre_prob_mod = []
            event_times_mod = []
            for likeli, times in zip(pre_prob, event_times):
                if sleep_chunk_interval[0] < times[0] and times[1] < sleep_chunk_interval[1]:
                    pre_prob_mod.append(likeli)
                    event_times_mod.append(times)

            pre_prob = pre_prob_mod


        # need to get model to convert array to 2D matrix for locations
        template_file_name = self.long_sleep[0].session_params.default_pre_ising_model

        with open(self.params.pre_proc_dir + 'awake_ising_maps/' + template_file_name + '.pkl', 'rb') as f:
            model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        res_2d = model_dic["occ_map"].shape

        # factor a.u. to cm
        # scaling_fac = self.session_params.data_params_dictionary["spatial_factor"]

        # ising maps are decoded using 5 a.u.
        # spatial_bin_size = 5 * scaling_fac

        # goal locations
        gl = self.long_sleep[0].session_params.goal_locations

        # offset for plotting:
        if self.session_name == "mjc163R4R_0114":
            x_off = 0
            y_off = 20
        elif self.session_name == "mjc163R2R_0114":
            x_off = -5
            y_off = 10
        elif self.session_name == "mjc169R1R_0114":
            x_off = -10
            y_off = 5
        elif self.session_name == "mjc169R4R_0114":
            x_off = 25
            y_off = 15
        elif self.session_name == "mjc163R1L_0114":
            x_off = 38
            y_off = 5
        elif self.session_name == "mjc148R4R_0113":
            x_off = 32
            y_off = 5
        elif self.session_name == "mjc163R3L_0114":
            x_off = 25
            y_off = 20

        if plotting:
            plt.imshow(model_dic["occ_map"].T)
            for g_l in gl:
                plt.scatter((g_l[0] - self.pre.x_min - x_off) / 5,
                            (g_l[1] - self.pre.y_min - y_off) / 5,
                            label="goal locations", marker="x", color="red", s=30, zorder=1000000)
            plt.show()

        goal_locations = []
        for g in gl:
            goal_locations.append(np.array([(g[0] - self.pre.x_min - x_off) / 5,
                                (g[1] - self.pre.y_min - y_off) / 5]))
        goal_locations = np.vstack(goal_locations)

        # record how often each goal was decoded
        goal_decoded = np.zeros(4)
        for event in pre_prob:
            for nr, bin in enumerate(event):
                b = np.reshape(bin,(res_2d[0], res_2d[1]))
                # find maximum
                pos = np.asarray(np.unravel_index(b.argmax(), b.shape))
                # check distance to all goals
                within_radius = np.linalg.norm(goal_locations - pos, axis=1) < max_dist

                goal_decoded[within_radius] += 1

        # for g_l in gl:
        #     plt.scatter((g_l[0] - self.pre.x_min - x_off) / 5,
        #                 (g_l[1] - self.pre.y_min - y_off) / 5,
        #                 label="goal locations", marker="x", color="red", s=30, zorder=1000000)
        # handles, labels = plt.gca().get_legend_handles_labels()
        # by_label = OrderedDict(zip(labels, handles))
        # plt.gca().legend(by_label.values(), by_label.keys())
        # plt.show()

        occ_post = self.post.occupancy_per_goal(first_half=first_half_of_post, radius=radius_around_goal)
        occ_post_amount = np.zeros(4)
        for goal_id, op in enumerate(occ_post):
            occ_post_amount[goal_id] = np.nansum(op)/np.sum(~np.isnan(op))


        goal_decoded_normalized = goal_decoded / goal_decoded.sum()
        occ_post_amount_normalized = occ_post_amount / occ_post_amount.sum()

        if plotting:
            plt.scatter(goal_decoded_normalized, occ_post_amount_normalized)
            plt.text(np.min(goal_decoded_normalized)+(np.max(goal_decoded_normalized)-np.min(goal_decoded_normalized))/2,
                     np.min(occ_post_amount_normalized)+(np.max(occ_post_amount_normalized)-np.min(occ_post_amount_normalized))/2,
                     "R="+str(np.round(pearsonr(goal_decoded_normalized, occ_post_amount_normalized)[0],2)))
            plt.xlabel("#goal reactivated during sleep (normalized)")
            plt.ylabel("occupancy per bin \n goal recall (normalized)")
            plt.title(self.session_name)
            plt.show()

        if z_score_occ:
            occ_post_amount_normalized = zscore(occ_post_amount_normalized)

        return goal_decoded_normalized, occ_post_amount_normalized

    def goal_reactivations_excess_path(self, pre_file_name=None, post_file_name=None, max_dist = 5,
                          pop_vec_threshold=1, cells_to_use="all", sleep_phase="rem", radius_goal=5, radius_start=10,
                                          plotting=False, third_of_sleep=1, z_score_excess=True, trial_to_use=0):
        """
        goal reactivation analysis


        :return: #times goal was decoded and occupancy in post
        :rtype:
        """
        # get length of sleep in seconds

        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        if sleep_phase == "rem":
            # get rem data first
            # --------------------------------------------------------------------------------------------------------------
            pre_prob, _, event_times = \
                self.memory_drift_long_sleep_get_raw_results(template_type="ising", pre_file_name=pre_file_name,
                                                         post_file_name=post_file_name, part_to_analyze="rem",
                                                         pop_vec_threshold=pop_vec_threshold, cells_to_use=cells_to_use)
        elif sleep_phase == "nrem":
            # get nrem data
            # ----------------------------------------------------------------------------------------------------------
            pre_prob, _, event_times = \
                self.memory_drift_long_sleep_get_raw_results(template_type="ising", pre_file_name=pre_file_name,
                                                         post_file_name=post_file_name, part_to_analyze="nrem",
                                                         pop_vec_threshold=2, cells_to_use=cells_to_use)

        # check if only a subset of sleep is supposed to be used
        if not third_of_sleep is None:
            if third_of_sleep == 1:
                # first third of sleep
                sleep_chunk_interval = [0, 1/3*len_sleep]
            elif third_of_sleep == 2:
                # second third of sleep
                sleep_chunk_interval = [1/3*len_sleep, 2/3*len_sleep]
            elif third_of_sleep == 3:
                # last third of sleep
                sleep_chunk_interval = [2/3*len_sleep, len_sleep]

            # only select REM or NREM phases that are within the interval
            pre_prob_mod = []
            event_times_mod = []
            for likeli, times in zip(pre_prob, event_times):
                if sleep_chunk_interval[0] < times[0] and times[1] < sleep_chunk_interval[1]:
                    pre_prob_mod.append(likeli)
                    event_times_mod.append(times)

            pre_prob = pre_prob_mod


        # need to get model to convert array to 2D matrix for locations
        template_file_name = self.long_sleep[0].session_params.default_pre_ising_model

        with open(self.params.pre_proc_dir + 'awake_ising_maps/' + template_file_name + '.pkl', 'rb') as f:
            model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        res_2d = model_dic["occ_map"].shape

        # factor a.u. to cm
        # scaling_fac = self.session_params.data_params_dictionary["spatial_factor"]

        # ising maps are decoded using 5 a.u.
        # spatial_bin_size = 5 * scaling_fac

        # goal locations
        gl = self.long_sleep[0].session_params.goal_locations

        # offset for plotting:
        if self.session_name == "mjc163R4R_0114":
            x_off = 0
            y_off = 20
        elif self.session_name == "mjc163R2R_0114":
            x_off = 0
            y_off = 0
        elif self.session_name == "mjc169R1R_0114":
            x_off = -10
            y_off = 5
        elif self.session_name == "mjc169R4R_0114":
            x_off = 25
            y_off = 15
        elif self.session_name == "mjc163R1L_0114":
            x_off = 38
            y_off = 5
        elif self.session_name == "mjc148R4R_0113":
            x_off = 32
            y_off = 5
        elif self.session_name == "mjc163R3L_0114":
            x_off = 25
            y_off = 20

        if plotting:
            plt.imshow(model_dic["occ_map"].T)
            for g_l in gl:
                plt.scatter((g_l[0] - self.pre.x_min - x_off) / 5,
                            (g_l[1] - self.pre.y_min - y_off) / 5,
                            label="goal locations", marker="x", color="red", s=30, zorder=1000000)
            plt.show()

        goal_locations = []
        for g in gl:
            goal_locations.append(np.array([(g[0] - self.pre.x_min - x_off) / 5,
                                (g[1] - self.pre.y_min - y_off) / 5]))
        goal_locations = np.vstack(goal_locations)

        # record how often each goal was decoded
        goal_decoded = np.zeros(4)
        for event in pre_prob:
            for nr, bin in enumerate(event):
                b = np.reshape(bin,(res_2d[0], res_2d[1]))
                # find maximum
                pos = np.asarray(np.unravel_index(b.argmax(), b.shape))
                # check distance to all goals
                within_radius = np.linalg.norm(goal_locations - pos, axis=1) < max_dist

                goal_decoded[within_radius] += 1

        # for g_l in gl:
        #     plt.scatter((g_l[0] - self.pre.x_min - x_off) / 5,
        #                 (g_l[1] - self.pre.y_min - y_off) / 5,
        #                 label="goal locations", marker="x", color="red", s=30, zorder=1000000)
        # handles, labels = plt.gca().get_legend_handles_labels()
        # by_label = OrderedDict(zip(labels, handles))
        # plt.gca().legend(by_label.values(), by_label.keys())
        # plt.show()

        excess_post = self.post.excess_path_per_goal(trial_to_use=trial_to_use, radius_goal=radius_goal, radius_start=radius_start)

        goal_decoded_normalized = goal_decoded / goal_decoded.sum()

        if plotting:
            plt.scatter(goal_decoded_normalized, excess_post)
            plt.text(np.min(goal_decoded_normalized)+(np.max(goal_decoded_normalized)-np.min(goal_decoded_normalized))/2,
                     np.min(excess_post)+(np.max(excess_post)-np.min(excess_post))/2,
                     "R="+str(np.round(pearsonr(goal_decoded_normalized, excess_post)[0],2)))
            plt.xlabel("#goal reactivated during sleep (normalized)")
            plt.ylabel("Excess path in post (1st trial)")
            plt.title(self.session_name)
            plt.show()

        if z_score_excess:
            excess_post = zscore(excess_post)

        return goal_decoded_normalized, excess_post

    def goal_reactivations_change_in_goal_coding(self, pre_file_name=None, post_file_name=None,
                          pop_vec_threshold=1, cells_to_use="all", radius_around_goal=10,
                          max_dist=5, sleep_phase="nrem", first_half_of_post=False, plotting=False,
                                                 third_of_sleep=None, z_score_goal_coding=True):
        """
        goal reactivation analysis


        :return: #times goal was decoded and occupancy in post
        :rtype:
        """
        # get length of sleep in seconds

        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        if sleep_phase == "rem":
            # get rem data first
            # --------------------------------------------------------------------------------------------------------------
            pre_prob, _, event_times = \
                self.memory_drift_long_sleep_get_raw_results(template_type="ising", pre_file_name=pre_file_name,
                                                         post_file_name=post_file_name, part_to_analyze="rem",
                                                         pop_vec_threshold=pop_vec_threshold, cells_to_use=cells_to_use)
        elif sleep_phase == "nrem":
            # get nrem data
            # ----------------------------------------------------------------------------------------------------------
            pre_prob, _, event_times = \
                self.memory_drift_long_sleep_get_raw_results(template_type="ising", pre_file_name=pre_file_name,
                                                         post_file_name=post_file_name, part_to_analyze="nrem",
                                                         pop_vec_threshold=2, cells_to_use=cells_to_use)

        # check if only a subset of sleep is supposed to be used
        if not third_of_sleep is None:
            if third_of_sleep == 1:
                # first third of sleep
                sleep_chunk_interval = [0, 1/3*len_sleep]
            elif third_of_sleep == 2:
                # second third of sleep
                sleep_chunk_interval = [1/3*len_sleep, 2/3*len_sleep]
            elif third_of_sleep == 3:
                # last third of sleep
                sleep_chunk_interval = [2/3*len_sleep, len_sleep]

            # only select REM or NREM phases that are within the interval
            pre_prob_mod = []
            event_times_mod = []
            for likeli, times in zip(pre_prob, event_times):
                if sleep_chunk_interval[0] < times[0] and times[1] < sleep_chunk_interval[1]:
                    pre_prob_mod.append(likeli)
                    event_times_mod.append(times)

            pre_prob = pre_prob_mod

        # need to get model to convert array to 2D matrix for locations

        template_file_name = self.long_sleep[0].session_params.default_pre_ising_model

        with open(self.params.pre_proc_dir + 'awake_ising_maps/' + template_file_name + '.pkl', 'rb') as f:
            model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        res_2d = model_dic["occ_map"].shape

        # factor a.u. to cm
        # scaling_fac = self.session_params.data_params_dictionary["spatial_factor"]

        # ising maps are decoded using 5 a.u.
        # spatial_bin_size = 5 * scaling_fac

        # goal locations
        gl = self.long_sleep[0].session_params.goal_locations

        # offset for plotting:
        if self.session_name == "mjc163R4R_0114":
            x_off = 0
            y_off = 20
        elif self.session_name == "mjc163R2R_0114":
            x_off = -5
            y_off = 10
        elif self.session_name == "mjc169R1R_0114":
            x_off = -10
            y_off = 5
        elif self.session_name == "mjc169R4R_0114":
            x_off = 25
            y_off = 15
        elif self.session_name == "mjc163R1L_0114":
            x_off = 38
            y_off = 5
        elif self.session_name == "mjc148R4R_0113":
            x_off = 32
            y_off = 5
        elif self.session_name == "mjc163R3L_0114":
            x_off = 25
            y_off = 20

        if plotting:
            plt.imshow(model_dic["occ_map"].T)
            for g_l in gl:
                plt.scatter((g_l[0] - self.pre.x_min - x_off) / 5,
                            (g_l[1] - self.pre.y_min - y_off) / 5,
                            label="goal locations", marker="x", color="red", s=30, zorder=1000000)
            plt.show()

        goal_locations = []
        for g in gl:
            goal_locations.append(np.array([(g[0] - self.pre.x_min - x_off) / 5,
                                (g[1] - self.pre.y_min - y_off) / 5]))
        goal_locations = np.vstack(goal_locations)

        # record how often each goal was decoded
        goal_decoded = np.zeros(4)
        for event in pre_prob:
            for nr, bin in enumerate(event):
                b = np.reshape(bin,(res_2d[0], res_2d[1]))
                # find maximum
                pos = np.asarray(np.unravel_index(b.argmax(), b.shape))
                # check distance to all goals
                within_radius = np.linalg.norm(goal_locations - pos, axis=1) < max_dist

                goal_decoded[within_radius] += 1

        # for g_l in gl:
        #     plt.scatter((g_l[0] - self.pre.x_min - x_off) / 5,
        #                 (g_l[1] - self.pre.y_min - y_off) / 5,
        #                 label="goal locations", marker="x", color="red", s=30, zorder=1000000)
        # handles, labels = plt.gca().get_legend_handles_labels()
        # by_label = OrderedDict(zip(labels, handles))
        # plt.gca().legend(by_label.values(), by_label.keys())
        # plt.show()

        env_dim_pre = self.pre.get_env_dim()
        pvs_pre, goal_ind_list = self.pre.pvs_around_goal(second_half=True)
        pvs_post, _ = self.post.pvs_around_goal(first_half=True, env_dim=env_dim_pre, goal_ind_list=goal_ind_list)

        all_goals_mean = []
        # go through all the goals
        for goal_pre, goal_post in zip(pvs_pre, pvs_post):
            # go through all pvs
            per_goal_pv = []
            for pv_pre, pv_post in zip(goal_pre, goal_post):
                per_goal_pv.append(pearsonr(pv_pre, pv_post)[0])
            all_goals_mean.append(np.nanmean(np.array(per_goal_pv)))

        goal_decoded_normalized = goal_decoded / goal_decoded.sum()
        if z_score_goal_coding:
            all_goals_mean = zscore(np.array(all_goals_mean))
        if plotting:
            # correlate with how often goal was reactivated
            plt.scatter(goal_decoded, all_goals_mean)
            plt.text(np.min(goal_decoded)+(np.max(goal_decoded)-np.min(goal_decoded))/2,
                     np.min(all_goals_mean)+(np.max(all_goals_mean)-np.min(all_goals_mean))/2,
                     "R="+str(np.round(pearsonr(goal_decoded, all_goals_mean)[0],2)))
            plt.xlabel("#times goal decoded in sleep")
            plt.ylabel("PV correlation around goal (PRE-POST)")
            plt.title(self.session_name)
            plt.show()
        # compare changes in goal coding to #times goal was reactivated
        return goal_decoded_normalized, all_goals_mean

    def goal_reactivations_strength_temporal(self, pre_file_name=None, post_file_name=None, window_size=200,
                          pop_vec_threshold=1, cells_to_use="all", max_dist=5, sleep_phase="rem"):
        """
        goal reactivation analysis


        :return: #times goal was decoded and occupancy in post
        :rtype:
        """
        # get length of sleep in seconds

        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob, _, _ = \
            self.memory_drift_long_sleep_get_raw_results(template_type="ising", pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze=sleep_phase,
                                                     pop_vec_threshold=pop_vec_threshold, cells_to_use=cells_to_use)

        # need to get model to convert array to 2D matrix for locations

        template_file_name = self.long_sleep[0].session_params.default_pre_ising_model

        with open(self.params.pre_proc_dir + 'awake_ising_maps/' + template_file_name + '.pkl', 'rb') as f:
            model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        res_2d = model_dic["occ_map"].shape

        # factor a.u. to cm
        # scaling_fac = self.session_params.data_params_dictionary["spatial_factor"]

        # ising maps are decoded using 5 a.u.
        # spatial_bin_size = 5 * scaling_fac

        # goal locations
        gl = self.long_sleep[0].session_params.goal_locations

        # offset for plotting:
        if self.session_name == "mjc163R4R_0114":
            x_off = 0
            y_off = 20
        elif self.session_name == "mjc163R2R_0114":
            x_off = 0
            y_off = 0
        elif self.session_name == "mjc169R1R_0114":
            x_off = -10
            y_off = 5
        elif self.session_name == "mjc169R4R_0114":
            x_off = 25
            y_off = 15
        elif self.session_name == "mjc163R1L_0114":
            x_off = 38
            y_off = 5
        elif self.session_name == "mjc148R4R_0113":
            x_off = 32
            y_off = 5
        elif self.session_name == "mjc163R3L_0114":
            x_off = 25
            y_off = 20

        # plt.imshow(model_dic["occ_map"].T)
        # for g_l in gl:
        #     plt.scatter((g_l[0] - self.pre.x_min - x_off) / 5,
        #                 (g_l[1] - self.pre.y_min - y_off) / 5,
        #                 label="goal locations", marker="x", color="red", s=30, zorder=1000000)
        # plt.show()
        # print("HERE")

        goal_locations = []
        for g in gl:
            goal_locations.append(np.array([(g[0] - self.pre.x_min - x_off) / 5,
                                (g[1] - self.pre.y_min - y_off) / 5]))
        goal_locations = np.vstack(goal_locations)

        # concatenate events
        pre_prob_arr = np.vstack(pre_prob)

        nr_windows = np.round(pre_prob_arr.shape[0]/window_size).astype(int)

        goal_coding_per_window = np.zeros(nr_windows)

        for window_id in range(nr_windows):
            window_dat = pre_prob_arr[window_id*window_size:(window_id+1)*window_size]
            # set goal reactivations to zero
            nr_goal_reactivations = 0
            # go trough all bins
            for nr, bin in enumerate(window_dat):
                b = np.reshape(bin,(res_2d[0], res_2d[1]))
                # find maximum
                pos = np.asarray(np.unravel_index(b.argmax(), b.shape))
                # check distance to all goals
                within_radius = np.linalg.norm(goal_locations - pos, axis=1) < max_dist
                # if at least one goal was reactivated (there shouldn't be an overlap actually)
                if np.sum(within_radius) > 0:
                    nr_goal_reactivations +=1

            goal_coding_per_window[window_id] = nr_goal_reactivations

        plt.plot(goal_coding_per_window/window_size)
        plt.ylabel("%bins coding for one goal")
        plt.xlabel("Window ID")
        plt.show()

    def nrem_outside_swr_rem_states_spatial_and_goal_coding(self, nr_spikes_per_jitter_window=2000,
                                                            outside_swr_min_bin_size_s=0.5, plotting=False):

        # get spatial information per mode for pre and post
        states_spatial_pre = self.pre.analyze_all_modes_spatial_information()
        states_spatial_post = self.post.analyze_all_modes_spatial_information()
        # get goal coding per mode for pre and post
        states_goal_coding_pre = self.pre.analyze_all_modes_goal_coding()
        states_goal_coding_post = self.post.analyze_all_modes_goal_coding()

        event_spike_rasters_swr = []
        event_spike_rasters_outside_swr = []
        event_spike_rasters_swr_jittered = []
        event_spike_rasters_rem = []
        bin_dur = []

        for l_s in self.long_sleep:
            # get SWR bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_swr_ = (
                l_s.get_spike_binned_raster_sleep_phase(sleep_phase="nrem"))

            _, event_spike_rasters_jittered_ = (
                l_s.get_spike_binned_raster_sleep_phase_jittered(sleep_phase="nrem",
                                                                 nr_spikes_per_jitter_window=nr_spikes_per_jitter_window))

            # get outside SWR bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_nrem_outside_swr_, bin_dur_ = (
                l_s.get_spike_binned_raster_around_swr(return_bin_duration=True))

            # get REM bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_rem_ = (
                l_s.get_spike_binned_raster_sleep_phase(sleep_phase="rem"))

            event_spike_rasters_swr.append(np.hstack(event_spike_rasters_swr_))
            event_spike_rasters_outside_swr.append(np.hstack(event_spike_rasters_nrem_outside_swr_))
            event_spike_rasters_swr_jittered.append(event_spike_rasters_jittered_)
            bin_dur.append(np.hstack(bin_dur_))
            event_spike_rasters_rem.append(np.hstack(event_spike_rasters_rem_))

        bin_dur = np.hstack(bin_dur)
        all_spike_rasters_outside_swr = np.hstack(event_spike_rasters_outside_swr)
        all_spike_rasters_swr = np.hstack(event_spike_rasters_swr)
        all_spike_rasters_swr_jittered = np.hstack(event_spike_rasters_swr_jittered)
        all_spike_rasters_rem = np.hstack(event_spike_rasters_rem)

        # only use long bins
        orig_nr_bins = all_spike_rasters_outside_swr.shape[1]
        all_spike_rasters_outside_swr = all_spike_rasters_outside_swr[:, bin_dur > outside_swr_min_bin_size_s]
        print(str(np.round(all_spike_rasters_outside_swr.shape[1] / (orig_nr_bins * 0.01), 2)) +
              "% of bins remained (threshold: " + str(outside_swr_min_bin_size_s) + "s)")

        # delete events which are too short --> TODO
        # --------------------------------------------------------------------------------------------------------------
        # nr_bins = np.array([x.shape[1] for x in merged_events])

        # get means of pre and post modes + compression factor
        pre_file_name = self.session_params.default_pre_phmm_model
        post_file_name = self.session_params.default_post_phmm_model
        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        # do decoding and plot result
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                  event_spike_rasters=all_spike_rasters_swr,
                                                  compression_factor=compression_factor)

        post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                   event_spike_rasters=all_spike_rasters_swr,
                                                   compression_factor=compression_factor)

        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        sim_ratio = (max_post_likeli - max_pre_likeli) / (max_post_likeli + max_pre_likeli)

        # do decoding of jittere swr_data
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_jittered = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                           event_spike_rasters=all_spike_rasters_swr_jittered,
                                                           compression_factor=compression_factor)

        post_likeli_jittered = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                            event_spike_rasters=all_spike_rasters_swr_jittered,
                                                            compression_factor=compression_factor)

        pre_likeli_jittered = np.vstack(pre_likeli_jittered)
        post_likeli_jittered = np.vstack(post_likeli_jittered)

        max_pre_likeli_jittered = np.max(pre_likeli_jittered, axis=1)
        max_post_likeli_jittered = np.max(post_likeli_jittered, axis=1)

        # do decoding for outside swr
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_out = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                      event_spike_rasters=all_spike_rasters_outside_swr,
                                                      compression_factor=compression_factor)

        post_likeli_out = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                       event_spike_rasters=all_spike_rasters_outside_swr,
                                                       compression_factor=compression_factor)

        pre_likeli_out = np.vstack(pre_likeli_out)
        post_likeli_out = np.vstack(post_likeli_out)

        max_pre_likeli_out = np.max(pre_likeli_out, axis=1)
        max_post_likeli_out = np.max(post_likeli_out, axis=1)

        # do decoding for rem
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_rem = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                      event_spike_rasters=all_spike_rasters_rem,
                                                      compression_factor=compression_factor)

        post_likeli_rem = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                       event_spike_rasters=all_spike_rasters_rem,
                                                       compression_factor=compression_factor)

        pre_likeli_rem = np.vstack(pre_likeli_rem)
        post_likeli_rem = np.vstack(post_likeli_rem)

        max_pre_likeli_rem = np.max(pre_likeli_rem, axis=1)
        max_post_likeli_rem = np.max(post_likeli_rem, axis=1)

        # check when pre/post was decoded
        pre_decoded = max_pre_likeli > max_post_likeli
        pre_decoded_out = max_pre_likeli_out > max_post_likeli_out
        pre_decoded_rem = max_pre_likeli_rem > max_post_likeli_rem

        # select model when it was decoded: original (nrem swr)
        pre_likeli_when_decoded = pre_likeli[pre_decoded, :]
        pre_likeli_when_decoded_states = np.argmax(pre_likeli_when_decoded, axis=1)
        post_likeli_when_decoded = post_likeli[~pre_decoded, :]
        post_likeli_when_decoded_states = np.argmax(post_likeli_when_decoded, axis=1)

        # select model when it was decoded: outside swr
        pre_likeli_out_when_decoded = pre_likeli_out[pre_decoded_out, :]
        pre_likeli_out_when_decoded_states = np.argmax(pre_likeli_out_when_decoded, axis=1)
        post_likeli_out_when_decoded = post_likeli_out[~pre_decoded_out, :]
        post_likeli_out_when_decoded_states = np.argmax(post_likeli_out_when_decoded, axis=1)

        # select model when it was decoded: rem
        pre_likeli_rem_when_decoded = pre_likeli_rem[pre_decoded_rem, :]
        pre_likeli_rem_when_decoded_states = np.argmax(pre_likeli_rem_when_decoded, axis=1)
        post_likeli_rem_when_decoded = post_likeli_rem[~pre_decoded_rem, :]
        post_likeli_rem_when_decoded_states = np.argmax(post_likeli_rem_when_decoded, axis=1)

        # compute how often a state was decoded --> use fraction to weigh the corresponding spatial coding/goal coding
        pre_states_decoded, pre_states_decoded_counts = np.unique(pre_likeli_when_decoded_states, return_counts=True)
        pre_states_decoded_fraction = pre_states_decoded_counts / pre_likeli_when_decoded_states.shape[0]
        post_states_decoded, post_states_decoded_counts = np.unique(post_likeli_when_decoded_states, return_counts=True)
        post_states_decoded_fraction = post_states_decoded_counts / post_likeli_when_decoded_states.shape[0]
        # outside swr
        pre_states_decoded_out, pre_states_decoded_out_counts = np.unique(pre_likeli_out_when_decoded_states,
                                                                          return_counts=True)
        pre_states_decoded_out_fraction = pre_states_decoded_out_counts / pre_likeli_out_when_decoded_states.shape[0]
        post_states_decoded_out, post_states_decoded_out_counts = np.unique(post_likeli_out_when_decoded_states, return_counts=True)
        post_states_decoded_out_fraction = post_states_decoded_out_counts / post_likeli_out_when_decoded_states.shape[0]
        # rem
        pre_states_decoded_rem, pre_states_decoded_rem_counts = np.unique(pre_likeli_rem_when_decoded_states,
                                                                          return_counts=True)
        pre_states_decoded_rem_fraction = pre_states_decoded_rem_counts / pre_likeli_rem_when_decoded_states.shape[0]
        post_states_decoded_rem, post_states_decoded_rem_counts = np.unique(post_likeli_rem_when_decoded_states, return_counts=True)
        post_states_decoded_rem_fraction = post_states_decoded_rem_counts / post_likeli_rem_when_decoded_states.shape[0]

        # maybe some states were not decoded: pre & post, original
        pre_states_decoded_fraction_temp = np.zeros(states_spatial_pre.shape[0])
        pre_states_decoded_fraction_temp[pre_states_decoded] = pre_states_decoded_fraction
        post_states_decoded_fraction_temp = np.zeros(states_spatial_post.shape[0])
        post_states_decoded_fraction_temp[post_states_decoded] = post_states_decoded_fraction
        # maybe some states were not decoded: pre & post, outside swr
        pre_states_decoded_out_fraction_temp = np.zeros(states_spatial_pre.shape[0])
        pre_states_decoded_out_fraction_temp[pre_states_decoded_out] = pre_states_decoded_out_fraction
        post_states_decoded_out_fraction_temp = np.zeros(states_spatial_post.shape[0])
        post_states_decoded_out_fraction_temp[post_states_decoded_out] = post_states_decoded_out_fraction
        # maybe some states was not decoded: pre
        pre_states_decoded_rem_fraction_temp = np.zeros(states_spatial_pre.shape[0])
        pre_states_decoded_rem_fraction_temp[pre_states_decoded_rem] = pre_states_decoded_rem_fraction
        post_states_decoded_rem_fraction_temp = np.zeros(states_spatial_post.shape[0])
        post_states_decoded_rem_fraction_temp[post_states_decoded_rem] = post_states_decoded_rem_fraction

        # now weigh each state's spatial coding with the fraction it was decoded
        # original (nrem swr)
        pre_states_decoded_median_distance_weighted = pre_states_decoded_fraction_temp*states_spatial_pre
        pre_states_decoded_median_distance_weighted_sum = np.sum(pre_states_decoded_median_distance_weighted)
        post_states_decoded_median_distance_weighted = post_states_decoded_fraction_temp*states_spatial_post
        post_states_decoded_median_distance_weighted_sum = np.sum(post_states_decoded_median_distance_weighted)
        # outside swr
        pre_states_decoded_out_median_distance_weighted = pre_states_decoded_out_fraction_temp*states_spatial_pre
        pre_states_decoded_out_median_distance_weighted_sum = np.sum(pre_states_decoded_out_median_distance_weighted)
        post_states_decoded_out_median_distance_weighted = post_states_decoded_out_fraction_temp*states_spatial_post
        post_states_decoded_out_median_distance_weighted_sum = np.sum(post_states_decoded_out_median_distance_weighted)
        # rem
        pre_states_decoded_rem_median_distance_weighted = pre_states_decoded_rem_fraction_temp*states_spatial_pre
        pre_states_decoded_rem_median_distance_weighted_sum = np.sum(pre_states_decoded_rem_median_distance_weighted)
        post_states_decoded_rem_median_distance_weighted = post_states_decoded_rem_fraction_temp*states_spatial_post
        post_states_decoded_rem_median_distance_weighted_sum = np.sum(post_states_decoded_rem_median_distance_weighted)

        # now weigh each state's goal coding with the fraction it was decoded
        # original (nrem swr)
        pre_states_decoded_goal_coding_weighted = pre_states_decoded_fraction_temp*states_goal_coding_pre
        pre_states_decoded_goal_coding_weighted_sum = np.sum(pre_states_decoded_goal_coding_weighted)
        post_states_decoded_goal_coding_weighted = post_states_decoded_fraction_temp*states_goal_coding_post
        post_states_decoded_goal_coding_weighted_sum = np.sum(post_states_decoded_goal_coding_weighted)
        # outside swr
        pre_states_decoded_out_goal_coding_weighted = pre_states_decoded_out_fraction_temp*states_goal_coding_pre
        pre_states_decoded_out_goal_coding_weighted_sum = np.sum(pre_states_decoded_out_goal_coding_weighted)
        post_states_decoded_out_goal_coding_weighted = post_states_decoded_out_fraction_temp*states_goal_coding_post
        post_states_decoded_out_goal_coding_weighted_sum = np.sum(post_states_decoded_out_goal_coding_weighted)
        # rem
        pre_states_decoded_rem_goal_coding_weighted = pre_states_decoded_rem_fraction_temp * states_goal_coding_pre
        pre_states_decoded_rem_goal_coding_weighted_sum = np.sum(pre_states_decoded_rem_goal_coding_weighted)
        post_states_decoded_rem_goal_coding_weighted = post_states_decoded_rem_fraction_temp * states_goal_coding_post
        post_states_decoded_rem_goal_coding_weighted_sum = np.sum(
        post_states_decoded_rem_goal_coding_weighted)

        if plotting:
            # plotting spatial information
            plt.figure(figsize=(6,6))
            c = "white"
            res = [pre_states_decoded_median_distance_weighted, pre_states_decoded_out_median_distance_weighted,
                   pre_states_decoded_rem_median_distance_weighted]
            bplot = plt.boxplot(res, positions=[1, 2, 3], patch_artist=True,
                                labels=["NREM SWRs", "NREM outside SWRs", "REM"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            colors = ["blue", 'grey', "grey"]
            plt.xticks(rotation=45)
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            plt.ylabel("Weighted median \n distance of activations")
            plt.grid(color="grey", axis="y")
            plt.title("Using Acquisition model")
            plt.tight_layout()
            plt.show()

            plt.figure(figsize=(6,6))
            c = "white"
            res = [post_states_decoded_median_distance_weighted, post_states_decoded_out_median_distance_weighted,
                   post_states_decoded_rem_median_distance_weighted]
            bplot = plt.boxplot(res, positions=[1, 2, 3], patch_artist=True,
                                labels=["NREM SWRs", "NREM outside SWRs", "REM"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            colors = ["blue", 'grey', "grey"]
            plt.xticks(rotation=45)
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            plt.ylabel("Weighted median \n distance of activations")
            plt.grid(color="grey", axis="y")
            plt.title("Using Recall model")
            plt.tight_layout()
            plt.show()
            # plotting goal coding
            # ----------------------------------------------------------------------------------------------------------
            plt.figure(figsize=(6,6))
            c = "white"
            res = [pre_states_decoded_goal_coding_weighted, pre_states_decoded_out_goal_coding_weighted,
                   pre_states_decoded_rem_goal_coding_weighted]
            bplot = plt.boxplot(res, positions=[1, 2, 3], patch_artist=True,
                                labels=["NREM SWRs", "NREM outside SWRs", "REM"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            colors = ["blue", 'grey', "grey"]
            plt.xticks(rotation=45)
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            plt.ylabel("Weighted goal coding")
            plt.grid(color="grey", axis="y")
            plt.title("Using Acquisition model")
            plt.tight_layout()
            plt.show()

            plt.figure(figsize=(6,6))
            c = "white"
            res = [post_states_decoded_goal_coding_weighted, post_states_decoded_out_goal_coding_weighted,
                   post_states_decoded_rem_goal_coding_weighted]
            bplot = plt.boxplot(res, positions=[1, 2, 3], patch_artist=True,
                                labels=["NREM SWRs", "NREM outside SWRs", "REM"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            colors = ["blue", 'grey', "grey"]
            plt.xticks(rotation=45)
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            plt.ylabel("Weighted goal coding")
            plt.grid(color="grey", axis="y")
            plt.title("Using Recall model")
            plt.tight_layout()
            plt.show()

        return [pre_states_decoded_median_distance_weighted, post_states_decoded_median_distance_weighted,
                pre_states_decoded_out_median_distance_weighted, post_states_decoded_out_median_distance_weighted,
                pre_states_decoded_rem_median_distance_weighted, post_states_decoded_rem_median_distance_weighted,
                pre_states_decoded_goal_coding_weighted,post_states_decoded_goal_coding_weighted,
                pre_states_decoded_out_goal_coding_weighted, post_states_decoded_out_goal_coding_weighted,
                pre_states_decoded_rem_goal_coding_weighted, post_states_decoded_rem_goal_coding_weighted]

    def nrem_outside_swr_rem_ising_goal_coding(self, outside_swr_min_bin_size_s=0.5, plotting=False,
                                               plot_for_control=False, bins_around_goal = 3, only_decoded=False):

        event_spike_rasters_swr = []
        event_spike_rasters_outside_swr = []
        event_spike_rasters_rem = []
        bin_dur = []

        for l_s in self.long_sleep:
            # get SWR bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_swr_ = (
                l_s.get_spike_binned_raster_sleep_phase(sleep_phase="nrem"))

            # get outside SWR bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_nrem_outside_swr_, bin_dur_ = (
                l_s.get_spike_binned_raster_around_swr(return_bin_duration=True))

            # get REM bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_rem_ = (
                l_s.get_spike_binned_raster_sleep_phase(sleep_phase="rem"))

            event_spike_rasters_swr.append(np.hstack(event_spike_rasters_swr_))
            event_spike_rasters_outside_swr.append(np.hstack(event_spike_rasters_nrem_outside_swr_))
            bin_dur.append(np.hstack(bin_dur_))
            event_spike_rasters_rem.append(np.hstack(event_spike_rasters_rem_))

        bin_dur = np.hstack(bin_dur)
        all_spike_rasters_outside_swr = np.hstack(event_spike_rasters_outside_swr)
        all_spike_rasters_swr = np.hstack(event_spike_rasters_swr)
        all_spike_rasters_rem = np.hstack(event_spike_rasters_rem)

        # only use long bins
        orig_nr_bins = all_spike_rasters_outside_swr.shape[1]
        all_spike_rasters_outside_swr = all_spike_rasters_outside_swr[:, bin_dur > outside_swr_min_bin_size_s]
        print(str(np.round(all_spike_rasters_outside_swr.shape[1] / (orig_nr_bins * 0.01), 2)) +
              "% of bins remained (threshold: " + str(outside_swr_min_bin_size_s) + "s)")

        # delete events which are too short --> TODO
        # --------------------------------------------------------------------------------------------------------------
        # nr_bins = np.array([x.shape[1] for x in merged_events])

        # get means of pre and post modes + compression factor
        pre_file_name = self.session_params.default_pre_ising_model
        post_file_name = self.session_params.default_post_ising_model

        # get pre and post model to do decoding
        with open(self.params.pre_proc_dir + "awake_ising_maps/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_template_map = pre_model_dic["res_map"]

        with open(self.params.pre_proc_dir + "awake_ising_maps/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_template_map = post_model_dic["res_map"]

        # get time_bin_size of encoding
        time_bin_size_encoding = pre_model_dic["time_bin_size"]

        # load correct compression factor (as defined in parameter file of the session)
        if time_bin_size_encoding == 0.01:
            compression_factor = \
                np.round(self.session_params.sleep_compression_factor_12spikes_100ms * 10, 3)
        elif time_bin_size_encoding == 0.1:
            compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

        # since there is a mismatch between the decoded map (because Ising maps were generatore introducing
        # accurate environment dimensions --> need to modify

        # ising maps are generated using 5 a.u. bins * 0.45 (spatial conversion factor)
        occ_map_at_resolution_pre = self.pre.get_occ_map(spatial_resolution=5*0.45)
        occ_map_at_resolution_post = self.post.get_occ_map(spatial_resolution=5*0.45)

        # goal locations at correct resolution
        gl_at_resolution_pre = np.vstack([[x[0]- self.pre.x_min, x[1]- self.pre.y_min] for x in self.pre.goal_locations])/(5*0.45)
        gl_at_resolution_post = np.vstack([[x[0]- self.post.x_min, x[1]- self.post.y_min] for x in self.post.goal_locations])/(5*0.45)

        # do decoding and plot result
        # --------------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli = decode_using_ising_map_fast(template_map=pre_template_map,
                                                  event_spike_rasters=all_spike_rasters_swr,
                                                  compression_factor=compression_factor)

        post_likeli = decode_using_ising_map_fast(template_map=post_template_map,
                                                   event_spike_rasters=all_spike_rasters_swr,
                                                   compression_factor=compression_factor)

        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)

        if only_decoded:
            max_pre_likeli = np.max(pre_likeli, axis=1)
            max_post_likeli = np.max(post_likeli, axis=1)

            pre_decoded = max_pre_likeli > max_post_likeli
            pre_likeli = pre_likeli[pre_decoded, :]
            post_likeli = post_likeli[~pre_decoded, :]

        # first do PRE
        # --------------------------------------------------------------------------------------------------------------
        # decoding function only returns results for non-empty bins --> need to have all bins to do the reshaping
        # reshape template (do not need 2D information --> just want to go through all spatial bins)
        template_map_pre = pre_template_map.reshape(-1, (pre_template_map.shape[1] * pre_template_map.shape[2]))

        # only select spatial bins where cells were firing during the model fit
        good_template_bins_pre = np.sum(template_map_pre, axis=0) > 0

        bins_decoded_pre = np.zeros((pre_likeli.shape[0], template_map_pre.shape[1]))
        bins_decoded_pre[:, good_template_bins_pre] = pre_likeli

        pre_spatial_bins_decoded = np.argmax(bins_decoded_pre, axis=1)

        bin_id, bin_count = np.unique(pre_spatial_bins_decoded, return_counts=True)
        pre_freq_decoded = np.zeros(bins_decoded_pre.shape[1])
        pre_freq_decoded[bin_id] = bin_count
        pre_freq_decoded_norm = pre_freq_decoded / np.sum(pre_freq_decoded)

        pre_freq_decoded_norm[~good_template_bins_pre] =np.nan
        pre_freq_spatial = np.reshape(pre_freq_decoded_norm,
                                                (pre_model_dic["res_map"].shape[1], pre_model_dic["res_map"].shape[2]))
        # cut decoded map --> ising models were generated not excluding points outside the environment dimensions
        pre_freq_spatial = pre_freq_spatial[-occ_map_at_resolution_pre.shape[0]:, :]

        if plot_for_control:
            # --> goal locations are in cm
            plt.subplot(2,1,1)
            plt.imshow(occ_map_at_resolution_pre.T)

            for g_l in gl_at_resolution_pre:
                plt.scatter(g_l[0], g_l[1])
            plt.colorbar()
            plt.subplot(2,1,2)
            plt.imshow(pre_freq_spatial.T)
            for g_l in gl_at_resolution_pre:
                plt.scatter(g_l[0], g_l[1])
            plt.colorbar()
            plt.show()

            pre_freq_spatial_temp = np.copy(pre_freq_spatial)
            for g_l in gl_at_resolution_pre:
                if (int(np.round(g_l[0]-bins_around_goal)) < 0) or (int(np.round(g_l[1]-bins_around_goal) < 0)):
                    if (int(np.round(g_l[0]-bins_around_goal)) < 0):
                        pre_freq_spatial_temp[0:int(np.round(g_l[0]+bins_around_goal)),
                        int(np.round(g_l[1]-bins_around_goal)):int(np.round(g_l[1]+bins_around_goal))]=10
                    elif (int(np.round(g_l[1]-bins_around_goal) < 0)):
                        pre_freq_spatial_temp[int(np.round(g_l[0]-bins_around_goal)):int(np.round(g_l[0]+bins_around_goal)),
                        0:int(np.round(g_l[1]+bins_around_goal))]=10
                else:
                    pre_freq_spatial_temp[int(np.round(g_l[0]-bins_around_goal)):int(np.round(g_l[0]+bins_around_goal)),
                    int(np.round(g_l[1]-bins_around_goal)):int(np.round(g_l[1]+bins_around_goal))]=10
            for g_l in gl_at_resolution_pre:
                plt.scatter(g_l[0], g_l[1])
            plt.imshow(pre_freq_spatial_temp.T)
            plt.show()
        # select bins around the goals
        pre_freq_spatial_around_goals_swr = 0

        for g_l in gl_at_resolution_pre:
            if (int(np.round(g_l[0]-bins_around_goal)) < 0) or (int(np.round(g_l[1]-bins_around_goal) < 0)):
                if (int(np.round(g_l[0]-bins_around_goal)) < 0):
                    pre_freq_spatial_around_goals_swr += \
                        np.nansum(pre_freq_spatial[0:int(np.round(g_l[0]+bins_around_goal)),
                                  int(np.round(g_l[1]-bins_around_goal)):int(np.round(g_l[1]+bins_around_goal))])
                elif (int(np.round(g_l[1]-bins_around_goal) < 0)):
                    pre_freq_spatial_around_goals_swr += \
                        np.nansum(pre_freq_spatial[int(np.round(g_l[0]-bins_around_goal)):int(np.round(g_l[0]+bins_around_goal)),
                                  0:int(np.round(g_l[1]+bins_around_goal))])
            else:
                pre_freq_spatial_around_goals_swr += \
                    np.nansum(pre_freq_spatial[int(np.round(g_l[0]-bins_around_goal)):int(np.round(g_l[0]+bins_around_goal)),
                              int(np.round(g_l[1]-bins_around_goal)):int(np.round(g_l[1]+bins_around_goal))])

        # do POST
        # --------------------------------------------------------------------------------------------------------------
        # decoding function only returns results for non-empty bins --> need to have all bins to do the reshaping
        # reshape template (do not need 2D information --> just want to go through all spatial bins)
        template_map_post = post_template_map.reshape(-1, (post_template_map.shape[1] * post_template_map.shape[2]))

        # only select spatial bins where cells were firing during the model fit
        good_template_bins_post = np.sum(template_map_post, axis=0) > 0

        bins_decoded_post = np.zeros((post_likeli.shape[0], template_map_post.shape[1]))
        bins_decoded_post[:, good_template_bins_post] = post_likeli

        post_spatial_bins_decoded = np.argmax(bins_decoded_post, axis=1)

        bin_id, bin_count = np.unique(post_spatial_bins_decoded, return_counts=True)
        post_freq_decoded = np.zeros(bins_decoded_post.shape[1])
        post_freq_decoded[bin_id] = bin_count
        post_freq_decoded_norm = post_freq_decoded / np.sum(post_freq_decoded)

        post_freq_decoded_norm[~good_template_bins_post] =np.nan
        post_freq_spatial = np.reshape(post_freq_decoded_norm,
                                      (post_model_dic["res_map"].shape[1], post_model_dic["res_map"].shape[2]))
        # cut decoded map --> ising models were generated not excluding points outside the environment dimensions
        post_freq_spatial = post_freq_spatial[-occ_map_at_resolution_post.shape[0]:, :]

        if plot_for_control:
            # --> goal locations are in cm
            plt.subplot(2,1,1)
            plt.imshow(occ_map_at_resolution_post.T)

            for g_l in gl_at_resolution_post:
                plt.scatter(g_l[0], g_l[1])
            plt.colorbar()
            plt.subplot(2,1,2)
            plt.imshow(post_freq_spatial.T)
            for g_l in gl_at_resolution_post:
                plt.scatter(g_l[0], g_l[1])
            plt.colorbar()
            plt.show()

            post_freq_spatial_temp = np.copy(post_freq_spatial)
            for g_l in gl_at_resolution_post:
                if (int(np.round(g_l[0]-bins_around_goal)) < 0) or (int(np.round(g_l[1]-bins_around_goal) < 0)):
                    if (int(np.round(g_l[0]-bins_around_goal)) < 0):
                        post_freq_spatial_temp[0:int(np.round(g_l[0]+bins_around_goal)),
                        int(np.round(g_l[1]-bins_around_goal)):int(np.round(g_l[1]+bins_around_goal))]=10
                    elif (int(np.round(g_l[1]-bins_around_goal) < 0)):
                        post_freq_spatial_temp[int(np.round(g_l[0]-bins_around_goal)):int(np.round(g_l[0]+bins_around_goal)),
                              0:int(np.round(g_l[1]+bins_around_goal))]=10
                else:
                    post_freq_spatial_temp[int(np.round(g_l[0]-bins_around_goal)):int(np.round(g_l[0]+bins_around_goal)),
                    int(np.round(g_l[1]-bins_around_goal)):int(np.round(g_l[1]+bins_around_goal))]=10
            for g_l in gl_at_resolution_post:
                plt.scatter(g_l[0], g_l[1])
            plt.imshow(post_freq_spatial_temp.T)
            plt.show()
        # select bins around the goals
        post_freq_spatial_around_goals_swr = 0

        for g_l in gl_at_resolution_post:
            if (int(np.round(g_l[0]-bins_around_goal)) < 0) or (int(np.round(g_l[1]-bins_around_goal) < 0)):
                if (int(np.round(g_l[0]-bins_around_goal)) < 0):
                    post_freq_spatial_around_goals_swr += \
                        np.nansum(post_freq_spatial[0:int(np.round(g_l[0]+bins_around_goal)),
                                  int(np.round(g_l[1]-bins_around_goal)):int(np.round(g_l[1]+bins_around_goal))])
                elif (int(np.round(g_l[1]-bins_around_goal) < 0)):
                    post_freq_spatial_around_goals_swr += \
                        np.nansum(post_freq_spatial[int(np.round(g_l[0]-bins_around_goal)):int(np.round(g_l[0]+bins_around_goal)),
                                  0:int(np.round(g_l[1]+bins_around_goal))])
            else:
                post_freq_spatial_around_goals_swr += \
                    np.nansum(post_freq_spatial[int(np.round(g_l[0]-bins_around_goal)):int(np.round(g_l[0]+bins_around_goal)),
                              int(np.round(g_l[1]-bins_around_goal)):int(np.round(g_l[1]+bins_around_goal))])

        # do decoding for outside swr
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_out = decode_using_ising_map_fast(template_map=pre_template_map,
                                                      event_spike_rasters=all_spike_rasters_outside_swr,
                                                      compression_factor=compression_factor)

        post_likeli_out = decode_using_ising_map_fast(template_map=post_template_map,
                                                       event_spike_rasters=all_spike_rasters_outside_swr,
                                                       compression_factor=compression_factor)

        pre_likeli_out = np.vstack(pre_likeli_out)
        post_likeli_out = np.vstack(post_likeli_out)

        if only_decoded:
            max_pre_likeli_out = np.max(pre_likeli_out, axis=1)
            max_post_likeli_out = np.max(post_likeli_out, axis=1)

            pre_decoded_out = max_pre_likeli_out > max_post_likeli_out
            pre_likeli_out = pre_likeli_out[pre_decoded_out, :]
            post_likeli_out = post_likeli_out[~pre_decoded_out, :]

        # first do PRE for outside SWRs
        # --------------------------------------------------------------------------------------------------------------
        bins_decoded_pre_out = np.zeros((pre_likeli_out.shape[0], template_map_pre.shape[1]))
        bins_decoded_pre_out[:, good_template_bins_pre] = pre_likeli_out

        pre_spatial_bins_decoded_out = np.argmax(bins_decoded_pre_out, axis=1)

        bin_id_out, bin_count_out = np.unique(pre_spatial_bins_decoded_out, return_counts=True)
        pre_freq_decoded_out = np.zeros(bins_decoded_pre_out.shape[1])
        pre_freq_decoded_out[bin_id_out] = bin_count_out
        pre_freq_decoded_norm_out = pre_freq_decoded_out / np.sum(pre_freq_decoded_out)

        pre_freq_decoded_norm_out[~good_template_bins_pre] =np.nan
        pre_freq_spatial_out = np.reshape(pre_freq_decoded_norm_out,
                                      (pre_model_dic["res_map"].shape[1], pre_model_dic["res_map"].shape[2]))
        # cut decoded map --> ising models were generated not excluding points outside the environment dimensions
        pre_freq_spatial_out = pre_freq_spatial_out[-occ_map_at_resolution_pre.shape[0]:, :]

        pre_freq_spatial_around_goals_out = 0

        for g_l in gl_at_resolution_pre:
            if (int(np.round(g_l[0]-bins_around_goal)) < 0) or (int(np.round(g_l[1]-bins_around_goal) < 0)):
                if (int(np.round(g_l[0]-bins_around_goal)) < 0):
                    pre_freq_spatial_around_goals_out += \
                        np.nansum(pre_freq_spatial_out[0:int(np.round(g_l[0]+bins_around_goal)),
                                  int(np.round(g_l[1]-bins_around_goal)):int(np.round(g_l[1]+bins_around_goal))])
                elif (int(np.round(g_l[1]-bins_around_goal) < 0)):
                    pre_freq_spatial_around_goals_out += \
                        np.nansum(pre_freq_spatial_out[int(np.round(g_l[0]-bins_around_goal)):int(np.round(g_l[0]+bins_around_goal)),
                                  0:int(np.round(g_l[1]+bins_around_goal))])
            else:
                pre_freq_spatial_around_goals_out += \
                    np.nansum(pre_freq_spatial_out[int(np.round(g_l[0]-bins_around_goal)):int(np.round(g_l[0]+bins_around_goal)),
                              int(np.round(g_l[1]-bins_around_goal)):int(np.round(g_l[1]+bins_around_goal))])
        # do POST
        # --------------------------------------------------------------------------------------------------------------

        bins_decoded_post_out = np.zeros((post_likeli_out.shape[0], template_map_post.shape[1]))
        bins_decoded_post_out[:, good_template_bins_post] = post_likeli_out

        post_spatial_bins_decoded_out = np.argmax(bins_decoded_post_out, axis=1)
        # need to adapt code below
        bin_id_out, bin_count_out = np.unique(post_spatial_bins_decoded_out, return_counts=True)
        post_freq_decoded_out = np.zeros(bins_decoded_post_out.shape[1])
        post_freq_decoded_out[bin_id_out] = bin_count_out
        post_freq_decoded_norm_out = post_freq_decoded_out / np.sum(post_freq_decoded_out)

        post_freq_decoded_norm_out[~good_template_bins_post] =np.nan
        post_freq_spatial_out = np.reshape(post_freq_decoded_norm_out,
                                       (post_model_dic["res_map"].shape[1], post_model_dic["res_map"].shape[2]))
        # cut decoded map --> ising models were generated not excluding points outside the environment dimensions
        post_freq_spatial_out = post_freq_spatial_out[-occ_map_at_resolution_post.shape[0]:, :]

        # select bins around the goals
        post_freq_spatial_around_goals_out = 0

        for g_l in gl_at_resolution_post:
            if (int(np.round(g_l[0]-bins_around_goal)) < 0) or (int(np.round(g_l[1]-bins_around_goal) < 0)):
                if (int(np.round(g_l[0]-bins_around_goal)) < 0):
                    post_freq_spatial_around_goals_out += \
                        np.nansum(post_freq_spatial_out[0:int(np.round(g_l[0]+bins_around_goal)),
                                  int(np.round(g_l[1]-bins_around_goal)):int(np.round(g_l[1]+bins_around_goal))])
                elif (int(np.round(g_l[1]-bins_around_goal) < 0)):
                    post_freq_spatial_around_goals_out += \
                        np.nansum(post_freq_spatial_out[int(np.round(g_l[0]-bins_around_goal)):int(np.round(g_l[0]+bins_around_goal)),
                                  0:int(np.round(g_l[1]+bins_around_goal))])
            else:
                post_freq_spatial_around_goals_out += \
                    np.nansum(post_freq_spatial_out[int(np.round(g_l[0]-bins_around_goal)):int(np.round(g_l[0]+bins_around_goal)),
                              int(np.round(g_l[1]-bins_around_goal)):int(np.round(g_l[1]+bins_around_goal))])

        # do decoding for rem
        # ----------------------------------------------------------------------------------------------------------
        # do the decoding using pre and post model
        pre_likeli_rem = decode_using_ising_map_fast(template_map=pre_template_map,
                                                      event_spike_rasters=all_spike_rasters_rem,
                                                      compression_factor=compression_factor)

        post_likeli_rem = decode_using_ising_map_fast(template_map=post_template_map,
                                                       event_spike_rasters=all_spike_rasters_rem,
                                                       compression_factor=compression_factor)

        pre_likeli_rem = np.vstack(pre_likeli_rem)
        post_likeli_rem = np.vstack(post_likeli_rem)

        if only_decoded:
            max_pre_likeli_rem = np.max(pre_likeli_rem, axis=1)
            max_post_likeli_rem = np.max(post_likeli_rem, axis=1)

            pre_decoded_rem = max_pre_likeli_rem > max_post_likeli_rem
            pre_likeli_rem = pre_likeli_rem[pre_decoded_rem, :]
            post_likeli_rem = post_likeli_rem[~pre_decoded_rem, :]

        # first do PRE for rem
        # --------------------------------------------------------------------------------------------------------------
        bins_decoded_pre_rem = np.zeros((pre_likeli_rem.shape[0], template_map_pre.shape[1]))
        bins_decoded_pre_rem[:, good_template_bins_pre] = pre_likeli_rem

        pre_spatial_bins_decoded_rem = np.argmax(bins_decoded_pre_rem, axis=1)

        bin_id_rem, bin_count_rem = np.unique(pre_spatial_bins_decoded_rem, return_counts=True)
        pre_freq_decoded_rem = np.zeros(bins_decoded_pre_rem.shape[1])
        pre_freq_decoded_rem[bin_id_rem] = bin_count_rem
        pre_freq_decoded_norm_rem = pre_freq_decoded_rem / np.sum(pre_freq_decoded_rem)

        pre_freq_decoded_norm_rem[~good_template_bins_pre] =np.nan
        pre_freq_spatial_rem = np.reshape(pre_freq_decoded_norm_rem,
                                          (pre_model_dic["res_map"].shape[1], pre_model_dic["res_map"].shape[2]))
        # cut decoded map --> ising models were generated not excluding points outside the environment dimensions
        pre_freq_spatial_rem = pre_freq_spatial_rem[-occ_map_at_resolution_pre.shape[0]:, :]

        pre_freq_spatial_around_goals_rem = 0

        for g_l in gl_at_resolution_pre:
            if (int(np.round(g_l[0]-bins_around_goal)) < 0) or (int(np.round(g_l[1]-bins_around_goal) < 0)):
                if (int(np.round(g_l[0]-bins_around_goal)) < 0):
                    pre_freq_spatial_around_goals_rem += \
                        np.nansum(pre_freq_spatial_rem[0:int(np.round(g_l[0]+bins_around_goal)),
                                  int(np.round(g_l[1]-bins_around_goal)):int(np.round(g_l[1]+bins_around_goal))])
                elif (int(np.round(g_l[1]-bins_around_goal) < 0)):
                    pre_freq_spatial_around_goals_rem += \
                        np.nansum(pre_freq_spatial_rem[int(np.round(g_l[0]-bins_around_goal)):int(np.round(g_l[0]+bins_around_goal)),
                                  0:int(np.round(g_l[1]+bins_around_goal))])
            else:
                pre_freq_spatial_around_goals_rem += \
                    np.nansum(pre_freq_spatial_rem[int(np.round(g_l[0]-bins_around_goal)):int(np.round(g_l[0]+bins_around_goal)),
                              int(np.round(g_l[1]-bins_around_goal)):int(np.round(g_l[1]+bins_around_goal))])
        # do POST
        # --------------------------------------------------------------------------------------------------------------

        bins_decoded_post_rem = np.zeros((post_likeli_rem.shape[0], template_map_post.shape[1]))
        bins_decoded_post_rem[:, good_template_bins_post] = post_likeli_rem

        post_spatial_bins_decoded_rem = np.argmax(bins_decoded_post_rem, axis=1)
        # need to adapt code below
        bin_id_rem, bin_count_rem = np.unique(post_spatial_bins_decoded_rem, return_counts=True)
        post_freq_decoded_rem = np.zeros(bins_decoded_post_rem.shape[1])
        post_freq_decoded_rem[bin_id_rem] = bin_count_rem
        post_freq_decoded_norm_rem = post_freq_decoded_rem / np.sum(post_freq_decoded_rem)

        post_freq_decoded_norm_rem[~good_template_bins_post] =np.nan
        post_freq_spatial_rem = np.reshape(post_freq_decoded_norm_rem,
                                           (post_model_dic["res_map"].shape[1], post_model_dic["res_map"].shape[2]))
        # cut decoded map --> ising models were generated not excluding points outside the environment dimensions
        post_freq_spatial_rem = post_freq_spatial_rem[-occ_map_at_resolution_post.shape[0]:, :]

        # select bins around the goals
        post_freq_spatial_around_goals_rem = 0

        for g_l in gl_at_resolution_post:
            if (int(np.round(g_l[0]-bins_around_goal)) < 0) or (int(np.round(g_l[1]-bins_around_goal) < 0)):
                if (int(np.round(g_l[0]-bins_around_goal)) < 0):
                    post_freq_spatial_around_goals_rem += \
                        np.nansum(post_freq_spatial_rem[0:int(np.round(g_l[0]+bins_around_goal)),
                                  int(np.round(g_l[1]-bins_around_goal)):int(np.round(g_l[1]+bins_around_goal))])
                elif (int(np.round(g_l[1]-bins_around_goal) < 0)):
                    post_freq_spatial_around_goals_rem += \
                        np.nansum(post_freq_spatial_rem[int(np.round(g_l[0]-bins_around_goal)):int(np.round(g_l[0]+bins_around_goal)),
                                  0:int(np.round(g_l[1]+bins_around_goal))])
            else:
                post_freq_spatial_around_goals_rem += \
                    np.nansum(post_freq_spatial_rem[int(np.round(g_l[0]-bins_around_goal)):int(np.round(g_l[0]+bins_around_goal)),
                              int(np.round(g_l[1]-bins_around_goal)):int(np.round(g_l[1]+bins_around_goal))])


        return pre_freq_spatial_around_goals_swr, post_freq_spatial_around_goals_swr, pre_freq_spatial_around_goals_out, \
                post_freq_spatial_around_goals_out, pre_freq_spatial_around_goals_rem, \
                post_freq_spatial_around_goals_rem


    # </editor-fold>

    # <editor-fold desc="Learning and non-stationarity during sleep">

    def learning_non_stationarity(self, time_bin_size=60, plasticity_measure="peak_loc", spatial_resolution=3,
                                  nr_fits=15, use_abs=True):

        if plasticity_measure == "peak_loc":
            peak_shift_pre = self.pre.learning_place_field_peak_shift(all_cells=True, plotting=False,
                                                                      spatial_resolution=1)

            less_learning = np.argwhere(peak_shift_pre < 0.5*np.median(peak_shift_pre)).flatten()
            more_learning = np.argwhere(peak_shift_pre > 1.5*np.median(peak_shift_pre)).flatten()

            peak_shift_post = self.post.learning_place_field_peak_shift(all_cells=True, plotting=False,
                                                                        spatial_resolution=1)

            peak_shift_pos_more_learning = peak_shift_post[more_learning]
            peak_shift_pos_less_learning = peak_shift_post[less_learning]

            peak_shift_pos_more_learning_sorted = np.sort(peak_shift_pos_more_learning)
            peak_shift_pos_less_learning_sorted = np.sort(peak_shift_pos_less_learning)

            p_more_learning = 1. * np.arange(peak_shift_pos_more_learning.shape[0]) / \
                              (peak_shift_pos_more_learning.shape[0] - 1)
            p_less_learning = 1. * np.arange(peak_shift_pos_less_learning.shape[0]) / (
                        peak_shift_pos_less_learning.shape[0] - 1)

            plt.plot(peak_shift_pos_more_learning_sorted, p_more_learning, label="More learning", color="magenta")
            plt.plot(peak_shift_pos_less_learning_sorted, p_less_learning, label="Less learning", color="aquamarine")
            plt.title("Peak firing shift POST")
            plt.xlabel("Peak firing location shift / cm")
            plt.ylabel("CDF")
            plt.legend()
            plt.show()

        if plasticity_measure == "rate_map_corr":
            rate_map_corr_pre = self.pre.learning_rate_map_corr(spatial_resolution=spatial_resolution)

            more_learning = np.argwhere(rate_map_corr_pre < 0.5*np.median(rate_map_corr_pre)).flatten()
            less_learning = np.argwhere(rate_map_corr_pre > 1.5*np.median(rate_map_corr_pre)).flatten()

            rate_map_corr_post = self.post.learning_rate_map_corr(spatial_resolution=spatial_resolution)

            rate_map_corr_post_more_learning = rate_map_corr_post[more_learning]
            rate_map_corr_post_less_learning = rate_map_corr_post[less_learning]

            rate_map_corr_post_more_learning_sorted = np.sort(rate_map_corr_post_more_learning)
            rate_map_corr_post_less_learning_sorted = np.sort(rate_map_corr_post_less_learning)

            p_more_learning = 1. * np.arange(rate_map_corr_post_more_learning.shape[0]) / \
                              (rate_map_corr_post_more_learning.shape[0] - 1)
            p_less_learning = 1. * np.arange(rate_map_corr_post_less_learning.shape[0]) / (
                        rate_map_corr_post_less_learning.shape[0] - 1)

            plt.plot(rate_map_corr_post_more_learning_sorted, p_more_learning, label="More learning", color="magenta")
            plt.plot(rate_map_corr_post_less_learning_sorted, p_less_learning, label="Less learning", color="aquamarine")
            plt.title("Rate map correlation POST (first vs. last 5 trials)")
            plt.xlabel("Pearson R")
            plt.ylabel("CDF")
            plt.legend()
            plt.show()

        elif plasticity_measure == "mean_firing":

            mean_diff_pre = self.pre.learning_mean_firing_rate()

            if use_abs:
                mean_diff_pre_abs = np.abs(mean_diff_pre)
                less_learning = np.argwhere(mean_diff_pre_abs < 0.25).flatten()
                more_learning = np.argwhere(mean_diff_pre_abs > 0.25).flatten()
            else:
                less_learning = np.argwhere((-0.25 < mean_diff_pre) & (mean_diff_pre < 0.25)).flatten()
                more_learning = np.argwhere((0.25 < mean_diff_pre) | (mean_diff_pre < -0.25)).flatten()

            mean_diff_post = self.post.learning_mean_firing_rate()

            if use_abs:
                mean_diff_post_abs = np.abs(mean_diff_post)

                mean_diff_post_more_learning = mean_diff_post_abs[more_learning]
                mean_diff_post_less_learning = mean_diff_post_abs[less_learning]

            else:
                mean_diff_post_more_learning = mean_diff_post[more_learning]
                mean_diff_post_less_learning = mean_diff_post[less_learning]

            mean_diff_post_more_learning_sorted = np.sort(mean_diff_post_more_learning)
            mean_diff_post_less_learning_sorted = np.sort(mean_diff_post_less_learning)

            p_more_learning = 1. * np.arange(mean_diff_post_more_learning.shape[0]) / \
                              (mean_diff_post_more_learning.shape[0] - 1)
            p_less_learning = 1. * np.arange(mean_diff_post_less_learning.shape[0]) / (
                    mean_diff_post_less_learning.shape[0] - 1)

            plt.plot(mean_diff_post_more_learning_sorted, p_more_learning, label="More learning", color="magenta")
            plt.plot(mean_diff_post_less_learning_sorted, p_less_learning, label="Less learning", color="aquamarine")
            plt.title("Mean firing diff. POST")
            if use_abs:
                plt.xlabel("Relative ABS. mean firing diff")
            else:
                plt.xlabel("Relative mean firing diff")
            plt.ylabel("CDF")
            plt.legend()
            plt.show()


        raster = []
        first = 0
        for l_s in self.long_sleep:
            duration = l_s.get_duration_sec()
            r = l_s.get_raster()
            raster.append(r)
            first += duration

        raster = np.hstack(raster)

        scaler = int(time_bin_size / self.params.time_bin_size)

        raster = down_sample_array_sum(x=raster, chunk_size=scaler)

        print(raster.shape)

        #
        times = np.arange(0, raster.shape[1]) * time_bin_size

        raster_more_learning = raster[more_learning, :]
        raster_less_learning = raster[less_learning, :]

        new_ml = MlMethodsOnePopulation()
        alpha = 1950

        r2_more_learning = []
        r2_less_learning = []

        for fit in range(nr_fits):
            random_seed = np.random.randint(0,1000,1)

            ml = new_ml.ridge_time_bin_progress(x=raster_more_learning, y=times, new_time_bin_size=time_bin_size,
                                                alpha_fitting=False, plotting=False, alpha=alpha, random_seed=random_seed)

            ll = new_ml.ridge_time_bin_progress(x=raster_less_learning, y=times, new_time_bin_size=time_bin_size,
                                                alpha_fitting=False, plotting=False, alpha=alpha, random_seed=random_seed)

            r2_more_learning.append(ml)
            r2_less_learning.append(ll)

        r2_more_learning = np.array(r2_more_learning)
        r2_less_learning = np.array(r2_less_learning)

        c = "white"


        res = np.vstack((r2_more_learning, r2_less_learning)).T

        bplot = plt.boxplot(res, positions=[1, 2], patch_artist=True,
                            labels=["More learning", "Less learning"],
                            boxprops=dict(color=c),
                            capprops=dict(color=c),
                            whiskerprops=dict(color=c),
                            flierprops=dict(color=c, markeredgecolor=c),
                            medianprops=dict(color=c),
                            )
        colors = ["yellow", 'blue']
        for patch, color in zip(bplot['boxes'], colors):
            patch.set_facecolor(color)
        plt.title("R2 VALUES OF RIDGE REGRESSION")
        plt.ylabel("R2 (15 SPLITS)")
        plt.grid(color="grey", axis="y")
        plt.show()

    def awake_cofiring_sleep_drift(self, time_bin_size=60):
        raster_pre = self.pre.get_raster()
        # make binary
        raster_pre[raster_pre > 0] = 1
        # raster_pre = raster_pre[:,:100]

        def count_co_firing(x,y):
            a = np.logical_and(x,y)
            return np.count_nonzero(a)

        nr_events = pairwise_distances(X=raster_pre, metric=count_co_firing)

        raster = []
        first = 0
        for l_s in self.long_sleep:
            duration = l_s.get_duration_sec()
            r = l_s.get_raster()
            raster.append(r)
            first += duration

        raster = np.hstack(raster)

        scaler = int(time_bin_size / self.params.time_bin_size)

        raster = down_sample_array_sum(x=raster, chunk_size=scaler)

        print(raster.shape)

        #
        times = np.arange(0, raster.shape[1]) * time_bin_size
        new_ml = MlMethodsOnePopulation()

        weights = new_ml.ridge_time_bin_progress(x=raster, y=times, new_time_bin_size=time_bin_size,
                                                alpha_fitting=True, plotting=False, return_weights=True)

        # compute all differences in weights
        weight_diff = pdist(np.expand_dims(weights, 1))

        weight_diff_abs = np.abs(weight_diff)

        weight_diff_norm = (weight_diff - np.mean(weight_diff))/np.std(weight_diff)

        weight_diff_norm_abs = np.abs(weight_diff_norm)

        nr_events_awake = upper_tri_without_diag(nr_events)

        plt.scatter(nr_events_awake, weight_diff_abs)
        plt.xlabel("Co-firing events awake")
        plt.ylabel("Regression weight distance")
        # plt.title("R = "+str(pearsonr(nr_events_awake, weight_diff_abs)))
        plt.show()

    # </editor-fold>

    # <editor-fold desc="Drift in correlations">

    def drift_correlation_structure_equalized_firing_rates(self, len_chunk_s = 200, filter_zero_spike_vectors=False,
                                                           plot_for_control=False, plotting=True, cells_to_use="stable"):
        # --------------------------------------------------------------------------------------------------------------
        # analyzes memory drift using correlation structure. Computes correlation matrix of awake behavior before and
        # correlation matrix of behavior after -> compares correlation matrix computed from sliding window during sleep
        # with before/after correlation matrix using Pearson correlation value
        #
        # parameters:   - correlation_window_size, int: size of sliding window (nr. time bins) to compute correlations
        #                 during sleep
        #
        # returns:      -
        # --------------------------------------------------------------------------------------------------------------

        # check if cheeseboard data or exploration data is used

        # get rasters from exploration before/after

        # load only stable cells
        with open(
                self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                "rb") as f:
            class_dic = pickle.load(f)

        if cells_to_use == "stable":
            cell_ids = class_dic["stable_cell_ids"]
        elif cells_to_use == "increasing":
            cell_ids = class_dic["increase_cell_ids"]
        elif cells_to_use == "decreasing":
            cell_ids = class_dic["decrease_cell_ids"]
        else:
           raise Exception("Cell subset name not defined!")

        raster_sleep = []

        for l_s in self.long_sleep:
            r = l_s.get_raster()
            raster_sleep.append(r)

        raster_sleep = np.hstack(raster_sleep)

        # only select stable cells
        raster_sleep = raster_sleep[cell_ids, :]

        # filter cells that are quiet most of the time
        per_cell_percent_firing = np.count_nonzero(raster_sleep, axis=1)/raster_sleep.shape[1]
        cells_to_exclude = np.where(per_cell_percent_firing < 0.01)
        raster_sleep = np.delete(raster_sleep, cells_to_exclude, axis=0)

        nr_chunks = raster_sleep.shape[1]*self.params.time_bin_size/len_chunk_s
        chunk_size = np.round(raster_sleep.shape[1]/nr_chunks).astype(int)
        # split into equal size parts to find min. number of spikes
        raster_sleep_chunks = down_sample_array_sum(x=raster_sleep, chunk_size=chunk_size)

        # nr of windows with zero spikes
        nr_time_bins_more_than_zero_spikes = np.count_nonzero(np.where(raster_sleep_chunks==0, 1, 0), axis=0)
        # binarize
        nr_time_bins_more_than_zero_spikes = np.where(nr_time_bins_more_than_zero_spikes==0, 1, 0).astype(bool)

        # remove time time windows if there is a single cell that doesn't spike
        raster_sleep_chunks_filtered = raster_sleep_chunks[:, nr_time_bins_more_than_zero_spikes]

        # min. number of spikes per window
        min_nr_spikes = np.min(raster_sleep_chunks_filtered, axis=1).astype(int)


        # need to filter periods where there is no spike for at least one cell in the initial raster
        nr_time_bins_more_than_zero_spikes_orig_resolution = nr_time_bins_more_than_zero_spikes.repeat(chunk_size).astype(bool)
        raster_sleep_filtered = raster_sleep[:,:nr_time_bins_more_than_zero_spikes_orig_resolution.shape[0]]
        raster_sleep_filtered = raster_sleep_filtered[:,nr_time_bins_more_than_zero_spikes_orig_resolution]


        additional_chunk_to_remove = []
        equalized_raster = np.zeros(raster_sleep_filtered.shape)
        # go through rasters and remove random spikes to equalize firing rates over time
        for chunk_id in range(int(np.round(nr_chunks))):
            # if end of chunk is bigger that filtered raster --> leave loop
            if (chunk_id+1)*chunk_size > raster_sleep_filtered.shape[1]:
                additional_chunk_to_remove.append(chunk_id)
                break
            # find all time bins with spikes
            raster_chunk = raster_sleep_filtered[:, chunk_id*chunk_size:(chunk_id+1)*chunk_size]
            raster_chunk_copy = np.copy(raster_chunk)
            # go trough all cells
            for cell_id, cell_min_nr_spikes in enumerate(min_nr_spikes):
                # keep removing spikes until min. nr. spikes is reached
                cell_chunk = np.copy(raster_chunk[cell_id,:]).astype(int)
                nr_spikes_in_chunk = np.sum(cell_chunk)
                while nr_spikes_in_chunk > cell_min_nr_spikes:
                    nr_spikes_to_remove = nr_spikes_in_chunk - cell_min_nr_spikes
                    # find time bins with spikes
                    cell_bins_with_spikes = np.argwhere(cell_chunk > 0).flatten()
                    # chunk with zero spikes for one cell
                    if cell_bins_with_spikes.shape[0] == 0:
                        additional_chunk_to_remove.append(chunk_id)
                        break
                    # there are less time bins with spikes than spikes to remove --> need to go through while loop
                    # another time
                    elif nr_spikes_to_remove > cell_bins_with_spikes.shape[0]:
                        spikes_to_remove = np.random.choice(a=cell_bins_with_spikes, size=cell_bins_with_spikes.shape[0],
                                                            replace=False)
                    else:
                        spikes_to_remove = np.random.choice(a=cell_bins_with_spikes, size=nr_spikes_to_remove,
                                                            replace=False)
                    # remove n spikes from bins to match min_nr_spikes
                    cell_chunk[spikes_to_remove] -= 1
                    # check if more spikes need to be removed
                    nr_spikes_in_chunk = np.sum(cell_chunk)

                equalized_raster[cell_id, chunk_id*chunk_size:(chunk_id+1)*chunk_size] = cell_chunk

            # # if last chunk is too small --> do not add it
            # if chunk_size == raster_chunk_copy.shape[1]:
            #     equalized_raster[:, chunk_id*chunk_size:(chunk_id+1)*chunk_size] = raster_chunk_copy
            # else:
            #     break
        if len(additional_chunk_to_remove)>0:
            # remove additional chunks, TODO: might be useless
            min_chunks_to_remove = min(additional_chunk_to_remove)
            equalized_raster = equalized_raster[:, :min_chunks_to_remove*chunk_size]

        if plot_for_control:
            # check equalized raster
            equalized_raster_test = down_sample_array_sum(x=equalized_raster, chunk_size=chunk_size)
            plt.figure(figsize=(7,3))
            plt.imshow(equalized_raster_test)
            plt.title("Spikes per chunk")
            plt.xlabel("Chunk ID")
            plt.ylabel("Cell ID")
            plt.show()
            plt.figure(figsize=(9,2))
            plt.title("Spikes per time bin")
            plt.xlabel("Time bin")
            plt.ylabel("Cell ID")
            plt.imshow(equalized_raster[:,:500])
            plt.show()

        # compute correlations from equalized rasters --> 60 seconds was a good estimate to get full rank corr. matrices
        len_corr_chunk_s = 200
        nr_chunks_corr = equalized_raster.shape[1]*self.params.time_bin_size/len_corr_chunk_s
        chunk_size = np.round(equalized_raster.shape[1]/nr_chunks_corr).astype(int)

        corr_matrices_sleep = []

        for i in range(int(nr_chunks_corr)):
            corr_matrices_sleep.append(upper_tri_without_diag(np.corrcoef(equalized_raster[:, i*chunk_size:(i+1)*chunk_size])))

        raster_pre = self.pre.get_raster()
        raster_post = self.post.get_raster()

        raster_pre = np.delete(raster_pre[cell_ids, :], cells_to_exclude, axis=0)
        raster_post = np.delete(raster_post[cell_ids, :], cells_to_exclude, axis=0)

        # compute correlations of exploration before/after
        corr_pre = np.nan_to_num(np.corrcoef(raster_pre))
        corr_post = np.nan_to_num(np.corrcoef(raster_post))

        corr_post = upper_tri_without_diag(corr_post)
        corr_pre = upper_tri_without_diag(corr_pre)

        sim_pearson = []
        # for each sliding window compute similarity with behavior before/after
        for corr in corr_matrices_sleep:
            sim_post = abs(pearsonr(corr, corr_post)[0])
            sim_pre = abs(pearsonr(corr, corr_pre)[0])
            sim_pearson.append((sim_post - sim_pre) / (sim_post + sim_pre))

        if plotting:
            x_axis = (np.arange(len(sim_pearson)) * len_corr_chunk_s)/60/60
            fig = plt.figure()
            ax = fig.add_subplot()
            ax.plot(x_axis, sim_pearson, color="red", label="PEARSON")
            plt.title("CORRELATION STRUCTURE SIMILARITY: BEFORE - AFTER\n #Duration to compute corr: " +
                      str(np.round(len_corr_chunk_s/60,1))+" min")
            plt.xlabel("TIME (h)")
            plt.ylabel("SIMILARITY BEFORE - AFTER / PEARSON")
            plt.ylim(-1, 1)
            plt.show()

            # apply some smoothing
            sim_pearson_smooth = moving_average(a=np.array(sim_pearson), n=20)

            x_axis = (np.arange(len(sim_pearson_smooth)) * len_corr_chunk_s/200)/60/60
            fig = plt.figure()
            ax = fig.add_subplot()
            ax.plot(x_axis, sim_pearson_smooth, color="red", label="PEARSON")
            plt.title("CORRELATION STRUCTURE SIMILARITY: BEFORE - AFTER\n #Duration to compute corr: " +
                      str(np.round(len_corr_chunk_s/60,1))+" min - smoothed")
            plt.xlabel("TIME")
            plt.ylabel("SIMILARITY BEFORE - AFTER / PEARSON")
            plt.ylim(-1, 1)
            plt.show()
        else:
            return sim_pearson

    def drift_correlation_structure_equalized_firing_rates_all_cells(self, len_chunk_s = 200,
                                                                     n_equalizing=1, n_smoothing=20,
                                                                     plot_for_control=False, plotting=True):

        raster_sleep = []

        for l_s in self.long_sleep:
            r = l_s.get_raster()
            raster_sleep.append(r)

        raster_sleep = np.hstack(raster_sleep)

        # filter cells that are quite most of the time
        per_cell_percent_firing = np.count_nonzero(raster_sleep, axis=1)/raster_sleep.shape[1]
        cells_to_exclude = np.where(per_cell_percent_firing < 0.01)
        raster_sleep = np.delete(raster_sleep, cells_to_exclude, axis=0)

        nr_chunks = raster_sleep.shape[1]*self.params.time_bin_size/len_chunk_s
        chunk_size = np.round(raster_sleep.shape[1]/nr_chunks).astype(int)
        # split into equal size parts to find min. number of spikes
        raster_sleep_chunks = down_sample_array_sum(x=raster_sleep, chunk_size=chunk_size)

        # nr of windows with zero spikes
        nr_time_bins_more_than_zero_spikes = np.count_nonzero(np.where(raster_sleep_chunks==0, 1, 0), axis=0)
        # binarize
        nr_time_bins_more_than_zero_spikes = np.where(nr_time_bins_more_than_zero_spikes==0, 1, 0).astype(bool)

        # remove time windows if there is a single cell that doesn't spike
        raster_sleep_chunks_filtered = raster_sleep_chunks[:, nr_time_bins_more_than_zero_spikes]

        # min. number of spikes per window
        min_nr_spikes = np.min(raster_sleep_chunks_filtered, axis=1).astype(int)

        # need to filter periods where there is no spike for at least one cell in the initial raster
        nr_time_bins_more_than_zero_spikes_orig_resolution = nr_time_bins_more_than_zero_spikes.repeat(chunk_size).astype(bool)
        raster_sleep_filtered = raster_sleep[:,:nr_time_bins_more_than_zero_spikes_orig_resolution.shape[0]]
        raster_sleep_filtered = raster_sleep_filtered[:,nr_time_bins_more_than_zero_spikes_orig_resolution]

        # need to generate many equalized rasters to compute mean later
        all_corr_matrices = []
        for n_eq in range(n_equalizing):
            additional_chunk_to_remove = []
            equalized_raster = np.zeros(raster_sleep_filtered.shape)
            # go through rasters and remove random spikes to equalize firing rates over time
            for chunk_id in range(int(np.round(nr_chunks))):
                # if end of chunk is bigger that filtered raster --> leave loop
                if (chunk_id+1)*chunk_size > raster_sleep_filtered.shape[1]:
                    additional_chunk_to_remove.append(chunk_id)
                    break
                # find all time bins with spikes
                raster_chunk = raster_sleep_filtered[:, chunk_id*chunk_size:(chunk_id+1)*chunk_size]
                raster_chunk_copy = np.copy(raster_chunk)
                # go trough all cells
                for cell_id, cell_min_nr_spikes in enumerate(min_nr_spikes):
                    # keep removing spikes until min. nr. spikes is reached
                    cell_chunk = np.copy(raster_chunk[cell_id,:]).astype(int)
                    nr_spikes_in_chunk = np.sum(cell_chunk)
                    while nr_spikes_in_chunk > cell_min_nr_spikes:
                        nr_spikes_to_remove = nr_spikes_in_chunk - cell_min_nr_spikes
                        # find time bins with spikes
                        cell_bins_with_spikes = np.argwhere(cell_chunk > 0).flatten()
                        # chunk with zero spikes for one cell
                        if cell_bins_with_spikes.shape[0] == 0:
                            additional_chunk_to_remove.append(chunk_id)
                            break
                        # there are less time bins with spikes than spikes to remove --> need to go through while loop
                        # another time
                        elif nr_spikes_to_remove > cell_bins_with_spikes.shape[0]:
                            spikes_to_remove = np.random.choice(a=cell_bins_with_spikes, size=cell_bins_with_spikes.shape[0],
                                                                replace=False)
                        else:
                            spikes_to_remove = np.random.choice(a=cell_bins_with_spikes, size=nr_spikes_to_remove,
                                                                replace=False)
                        # remove n spikes from bins to match min_nr_spikes
                        cell_chunk[spikes_to_remove] -= 1
                        # check if more spikes need to be removed
                        nr_spikes_in_chunk = np.sum(cell_chunk)

                    equalized_raster[cell_id, chunk_id*chunk_size:(chunk_id+1)*chunk_size] = cell_chunk

                # # if last chunk is too small --> do not add it
                # if chunk_size == raster_chunk_copy.shape[1]:
                #     equalized_raster[:, chunk_id*chunk_size:(chunk_id+1)*chunk_size] = raster_chunk_copy
                # else:
                #     break
            if len(additional_chunk_to_remove)>0:
                # remove additional chunks, TODO: might be useless
                min_chunks_to_remove = min(additional_chunk_to_remove)
                equalized_raster = equalized_raster[:, :min_chunks_to_remove*chunk_size]
            if plot_for_control:
                # check equalized raster
                equalized_raster_test = down_sample_array_sum(x=equalized_raster, chunk_size=chunk_size)
                plt.figure(figsize=(7,3))
                plt.imshow(equalized_raster_test)
                plt.title("Spikes per chunk")
                plt.xlabel("Chunk ID")
                plt.ylabel("Cell ID")
                plt.show()
                plt.figure(figsize=(9,2))
                plt.title("Spikes per time bin")
                plt.xlabel("Time bin")
                plt.ylabel("Cell ID")
                plt.imshow(equalized_raster[:,:500])
                plt.show()

            # compute correlations from equalized rasters --> 60 seconds was a good estimate to get full rank corr. matrices
            len_corr_chunk_s = 200
            nr_chunks_corr = equalized_raster.shape[1]*self.params.time_bin_size/len_corr_chunk_s
            chunk_size = np.round(equalized_raster.shape[1]/nr_chunks_corr).astype(int)

            corr_matrices_sleep = []

            for i in range(int(nr_chunks_corr)):
                corr_matrices_sleep.append(upper_tri_without_diag(np.corrcoef(equalized_raster[:, i*chunk_size:(i+1)*chunk_size])))

            all_corr_matrices.append(corr_matrices_sleep)

        raster_pre = self.pre.get_raster()
        raster_post = self.post.get_raster()

        raster_pre = np.delete(raster_pre, cells_to_exclude, axis=0)
        raster_post = np.delete(raster_post, cells_to_exclude, axis=0)

        corr_pre = np.nan_to_num(np.corrcoef(raster_pre))
        corr_post = np.nan_to_num(np.corrcoef(raster_post))

        corr_post = upper_tri_without_diag(corr_post)
        corr_pre = upper_tri_without_diag(corr_pre)

        all_sim_pearson = []
        for corr_matrices_sleep in all_corr_matrices:
            sim_pearson = []
            # for each sliding window compute similarity with behavior before/after
            for corr in corr_matrices_sleep:
                sim_post = abs(pearsonr(corr, corr_post)[0])
                sim_pre = abs(pearsonr(corr, corr_pre)[0])
                sim_pearson.append((sim_post - sim_pre) / (sim_post + sim_pre))
            all_sim_pearson.append(sim_pearson)

        all_sim_pearson = np.vstack(all_sim_pearson)

        all_sim_pearson_smooth = []
        for sim_pearson in all_sim_pearson:
            # apply some smoothing
            all_sim_pearson_smooth.append(moving_average(a=np.array(sim_pearson), n=n_smoothing))

        all_sim_pearson_smooth = np.vstack(all_sim_pearson_smooth)

        if n_equalizing > 1:
            mean = np.mean(all_sim_pearson_smooth, axis=0)
            std = np.std(all_sim_pearson_smooth, axis=0)
        else:
            mean = all_sim_pearson_smooth
            std=None

        if plotting:
            x_axis = (np.arange(len(all_sim_pearson[0])) * len_corr_chunk_s)/60/60
            fig = plt.figure()
            ax = fig.add_subplot()
            ax.errorbar(x=x_axis, y=np.mean(all_sim_pearson, axis=0), yerr=np.std(all_sim_pearson, axis=0),
                        color="red", label="PEARSON")
            plt.title("CORRELATION STRUCTURE SIMILARITY: BEFORE - AFTER\n #Duration to compute corr: " +
                      str(np.round(len_corr_chunk_s/60,1))+" min")
            plt.xlabel("TIME (h)")
            plt.ylabel("SIMILARITY BEFORE - AFTER / PEARSON")
            plt.ylim(-1, 1)
            plt.show()

            x_axis = (np.arange(all_sim_pearson_smooth.shape[1]) * len_corr_chunk_s/200)/60/60
            fig = plt.figure()
            ax = fig.add_subplot()
            ax.errorbar(x=x_axis, y=np.mean(all_sim_pearson_smooth, axis=0), yerr=np.std(all_sim_pearson_smooth, axis=0),
                        color="red", label="PEARSON")
            plt.title("CORRELATION STRUCTURE SIMILARITY: BEFORE - AFTER\n #Duration to compute corr: " +
                      str(np.round(len_corr_chunk_s/60,1))+" min - smoothed")
            plt.xlabel("TIME")
            plt.ylabel("SIMILARITY BEFORE - AFTER / PEARSON")
            plt.ylim(-1, 1)
            plt.show()
        else:
            return mean, std

    def drift_correlation_structure_all_cells(self, len_chunk_s = 200, n_smoothing=20, plot_for_control=False,
                                              plotting=True):

        raster_sleep = []

        for l_s in self.long_sleep:
            r = l_s.get_raster()
            raster_sleep.append(r)

        raster_sleep = np.hstack(raster_sleep)

        # filter cells that are quite most of the time
        per_cell_percent_firing = np.count_nonzero(raster_sleep, axis=1)/raster_sleep.shape[1]
        cells_to_exclude = np.where(per_cell_percent_firing < 0.01)
        raster_sleep = np.delete(raster_sleep, cells_to_exclude, axis=0)

        nr_chunks = raster_sleep.shape[1]*self.params.time_bin_size/len_chunk_s
        chunk_size = np.round(raster_sleep.shape[1]/nr_chunks).astype(int)
        # split into equal size parts to find min. number of spikes
        raster_sleep_chunks = down_sample_array_sum(x=raster_sleep, chunk_size=chunk_size)

        # nr of windows with zero spikes
        nr_time_bins_more_than_zero_spikes = np.count_nonzero(np.where(raster_sleep_chunks==0, 1, 0), axis=0)
        # binarize
        nr_time_bins_more_than_zero_spikes = np.where(nr_time_bins_more_than_zero_spikes==0, 1, 0).astype(bool)

        # remove time time windows if there is a single cell that doesn't spike
        raster_sleep_chunks_filtered = raster_sleep_chunks[:, nr_time_bins_more_than_zero_spikes]

        # min. number of spikes per window
        min_nr_spikes = np.min(raster_sleep_chunks_filtered, axis=1).astype(int)

        # need to filter periods where there is no spike for at least one cell in the initial raster
        nr_time_bins_more_than_zero_spikes_orig_resolution = nr_time_bins_more_than_zero_spikes.repeat(chunk_size).astype(bool)
        raster_sleep_filtered = raster_sleep[:,:nr_time_bins_more_than_zero_spikes_orig_resolution.shape[0]]
        raster_sleep_filtered = raster_sleep_filtered[:,nr_time_bins_more_than_zero_spikes_orig_resolution]

        # compute correlations from rasters --> 60 seconds was a good estimate to get full rank corr. matrices
        len_corr_chunk_s = 200
        nr_chunks_corr = raster_sleep_filtered.shape[1]*self.params.time_bin_size/len_corr_chunk_s
        chunk_size = np.round(raster_sleep_filtered.shape[1]/nr_chunks_corr).astype(int)

        corr_matrices_sleep = []

        for i in range(int(nr_chunks_corr)):
            corr_matrices_sleep.append(upper_tri_without_diag(np.corrcoef(raster_sleep_filtered[:, i*chunk_size:(i+1)*chunk_size])))

        raster_pre = self.pre.get_raster()
        raster_post = self.post.get_raster()

        raster_pre = np.delete(raster_pre, cells_to_exclude, axis=0)
        raster_post = np.delete(raster_post, cells_to_exclude, axis=0)

        corr_pre = np.nan_to_num(np.corrcoef(raster_pre))
        corr_post = np.nan_to_num(np.corrcoef(raster_post))

        corr_post = upper_tri_without_diag(corr_post)
        corr_pre = upper_tri_without_diag(corr_pre)

        all_sim_pearson = []

        sim_pearson = []
        # for each sliding window compute similarity with behavior before/after
        for corr in corr_matrices_sleep:
            sim_post = abs(pearsonr(corr, corr_post)[0])
            sim_pre = abs(pearsonr(corr, corr_pre)[0])
            sim_pearson.append((sim_post - sim_pre) / (sim_post + sim_pre))
        all_sim_pearson.append(sim_pearson)

        all_sim_pearson = np.vstack(all_sim_pearson)

        all_sim_pearson_smooth = []
        for sim_pearson in all_sim_pearson:
            # apply some smoothing
            all_sim_pearson_smooth.append(moving_average(a=np.array(sim_pearson), n=n_smoothing))

        all_sim_pearson_smooth = np.vstack(all_sim_pearson_smooth)

        mean = all_sim_pearson_smooth
        std=None

        if plotting:
            x_axis = (np.arange(len(all_sim_pearson[0])) * len_corr_chunk_s)/60/60
            fig = plt.figure()
            ax = fig.add_subplot()
            ax.errorbar(x=x_axis, y=np.mean(all_sim_pearson, axis=0), yerr=np.std(all_sim_pearson, axis=0),
                        color="red", label="PEARSON")
            plt.title("CORRELATION STRUCTURE SIMILARITY: BEFORE - AFTER\n #Duration to compute corr: " +
                      str(np.round(len_corr_chunk_s/60,1))+" min")
            plt.xlabel("TIME (h)")
            plt.ylabel("SIMILARITY BEFORE - AFTER / PEARSON")
            plt.ylim(-1, 1)
            plt.show()

            x_axis = (np.arange(all_sim_pearson_smooth.shape[1]) * len_corr_chunk_s/200)/60/60
            fig = plt.figure()
            ax = fig.add_subplot()
            ax.errorbar(x=x_axis, y=np.mean(all_sim_pearson_smooth, axis=0), yerr=np.std(all_sim_pearson_smooth, axis=0),
                        color="red", label="PEARSON")
            plt.title("CORRELATION STRUCTURE SIMILARITY: BEFORE - AFTER\n #Duration to compute corr: " +
                      str(np.round(len_corr_chunk_s/60,1))+" min - smoothed")
            plt.xlabel("TIME")
            plt.ylabel("SIMILARITY BEFORE - AFTER / PEARSON")
            plt.ylim(-1, 1)
            plt.show()
        else:
            return mean, std

    # </editor-fold>

    # <editor-fold desc="Others">

    def cell_type_correlations(self, awake_smoothing=3, sleep_smoothing=5, part_to_analyze="rem", nr_blocks=5,
                               plotting=True):

        # load cell labels
        # --------------------------------------------------------------------------------------------------------------
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.params.session_name + "_" + self.params.stable_cell_method + ".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_cells = class_dic["stable_cell_ids"].flatten()
        inc_cells = class_dic["increase_cell_ids"].flatten()
        dec_cells = class_dic["decrease_cell_ids"].flatten()

        n_stable_cells = len(stable_cells)

        # get rasters
        raster_pre = self.pre.get_raster()
        raster_post = self.post.get_raster()

        # filter cells that don't fire
        pre_to_delete = np.argwhere(np.sum(raster_pre, axis=1) == 0).flatten()
        post_to_delete = np.argwhere(np.sum(raster_post, axis=1) == 0).flatten()

        # delete stable cells that don't fire in pre or post
        for to_delete in np.hstack((pre_to_delete, post_to_delete)):
            stable_cells = stable_cells[stable_cells != to_delete]

        # delete decreasing cells that don't fire in pre
        for to_delete in pre_to_delete:
            dec_cells = dec_cells[dec_cells != to_delete]

        # delete increasing cells that don't fire in post
        for to_delete in post_to_delete:
            inc_cells = inc_cells[inc_cells != to_delete]

        # PRE DATA
        # --------------------------------------------------------------------------------------------------------------
        # apply smoothing
        raster_pre = uniform_filter1d(raster_pre, size=awake_smoothing, axis=1)

        # select stable and dec for pre correlations
        raster_stable_dec_pre = np.vstack((raster_pre[stable_cells, :], raster_pre[dec_cells, :]))
        # filter silent periods, z-score and compute correlations
        raster_stable_dec_pre = zscore(raster_stable_dec_pre[:, np.sum(raster_stable_dec_pre, axis=0) > 1], axis=1)
        corr_stable_dec_pre = squareform(1 - pdist(raster_stable_dec_pre,
                                                   "correlation"))[:n_stable_cells, n_stable_cells:].flatten()
        # stable correlations
        raster_stable_pre = raster_pre[stable_cells, :]
        raster_stable_pre = zscore(raster_stable_pre[:, np.sum(raster_stable_pre, axis=0) > 1], axis=1)
        corr_stable_pre = 1 - pdist(raster_stable_pre, "correlation")

        # decreasing correlations
        raster_dec_pre = raster_pre[dec_cells, :]
        raster_dec_pre = raster_dec_pre[:, np.sum(raster_dec_pre, axis=0) > 1]
        raster_dec_pre = zscore(raster_dec_pre, axis=1)
        corr_dec_pre = 1 - pdist(raster_dec_pre, 'correlation')

        # POST DATA
        # --------------------------------------------------------------------------------------------------------------
        # apply smoothing
        raster_post = uniform_filter1d(raster_post, size=awake_smoothing, axis=1)

        # select stable and dec for pre correlations
        raster_stable_inc_post = np.vstack((raster_post[stable_cells, :], raster_post[inc_cells, :]))
        # filter silent periods, z-score and compute correlations
        raster_stable_inc_post = zscore(raster_stable_inc_post[:, np.sum(raster_stable_inc_post, axis=0) > 1], axis=1)
        corr_stable_inc_post = squareform(1 - pdist(raster_stable_inc_post,
                                                    "correlation"))[:n_stable_cells, n_stable_cells:].flatten()
        # stable correlations
        raster_stable_post = raster_post[stable_cells, :]
        raster_stable_post = zscore(raster_stable_post[:, np.sum(raster_stable_post, axis=0) > 1], axis=1)
        corr_stable_post = 1 - pdist(raster_stable_post, "correlation")

        # decreasing correlations
        raster_inc_post = raster_post[inc_cells, :]
        raster_inc_post = raster_inc_post[:, np.sum(raster_inc_post, axis=0) > 1]
        raster_inc_post = zscore(raster_inc_post, axis=1)
        corr_inc_post = 1 - pdist(raster_inc_post, 'correlation')

        # GET SLEEP DATA CORRELATIONS
        # --------------------------------------------------------------------------------------------------------------
        rasters_sleep = []
        for sleep in self.long_sleep:
            rasters, _ = sleep.get_event_spike_rasters(part_to_analyze=part_to_analyze)
            rasters_sleep.append(np.hstack(rasters))

        rasters_sleep = np.hstack(rasters_sleep)

        block_length = int(rasters_sleep.shape[1] / nr_blocks)

        dec_pre = []
        inc_post = []
        stable_pre = []
        stable_post = []
        stable_dec_pre = []
        stable_inc_post = []

        for block_id in range(nr_blocks):
            data = rasters_sleep[:, block_id * block_length:(block_id + 1) * block_length]
            # smoothen data
            data = uniform_filter1d(data, size=sleep_smoothing, axis=1)

            # select decreasing cells
            dec_data = data[dec_cells, :]
            dec_data = dec_data[:, np.sum(dec_data, axis=0) > 1]
            dec_data = np.nan_to_num(zscore(dec_data, axis=1))
            dec_pre.append(pearsonr(np.nan_to_num(1 - pdist(dec_data, "correlation")), corr_dec_pre)[0])

            # select increasing cells
            inc_data = data[inc_cells, :]
            inc_data = inc_data[:, np.sum(inc_data, axis=0) > 1]
            inc_data = np.nan_to_num(zscore(inc_data, axis=1))
            inc_post.append(pearsonr(np.nan_to_num(1 - pdist(inc_data, "correlation")), corr_inc_post)[0])

            # select stable cells
            stable_data = data[stable_cells, :]
            stable_data = stable_data[:, np.sum(stable_data, axis=0) > 1]
            stable_data = np.nan_to_num(zscore(stable_data, axis=1))
            stable_pre.append(pearsonr(np.nan_to_num(1 - pdist(stable_data, "correlation")), corr_stable_pre)[0])
            stable_post.append(pearsonr(np.nan_to_num(1 - pdist(stable_data, "correlation")), corr_stable_post)[0])

            # select stable & decreasing cells for PRE
            stable_dec_data = np.vstack((data[stable_cells, :], data[dec_cells, :]))
            stable_dec_data = np.nan_to_num(zscore(stable_dec_data[:, np.sum(stable_dec_data, axis=0) > 1], axis=1))
            corr_stable_dec = squareform(np.nan_to_num(1 - pdist(stable_dec_data,
                                                                 "correlation")))[:n_stable_cells,
                              n_stable_cells:].flatten()
            stable_dec_pre.append(pearsonr(corr_stable_dec, corr_stable_dec_pre)[0])

            # select stable & increasing cells for POST
            stable_inc_data = np.vstack((data[stable_cells, :], data[inc_cells, :]))
            stable_inc_data = np.nan_to_num(zscore(stable_inc_data[:, np.sum(stable_inc_data, axis=0) > 1], axis=1))
            corr_stable_inc = squareform(np.nan_to_num(1 - pdist(stable_inc_data,
                                                                 "correlation")))[:n_stable_cells,
                              n_stable_cells:].flatten()
            stable_inc_post.append(pearsonr(corr_stable_inc, corr_stable_inc_post)[0])

        if plotting:
            plt.figure(figsize=(5, 15))
            plt.subplot(5, 1, 1)
            plt.plot(stable_pre, color="blue", label="PRE")
            plt.plot(stable_post, color="orange", label="POST")
            plt.ylabel("PEARSON R")
            plt.title("STABLE")
            plt.legend()
            # plt.ylim([0, y_max])
            plt.subplot(5, 1, 2)
            plt.plot(inc_post, color="orange")
            plt.title("INC")
            # plt.ylim([0, y_max])
            plt.subplot(5, 1, 3)
            plt.plot(dec_pre, color="blue")
            plt.title("DEC")
            # plt.ylim([0,y_max])
            plt.subplot(5, 1, 4)
            plt.plot(stable_inc_post, color="orange")
            plt.title("STABLE-INC")
            # plt.ylim([0, y_max])
            plt.subplot(5, 1, 5)
            plt.plot(stable_dec_pre, color="blue")
            plt.title("STABLE-DEC")
            # plt.ylim([0, y_max])
            plt.show()

        else:
            return stable_pre, stable_post, inc_post, dec_pre, stable_inc_post, stable_dec_pre

    def sleep_k_means_clustering_post_correlations(self, n_clusters=5):

        # get rasters
        raster = []
        first = 0
        for l_s in self.long_sleep:
            duration = l_s.get_duration_sec()
            r, t = l_s.get_spike_binned_raster(return_estimated_times=True)
            raster.append(r)
            first += duration
        raster = np.hstack(raster)

        # down sample raster
        raster_ds = down_sample_array_mean(x=raster, chunk_size=5000)
        raster_ds = raster_ds / np.max(raster_ds, axis=1, keepdims=True)

        # smooth down sampled rasters
        raster_ds_smoothed = []

        for cell_arr in raster_ds:
            s = moving_average(a=cell_arr, n=50)
            s = s / np.max(s)
            raster_ds_smoothed.append(s)

        raster_ds_smoothed = np.array(raster_ds_smoothed)

        # k-means clustering
        kmeans = KMeans(n_clusters=n_clusters).fit(X=raster_ds_smoothed)
        k_labels = kmeans.labels_

        stable = []
        increase = []
        decrease = []
        # find clusters with constant/decreasing/increasing firing rates
        # compare firing during first 20% with firing during last 20%
        for cl_id in np.unique(k_labels):
            cl_data = raster_ds_smoothed[k_labels == cl_id, :]
            diff = np.mean(cl_data[:, -int(0.2 * cl_data.shape[1]):].flatten()) - np.mean(
                cl_data[:, :int(0.2 * cl_data.shape[1])].flatten())
            if diff < -0.09:
                decrease.append(cl_id)
            elif diff > 0.09:
                increase.append(cl_id)
            else:
                stable.append(cl_id)

        decrease = np.array(decrease)
        increase = np.array(increase)
        stable = np.array(stable)

        print("STABLE CLUSTER IDS: " + str(stable))
        print("INCREASING CLUSTER IDS: " + str(increase))
        print("DECREASING CLUSTER IDS: " + str(decrease))

        stable_cell_ids = []
        decrease_cell_ids = []
        increase_cell_ids = []
        for stable_cluster_id in stable:
            stable_cell_ids.append(np.where(k_labels == stable_cluster_id)[0])
        for dec_cluster_id in decrease:
            decrease_cell_ids.append(np.where(k_labels == dec_cluster_id)[0])
        for inc_cluster_id in increase:
            increase_cell_ids.append(np.where(k_labels == inc_cluster_id)[0])

        # stable_cell_ids = np.hstack(stable_cell_ids)
        # decrease_cell_ids = np.hstack(decrease_cell_ids)
        # increase_cell_ids = np.hstack(increase_cell_ids)
        k_labels_plotting = np.copy(k_labels)
        k_labels_plotting[k_labels_plotting % 2 == 1] = 1
        k_labels_plotting[k_labels_plotting % 2 != 1] = 2
        k_labels_sorted = k_labels.argsort()

        fig = plt.figure(figsize=(8, 6))
        gs = fig.add_gridspec(6, 20)

        ax1 = fig.add_subplot(gs[:, 0])
        ax1.set_title("CLUSTERS")
        ax2 = fig.add_subplot(gs[:, 3:-2])
        ax3 = fig.add_subplot(gs[:, -1:])

        # plotting

        ax1.imshow(np.expand_dims(k_labels_plotting[k_labels_sorted], 1), aspect="auto", cmap="Set1")
        ax1.axis("off")
        rate_map = ax2.imshow(raster_ds_smoothed[k_labels_sorted, :], interpolation='nearest', aspect='auto')
        ax2.set_xlabel("BIN ID")
        ax2.set_ylabel("CELLS SORTED")
        a = plt.colorbar(rate_map, cax=ax3)
        a.set_label("#SPIKES / NORMALIZED")
        plt.show()

        # get post raster
        post_raster = self.post.get_raster()

        # go through all increasing clusters and compute correlations
        # in post
        inc_corr = []
        for inc_cluster_cells in increase_cell_ids:
            if inc_cluster_cells.shape[0] > 1:
                inc_clust_raster = post_raster[inc_cluster_cells, :]
                correlations = upper_tri_without_diag(np.corrcoef(inc_clust_raster))
                inc_corr.append(correlations)

        all_corr = upper_tri_without_diag(np.corrcoef(post_raster))
        all_corr = np.nan_to_num(np.array(all_corr))
        # plot distributions
        for i, corr in enumerate(inc_corr):
            corr = np.nan_to_num(np.array(corr))
            plt.hist(all_corr, np.arange(np.min(np.hstack((corr, all_corr))),
                                         np.max(np.hstack((corr, all_corr))), 0.02), color="gray", density=True,
                     label="ALL CELLS")
            plt.hist(corr, np.arange(np.min(np.hstack((corr, all_corr))),
                                     np.max(np.hstack((corr, all_corr))), 0.02), color="red", alpha=0.7,
                     label="INCREASING CELLS, CLUSTER " + str(i), density=True)
            plt.legend()
            plt.xlabel("CORRELATIONS")
            plt.ylabel("DENSITY")
            plt.show()

    def cell_classification_mean_firing_rates_awake(self, sorting="awake_average", normalize=False, smooth_sleep=False,
                                                    save_fig=False, plotting=True, n_chunks_for_sleep=10, n_smoothing=2,
                                                    log_scale=False):

        pre_raster_mean = np.mean(self.pre.get_raster(), axis=1)/self.params.time_bin_size
        post_raster_mean = np.mean(self.post.get_raster(), axis=1)/self.params.time_bin_size

        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_cell_ids = class_dic["stable_cell_ids"].flatten()
        increase_cell_ids = class_dic["increase_cell_ids"].flatten()
        decrease_cell_ids = class_dic["decrease_cell_ids"].flatten()

        # get sleep raster
        sleep_raster = []
        duration_sleep = 0
        for l_s in self.long_sleep:
            duration = l_s.get_duration_sec()
            r = l_s.get_raster()
            sleep_raster.append(r)
            duration_sleep += duration
        sleep_raster = np.hstack(sleep_raster)
        duration_sleep_h = np.round(duration_sleep / 60 / 60, 0).astype(int)

        # compute mean firing per chunk during sleep
        chunk_size = int(sleep_raster.shape[1] / n_chunks_for_sleep)
        sleep_raster_mean = np.zeros((sleep_raster.shape[0], n_chunks_for_sleep))

        for n_chunk in range(n_chunks_for_sleep):
            sleep_raster_mean[:, n_chunk] = \
                np.mean(sleep_raster[:, n_chunk*chunk_size:(n_chunk+1)*chunk_size], axis=1)/self.params.time_bin_size

        # normalize if requested
        if normalize:
            max_fir = np.max(np.hstack(
                (np.expand_dims(pre_raster_mean, 1), sleep_raster_mean, np.expand_dims(post_raster_mean, 1))),
                axis=1, keepdims=True)
            sleep_raster_mean = sleep_raster_mean / max_fir[:, :sleep_raster_mean.shape[1]]
            pre_raster_mean = pre_raster_mean / max_fir[:, 0]
            post_raster_mean = post_raster_mean / max_fir[:, 0]

        if smooth_sleep:
            # smooth down sampled rasters
            sleep_raster_mean_smooth = []
            for cell_arr in sleep_raster_mean:
                s = moving_average(a=cell_arr, n=n_smoothing)
                sleep_raster_mean_smooth.append(s)
            sleep_raster_mean = np.array(sleep_raster_mean_smooth)

        sleep_raster_mean_stable = sleep_raster_mean[stable_cell_ids, :]
        pre_raster_mean_stable = pre_raster_mean[stable_cell_ids]
        post_raster_mean_stable = post_raster_mean[stable_cell_ids]
        # sort stable cells using k-means
        if sorting == "k_means_sorting":
            kmeans = KMeans(n_clusters=10).fit(X=sleep_raster_mean_stable)
            k_labels = kmeans.labels_
            sorted_ind = k_labels.argsort()
            sleep_raster_mean_stable_sorted = sleep_raster_mean_stable[sorted_ind[::-1]]
            pre_raster_mean_stable_sorted = pre_raster_mean_stable[sorted_ind[::-1]]
            post_raster_mean_stable_sorted = post_raster_mean_stable[sorted_ind[::-1]]
        # sort cells according to awake firing rate
        elif sorting == "awake_average":
            sorted_ids = pre_raster_mean_stable.argsort()
            sleep_raster_mean_stable_sorted = sleep_raster_mean_stable[sorted_ids, :]
            pre_raster_mean_stable_sorted = pre_raster_mean_stable[sorted_ids]
            post_raster_mean_stable_sorted = post_raster_mean_stable[sorted_ids]

        # sort increasing cells using k-means
        sleep_raster_mean_increasing = sleep_raster_mean[increase_cell_ids]
        pre_raster_mean_increasing = pre_raster_mean[increase_cell_ids]
        post_raster_mean_increasing = post_raster_mean[increase_cell_ids]
        sleep_raster_mean_increasing = np.nan_to_num(sleep_raster_mean_increasing)
        if sorting == "k_means_sorting":
            kmeans = KMeans(n_clusters=10).fit(X=sleep_raster_mean_increasing)
            k_labels = kmeans.labels_
            sorted_ind = k_labels.argsort()
            sleep_raster_mean_increasing_sorted = sleep_raster_mean_increasing[sorted_ind[::-1]]
            pre_raster_mean_increasing_sorted = pre_raster_mean_increasing[sorted_ind[::-1]]
            post_raster_mean_increasing_sorted = post_raster_mean_increasing[sorted_ind[::-1]]
        elif sorting == "awake_average":
            sorted_ids = pre_raster_mean_increasing.argsort()
            sleep_raster_mean_increasing_sorted = sleep_raster_mean_increasing[sorted_ids,:]
            pre_raster_mean_increasing_sorted = pre_raster_mean_increasing[sorted_ids]
            post_raster_mean_increasing_sorted = post_raster_mean_increasing[sorted_ids]

        # sort decreasing cells using k-means
        sleep_raster_mean_decreasing = sleep_raster_mean[decrease_cell_ids]
        pre_raster_mean_decreasing = pre_raster_mean[decrease_cell_ids]
        post_raster_mean_decreasing = post_raster_mean[decrease_cell_ids]
        if sorting == "k_means_sorting":
            kmeans = KMeans(n_clusters=10).fit(X=sleep_raster_mean_decreasing)
            k_labels = kmeans.labels_
            sorted_ind = k_labels.argsort()
            sleep_raster_mean_decreasing_sorted = sleep_raster_mean_decreasing[sorted_ind[::-1]]
            pre_raster_mean_decreasing_sorted = pre_raster_mean_decreasing[sorted_ind[::-1]]
            post_raster_mean_decreasing_sorted = post_raster_mean_decreasing[sorted_ind[::-1]]
        elif sorting == "awake_average":
            sorted_ids = pre_raster_mean_decreasing.argsort()
            sleep_raster_mean_decreasing_sorted = sleep_raster_mean_decreasing[sorted_ids, :]
            pre_raster_mean_decreasing_sorted = pre_raster_mean_decreasing[sorted_ids]
            post_raster_mean_decreasing_sorted = post_raster_mean_decreasing[sorted_ids]

        # stack them back together for plotting
        pre_raster_mean_sorted = np.hstack(
            (pre_raster_mean_stable_sorted, pre_raster_mean_increasing_sorted, pre_raster_mean_decreasing_sorted))
        post_raster_mean_sorted = np.hstack(
            (post_raster_mean_stable_sorted, post_raster_mean_increasing_sorted, post_raster_mean_decreasing_sorted))

        sleep_raster_mean_sorted = np.vstack((sleep_raster_mean_stable_sorted, sleep_raster_mean_increasing_sorted,
                                               sleep_raster_mean_decreasing_sorted))

        if save_fig or plotting:

            # transform data for better visualization
            # ----------------------------------------------------------------------------------------------------------
            # pre_data = np.expand_dims(pre_raster_mean_sorted, 1) ** 2
            # post_data = np.expand_dims(post_raster_mean_sorted, 1) ** 2
            # sleep_data = sleep_raster_mean_sorted ** 2

            pre_data = np.expand_dims(pre_raster_mean_sorted, 1)
            post_data = np.expand_dims(post_raster_mean_sorted, 1)
            sleep_data = sleep_raster_mean_sorted

            v_max = np.max(np.hstack((pre_data.flatten(), sleep_data.flatten(),
                                      post_data.flatten())))
            v_min = np.min(np.hstack((pre_data.flatten(), sleep_data.flatten(),
                                      post_data.flatten())))

            if save_fig:
                plt.style.use('default')

            fig = plt.figure(figsize=(8, 6))
            gs = fig.add_gridspec(15, 20)
            ax1 = fig.add_subplot(gs[:-3, :2])
            ax2 = fig.add_subplot(gs[:-3, 2:-2])
            ax3 = fig.add_subplot(gs[:-3, -2:])
            ax4 = fig.add_subplot(gs[-1, :10])
            if log_scale:
                ax1.imshow(pre_data, norm=LogNorm(0.01, v_max), interpolation='nearest', aspect='auto')
            else:
                ax1.imshow(pre_data, vmin=v_min, vmax=v_max, interpolation='nearest', aspect='auto')

            ax1.hlines(stable_cell_ids.shape[0], -0.5, 0.5, color="red")
            ax1.hlines(stable_cell_ids.shape[0] + increase_cell_ids.shape[0], -0.5, 0.5, color="red")
            ax1.set_xticks([])
            ax1.set_xlabel("Cells")

            if log_scale:
                cax = ax2.imshow(sleep_data, norm=LogNorm(0.01, v_max),
                               interpolation='none', aspect='auto')
            else:
                cax = ax2.imshow(sleep_data, vmin=v_min, vmax=v_max, interpolation='nearest', aspect='auto')
            ax2.hlines(stable_cell_ids.shape[0], -0.5, sleep_raster_mean_sorted.shape[1] - 0.5, color="red")
            ax2.hlines(stable_cell_ids.shape[0] + increase_cell_ids.shape[0], -0.5,
                       sleep_raster_mean_sorted.shape[1] - 0.5,
                       color="red")
            ax2.set_yticks([])
            ax2.set_xticks([0, 0.33 * sleep_raster_mean_sorted.shape[1], 0.66 * sleep_raster_mean_sorted.shape[1],
                            sleep_raster_mean_sorted.shape[1] - 0.5])
            ax2.set_xticklabels(
                ["0", str(int(np.round(duration_sleep_h * 0.33, 0))), str(int(np.round(duration_sleep_h * 0.66, 0))),
                 str(duration_sleep_h)])
            ax2.set_xlabel("Sleep duration (h)")

            if log_scale:
                ax3.imshow(post_data, norm=LogNorm(0.01, v_max), interpolation='nearest',
                           aspect='auto')
            else:
                ax3.imshow(post_data, vmin=v_min, vmax=v_max, interpolation='nearest',
                           aspect='auto')
            ax3.hlines(stable_cell_ids.shape[0], -0.5, 0.5, color="red")
            ax3.hlines(stable_cell_ids.shape[0] + increase_cell_ids.shape[0], -0.5, 0.5, color="red")
            ax3.set_yticks([])
            ax3.set_xticks([])
            a = fig.colorbar(mappable=cax, cax=ax4, orientation="horizontal")
            a.ax.set_xlabel("Firing rates (Hz)")
            # a.ax.set_xticklabels(["0", "1"])
            # a.ax.set_xlabel("Normalized firing rate")
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "cell_classification.svg"), transparent="True")
            else:
                plt.show()
        else:

            return sleep_raster_mean_stable, pre_raster_mean_stable, post_raster_mean_stable, \
                   sleep_raster_mean_decreasing, pre_raster_mean_decreasing, post_raster_mean_decreasing, \
                   sleep_raster_mean_increasing, pre_raster_mean_increasing, post_raster_mean_increasing,

    def plot_likelihoods_ising(self, save_fig=False):

        # get pre map for dimensions
        pre_template = self.pre.load_glm_awake(plotting=False)
        post_template = self.post.load_glm_awake(plotting=False)

        pre_occ = self.pre.get_occ_map(spatial_resolution=5)
        post_occ = self.post.get_occ_map(spatial_resolution=5)

        pre_prob, post_prob, event_times, swr_to_nrem = self.long_sleep[0].decode_activity_using_pre_post(
            template_type="ising",
            part_to_analyze="rem")

        print("HERE")
        # use only second time bin to plot likelihoods
        pre_like = pre_prob[0][1, :]
        post_like = post_prob[0][1, :]

        pre_like_2d = np.reshape(pre_like, (pre_template.shape[1], pre_template.shape[2]))
        post_like_2d = np.reshape(post_like, (post_template.shape[1], post_template.shape[2]))

        pre_like_2d[pre_occ == 0] = np.nan
        post_like_2d[post_occ == 0] = np.nan
        plt.style.use('default')
        plt.imshow(pre_like_2d, cmap="YlOrRd")
        plt.title("PRE")
        plt.colorbar()
        plt.rcParams['svg.fonttype'] = 'none'
        if save_fig:
            plt.savefig(os.path.join(save_path, "pre_likeli_ising.svg"))
        else:
            plt.show()

        plt.imshow(post_like_2d, cmap="YlOrRd")
        plt.title("POST")
        plt.colorbar()
        plt.rcParams['svg.fonttype'] = 'none'
        if save_fig:
            plt.savefig(os.path.join(save_path, "post_likeli_ising.svg"))
        else:
            plt.show()

    def predict_pre_or_post(self):

        pre_act, _, _ = self.pre.get_raster_location_speed()
        post_act, _, _ = self.post.get_raster_location_speed()

        y = np.hstack((np.zeros(pre_act.shape[1]), np.ones(post_act.shape[1])))
        x = np.hstack((pre_act, post_act))

        per_ind = np.random.permutation(np.arange(x.shape[1]))
        x_shuffled = x[:, per_ind].T
        y_shuffled = y[per_ind]

        # split into test and training
        x_train = x_shuffled[:int(x_shuffled.shape[0] * 0.7)]
        x_test = x_shuffled[int(x_shuffled.shape[0] * 0.7):]

        y_train = y_shuffled[:int(y_shuffled.shape[0] * 0.7)]
        y_test = y_shuffled[int(y_shuffled.shape[0] * 0.7):]

        clf = svm.SVC()
        clf.fit(x_train, y_train)

        print(clf.score(x_test, y_test))

        # get sleep data
        raster_sleep = self.get_sleep_raster()

        predicted = clf.predict(raster_sleep.T)
        plt.plot(moving_average(a=predicted, n=10000))
        plt.show()

    def mode_occurence_pre_sleep_post(self):

        m_sleep, occurrences_sleep = self.phmm_mode_occurrence(part_to_analyze="nrem", data_length=1)

        m_pre, _ = self.pre.phmm_mode_occurrence()
        m_post, occurrences_post = self.post.phmm_mode_occurrence()

        plt.scatter(m_pre, occurrences_post)
        plt.xlabel("m_pre")
        plt.ylabel("occurrences sleep")
        plt.title(pearsonr(m_pre, occurrences_sleep))
        plt.show()

    def sparsity_and_distance_of_modes(self, metric="cosine", plotting=True):

        sparsity_reactivations_rem_pre, sparsity_reactivations_nrem_pre, \
            distance_reactivations_rem_pre, distance_reactivations_nrem_pre = \
            self.memory_drift_sparsity_and_distance_of_modes(pre_or_post="pre", n_segments=1, metric=metric)

        sparsity_reactivations_rem_post, sparsity_reactivations_nrem_post, \
            distance_reactivations_rem_post, distance_reactivations_nrem_post = \
            self.memory_drift_sparsity_and_distance_of_modes(pre_or_post="post", n_segments=1, metric=metric)

        sparsity_pre, distance_pre, = \
            self.pre.sparsity_and_distance_of_modes(use_spike_bins=True, metric=metric)

        sparsity_post, distance_post, = \
            self.post.sparsity_and_distance_of_modes(use_spike_bins=True, metric=metric)

        # plot for distance
        # clean up results
        distance_pre = distance_pre.flatten()[~np.isnan(distance_pre.flatten())]
        distance_post = distance_post.flatten()[~np.isnan(distance_post.flatten())]
        distance_reactivations_rem_pre = \
            distance_reactivations_rem_pre.flatten()[~np.isnan(distance_reactivations_rem_pre.flatten())]
        distance_reactivations_nrem_pre = \
            distance_reactivations_nrem_pre.flatten()[~np.isnan(distance_reactivations_nrem_pre.flatten())]
        distance_reactivations_rem_post = \
            distance_reactivations_rem_post.flatten()[~np.isnan(distance_reactivations_rem_post.flatten())]
        distance_reactivations_nrem_post = \
            distance_reactivations_nrem_post.flatten()[~np.isnan(distance_reactivations_nrem_post.flatten())]

        if plotting:
            plt.figure(figsize=(12, 4))
            c = "white"
            res = [distance_pre, distance_reactivations_rem_pre, distance_reactivations_nrem_pre,
                   distance_reactivations_rem_post, distance_reactivations_nrem_post, distance_post]
            bplot = plt.boxplot(res, positions=[1, 2, 3, 4, 5, 6], patch_artist=True,
                                labels=["PRE_modes", "PRE_modes_REM", "PRE_modes_NREM", "POST_modes_REM",
                                        "POST_modes_NREM", "POST_modes"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False, widths=0.2)
            plt.ylabel("Distance (cos)")
            plt.ylim(0, 2)
            plt.hlines(1.8, 1, 6)
            if mannwhitneyu(distance_pre, distance_post)[1] < 0.001:
                plt.text(3.5, 1.85, "***")
            elif mannwhitneyu(distance_pre, distance_post)[1] < 0.01:
                plt.text(3.5, 1.85, "**")
            elif mannwhitneyu(distance_pre, distance_post)[1] < 0.05:
                plt.text(3.5, 1.85, "*")
            else:
                plt.text(3.5, 1.85, "n.s.")
            plt.hlines(1.7, 1, 2)
            if mannwhitneyu(distance_pre, distance_reactivations_rem_pre)[1] < 0.001:
                plt.text(1.5, 1.71, "***")
            elif mannwhitneyu(distance_pre, distance_reactivations_rem_pre)[1] < 0.01:
                plt.text(1.5, 1.71, "**")
            elif mannwhitneyu(distance_pre, distance_reactivations_rem_pre)[1] < 0.05:
                plt.text(1.5, 1.71, "*")
            else:
                plt.text(3.5, 1.71, "n.s.")

            plt.hlines(1.6, 1, 3)
            if mannwhitneyu(distance_pre, distance_reactivations_nrem_pre)[1] < 0.001:
                plt.text(2, 1.6, "***")
            elif mannwhitneyu(distance_pre, distance_reactivations_nrem_pre)[1] < 0.01:
                plt.text(2, 1.61, "**")
            elif mannwhitneyu(distance_pre, distance_reactivations_nrem_pre)[1] < 0.05:
                plt.text(2, 1.61, "*")
            else:
                plt.text(2, 1.61, "n.s.")

            plt.hlines(1.7, 5, 6)
            if mannwhitneyu(distance_post, distance_reactivations_nrem_post)[1] < 0.001:
                plt.text(5.5, 1.71, "***")
            elif mannwhitneyu(distance_post, distance_reactivations_nrem_post)[1] < 0.01:
                plt.text(5.5, 1.71, "**")
            elif mannwhitneyu(distance_post, distance_reactivations_nrem_post)[1] < 0.05:
                plt.text(5.5, 1.71, "*")
            else:
                plt.text(5.5, 1.71, "n.s.")

            plt.hlines(1.6, 4, 6)
            if mannwhitneyu(distance_post, distance_reactivations_rem_post)[1] < 0.001:
                plt.text(5, 1.6, "***")
            elif mannwhitneyu(distance_post, distance_reactivations_rem_post)[1] < 0.01:
                plt.text(5, 1.61, "**")
            elif mannwhitneyu(distance_post, distance_reactivations_rem_post)[1] < 0.05:
                plt.text(5, 1.61, "*")
            else:
                plt.text(5, 1.61, "n.s.")

            plt.tight_layout()
            plt.grid(color="dimgrey", axis="y")
            plt.show()

        # plot for sparsity
        # clean up results
        sparsity_pre = sparsity_pre.flatten()[~np.isnan(sparsity_pre.flatten())]
        sparsity_post = sparsity_post.flatten()[~np.isnan(sparsity_post.flatten())]
        sparsity_reactivations_rem_pre = \
            sparsity_reactivations_rem_pre.flatten()[~np.isnan(sparsity_reactivations_rem_pre.flatten())]
        sparsity_reactivations_nrem_pre = \
            sparsity_reactivations_nrem_pre.flatten()[~np.isnan(sparsity_reactivations_nrem_pre.flatten())]
        sparsity_reactivations_rem_post = \
            sparsity_reactivations_rem_post.flatten()[~np.isnan(sparsity_reactivations_rem_post.flatten())]
        sparsity_reactivations_nrem_post = \
            sparsity_reactivations_nrem_post.flatten()[~np.isnan(sparsity_reactivations_nrem_post.flatten())]

        if plotting:
            plt.figure(figsize=(12,4))
            c = "white"
            res = [sparsity_pre, sparsity_reactivations_rem_pre, sparsity_reactivations_nrem_pre,
                   sparsity_reactivations_rem_post, sparsity_reactivations_nrem_post, sparsity_post]
            bplot = plt.boxplot(res, positions=[1, 2, 3, 4, 5, 6], patch_artist=True,
                                labels=["PRE", "PRE_modes_REM", "PRE_modes_NREM", "POST_modes_REM",
                                        "POST_modes_NREM", "POST"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False)
            y_min, y_max = plt.gca().get_ylim()
            y_span = y_max-y_min
            plt.ylabel("Sparsity")
            plt.ylim(y_min, y_min+1.5*y_span)
            plt.hlines(y_min+1.4*y_span, 1, 6)
            if mannwhitneyu(sparsity_pre, sparsity_post)[1] < 0.001:
                plt.text(3.5, y_min+1.42*y_span, "***")
            elif mannwhitneyu(sparsity_pre, sparsity_post)[1] < 0.01:
                plt.text(3.5, y_min+1.42*y_span, "**")
            elif mannwhitneyu(sparsity_pre, sparsity_post)[1] < 0.05:
                plt.text(3.5, y_min+1.42*y_span, "*")
            else:
                plt.text(3.5, y_min+1.42*y_span, "n.s.")
            plt.hlines(y_min+1.3*y_span, 1, 2)
            if mannwhitneyu(sparsity_pre, sparsity_reactivations_rem_pre)[1] < 0.001:
                plt.text(1.5,y_min+1.32*y_span, "***")
            elif mannwhitneyu(sparsity_pre, sparsity_reactivations_rem_pre)[1] < 0.01:
                plt.text(1.5, y_min+1.32*y_span, "**")
            elif mannwhitneyu(sparsity_pre, sparsity_reactivations_rem_pre)[1] < 0.05:
                plt.text(1.5, y_min+1.32*y_span, "*")
            else:
                plt.text(3.5, y_min+1.32*y_span, "n.s.")

            plt.hlines(y_min+1.2*y_span, 1, 3)
            if mannwhitneyu(sparsity_pre, sparsity_reactivations_nrem_pre)[1] < 0.001:
                plt.text(2, y_min+1.22*y_span, "***")
            elif mannwhitneyu(sparsity_pre, sparsity_reactivations_nrem_pre)[1] < 0.01:
                plt.text(2, y_min+1.22*y_span, "**")
            elif mannwhitneyu(sparsity_pre, sparsity_reactivations_nrem_pre)[1] < 0.05:
                plt.text(2, y_min+1.22*y_span, "*")
            else:
                plt.text(2, y_min+1.22*y_span, "n.s.")

            plt.hlines(y_min+1.3*y_span, 5, 6)
            if mannwhitneyu(sparsity_post, sparsity_reactivations_nrem_post)[1] < 0.001:
                plt.text(5.5, y_min+1.32*y_span, "***")
            elif mannwhitneyu(sparsity_post, sparsity_reactivations_nrem_post)[1] < 0.01:
                plt.text(5.5, y_min+1.32*y_span, "**")
            elif mannwhitneyu(sparsity_post, sparsity_reactivations_nrem_post)[1] < 0.05:
                plt.text(5.5, y_min+1.32*y_span, "*")
            else:
                plt.text(5.5, y_min+1.32*y_span, "n.s.")

            plt.hlines(y_min+1.2*y_span, 4, 6)
            if mannwhitneyu(sparsity_post, sparsity_reactivations_rem_post)[1] < 0.001:
                plt.text(5, y_min+1.22*y_span, "***")
            elif mannwhitneyu(sparsity_post, sparsity_reactivations_rem_post)[1] < 0.01:
                plt.text(5, y_min+1.22*y_span, "**")
            elif mannwhitneyu(sparsity_post, sparsity_reactivations_rem_post)[1] < 0.05:
                plt.text(5, y_min+1.22*y_span, "*")
            else:
                plt.text(5, y_min+1.22*y_span, "n.s.")
            plt.tight_layout()
            plt.grid(color="dimgrey", axis="y")
            plt.show()

        return sparsity_pre, sparsity_post, sparsity_reactivations_rem_pre, sparsity_reactivations_nrem_pre, \
            sparsity_reactivations_rem_post, sparsity_reactivations_nrem_post, distance_pre, distance_post, \
            distance_reactivations_rem_pre, distance_reactivations_nrem_pre, distance_reactivations_rem_post, \
            distance_reactivations_nrem_post

    def memory_drift_control_poisson(self):

        # get mean firing per cell from pre and post
        pre_raster = self.pre.get_raster()
        post_raster = self.post.get_raster()

        # mean: #spikes / 100 ms bin
        pre_mean = np.mean(pre_raster, axis=1)
        post_mean = np.mean(post_raster, axis=1)

        # interpolate from pre to post firing
        nr_data_points = 100
        interpol_mean_fir = np.zeros((pre_mean.shape[0], nr_data_points))

        for i, (pre, post) in enumerate(zip(pre_mean, post_mean)):
            interpol_mean_fir[i, :] = np.interp(np.linspace(0, 1, nr_data_points), np.array([0, 1]),
                                                np.array([pre, post]))

        # sample from mean using log normal
        interpol_mean_fir = np.random.lognormal(interpol_mean_fir)

        # generate 12-spike bins
        spike_bins = constant_nr_spike_bin_from_mean_firing(mean_firing_vector=interpol_mean_fir, return_mean_vector=True)

        # get means of pre and post modes + compression factor
        pre_file_name = self.session_params.default_pre_phmm_model
        post_file_name = self.session_params.default_post_phmm_model
        compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            pre_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        pre_mode_means = pre_model_dic.means_

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            post_model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        post_mode_means = post_model_dic.means_

        # do the decoding using pre and post model
        pre_likeli = decode_using_phmm_modes(mode_means=pre_mode_means,
                                               event_spike_rasters=[spike_bins],
                                               compression_factor=compression_factor,
                                               cell_selection="all")[0]

        max_pre_likeli = np.max(pre_likeli, axis=1)

        post_likeli = decode_using_phmm_modes(mode_means=post_mode_means,
                                               event_spike_rasters=[spike_bins],
                                               compression_factor=compression_factor,
                                               cell_selection="all")[0]

        max_post_likeli = np.max(post_likeli, axis=1)

        sim_ratio = (max_post_likeli - max_pre_likeli)/(max_post_likeli + max_pre_likeli)

        plt.plot(sim_ratio)
        plt.xlabel("relative time")
        plt.xticks(ticks=np.arange(sim_ratio.shape[0])[::10], labels=np.round(np.linspace(0, 1, nr_data_points)[::10],1))
        plt.ylabel("sim_ratio")
        plt.tight_layout()
        plt.show()

        dat = moving_average(sim_ratio, 20)
        plt.plot(dat)
        plt.xlabel("relative time")
        plt.xticks(ticks=np.arange(dat.shape[0])[::10], labels=np.round(np.linspace(0, 1, dat.shape[0])[::10],1))
        plt.ylabel("sim_ratio - smooth")
        plt.tight_layout()
        plt.ylim(-1,1)
        plt.show()

        print("HERE")
    # </editor-fold>


class SleepLongSleep(LongSleep):
    """Class for long sleep"""

    def __init__(self, sleep_data_obj, sleep, params, session_params=None):

        # initialize parent classes
        LongSleep.__init__(self, sleep_data_obj=sleep_data_obj, params=params, session_params=session_params)

        # add sleep
        self.sleep = sleep

        self.cell_type = sleep_data_obj.get_cell_type()

    def memory_drift_sleep_long_sleep(self, n_moving_average_pop_vec=2000,plotting=False, smoothing=False,
                                    save_fig=False, model="phmm", z_score=False, only_decoded=True):

        # get spike binned rasters from long sleep
        event_spike_rasters_long_sleep = []
        for l_s in self.long_sleep:

            # get original bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_ = (
                l_s.get_spike_binned_raster_sleep_phase(sleep_phase="nrem"))

            #
            # _, event_spike_rasters_jittered_ = (
            #     l_s.get_spike_binned_raster_combined_sleep_phases_jittered(nr_spikes_per_jitter_window=np.hstack(event_spike_rasters_).shape[1]))

            event_spike_rasters_long_sleep.append(np.hstack(event_spike_rasters_))

        all_spike_rasters = np.hstack(event_spike_rasters_long_sleep)

        # get spike binned rasters from pre
        _, spike_rasters_fam = self.sleep.get_spike_binned_raster_sleep_phase(sleep_phase="nrem")

        if model == "phmm":
            # get means of pre and post modes + compression factor
            pre_file_name = self.session_params.default_pre_phmm_model
            post_file_name = self.session_params.default_post_phmm_model
            compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

            # get pre and post model to do decoding
            with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
                pre_model_dic = pickle.load(f)
            # get means of model (lambdas) for decoding
            pre_mode_means = pre_model_dic.means_

            with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
                post_model_dic = pickle.load(f)
            # get means of model (lambdas) for decoding
            post_mode_means = post_model_dic.means_

            # do decoding and plot result
            # ----------------------------------------------------------------------------------------------------------
            # do the decoding using pre and post model
            pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                      event_spike_rasters=all_spike_rasters,
                                                      compression_factor=compression_factor)

            post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                       event_spike_rasters=all_spike_rasters,
                                                       compression_factor=compression_factor)


            # do decoding and plot result for familiar sleep
            # ----------------------------------------------------------------------------------------------------------
            # do the decoding using pre and post model
            pre_likeli_fam = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                          event_spike_rasters=spike_rasters_fam,
                                                          compression_factor=compression_factor)

            post_likeli_fam = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                           event_spike_rasters=spike_rasters_fam,
                                                           compression_factor=compression_factor)

        elif model == "ising":

            pre_file_name = self.session_params.default_pre_ising_model
            post_file_name = self.session_params.default_post_ising_model

            with open(self.params.pre_proc_dir + 'awake_ising_maps/' + pre_file_name + '.pkl',
                      'rb') as f:
                pre_model_dic = pickle.load(f)
            pre_template_map = pre_model_dic["res_map"]

            with open(self.params.pre_proc_dir + 'awake_ising_maps/' + post_file_name + '.pkl',
                      'rb') as f:
                post_model_dic = pickle.load(f)
            post_template_map = post_model_dic["res_map"]


            # get time_bin_size of encoding
            time_bin_size_encoding = pre_model_dic["time_bin_size"]

            # load correct compression factor (as defined in parameter file of the session)
            if time_bin_size_encoding == 0.01:
                compression_factor = \
                    np.round(self.session_params.sleep_compression_factor_12spikes_100ms * 10, 3)
            elif time_bin_size_encoding == 0.1:
                compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

            pre_likeli_j = decode_using_ising_map(template_map=pre_template_map,
                                                  event_spike_rasters=[spike_rasters_pre],
                                                  compression_factor=compression_factor,
                                                  cell_selection="all")
            post_likeli_j = decode_using_ising_map(template_map=post_template_map,
                                                   event_spike_rasters=[spike_rasters_pre],
                                                   compression_factor=compression_factor,
                                                   cell_selection="all")
            pre_likeli = decode_using_ising_map(template_map=pre_template_map,
                                                event_spike_rasters=[all_spike_rasters],
                                                compression_factor=compression_factor,
                                                cell_selection="all")
            post_likeli = decode_using_ising_map(template_map=post_template_map,
                                                 event_spike_rasters=[all_spike_rasters],
                                                 compression_factor=compression_factor,
                                                 cell_selection="all")

        # make arrays
        pre_likeli = np.vstack(pre_likeli)
        post_likeli = np.vstack(post_likeli)
        pre_likeli_fam = np.vstack(pre_likeli_fam)
        post_likeli_fam = np.vstack(post_likeli_fam)

        # compute maximum
        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)
        max_pre_likeli_fam = np.max(pre_likeli_fam, axis=1)
        max_post_likeli_fam = np.max(post_likeli_fam, axis=1)

        # look at distribution of decoded PRE modes
        # mode_decoded_pre, mode_decoded_pre_counts = np.unique(np.argmax(pre_likeli, axis=1), return_counts=True)
        # mode_decoded_pre_fam, mode_decoded_pre_counts_fam = np.unique(np.argmax(pre_likeli_fam, axis=1),
        #                                                                       return_counts=True)
        # modes_original_pre =np.zeros(pre_likeli.shape[1])
        # modes_original_pre[mode_decoded_pre] = mode_decoded_pre_counts/np.sum(mode_decoded_pre_counts)
        # modes_fam_pre =np.zeros(pre_likeli.shape[1])
        # modes_fam_pre[mode_decoded_pre_fam] = mode_decoded_pre_counts_fam/np.sum(mode_decoded_pre_counts_fam)
        #
        # # look at distribution of decoded POST modes
        # mode_decoded_post, mode_decoded_post_counts = np.unique(np.argmax(post_likeli, axis=1), return_counts=True)
        # mode_decoded_post_fam, mode_decoded_post_counts_fam = np.unique(np.argmax(post_likeli_fam, axis=1),
        #                                                                         return_counts=True)
        # modes_original_post =np.zeros(post_likeli.shape[1])
        # modes_original_post[mode_decoded_post] = mode_decoded_post_counts/np.sum(mode_decoded_post_counts)
        # modes_fam_post =np.zeros(post_likeli.shape[1])
        # modes_fam_post[mode_decoded_post_fam] = mode_decoded_post_counts_fam/np.sum(mode_decoded_post_counts_fam)
        #
        # plt.scatter(modes_original_pre, modes_fam_pre)
        # plt.xlabel("long_sleep")
        # plt.ylabel("fam_sleep")
        # plt.title("PRE")
        # plt.tight_layout()
        # plt.show()
        #
        # plt.scatter(modes_original_post, modes_fam_post)
        # plt.xlabel("long_sleep")
        # plt.ylabel("fam_sleep")
        # plt.title("POST")
        # plt.tight_layout()
        # plt.show()

        # compute drift score
        sim_ratio = (max_post_likeli - max_pre_likeli) / (max_post_likeli + max_pre_likeli)
        sim_ratio_fam = (max_post_likeli_fam - max_pre_likeli_fam) / (max_post_likeli_fam + max_pre_likeli_fam)

        # take the log
        max_pre_log_likeli = np.log(max_pre_likeli)
        max_pre_log_likeli_fam = np.log(max_pre_likeli_fam)
        max_post_log_likeli = np.log(max_post_likeli)
        max_post_log_likeli_fam = np.log(max_post_likeli_fam)

        # only use when decoded
        pre_decoded_long_sleep = max_pre_likeli > max_post_likeli
        pre_decoded_fam_sleep = max_pre_likeli_fam > max_post_likeli_fam
        frac_pre_decoded_long_sleep = np.count_nonzero(pre_decoded_long_sleep)/pre_decoded_long_sleep.shape[0]
        frac_pre_decoded_fam_sleep = np.count_nonzero(pre_decoded_fam_sleep)/pre_decoded_fam_sleep.shape[0]
        #
        if only_decoded:
            max_pre_log_likeli = max_pre_log_likeli[pre_decoded_long_sleep]
            max_post_log_likeli = max_post_log_likeli[~pre_decoded_long_sleep]
            max_pre_log_likeli_fam = max_pre_log_likeli_fam[pre_decoded_fam_sleep]
            max_post_log_likeli_fam = max_post_log_likeli_fam[~pre_decoded_fam_sleep]
        #
        # print("likelihood size")
        # print(max_post_log_likeli.shape)
        # print(max_post_log_likeli_fam.shape)

        if smoothing:
            # apply smoothing
            max_pre_log_likeli = uniform_filter1d(max_pre_log_likeli, int(max_pre_log_likeli.shape[0]/10))
            max_pre_log_likeli_fam = uniform_filter1d(max_pre_log_likeli_fam, int(max_pre_log_likeli_fam.shape[0]/10))
            max_post_log_likeli = uniform_filter1d(max_post_log_likeli, int(max_post_log_likeli.shape[0]/10))
            max_post_log_likeli_fam = uniform_filter1d(max_post_log_likeli_fam, int(max_post_log_likeli_fam.shape[0]/10))
        #
        # max_pre_log_likeli = uniform_filter1d(max_pre_log_likeli, n_moving_average_pop_vec)
        # max_pre_log_likeli_fam = uniform_filter1d(max_pre_log_likeli_fam, n_moving_average_pop_vec)
        # max_post_log_likeli = uniform_filter1d(max_post_log_likeli, n_moving_average_pop_vec)
        # max_post_log_likeli_fam = uniform_filter1d(max_post_log_likeli_fam, n_moving_average_pop_vec)
        #

        if plotting or save_fig:

            if save_fig:
                plt.figure(figsize=(4,3))
                plt.style.use('default')
            plt.plot(np.linspace(0,1,moving_average(sim_ratio, n_moving_average_pop_vec).shape[0]),
                     moving_average(sim_ratio, n_moving_average_pop_vec), label="long_sleep", color="grey")
            plt.plot(np.linspace(0,1,moving_average(sim_ratio_fam, int(n_moving_average_pop_vec/10)).shape[0]),
                     moving_average(sim_ratio_fam, int(n_moving_average_pop_vec/10)), label="fam_sleep", color="yellow")
            plt.xlabel("Normalized duration")
            plt.ylabel("Drift score")
            plt.legend()
            plt.tight_layout()
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "drift_score_equalized"+self.session_name+".svg"), transparent="True")
            else:
                plt.show()

            plt.figure(figsize=(12,8))
            plt.subplot(1,2, 1)
            plt.plot(max_pre_log_likeli_s, color="white", label="PRE long_sleep")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_pre_log_likeli_pre_s, color="salmon", linestyle="--", label="PRE pre_sleep")
            plt.plot(max_pre_log_likeli_fam_s, color="lightblue", linestyle="--", label="PRE fam_sleep")
            plt.ylabel("Max log likelihood")
            plt.legend()
            plt.subplot(1,2, 2)
            plt.plot(max_post_log_likeli_s, color="white", label="POST long_sleep")
            plt.plot(max_post_log_likeli_fam_s, color="lightblue", linestyle="--", label="POST fam_sleep", alpha=0.8)
            plt.ylabel("Max log likelihood")
            plt.legend()
            plt.tight_layout()
            plt.show()

        if z_score:
            # z-score per model --> to be able to lump data from different sessions together
            temp_pre = np.hstack((max_pre_log_likeli, max_pre_log_likeli_fam))
            temp_pre_z = zscore(temp_pre)
            max_pre_log_likeli_z = temp_pre_z[:max_pre_log_likeli.shape[0]]
            max_pre_log_likeli_fam_z = temp_pre_z[max_pre_log_likeli.shape[0]:]
            # post
            temp_post = np.hstack((max_post_log_likeli, max_post_log_likeli_fam))
            temp_post_z = zscore(temp_post)
            max_post_log_likeli_z = temp_post_z[:max_post_log_likeli.shape[0]]
            max_post_log_likeli_fam_z = temp_post_z[max_post_log_likeli.shape[0]:]
        else:
            max_pre_log_likeli_z = max_pre_log_likeli
            max_pre_log_likeli_fam_z = max_pre_log_likeli_fam
            max_post_log_likeli_z = max_post_log_likeli
            max_post_log_likeli_fam_z = max_post_log_likeli_fam

        return (max_pre_log_likeli_z, max_pre_log_likeli_fam_z, max_post_log_likeli_z,
                max_post_log_likeli_fam_z, sim_ratio, sim_ratio_fam, frac_pre_decoded_long_sleep,
                frac_pre_decoded_fam_sleep)


class SleepSleepLongSleep(LongSleep):
    """Class for long sleep"""

    def __init__(self, sleep_data_obj, sleep_1, sleep_2, params, session_params=None):

        # initialize parent classes
        LongSleep.__init__(self, sleep_data_obj=sleep_data_obj, params=params, session_params=session_params)

        # add sleep
        self.sleep_1 = sleep_1
        self.sleep_2 = sleep_2
        self.cell_type = sleep_data_obj.get_cell_type()


class CheeseboardLongSleep(LongSleep):
    """Class for long sleep"""

    def __init__(self, sleep_data_obj, cheeseboard, params, session_params=None):

        # initialize parent classes
        LongSleep.__init__(self, sleep_data_obj=sleep_data_obj, params=params, session_params=session_params)

        # add sleep
        self.cheeseboard = cheeseboard

        self.cell_type = sleep_data_obj.get_cell_type()

    def memory_drift_cheeseboard_vs_long_sleep(self, n_moving_average_pop_vec=2000,plotting=True, save_fig=False, model="phmm"):

        # get spike binned rasters from long sleep
        event_spike_rasters_long_sleep = []
        for l_s in self.long_sleep:

            # get original bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_ = (
                l_s.get_spike_binned_raster_combined_sleep_phases())

            #
            # _, event_spike_rasters_jittered_ = (
            #     l_s.get_spike_binned_raster_combined_sleep_phases_jittered(nr_spikes_per_jitter_window=np.hstack(event_spike_rasters_).shape[1]))

            event_spike_rasters_long_sleep.append(np.hstack(event_spike_rasters_))

        all_spike_rasters = np.hstack(event_spike_rasters_long_sleep)

        # get spike binned rasters from pre
        spike_rasters_cb = self.cheeseboard.get_spike_bin_raster(return_estimated_times=False)
        spike_rasters_cb = np.hstack(spike_rasters_cb)
        # delete events which are too short --> TODO
        # --------------------------------------------------------------------------------------------------------------
        # nr_bins = np.array([x.shape[1] for x in merged_events])

        if model == "phmm":
            # get means of pre and post modes + compression factor
            pre_file_name = self.session_params.default_pre_phmm_model
            post_file_name = self.session_params.default_post_phmm_model
            compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

            # get pre and post model to do decoding
            with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
                pre_model_dic = pickle.load(f)
            # get means of model (lambdas) for decoding
            pre_mode_means = pre_model_dic.means_

            with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
                post_model_dic = pickle.load(f)
            # get means of model (lambdas) for decoding
            post_mode_means = post_model_dic.means_

            # do decoding and plot result
            # ----------------------------------------------------------------------------------------------------------
            # do the decoding using pre and post model
            pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                      event_spike_rasters=all_spike_rasters,
                                                      compression_factor=compression_factor)

            post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                       event_spike_rasters=all_spike_rasters,
                                                       compression_factor=compression_factor)

            # do decoding and plot result for pre sleep
            # ----------------------------------------------------------------------------------------------------------
            # do the decoding using pre and post model
            pre_likeli_j = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                        event_spike_rasters=spike_rasters_cb,
                                                        compression_factor=compression_factor)

            post_likeli_j = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                         event_spike_rasters=spike_rasters_cb,
                                                         compression_factor=compression_factor)

        elif model == "ising":

            pre_file_name = self.session_params.default_pre_ising_model
            post_file_name = self.session_params.default_post_ising_model

            with open(self.params.pre_proc_dir + 'awake_ising_maps/' + pre_file_name + '.pkl',
                      'rb') as f:
                pre_model_dic = pickle.load(f)
            pre_template_map = pre_model_dic["res_map"]

            with open(self.params.pre_proc_dir + 'awake_ising_maps/' + post_file_name + '.pkl',
                      'rb') as f:
                post_model_dic = pickle.load(f)
            post_template_map = post_model_dic["res_map"]


            # get time_bin_size of encoding
            time_bin_size_encoding = pre_model_dic["time_bin_size"]

            # load correct compression factor (as defined in parameter file of the session)
            if time_bin_size_encoding == 0.01:
                compression_factor = \
                    np.round(self.session_params.sleep_compression_factor_12spikes_100ms * 10, 3)
            elif time_bin_size_encoding == 0.1:
                compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

            pre_likeli_j = decode_using_ising_map_fast(template_map=pre_template_map,
                                                  event_spike_rasters=spike_rasters_cb,
                                                  compression_factor=compression_factor,
                                                  cell_selection="all")
            post_likeli_j = decode_using_ising_map_fast(template_map=post_template_map,
                                                   event_spike_rasters=spike_rasters_cb,
                                                   compression_factor=compression_factor,
                                                   cell_selection="all")
            pre_likeli = decode_using_ising_map_fast(template_map=pre_template_map,
                                                event_spike_rasters=all_spike_rasters,
                                                compression_factor=compression_factor,
                                                cell_selection="all")
            post_likeli = decode_using_ising_map_fast(template_map=post_template_map,
                                                 event_spike_rasters=all_spike_rasters,
                                                 compression_factor=compression_factor,
                                                 cell_selection="all")

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        mean_pre_likeli = np.mean(np.log(pre_likeli), axis=1)
        mean_post_likeli = np.mean(np.log(post_likeli), axis=1)

        sim_ratio = (max_post_likeli - max_pre_likeli) / (max_post_likeli + max_pre_likeli)

        max_pre_likeli_j = np.max(pre_likeli_j, axis=1)
        max_post_likeli_j = np.max(post_likeli_j, axis=1)

        mean_pre_likeli_j = np.mean(np.log(pre_likeli_j), axis=1)
        mean_post_likeli_j = np.mean(np.log(post_likeli_j), axis=1)

        sim_ratio_j = (max_post_likeli_j - max_pre_likeli_j) / (max_post_likeli_j + max_pre_likeli_j)

        # look at distribution of decoded PRE modes
        mode_decoded_pre, mode_decoded_pre_counts = np.unique(np.argmax(pre_likeli, axis=1), return_counts=True)
        mode_decoded_pre_outside, mode_decoded_pre_counts_outside = np.unique(np.argmax(pre_likeli_j, axis=1),
                                                                              return_counts=True)
        modes_original =np.zeros(pre_likeli.shape[1])
        modes_original[mode_decoded_pre] = mode_decoded_pre_counts/np.sum(mode_decoded_pre_counts)
        modes_excluded =np.zeros(pre_likeli.shape[1])
        modes_excluded[mode_decoded_pre_outside] = mode_decoded_pre_counts_outside/np.sum(mode_decoded_pre_counts_outside)

        plt.scatter(modes_original, modes_excluded)
        plt.title("PRE mode decoding (fraction)")
        plt.xlabel("Long_sleep")
        plt.ylabel("Learning")
        plt.tight_layout()
        plt.show()


        # cut to same length for the ratio
        # --------------------------------------------------------------------------------------------------------------

        # take the log
        max_pre_log_likeli = np.log(max_pre_likeli)
        max_pre_log_likeli_j = np.log(max_pre_likeli_j)
        max_post_log_likeli = np.log(max_post_likeli)
        max_post_log_likeli_j = np.log(max_post_likeli_j)

        # interpolate values --> likelihoods
        # --------------------------------------------------------------------------------------------------------------
        max_pre_log_likeli_j = np.interp(np.linspace(0, len(max_pre_log_likeli_j), len(max_pre_log_likeli)),
                                         np.arange(len(max_pre_log_likeli_j)), max_pre_log_likeli_j)

        max_post_log_likeli_j = np.interp(np.linspace(0, len(max_post_log_likeli_j), len(max_post_log_likeli)),
                                          np.arange(len(max_post_log_likeli_j)), max_post_log_likeli_j)


        # interpolate values --> sim_ratio
        # --------------------------------------------------------------------------------------------------------------
        sim_ratio_j = np.interp(np.linspace(0, len(sim_ratio_j), len(sim_ratio)),
                                np.arange(len(sim_ratio_j)), sim_ratio_j)

        # apply smoothing
        max_pre_log_likeli_s = uniform_filter1d(max_pre_log_likeli, n_moving_average_pop_vec)
        max_pre_log_likeli_j_s = uniform_filter1d(max_pre_log_likeli_j, n_moving_average_pop_vec)
        max_post_log_likeli_s = uniform_filter1d(max_post_log_likeli, n_moving_average_pop_vec)
        max_post_log_likeli_j_s = uniform_filter1d(max_post_log_likeli_j, n_moving_average_pop_vec)

        # compute difference
        diff_pre = max_pre_log_likeli_s - max_pre_log_likeli_j_s
        diff_post = max_post_log_likeli_s - max_post_log_likeli_j_s


        if plotting or save_fig:

            c = "white"
            res = [mean_pre_likeli, mean_pre_likeli_j, mean_post_likeli, mean_post_likeli_j]
            bplot = plt.boxplot(res, positions=[1, 2, 3, 4], patch_artist=True,
                                labels=["PRE long_sleep", "PRE Learning", "POST long_sleep", "POST Learning"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            colors = ["magenta", 'magenta', "blue", "blue"]
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            plt.title("Mean log-likelihood")
            plt.ylabel("Mean log-likelihood")
            plt.grid(color="grey", axis="y")
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.show()


            if save_fig:
                plt.figure(figsize=(4,3))
                plt.style.use('default')
            plt.plot(np.linspace(0,1,moving_average(sim_ratio, n_moving_average_pop_vec).shape[0]),
                     moving_average(sim_ratio, n_moving_average_pop_vec), label="long_sleep", color="grey")
            plt.plot(np.linspace(0,1,moving_average(sim_ratio_j, n_moving_average_pop_vec).shape[0]),
                     moving_average(sim_ratio_j, n_moving_average_pop_vec), label="Learning", color="red", alpha=0.5)
            plt.xlabel("Normalized duration")
            plt.ylabel("Drift score")
            plt.legend()
            plt.tight_layout()
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "drift_score_equalized"+self.session_name+".svg"), transparent="True")
            else:
                plt.show()

            plt.figure(figsize=(12,8))
            plt.subplot(2,2,1)
            plt.plot(max_pre_log_likeli_s, color="red", label="PRE long_sleep")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_pre_log_likeli_j_s, color="salmon", linestyle="--", label="PRE Learning")
            plt.xticks([])
            plt.ylabel("Max log likelihood")
            plt.legend()
            plt.subplot(2,2,3)
            plt.plot(np.linspace(0,1, diff_pre.shape[0]), diff_pre)
            plt.ylabel("long_sleep - Learning")
            plt.xlabel("relative time")
            plt.subplot(2,2,2)
            plt.plot(max_post_log_likeli_s, color="white", label="POST long_sleep")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_post_log_likeli_j_s, color="lightblue", linestyle="--", label="POST Learning", alpha=0.8)
            plt.xticks([])
            plt.legend()
            plt.subplot(2,2,4)
            plt.plot(np.linspace(0,1, diff_post.shape[0]), diff_post)
            plt.ylabel("long_sleep - Learning")
            plt.xlabel("relative time")
            plt.tight_layout()
            plt.show()

        return sim_ratio, sim_ratio_j

    def theta_phase_preference_subsets(self, plotting=False, save_fig=False, return_diff=True, save_results=False):

        print("Computing phase preference Awake .. ")
        # get phase preference for cheeseboard
        cb_stable, cb_dec, cb_inc =self.cheeseboard.phase_preference_analysis(plotting=False)
        print("Done")

        long_sleep_stable = []
        long_sleep_dec = []
        long_sleep_inc = []
        # go through all sleep files and compute theta locking
        for i, sleep in enumerate(self.long_sleep):
            print("Computing phase preference sleep"+str(i)+" ... ")
            s,d,i = sleep.phase_preference_analysis(plotting=False)
            long_sleep_stable.append(s)
            long_sleep_dec.append(d)
            long_sleep_inc.append(i)
            del sleep
            print("Done")
        long_sleep_stable =np.vstack(long_sleep_stable)
        long_sleep_dec =np.vstack(long_sleep_dec)
        long_sleep_inc =np.vstack(long_sleep_inc)

        long_sleep_stable_mean =np.mean(long_sleep_stable, axis=0)
        long_sleep_inc_mean =np.mean(long_sleep_inc, axis=0)
        long_sleep_dec_mean =np.mean(long_sleep_dec, axis=0)
        
        diff_stable = long_sleep_stable_mean - cb_stable
        diff_inc = long_sleep_inc_mean - cb_inc
        diff_dec = long_sleep_dec_mean - cb_dec

        if save_results:
            res_dic = {}
            res_dic["diff_stable"] = diff_stable
            res_dic["diff_inc"] = diff_inc
            res_dic["diff_dec"] = diff_dec


            save_path = os.path.dirname(
                os.path.realpath(__file__)) + "/../temp_data/theta_phase"

            filename = save_path + "/" + self.session_name

            outfile = open(filename, 'wb')
            pickle.dump(res_dic, outfile)
            outfile.close()

        if plotting:
            bins_number = 10  # the [0, 360) interval will be subdivided into this
            # number of equal bins
            bins = np.linspace(0.0, 2 * np.pi, bins_number + 1)
            plt.clf()
            width = 2 * np.pi / bins_number
            ax = plt.subplot(1, 1, 1, projection='polar')
            n, _, _ = plt.hist(long_sleep_stable_mean, bins, density=True, alpha=0)
            bars = ax.bar(bins[:bins_number], n, width=width, bottom=0.0, label="Awake")
            for bar in bars:
                bar.set_alpha(1)
                bar.set_color("grey")
            n, _, _ = plt.hist(cb_stable, bins, density=True, alpha=0)
            bars = ax.bar(bins[:bins_number], n, width=width, bottom=0.0, label="REM")
            for bar in bars:
                bar.set_alpha(0.5)
                bar.set_color("red")
            ax.set_title("stable cells")
            plt.legend(loc=4)
            plt.tight_layout()
            plt.show()

            ax = plt.subplot(1, 1, 1, projection='polar')
            n, _, _ = plt.hist(diff_stable, bins, density=True, alpha=0)
            bars = ax.bar(bins[:bins_number], n, width=width, bottom=0.0, label="Diff per")
            for bar in bars:
                bar.set_alpha(1)
                bar.set_color("violet")
            n, _, _ = plt.hist(diff_inc, bins, density=True, alpha=0)
            bars = ax.bar(bins[:bins_number], n, width=width, bottom=0.0, label="Diff inc")
            for bar in bars:
                bar.set_alpha(0.5)
                bar.set_color("turquoise")
            n, _, _ = plt.hist(diff_dec, bins, density=True, alpha=0)
            bars = ax.bar(bins[:bins_number], n, width=width, bottom=0.0, label="Diff dec")
            for bar in bars:
                bar.set_alpha(0.5)
                bar.set_color("orange")
            ax.set_title("stable cells")
            plt.legend(loc=4)
            plt.tight_layout()
            plt.show()

            if save_fig:
                plt.style.use('default')
                c="black"
            else:
                c="white"
            res =[diff_stable, diff_dec, diff_inc]
            plt.figure(figsize=(4,5))
            bplot=plt.boxplot(res, positions=[1, 2, 3], patch_artist=True,
                              labels=["persistent", "decreasing", "increasing"],
                              boxprops=dict(color=c),
                              capprops=dict(color=c),
                              whiskerprops=dict(color=c),
                              flierprops=dict(color=c, markeredgecolor=c),
                              medianprops=dict(color=c), showfliers=False)
            plt.tick_params(axis='x', labelrotation=45)
            plt.ylim(0,9)
            y_base=7
            n_comp =3
            plt.hlines(y_base, 1, 1.9, color=c)
            if mannwhitneyu(res[0], res[1])[1] > 0.05/n_comp:
                plt.text(1.4, y_base, "n.s.", color=c)
            elif mannwhitneyu(res[0], res[1])[1] < 0.001/n_comp:
                plt.text(1.4, y_base, "***", color=c)
            elif mannwhitneyu(res[0], res[1])[1] < 0.01/n_comp:
                plt.text(1.4, y_base, "**", color=c)
            elif mannwhitneyu(res[0], res[1])[1] < 0.05/n_comp:
                plt.text(1.4, y_base, "*", color=c)
            plt.hlines(y_base, 2.1, 3, color=c)
            if mannwhitneyu(res[2], res[1])[1] > 0.05/n_comp:
                plt.text(2.4, y_base, "n.s.", color=c)
            elif mannwhitneyu(res[2], res[1])[1] < 0.001/n_comp:
                plt.text(2.4, y_base, "***", color=c)
            elif mannwhitneyu(res[2], res[1])[1] < 0.01/n_comp:
                plt.text(2.4, y_base, "**", color=c)
            elif mannwhitneyu(res[2], res[1])[1] < 0.05/n_comp:
                plt.text(2.4, y_base, "*", color=c)
            # first vs. second half NREM
            y_base=8
            plt.hlines(y_base, 1, 3, color=c)
            if mannwhitneyu(res[0], res[2])[1] > 0.05/n_comp:
                plt.text(2, y_base, "n.s.", color=c)
            elif mannwhitneyu(res[0], res[2])[1] < 0.001/n_comp:
                plt.text(2, y_base, "***", color=c)
            elif mannwhitneyu(res[0], res[2])[1] < 0.01/n_comp:
                plt.text(2, y_base, "**", color=c)
            elif mannwhitneyu(res[0], res[2])[1] < 0.05/n_comp:
                plt.text(2, y_base, "*", color=c)
            for patch, color in zip(bplot['boxes'], ["violet", "turquoise", "orange"]):
                patch.set_facecolor(color)

            plt.ylabel("Theta_REM_phase - \n Theta_awake_phase (rad)")
            plt.tight_layout()
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig("theta_rem_awake"+self.session_name+".svg", transparent="True")
            else:
                plt.show()

        if not return_diff:
            return cb_stable, cb_inc, cb_dec, long_sleep_stable_mean, long_sleep_inc_mean, long_sleep_dec_mean
        else:
            return diff_stable, diff_dec, diff_inc


class ExplorationLongSleep(LongSleep):
    """Class for long sleep"""

    def __init__(self, sleep_data_obj, exploration, params, session_params=None):

        # initialize parent classes
        LongSleep.__init__(self, sleep_data_obj=sleep_data_obj, params=params, session_params=session_params)

        # add sleep
        self.exploration = exploration

        self.cell_type = sleep_data_obj.get_cell_type()

    def memory_drift_exploration_vs_long_sleep(self, n_moving_average_pop_vec=2000,plotting=True, save_fig=False, model="phmm"):

        # get spike binned rasters from long sleep
        event_spike_rasters_long_sleep = []
        for l_s in self.long_sleep:

            # get original bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_ = (
                l_s.get_spike_binned_raster_combined_sleep_phases())

            #
            # _, event_spike_rasters_jittered_ = (
            #     l_s.get_spike_binned_raster_combined_sleep_phases_jittered(nr_spikes_per_jitter_window=np.hstack(event_spike_rasters_).shape[1]))

            event_spike_rasters_long_sleep.append(np.hstack(event_spike_rasters_))

        all_spike_rasters = np.hstack(event_spike_rasters_long_sleep)

        # get spike binned rasters from pre
        spike_rasters_cb = self.exploration.get_spike_bin_raster(return_estimated_times=False)

        # delete events which are too short --> TODO
        # --------------------------------------------------------------------------------------------------------------
        # nr_bins = np.array([x.shape[1] for x in merged_events])

        if model == "phmm":
            # get means of pre and post modes + compression factor
            pre_file_name = self.session_params.default_pre_phmm_model
            post_file_name = self.session_params.default_post_phmm_model
            compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

            # get pre and post model to do decoding
            with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
                pre_model_dic = pickle.load(f)
            # get means of model (lambdas) for decoding
            pre_mode_means = pre_model_dic.means_

            with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
                post_model_dic = pickle.load(f)
            # get means of model (lambdas) for decoding
            post_mode_means = post_model_dic.means_

            # do decoding and plot result
            # ----------------------------------------------------------------------------------------------------------
            # do the decoding using pre and post model
            pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                      event_spike_rasters=all_spike_rasters,
                                                      compression_factor=compression_factor)

            post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                       event_spike_rasters=all_spike_rasters,
                                                       compression_factor=compression_factor)

            # do decoding and plot result for pre sleep
            # ----------------------------------------------------------------------------------------------------------
            # do the decoding using pre and post model
            pre_likeli_j = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                        event_spike_rasters=spike_rasters_cb,
                                                        compression_factor=compression_factor)

            post_likeli_j = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                         event_spike_rasters=spike_rasters_cb,
                                                         compression_factor=compression_factor)

        elif model == "ising":

            pre_file_name = self.session_params.default_pre_ising_model
            post_file_name = self.session_params.default_post_ising_model

            with open(self.params.pre_proc_dir + 'awake_ising_maps/' + pre_file_name + '.pkl',
                      'rb') as f:
                pre_model_dic = pickle.load(f)
            pre_template_map = pre_model_dic["res_map"]

            with open(self.params.pre_proc_dir + 'awake_ising_maps/' + post_file_name + '.pkl',
                      'rb') as f:
                post_model_dic = pickle.load(f)
            post_template_map = post_model_dic["res_map"]


            # get time_bin_size of encoding
            time_bin_size_encoding = pre_model_dic["time_bin_size"]

            # load correct compression factor (as defined in parameter file of the session)
            if time_bin_size_encoding == 0.01:
                compression_factor = \
                    np.round(self.session_params.sleep_compression_factor_12spikes_100ms * 10, 3)
            elif time_bin_size_encoding == 0.1:
                compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

            pre_likeli_j = decode_using_ising_map_fast(template_map=pre_template_map,
                                                  event_spike_rasters=spike_rasters_cb,
                                                  compression_factor=compression_factor,
                                                  cell_selection="all")
            post_likeli_j = decode_using_ising_map_fast(template_map=post_template_map,
                                                   event_spike_rasters=spike_rasters_cb,
                                                   compression_factor=compression_factor,
                                                   cell_selection="all")
            pre_likeli = decode_using_ising_map_fast(template_map=pre_template_map,
                                                event_spike_rasters=all_spike_rasters,
                                                compression_factor=compression_factor,
                                                cell_selection="all")
            post_likeli = decode_using_ising_map_fast(template_map=post_template_map,
                                                 event_spike_rasters=all_spike_rasters,
                                                 compression_factor=compression_factor,
                                                 cell_selection="all")

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        mean_pre_likeli = np.mean(np.log(pre_likeli), axis=1)
        mean_post_likeli = np.mean(np.log(post_likeli), axis=1)

        sim_ratio = (max_post_likeli - max_pre_likeli) / (max_post_likeli + max_pre_likeli)

        max_pre_likeli_j = np.max(pre_likeli_j, axis=1)
        max_post_likeli_j = np.max(post_likeli_j, axis=1)

        mean_pre_likeli_j = np.mean(np.log(pre_likeli_j), axis=1)
        mean_post_likeli_j = np.mean(np.log(post_likeli_j), axis=1)

        sim_ratio_j = (max_post_likeli_j - max_pre_likeli_j) / (max_post_likeli_j + max_pre_likeli_j)

        # look at distribution of decoded PRE modes
        mode_decoded_pre, mode_decoded_pre_counts = np.unique(np.argmax(pre_likeli, axis=1), return_counts=True)
        mode_decoded_pre_outside, mode_decoded_pre_counts_outside = np.unique(np.argmax(pre_likeli_j, axis=1),
                                                                              return_counts=True)
        modes_original =np.zeros(pre_likeli.shape[1])
        modes_original[mode_decoded_pre] = mode_decoded_pre_counts/np.sum(mode_decoded_pre_counts)
        modes_excluded =np.zeros(pre_likeli.shape[1])
        modes_excluded[mode_decoded_pre_outside] = mode_decoded_pre_counts_outside/np.sum(mode_decoded_pre_counts_outside)

        plt.scatter(modes_original, modes_excluded)
        plt.title("PRE mode decoding (fraction)")
        plt.xlabel("Long_sleep")
        plt.ylabel("Familiar")
        plt.tight_layout()
        plt.show()

        # look at distribution of decoded POST modes
        mode_decoded_post, mode_decoded_post_counts = np.unique(np.argmax(post_likeli, axis=1), return_counts=True)
        mode_decoded_post_outside, mode_decoded_post_counts_outside = np.unique(np.argmax(post_likeli_j, axis=1),
                                                                              return_counts=True)
        modes_original =np.zeros(post_likeli.shape[1])
        modes_original[mode_decoded_post] = mode_decoded_post_counts/np.sum(mode_decoded_post_counts)
        modes_excluded =np.zeros(post_likeli.shape[1])
        modes_excluded[mode_decoded_post_outside] = mode_decoded_post_counts_outside/np.sum(mode_decoded_post_counts_outside)

        plt.scatter(modes_original, modes_excluded)
        plt.title("POST mode decoding (fraction)")
        plt.xlabel("Long_sleep")
        plt.ylabel("Familiar")
        plt.tight_layout()
        plt.show()

        # cut to same length for the ratio
        # --------------------------------------------------------------------------------------------------------------

        # take the log
        max_pre_log_likeli = np.log(max_pre_likeli)
        max_pre_log_likeli_j = np.log(max_pre_likeli_j)
        max_post_log_likeli = np.log(max_post_likeli)
        max_post_log_likeli_j = np.log(max_post_likeli_j)

        # interpolate values --> likelihoods
        # --------------------------------------------------------------------------------------------------------------
        max_pre_log_likeli_j = np.interp(np.linspace(0, len(max_pre_log_likeli_j), len(max_pre_log_likeli)),
                                         np.arange(len(max_pre_log_likeli_j)), max_pre_log_likeli_j)

        max_post_log_likeli_j = np.interp(np.linspace(0, len(max_post_log_likeli_j), len(max_post_log_likeli)),
                                          np.arange(len(max_post_log_likeli_j)), max_post_log_likeli_j)


        # interpolate values --> sim_ratio
        # --------------------------------------------------------------------------------------------------------------
        sim_ratio_j = np.interp(np.linspace(0, len(sim_ratio_j), len(sim_ratio)),
                                np.arange(len(sim_ratio_j)), sim_ratio_j)

        # apply smoothing
        max_pre_log_likeli_s = uniform_filter1d(max_pre_log_likeli, n_moving_average_pop_vec)
        max_pre_log_likeli_j_s = uniform_filter1d(max_pre_log_likeli_j, n_moving_average_pop_vec)
        max_post_log_likeli_s = uniform_filter1d(max_post_log_likeli, n_moving_average_pop_vec)
        max_post_log_likeli_j_s = uniform_filter1d(max_post_log_likeli_j, n_moving_average_pop_vec)

        # compute difference
        diff_pre = max_pre_log_likeli_s - max_pre_log_likeli_j_s
        diff_post = max_post_log_likeli_s - max_post_log_likeli_j_s


        if plotting or save_fig:

            c = "white"
            res = [mean_pre_likeli, mean_pre_likeli_j, mean_post_likeli, mean_post_likeli_j]
            bplot = plt.boxplot(res, positions=[1, 2, 3, 4], patch_artist=True,
                                labels=["PRE long_sleep", "PRE familiar", "POST long_sleep", "POST familiar"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            colors = ["magenta", 'magenta', "blue", "blue"]
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            plt.title("Mean log-likelihood")
            plt.ylabel("Mean log-likelihood")
            plt.grid(color="grey", axis="y")
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.show()


            if save_fig:
                plt.figure(figsize=(4,3))
                plt.style.use('default')
            plt.plot(np.linspace(0,1,moving_average(sim_ratio, n_moving_average_pop_vec).shape[0]),
                     moving_average(sim_ratio, n_moving_average_pop_vec), label="long_sleep", color="grey")
            plt.plot(np.linspace(0,1,moving_average(sim_ratio_j, n_moving_average_pop_vec).shape[0]),
                     moving_average(sim_ratio_j, n_moving_average_pop_vec), label="familiar", color="red", alpha=0.5)
            plt.xlabel("Normalized duration")
            plt.ylabel("Drift score")
            plt.legend()
            plt.tight_layout()
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "drift_score_equalized"+self.session_name+".svg"), transparent="True")
            else:
                plt.show()

            plt.figure(figsize=(12,8))
            plt.subplot(2,2,1)
            plt.plot(max_pre_log_likeli_s, color="red", label="PRE long_sleep")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_pre_log_likeli_j_s, color="salmon", linestyle="--", label="PRE familiar")
            plt.xticks([])
            plt.ylabel("Max log likelihood")
            plt.legend()
            plt.subplot(2,2,3)
            plt.plot(np.linspace(0,1, diff_pre.shape[0]), diff_pre)
            plt.ylabel("long_sleep - familiar")
            plt.xlabel("relative time")
            plt.subplot(2,2,2)
            plt.plot(max_post_log_likeli_s, color="white", label="POST long_sleep")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_post_log_likeli_j_s, color="lightblue", linestyle="--", label="POST familiar", alpha=0.8)
            plt.xticks([])
            plt.legend()
            plt.subplot(2,2,4)
            plt.plot(np.linspace(0,1, diff_post.shape[0]), diff_post)
            plt.ylabel("long_sleep - familiar")
            plt.xlabel("relative time")
            plt.tight_layout()
            plt.show()

        return sim_ratio, sim_ratio_j

    def memory_drift_exploration_vs_long_sleep_excluded(self, n_moving_average_pop_vec=2000,plotting=True, save_fig=False, model="phmm"):

        # get spike binned rasters from long sleep
        event_spike_rasters_long_sleep = []
        for l_s in self.long_sleep:

            # get original bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_ = (
                l_s.get_spike_binned_raster_excluded_periods())

            #
            # _, event_spike_rasters_jittered_ = (
            #     l_s.get_spike_binned_raster_combined_sleep_phases_jittered(nr_spikes_per_jitter_window=np.hstack(event_spike_rasters_).shape[1]))

            event_spike_rasters_long_sleep.append(np.hstack(event_spike_rasters_))

        all_spike_rasters = np.hstack(event_spike_rasters_long_sleep)

        # get spike binned rasters from pre
        spike_rasters_cb = self.exploration.get_spike_bin_raster(return_estimated_times=False)

        # delete events which are too short --> TODO
        # --------------------------------------------------------------------------------------------------------------
        # nr_bins = np.array([x.shape[1] for x in merged_events])

        if model == "phmm":
            # get means of pre and post modes + compression factor
            pre_file_name = self.session_params.default_pre_phmm_model
            post_file_name = self.session_params.default_post_phmm_model
            compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

            # get pre and post model to do decoding
            with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
                pre_model_dic = pickle.load(f)
            # get means of model (lambdas) for decoding
            pre_mode_means = pre_model_dic.means_

            with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
                post_model_dic = pickle.load(f)
            # get means of model (lambdas) for decoding
            post_mode_means = post_model_dic.means_

            # do decoding and plot result
            # ----------------------------------------------------------------------------------------------------------
            # do the decoding using pre and post model
            pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                      event_spike_rasters=all_spike_rasters,
                                                      compression_factor=compression_factor)

            post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                       event_spike_rasters=all_spike_rasters,
                                                       compression_factor=compression_factor)

            # do decoding and plot result for pre sleep
            # ----------------------------------------------------------------------------------------------------------
            # do the decoding using pre and post model
            pre_likeli_j = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                        event_spike_rasters=spike_rasters_cb,
                                                        compression_factor=compression_factor)

            post_likeli_j = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                         event_spike_rasters=spike_rasters_cb,
                                                         compression_factor=compression_factor)

        elif model == "ising":

            pre_file_name = self.session_params.default_pre_ising_model
            post_file_name = self.session_params.default_post_ising_model

            with open(self.params.pre_proc_dir + 'awake_ising_maps/' + pre_file_name + '.pkl',
                      'rb') as f:
                pre_model_dic = pickle.load(f)
            pre_template_map = pre_model_dic["res_map"]

            with open(self.params.pre_proc_dir + 'awake_ising_maps/' + post_file_name + '.pkl',
                      'rb') as f:
                post_model_dic = pickle.load(f)
            post_template_map = post_model_dic["res_map"]


            # get time_bin_size of encoding
            time_bin_size_encoding = pre_model_dic["time_bin_size"]

            # load correct compression factor (as defined in parameter file of the session)
            if time_bin_size_encoding == 0.01:
                compression_factor = \
                    np.round(self.session_params.sleep_compression_factor_12spikes_100ms * 10, 3)
            elif time_bin_size_encoding == 0.1:
                compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

            pre_likeli_j = decode_using_ising_map_fast(template_map=pre_template_map,
                                                  event_spike_rasters=spike_rasters_cb,
                                                  compression_factor=compression_factor,
                                                  cell_selection="all")
            post_likeli_j = decode_using_ising_map_fast(template_map=post_template_map,
                                                   event_spike_rasters=spike_rasters_cb,
                                                   compression_factor=compression_factor,
                                                   cell_selection="all")
            pre_likeli = decode_using_ising_map_fast(template_map=pre_template_map,
                                                event_spike_rasters=all_spike_rasters,
                                                compression_factor=compression_factor,
                                                cell_selection="all")
            post_likeli = decode_using_ising_map_fast(template_map=post_template_map,
                                                 event_spike_rasters=all_spike_rasters,
                                                 compression_factor=compression_factor,
                                                 cell_selection="all")

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        mean_pre_likeli = np.mean(np.log(pre_likeli), axis=1)
        mean_post_likeli = np.mean(np.log(post_likeli), axis=1)

        sim_ratio = (max_post_likeli - max_pre_likeli) / (max_post_likeli + max_pre_likeli)

        max_pre_likeli_j = np.max(pre_likeli_j, axis=1)
        max_post_likeli_j = np.max(post_likeli_j, axis=1)

        mean_pre_likeli_j = np.mean(np.log(pre_likeli_j), axis=1)
        mean_post_likeli_j = np.mean(np.log(post_likeli_j), axis=1)

        sim_ratio_j = (max_post_likeli_j - max_pre_likeli_j) / (max_post_likeli_j + max_pre_likeli_j)

        # look at distribution of decoded PRE modes
        mode_decoded_pre, mode_decoded_pre_counts = np.unique(np.argmax(pre_likeli, axis=1), return_counts=True)
        mode_decoded_pre_outside, mode_decoded_pre_counts_outside = np.unique(np.argmax(pre_likeli_j, axis=1),
                                                                              return_counts=True)
        modes_original =np.zeros(pre_likeli.shape[1])
        modes_original[mode_decoded_pre] = mode_decoded_pre_counts/np.sum(mode_decoded_pre_counts)
        modes_excluded =np.zeros(pre_likeli.shape[1])
        modes_excluded[mode_decoded_pre_outside] = mode_decoded_pre_counts_outside/np.sum(mode_decoded_pre_counts_outside)

        plt.scatter(modes_original, modes_excluded)
        plt.title("PRE mode decoding (fraction)")
        plt.xlabel("Long_sleep - Excluded")
        plt.ylabel("Familiar")
        plt.tight_layout()
        plt.show()

        # look at distribution of decoded POST modes
        mode_decoded_post, mode_decoded_post_counts = np.unique(np.argmax(post_likeli, axis=1), return_counts=True)
        mode_decoded_post_outside, mode_decoded_post_counts_outside = np.unique(np.argmax(post_likeli_j, axis=1),
                                                                              return_counts=True)
        modes_original =np.zeros(post_likeli.shape[1])
        modes_original[mode_decoded_post] = mode_decoded_post_counts/np.sum(mode_decoded_post_counts)
        modes_excluded =np.zeros(post_likeli.shape[1])
        modes_excluded[mode_decoded_post_outside] = mode_decoded_post_counts_outside/np.sum(mode_decoded_post_counts_outside)

        plt.scatter(modes_original, modes_excluded)
        plt.title("POST mode decoding (fraction)")
        plt.xlabel("Long_sleep - Excluded")
        plt.ylabel("Familiar")
        plt.tight_layout()
        plt.show()

        # cut to same length for the ratio
        # --------------------------------------------------------------------------------------------------------------

        # take the log
        max_pre_log_likeli = np.log(max_pre_likeli)
        max_pre_log_likeli_j = np.log(max_pre_likeli_j)
        max_post_log_likeli = np.log(max_post_likeli)
        max_post_log_likeli_j = np.log(max_post_likeli_j)

        # interpolate values --> likelihoods
        # --------------------------------------------------------------------------------------------------------------
        max_pre_log_likeli_j = np.interp(np.linspace(0, len(max_pre_log_likeli_j), len(max_pre_log_likeli)),
                                         np.arange(len(max_pre_log_likeli_j)), max_pre_log_likeli_j)

        max_post_log_likeli_j = np.interp(np.linspace(0, len(max_post_log_likeli_j), len(max_post_log_likeli)),
                                          np.arange(len(max_post_log_likeli_j)), max_post_log_likeli_j)


        # interpolate values --> sim_ratio
        # --------------------------------------------------------------------------------------------------------------
        sim_ratio_j = np.interp(np.linspace(0, len(sim_ratio_j), len(sim_ratio)),
                                np.arange(len(sim_ratio_j)), sim_ratio_j)

        # apply smoothing
        max_pre_log_likeli_s = uniform_filter1d(max_pre_log_likeli, n_moving_average_pop_vec)
        max_pre_log_likeli_j_s = uniform_filter1d(max_pre_log_likeli_j, n_moving_average_pop_vec)
        max_post_log_likeli_s = uniform_filter1d(max_post_log_likeli, n_moving_average_pop_vec)
        max_post_log_likeli_j_s = uniform_filter1d(max_post_log_likeli_j, n_moving_average_pop_vec)

        # compute difference
        diff_pre = max_pre_log_likeli_s - max_pre_log_likeli_j_s
        diff_post = max_post_log_likeli_s - max_post_log_likeli_j_s


        if plotting or save_fig:

            c = "white"
            res = [mean_pre_likeli, mean_pre_likeli_j, mean_post_likeli, mean_post_likeli_j]
            bplot = plt.boxplot(res, positions=[1, 2, 3, 4], patch_artist=True,
                                labels=["PRE long_sleep - Excluded", "PRE familiar", "POST long_sleep - Excluded", "POST familiar"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            colors = ["magenta", 'magenta', "blue", "blue"]
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            plt.title("Mean log-likelihood")
            plt.ylabel("Mean log-likelihood")
            plt.grid(color="grey", axis="y")
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.show()


            if save_fig:
                plt.figure(figsize=(4,3))
                plt.style.use('default')
            plt.plot(np.linspace(0,1,moving_average(sim_ratio, n_moving_average_pop_vec).shape[0]),
                     moving_average(sim_ratio, n_moving_average_pop_vec), label="long_sleep - Excluded", color="grey")
            plt.plot(np.linspace(0,1,moving_average(sim_ratio_j, n_moving_average_pop_vec).shape[0]),
                     moving_average(sim_ratio_j, n_moving_average_pop_vec), label="familiar", color="red", alpha=0.5)
            plt.xlabel("Normalized duration")
            plt.ylabel("Drift score")
            plt.legend()
            plt.tight_layout()
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "drift_score_equalized"+self.session_name+".svg"), transparent="True")
            else:
                plt.show()

            plt.figure(figsize=(12,8))
            plt.subplot(2,2,1)
            plt.plot(max_pre_log_likeli_s, color="red", label="PRE long_sleep - Excluded")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_pre_log_likeli_j_s, color="salmon", linestyle="--", label="PRE familiar")
            plt.xticks([])
            plt.ylabel("Max log likelihood")
            plt.legend()
            plt.subplot(2,2,3)
            plt.plot(np.linspace(0,1, diff_pre.shape[0]), diff_pre)
            plt.ylabel("long_sleep (excl) - familiar")
            plt.xlabel("relative time")
            plt.subplot(2,2,2)
            plt.plot(max_post_log_likeli_s, color="white", label="POST long_sleep - Excluded")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_post_log_likeli_j_s, color="lightblue", linestyle="--", label="POST familiar", alpha=0.8)
            plt.xticks([])
            plt.legend()
            plt.subplot(2,2,4)
            plt.plot(np.linspace(0,1, diff_post.shape[0]), diff_post)
            plt.ylabel("long_sleep (excl) - familiar")
            plt.xlabel("relative time")
            plt.tight_layout()
            plt.show()

        return sim_ratio, sim_ratio_j

    def memory_drift_exploration_vs_long_sleep_around_swr(self, n_moving_average_pop_vec=2000,plotting=True, save_fig=False, model="phmm"):

        # get spike binned rasters from long sleep
        event_spike_rasters_long_sleep = []
        for l_s in self.long_sleep:

            # get original bins
            # ----------------------------------------------------------------------------------------------------------
            _, event_spike_rasters_ = (
                l_s.get_spike_binned_raster_around_swr())

            #
            # _, event_spike_rasters_jittered_ = (
            #     l_s.get_spike_binned_raster_combined_sleep_phases_jittered(nr_spikes_per_jitter_window=np.hstack(event_spike_rasters_).shape[1]))

            event_spike_rasters_long_sleep.append(np.hstack(event_spike_rasters_))

        all_spike_rasters = np.hstack(event_spike_rasters_long_sleep)

        # get spike binned rasters from pre
        spike_rasters_cb = self.exploration.get_spike_bin_raster(return_estimated_times=False)

        # delete events which are too short --> TODO
        # --------------------------------------------------------------------------------------------------------------
        # nr_bins = np.array([x.shape[1] for x in merged_events])

        if model == "phmm":
            # get means of pre and post modes + compression factor
            pre_file_name = self.session_params.default_pre_phmm_model
            post_file_name = self.session_params.default_post_phmm_model
            compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

            # get pre and post model to do decoding
            with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
                pre_model_dic = pickle.load(f)
            # get means of model (lambdas) for decoding
            pre_mode_means = pre_model_dic.means_

            with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
                post_model_dic = pickle.load(f)
            # get means of model (lambdas) for decoding
            post_mode_means = post_model_dic.means_

            # do decoding and plot result
            # ----------------------------------------------------------------------------------------------------------
            # do the decoding using pre and post model
            pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                      event_spike_rasters=all_spike_rasters,
                                                      compression_factor=compression_factor)

            post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                       event_spike_rasters=all_spike_rasters,
                                                       compression_factor=compression_factor)

            # do decoding and plot result for pre sleep
            # ----------------------------------------------------------------------------------------------------------
            # do the decoding using pre and post model
            pre_likeli_j = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                        event_spike_rasters=spike_rasters_cb,
                                                        compression_factor=compression_factor)

            post_likeli_j = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                         event_spike_rasters=spike_rasters_cb,
                                                         compression_factor=compression_factor)

        elif model == "ising":

            pre_file_name = self.session_params.default_pre_ising_model
            post_file_name = self.session_params.default_post_ising_model

            with open(self.params.pre_proc_dir + 'awake_ising_maps/' + pre_file_name + '.pkl',
                      'rb') as f:
                pre_model_dic = pickle.load(f)
            pre_template_map = pre_model_dic["res_map"]

            with open(self.params.pre_proc_dir + 'awake_ising_maps/' + post_file_name + '.pkl',
                      'rb') as f:
                post_model_dic = pickle.load(f)
            post_template_map = post_model_dic["res_map"]


            # get time_bin_size of encoding
            time_bin_size_encoding = pre_model_dic["time_bin_size"]

            # load correct compression factor (as defined in parameter file of the session)
            if time_bin_size_encoding == 0.01:
                compression_factor = \
                    np.round(self.session_params.sleep_compression_factor_12spikes_100ms * 10, 3)
            elif time_bin_size_encoding == 0.1:
                compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

            pre_likeli_j = decode_using_ising_map_fast(template_map=pre_template_map,
                                                  event_spike_rasters=spike_rasters_cb,
                                                  compression_factor=compression_factor,
                                                  cell_selection="all")
            post_likeli_j = decode_using_ising_map_fast(template_map=post_template_map,
                                                   event_spike_rasters=spike_rasters_cb,
                                                   compression_factor=compression_factor,
                                                   cell_selection="all")
            pre_likeli = decode_using_ising_map_fast(template_map=pre_template_map,
                                                event_spike_rasters=all_spike_rasters,
                                                compression_factor=compression_factor,
                                                cell_selection="all")
            post_likeli = decode_using_ising_map_fast(template_map=post_template_map,
                                                 event_spike_rasters=all_spike_rasters,
                                                 compression_factor=compression_factor,
                                                 cell_selection="all")

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        mean_pre_likeli = np.mean(np.log(pre_likeli), axis=1)
        mean_post_likeli = np.mean(np.log(post_likeli), axis=1)

        sim_ratio = (max_post_likeli - max_pre_likeli) / (max_post_likeli + max_pre_likeli)

        max_pre_likeli_j = np.max(pre_likeli_j, axis=1)
        max_post_likeli_j = np.max(post_likeli_j, axis=1)

        mean_pre_likeli_j = np.mean(np.log(pre_likeli_j), axis=1)
        mean_post_likeli_j = np.mean(np.log(post_likeli_j), axis=1)

        sim_ratio_j = (max_post_likeli_j - max_pre_likeli_j) / (max_post_likeli_j + max_pre_likeli_j)

        # look at distribution of decoded PRE modes
        mode_decoded_pre, mode_decoded_pre_counts = np.unique(np.argmax(pre_likeli, axis=1), return_counts=True)
        mode_decoded_pre_outside, mode_decoded_pre_counts_outside = np.unique(np.argmax(pre_likeli_j, axis=1),
                                                                              return_counts=True)
        modes_original =np.zeros(pre_likeli.shape[1])
        modes_original[mode_decoded_pre] = mode_decoded_pre_counts/np.sum(mode_decoded_pre_counts)
        modes_excluded =np.zeros(pre_likeli.shape[1])
        modes_excluded[mode_decoded_pre_outside] = mode_decoded_pre_counts_outside/np.sum(mode_decoded_pre_counts_outside)

        plt.scatter(modes_original, modes_excluded)
        plt.title("PRE mode decoding (fraction)")
        plt.xlabel("Long_sleep - around SWR")
        plt.ylabel("Familiar")
        plt.tight_layout()
        plt.show()

        # look at distribution of decoded POST modes
        mode_decoded_post, mode_decoded_post_counts = np.unique(np.argmax(post_likeli, axis=1), return_counts=True)
        mode_decoded_post_outside, mode_decoded_post_counts_outside = np.unique(np.argmax(post_likeli_j, axis=1),
                                                                              return_counts=True)
        modes_original =np.zeros(post_likeli.shape[1])
        modes_original[mode_decoded_post] = mode_decoded_post_counts/np.sum(mode_decoded_post_counts)
        modes_excluded =np.zeros(post_likeli.shape[1])
        modes_excluded[mode_decoded_post_outside] = mode_decoded_post_counts_outside/np.sum(mode_decoded_post_counts_outside)

        plt.scatter(modes_original, modes_excluded)
        plt.title("POST mode decoding (fraction)")
        plt.xlabel("Long_sleep - around SWR")
        plt.ylabel("Familiar")
        plt.tight_layout()
        plt.show()

        # cut to same length for the ratio
        # --------------------------------------------------------------------------------------------------------------

        # take the log
        max_pre_log_likeli = np.log(max_pre_likeli)
        max_pre_log_likeli_j = np.log(max_pre_likeli_j)
        max_post_log_likeli = np.log(max_post_likeli)
        max_post_log_likeli_j = np.log(max_post_likeli_j)

        # interpolate values --> likelihoods
        # --------------------------------------------------------------------------------------------------------------
        max_pre_log_likeli_j = np.interp(np.linspace(0, len(max_pre_log_likeli_j), len(max_pre_log_likeli)),
                                         np.arange(len(max_pre_log_likeli_j)), max_pre_log_likeli_j)

        max_post_log_likeli_j = np.interp(np.linspace(0, len(max_post_log_likeli_j), len(max_post_log_likeli)),
                                          np.arange(len(max_post_log_likeli_j)), max_post_log_likeli_j)


        # interpolate values --> sim_ratio
        # --------------------------------------------------------------------------------------------------------------
        sim_ratio_j = np.interp(np.linspace(0, len(sim_ratio_j), len(sim_ratio)),
                                np.arange(len(sim_ratio_j)), sim_ratio_j)

        # apply smoothing
        max_pre_log_likeli_s = uniform_filter1d(max_pre_log_likeli, n_moving_average_pop_vec)
        max_pre_log_likeli_j_s = uniform_filter1d(max_pre_log_likeli_j, n_moving_average_pop_vec)
        max_post_log_likeli_s = uniform_filter1d(max_post_log_likeli, n_moving_average_pop_vec)
        max_post_log_likeli_j_s = uniform_filter1d(max_post_log_likeli_j, n_moving_average_pop_vec)

        # compute difference
        diff_pre = max_pre_log_likeli_s - max_pre_log_likeli_j_s
        diff_post = max_post_log_likeli_s - max_post_log_likeli_j_s


        if plotting or save_fig:

            c = "white"
            res = [mean_pre_likeli, mean_pre_likeli_j, mean_post_likeli, mean_post_likeli_j]
            bplot = plt.boxplot(res, positions=[1, 2, 3, 4], patch_artist=True,
                                labels=["PRE long_sleep - around SWR", "PRE familiar", "POST long_sleep - around SWR", "POST familiar"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            colors = ["magenta", 'magenta', "blue", "blue"]
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            plt.title("Mean log-likelihood")
            plt.ylabel("Mean log-likelihood")
            plt.grid(color="grey", axis="y")
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.show()


            if save_fig:
                plt.figure(figsize=(4,3))
                plt.style.use('default')
            plt.plot(np.linspace(0,1,moving_average(sim_ratio, n_moving_average_pop_vec).shape[0]),
                     moving_average(sim_ratio, n_moving_average_pop_vec), label="long_sleep - around SWR", color="grey")
            plt.plot(np.linspace(0,1,moving_average(sim_ratio_j, n_moving_average_pop_vec).shape[0]),
                     moving_average(sim_ratio_j, n_moving_average_pop_vec), label="familiar", color="red", alpha=0.5)
            plt.xlabel("Normalized duration")
            plt.ylabel("Drift score")
            plt.legend()
            plt.tight_layout()
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "drift_score_equalized"+self.session_name+".svg"), transparent="True")
            else:
                plt.show()

            plt.figure(figsize=(12,8))
            plt.subplot(2,2,1)
            plt.plot(max_pre_log_likeli_s, color="red", label="PRE long_sleep - around SWR")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_pre_log_likeli_j_s, color="salmon", linestyle="--", label="PRE familiar")
            plt.xticks([])
            plt.ylabel("Max log likelihood")
            plt.legend()
            plt.subplot(2,2,3)
            plt.plot(np.linspace(0,1, diff_pre.shape[0]), diff_pre)
            plt.ylabel("long_sleep (around SWR) - familiar")
            plt.xlabel("relative time")
            plt.subplot(2,2,2)
            plt.plot(max_post_log_likeli_s, color="white", label="POST long_sleep - around SWR")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_post_log_likeli_j_s, color="lightblue", linestyle="--", label="POST familiar", alpha=0.8)
            plt.xticks([])
            plt.legend()
            plt.subplot(2,2,4)
            plt.plot(np.linspace(0,1, diff_post.shape[0]), diff_post)
            plt.ylabel("long_sleep (around SWR) - familiar")
            plt.xlabel("relative time")
            plt.tight_layout()
            plt.show()

        return sim_ratio, sim_ratio_j


class ExplorationCheeseboard():

    def __init__(self, cheeseboard, exploration, params, session_params=None):

        self.params = params
        self.session_params = session_params
        # add sleep
        self.exploration = exploration
        self.cheeseboard = cheeseboard

    def memory_drift_exploration_vs_learning(self, model="phmm", save_fig=False, plotting=True,
                                             n_moving_average_pop_vec=2000):
        # get spike binned rasters from pre
        spike_rasters_ex = self.exploration.get_spike_bin_raster(return_estimated_times=False)
        # get spike binned rasters from pre
        spike_rasters_cb = self.cheeseboard.get_spike_bin_raster(return_estimated_times=False)
        spike_rasters_cb = np.hstack(spike_rasters_cb)

        # delete events which are too short --> TODO
        # --------------------------------------------------------------------------------------------------------------
        # nr_bins = np.array([x.shape[1] for x in merged_events])

        if model == "phmm":
            # get means of pre and post modes + compression factor
            pre_file_name = self.session_params.default_pre_phmm_model
            post_file_name = self.session_params.default_post_phmm_model
            compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

            # get pre and post model to do decoding
            with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
                pre_model_dic = pickle.load(f)
            # get means of model (lambdas) for decoding
            pre_mode_means = pre_model_dic.means_

            with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
                post_model_dic = pickle.load(f)
            # get means of model (lambdas) for decoding
            post_mode_means = post_model_dic.means_

            # do decoding and plot result
            # ----------------------------------------------------------------------------------------------------------
            # do the decoding using pre and post model
            pre_likeli = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                      event_spike_rasters=spike_rasters_cb,
                                                      compression_factor=compression_factor)

            post_likeli = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                       event_spike_rasters=spike_rasters_cb,
                                                       compression_factor=compression_factor)

            # do decoding and plot result for pre sleep
            # ----------------------------------------------------------------------------------------------------------
            # do the decoding using pre and post model
            pre_likeli_j = decode_using_phmm_modes_fast(mode_means=pre_mode_means,
                                                        event_spike_rasters=spike_rasters_ex,
                                                        compression_factor=compression_factor)

            post_likeli_j = decode_using_phmm_modes_fast(mode_means=post_mode_means,
                                                         event_spike_rasters=spike_rasters_ex,
                                                         compression_factor=compression_factor)

        elif model == "ising":

            pre_file_name = self.session_params.default_pre_ising_model
            post_file_name = self.session_params.default_post_ising_model

            with open(self.params.pre_proc_dir + 'awake_ising_maps/' + pre_file_name + '.pkl',
                      'rb') as f:
                pre_model_dic = pickle.load(f)
            pre_template_map = pre_model_dic["res_map"]

            with open(self.params.pre_proc_dir + 'awake_ising_maps/' + post_file_name + '.pkl',
                      'rb') as f:
                post_model_dic = pickle.load(f)
            post_template_map = post_model_dic["res_map"]


            # get time_bin_size of encoding
            time_bin_size_encoding = pre_model_dic["time_bin_size"]

            # load correct compression factor (as defined in parameter file of the session)
            if time_bin_size_encoding == 0.01:
                compression_factor = \
                    np.round(self.session_params.sleep_compression_factor_12spikes_100ms * 10, 3)
            elif time_bin_size_encoding == 0.1:
                compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms

            pre_likeli_j = decode_using_ising_map_fast(template_map=pre_template_map,
                                                  event_spike_rasters=spike_rasters_ex,
                                                  compression_factor=compression_factor,
                                                  cell_selection="all")
            post_likeli_j = decode_using_ising_map_fast(template_map=post_template_map,
                                                   event_spike_rasters=spike_rasters_ex,
                                                   compression_factor=compression_factor,
                                                   cell_selection="all")
            pre_likeli = decode_using_ising_map_fast(template_map=pre_template_map,
                                                event_spike_rasters=spike_rasters_cb,
                                                compression_factor=compression_factor,
                                                cell_selection="all")
            post_likeli = decode_using_ising_map_fast(template_map=post_template_map,
                                                 event_spike_rasters=spike_rasters_cb,
                                                 compression_factor=compression_factor,
                                                 cell_selection="all")

        max_pre_likeli = np.max(pre_likeli, axis=1)
        max_post_likeli = np.max(post_likeli, axis=1)

        mean_pre_likeli = np.mean(np.log(pre_likeli), axis=1)
        mean_post_likeli = np.mean(np.log(post_likeli), axis=1)

        sim_ratio = (max_post_likeli - max_pre_likeli) / (max_post_likeli + max_pre_likeli)

        max_pre_likeli_j = np.max(pre_likeli_j, axis=1)
        max_post_likeli_j = np.max(post_likeli_j, axis=1)

        mean_pre_likeli_j = np.mean(np.log(pre_likeli_j), axis=1)
        mean_post_likeli_j = np.mean(np.log(post_likeli_j), axis=1)

        sim_ratio_j = (max_post_likeli_j - max_pre_likeli_j) / (max_post_likeli_j + max_pre_likeli_j)

        # look at distribution of decoded PRE modes
        mode_decoded_pre, mode_decoded_pre_counts = np.unique(np.argmax(pre_likeli, axis=1), return_counts=True)
        mode_decoded_pre_outside, mode_decoded_pre_counts_outside = np.unique(np.argmax(pre_likeli_j, axis=1),
                                                                              return_counts=True)
        modes_original =np.zeros(pre_likeli.shape[1])
        modes_original[mode_decoded_pre] = mode_decoded_pre_counts/np.sum(mode_decoded_pre_counts)
        modes_excluded =np.zeros(pre_likeli.shape[1])
        modes_excluded[mode_decoded_pre_outside] = mode_decoded_pre_counts_outside/np.sum(mode_decoded_pre_counts_outside)

        plt.scatter(modes_original, modes_excluded)
        plt.title("PRE mode decoding (fraction)")
        plt.xlabel("Learning")
        plt.ylabel("Familiar")
        plt.tight_layout()
        plt.show()

        # look at distribution of decoded POST modes
        mode_decoded_post, mode_decoded_post_counts = np.unique(np.argmax(post_likeli, axis=1), return_counts=True)
        mode_decoded_post_outside, mode_decoded_post_counts_outside = np.unique(np.argmax(post_likeli_j, axis=1),
                                                                              return_counts=True)
        modes_original =np.zeros(post_likeli.shape[1])
        modes_original[mode_decoded_post] = mode_decoded_post_counts/np.sum(mode_decoded_post_counts)
        modes_excluded =np.zeros(post_likeli.shape[1])
        modes_excluded[mode_decoded_post_outside] = mode_decoded_post_counts_outside/np.sum(mode_decoded_post_counts_outside)

        plt.scatter(modes_original, modes_excluded)
        plt.title("POST mode decoding (fraction)")
        plt.xlabel("Learning")
        plt.ylabel("Familiar")
        plt.tight_layout()
        plt.show()

        # cut to same length for the ratio
        # --------------------------------------------------------------------------------------------------------------

        # take the log
        max_pre_log_likeli = np.log(max_pre_likeli)
        max_pre_log_likeli_j = np.log(max_pre_likeli_j)
        max_post_log_likeli = np.log(max_post_likeli)
        max_post_log_likeli_j = np.log(max_post_likeli_j)

        # interpolate values --> likelihoods
        # --------------------------------------------------------------------------------------------------------------
        max_pre_log_likeli_j = np.interp(np.linspace(0, len(max_pre_log_likeli_j), len(max_pre_log_likeli)),
                                         np.arange(len(max_pre_log_likeli_j)), max_pre_log_likeli_j)

        max_post_log_likeli_j = np.interp(np.linspace(0, len(max_post_log_likeli_j), len(max_post_log_likeli)),
                                          np.arange(len(max_post_log_likeli_j)), max_post_log_likeli_j)


        # interpolate values --> sim_ratio
        # --------------------------------------------------------------------------------------------------------------
        sim_ratio_j = np.interp(np.linspace(0, len(sim_ratio_j), len(sim_ratio)),
                                np.arange(len(sim_ratio_j)), sim_ratio_j)

        # apply smoothing
        max_pre_log_likeli_s = uniform_filter1d(max_pre_log_likeli, n_moving_average_pop_vec)
        max_pre_log_likeli_j_s = uniform_filter1d(max_pre_log_likeli_j, n_moving_average_pop_vec)
        max_post_log_likeli_s = uniform_filter1d(max_post_log_likeli, n_moving_average_pop_vec)
        max_post_log_likeli_j_s = uniform_filter1d(max_post_log_likeli_j, n_moving_average_pop_vec)

        # compute difference
        diff_pre = max_pre_log_likeli_s - max_pre_log_likeli_j_s
        diff_post = max_post_log_likeli_s - max_post_log_likeli_j_s


        if plotting or save_fig:

            c = "white"
            res = [mean_pre_likeli, mean_pre_likeli_j, mean_post_likeli, mean_post_likeli_j]
            bplot = plt.boxplot(res, positions=[1, 2, 3, 4], patch_artist=True,
                                labels=["PRE Learning", "PRE familiar", "POST Learning", "POST familiar"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            colors = ["magenta", 'magenta', "blue", "blue"]
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            plt.title("Mean log-likelihood")
            plt.ylabel("Mean log-likelihood")
            plt.grid(color="grey", axis="y")
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.show()


            if save_fig:
                plt.figure(figsize=(4,3))
                plt.style.use('default')
            plt.plot(np.linspace(0,1,moving_average(sim_ratio, n_moving_average_pop_vec).shape[0]),
                     moving_average(sim_ratio, n_moving_average_pop_vec), label="Learning", color="grey")
            plt.plot(np.linspace(0,1,moving_average(sim_ratio_j, n_moving_average_pop_vec).shape[0]),
                     moving_average(sim_ratio_j, n_moving_average_pop_vec), label="familiar", color="red", alpha=0.5)
            plt.xlabel("Normalized duration")
            plt.ylabel("Drift score")
            plt.legend()
            plt.tight_layout()
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "drift_score_equalized"+self.session_name+".svg"), transparent="True")
            else:
                plt.show()

            plt.figure(figsize=(12,8))
            plt.subplot(2,2,1)
            plt.plot(max_pre_log_likeli_s, color="red", label="PRE Learning")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_pre_log_likeli_j_s, color="salmon", linestyle="--", label="PRE familiar")
            plt.xticks([])
            plt.ylabel("Max log likelihood")
            plt.legend()
            plt.subplot(2,2,3)
            plt.plot(np.linspace(0,1, diff_pre.shape[0]), diff_pre)
            plt.ylabel("Learning - familiar")
            plt.xlabel("relative time")
            plt.subplot(2,2,2)
            plt.plot(max_post_log_likeli_s, color="white", label="POST Learning")
            # plt.plot(moving_average(max_pre_likeli_j, 2000), color="grey")
            plt.plot(max_post_log_likeli_j_s, color="lightblue", linestyle="--", label="POST familiar", alpha=0.8)
            plt.xticks([])
            plt.legend()
            plt.subplot(2,2,4)
            plt.plot(np.linspace(0,1, diff_post.shape[0]), diff_post)
            plt.ylabel("Learning - familiar")
            plt.xlabel("relative time")
            plt.tight_layout()
            plt.show()


class ExplFamPrePostCheeseboardExplFam:
    """Class to compare PRE and POST"""

    def __init__(self, exp_fam_1, pre, post, exp_fam_2, params, session_params=None):
        self.params = params
        self.session_params = session_params
        self.cell_type = self.params.cell_type
        self.session_name = session_params.session_name

        # initialize each phase
        self.pre = pre
        self.post = post
        self.exp_fam_1 = exp_fam_1
        self.exp_fam_2 = exp_fam_2

    def rate_map_stability(self, spatial_resolution=5, nr_of_splits=3, cells_to_use="all", plotting=True):

        # get subsets
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name +"_"+self.params.stable_cell_method + ".pickle", "rb") as f:
            class_dic = pickle.load(f)

        if cells_to_use == "stable":
            cell_ids = class_dic["stable_cell_ids"].flatten()
        elif cells_to_use == "decreasing":
            cell_ids = class_dic["decrease_cell_ids"].flatten()
        elif cells_to_use == "increasing":
            cell_ids = class_dic["increase_cell_ids"].flatten()
        elif cells_to_use == "all":
            print(" --> using all cells")

        # get rate maps and occ maps from pre-probe, pre, post, post-probe
        pre_prob_rate_maps, pre_prob_occ_maps = \
            self.exp_fam_1.get_rate_maps_occ_maps_temporal_splits(spatial_resolution=spatial_resolution,
                                                                  nr_of_splits=nr_of_splits)
        env_dim = self.exp_fam_1.get_env_dim()
        post_prob_rate_maps, post_prob_occ_maps = \
            self.exp_fam_2.get_rate_maps_occ_maps_temporal_splits(env_dim=env_dim,
                                                                  spatial_resolution=spatial_resolution,
                                                                  nr_of_splits=nr_of_splits)
        pre_rate_maps, pre_occ_maps = \
            self.pre.get_rate_maps_occ_maps_temporal_splits(env_dim=env_dim, spatial_resolution=spatial_resolution,
                                                            nr_of_splits=nr_of_splits)
        post_rate_maps, post_occ_maps = \
            self.post.get_rate_maps_occ_maps_temporal_splits(env_dim=env_dim, spatial_resolution=spatial_resolution,
                                                             nr_of_splits=nr_of_splits)

        all_occ_maps = pre_prob_occ_maps+ pre_occ_maps+post_occ_maps+ post_prob_occ_maps
        all_occ_maps_arr = np.array(all_occ_maps)

        all_rate_maps = pre_prob_rate_maps + pre_rate_maps + post_rate_maps + post_prob_rate_maps

        # check which bins were visited in all maps
        nr_non_zero = np.count_nonzero(all_occ_maps_arr, axis=0)

        # set all good bins to 1
        good_bins = np.zeros((all_occ_maps_arr.shape[1], all_occ_maps_arr.shape[2]))
        good_bins[nr_non_zero==all_occ_maps_arr.shape[0]] = 1
        good_bins_rep = np.repeat(good_bins[:, :, np.newaxis], pre_prob_rate_maps[0].shape[2], axis=2)

        # only select good bins for rate maps
        map_similarity = np.zeros((all_occ_maps_arr.shape[0], all_occ_maps_arr.shape[0]))

        for id_1, rate_map_comp_1 in enumerate(all_rate_maps):
            for id_2, rate_map_comp_2 in enumerate(all_rate_maps):
                # compute correlation --> cdist computes all pairs: we only need mean of diagonal
                if not cells_to_use == "all":
                    rate_map_comp_1_good = rate_map_comp_1[good_bins == 1]
                    rate_map_comp_1_subset = rate_map_comp_1_good[:,cell_ids]
                    rate_map_comp_2_good = rate_map_comp_2[good_bins == 1]
                    rate_map_comp_2_subset = rate_map_comp_2_good[:,cell_ids]
                    a = 1 - cdist(rate_map_comp_1_subset, rate_map_comp_2_subset,
                                  metric="correlation")
                else:
                    a = 1-cdist(rate_map_comp_1[good_bins==1], rate_map_comp_2[good_bins==1], metric="correlation")
                map_similarity[id_1, id_2] = np.mean(np.diag(a))

        if plotting:
            # generate x/y label entries
            labels = []
            phases = np.array(["Exp.fam_before", "learn-PRE", "POST", "Exp.fam_after"])
            for phase in phases:
                for subdiv in range(nr_of_splits):
                    labels.append(phase+"_"+str(subdiv))

            # plt.figure(figsize=(6,5))
            plt.imshow(map_similarity, vmin=0, vmax=1)
            plt.yticks(np.arange(map_similarity.shape[0]), labels)
            plt.xticks(np.arange(map_similarity.shape[0]), labels, rotation='vertical')
            plt.xlim(-0.5, map_similarity.shape[0]-0.5)
            plt.ylim(-0.5, map_similarity.shape[0]-0.5)
            # plt.ylim(0, map_similarity.shape[0])
            a = plt.colorbar()
            a.set_label("Mean population vector correlation")
            plt.show()

        else:
            return map_similarity

    def compare_firing_rate_distributions(self, alpha=0.01):
        fir_exp_fam = self.exp_fam_1.get_raster()
        fir_pre = self.pre.get_raster()
        fir_post = self.post.get_raster()

        p_less_pre_post = []
        p_greater_pre_post = []
        p_two_sided_pre_post = []
        p_less_fam_pre = []
        p_greater_fam_pre = []
        p_two_sided_fam_pre = []
        for cell_id, (cell_fir_exp_fam, cell_fir_bef, cell_fir_aft) in enumerate(zip(fir_exp_fam, fir_pre, fir_post)):
            p_less_pre_post.append(mannwhitneyu(x=cell_fir_aft, y=cell_fir_bef, alternative="less")[1])
            p_greater_pre_post.append(mannwhitneyu(x=cell_fir_aft, y=cell_fir_bef, alternative="greater")[1])
            p_two_sided_pre_post.append(mannwhitneyu(x=cell_fir_aft, y=cell_fir_bef)[1])
            p_less_fam_pre.append(mannwhitneyu(x=cell_fir_bef, y=cell_fir_exp_fam, alternative="less")[1])
            p_greater_fam_pre.append(mannwhitneyu(x=cell_fir_bef, y=cell_fir_exp_fam, alternative="greater")[1])
            p_two_sided_fam_pre.append(mannwhitneyu(x=cell_fir_bef, y=cell_fir_exp_fam)[1])

        plt.scatter(np.arange(fir_post.shape[0]), p_two_sided_pre_post, color="green", label="pre_post")
        plt.scatter(np.arange(fir_post.shape[0]), p_two_sided_fam_pre, color="red", label="fam_pre")
        plt.legend()
        plt.show()
        print("HERE")

    def plot_rasters_contexts(self, down_sample_expl=False, cells_to_use="stable"):

            with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle", "rb") as f:
                class_dic = pickle.load(f)

            if cells_to_use == "stable":
                cell_ids = class_dic["stable_cell_ids"].flatten()
            elif cells_to_use == "decreasing":
                cell_ids = class_dic["decrease_cell_ids"].flatten()
            elif cells_to_use == "increasing":
                cell_ids = class_dic["increase_cell_ids"].flatten()

            raster_exp_fam_1 = self.exp_fam_1.get_raster()
            raster_pre = self.pre.get_raster(trials_to_use="all")
            raster_post = self.post.get_raster(trials_to_use="all")
            raster_exp_fam_2 = self.exp_fam_2.get_raster()

            if down_sample_expl:
                # down sample exploration familiar (much more data)
                raster_exp_fam_1 = raster_exp_fam_1[:, ::2]
                raster_exp_fam_2 = raster_exp_fam_2[:, ::2]

            all_data = np.hstack((raster_exp_fam_1, raster_pre, raster_post, raster_exp_fam_2))
            all_data = all_data[cell_ids, :]

            plt.figure(figsize=(15,5))
            plt.imshow(all_data, interpolation='nearest', aspect='auto')
            plt.vlines(raster_exp_fam_1.shape[1], 0, all_data.shape[0]-1, color="r")
            plt.vlines(raster_exp_fam_1.shape[1]+raster_pre.shape[1], 0, all_data.shape[0] - 1, color="r")
            plt.vlines(raster_exp_fam_1.shape[1] + raster_pre.shape[1] + raster_post.shape[1], 0, all_data.shape[0] - 1, color="r")
            plt.title(cells_to_use+" (EXP_FAM, PRE, POST, EXP_FAM)")
            plt.show()

    def distinguish_contexts(self, nr_splits=10, cells_to_use="stable"):

            with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle", "rb") as f:
                class_dic = pickle.load(f)

            cell_ids_stable = class_dic["stable_cell_ids"].flatten()
            cell_ids_decrease = class_dic["decrease_cell_ids"].flatten()
            cell_ids_increase = class_dic["increase_cell_ids"].flatten()

            raster_exp_fam_1 = self.exp_fam_1.get_raster()
            raster_pre = self.pre.get_raster(trials_to_use="all")
            raster_post = self.post.get_raster(trials_to_use="all")
            raster_exp_fam_2 = self.exp_fam_2.get_raster()

            # down sample exploration familiar (much more data)
            raster_exp_fam_1 = raster_exp_fam_1[:, ::2]
            raster_exp_fam_2 = raster_exp_fam_2[:, ::2]

            if cells_to_use == "stable":
                raster_exp_fam_1 = raster_exp_fam_1[cell_ids_stable,:]
                raster_exp_fam_2 = raster_exp_fam_2[cell_ids_stable, :]
                raster_post = raster_post[cell_ids_stable, :]
                raster_pre = raster_pre[cell_ids_stable, :]
            elif cells_to_use == "decreasing":
                raster_exp_fam_1 = raster_exp_fam_1[cell_ids_decrease,:]
                raster_exp_fam_2 = raster_exp_fam_2[cell_ids_decrease, :]
                raster_post = raster_post[cell_ids_decrease, :]
                raster_pre = raster_pre[cell_ids_decrease, :]
            elif cells_to_use == "increasing":
                raster_exp_fam_1 = raster_exp_fam_1[cell_ids_increase, :]
                raster_exp_fam_2 = raster_exp_fam_2[cell_ids_increase, :]
                raster_post = raster_post[cell_ids_increase, :]
                raster_pre = raster_pre[cell_ids_increase, :]

            contexts = [raster_exp_fam_1, raster_pre, raster_post, raster_exp_fam_2]


            context_ids = [0, 1, 2, 3]

            mean_acc = np.zeros((4, 4))
            mean_acc[:] = np.nan

            for combination in itertools.combinations(context_ids, r=2):
                context_a = contexts[combination[0]]
                context_b = contexts[combination[1]]

                X = np.hstack((context_a, context_b)).T
                y = np.zeros(X.shape[0])
                y[:context_a.shape[1]] = 1

                train_index_list = []
                test_index_list = []

                sss = StratifiedShuffleSplit(n_splits=nr_splits, test_size=0.3, random_state=0)
                for split_id, (train_index, test_index) in enumerate(sss.split(X, y)):
                    train_index_list.append(train_index)
                    test_index_list.append(test_index)

                train_test_indices = list(zip(train_index_list, test_index_list))

                with mp.Pool(nr_splits) as p:
                    # compute results for different splits in parallel
                    multi_arg = partial(self.svm_with_test_train_index, X=X, y=y)
                    mean_acc_splits = p.map(multi_arg, train_test_indices)

                mean_acc[combination[0], combination[1]] = np.mean(np.array(mean_acc_splits))

            plt.imshow(mean_acc)
            plt.xticks([0, 1, 2, 3], ["exp_fam_1", "pre", "post", "exp_fam_2"])
            plt.yticks([0, 1, 2, 3], ["exp_fam_1", "pre", "post", "exp_fam_2"])
            plt.colorbar()
            plt.show()

    def distinguish_contexts_subsets(self, nr_splits=2, nr_cells_in_subset=None, nr_subsets=10,
                                     cells_to_use="decreasing", save_fig=False, min_nr_spikes_per_pv=2, plotting=False):
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle", "rb") as f:
            class_dic = pickle.load(f)

        cell_ids_stable = class_dic["stable_cell_ids"].flatten()
        cell_ids_decrease = class_dic["decrease_cell_ids"].flatten()
        cell_ids_increase = class_dic["increase_cell_ids"].flatten()
        if nr_cells_in_subset is None:
            nr_cells_in_subset = np.min(np.array([cell_ids_stable.shape[0], cell_ids_decrease.shape[0],
                                                  cell_ids_increase.shape[0]]))

        raster_exp_fam_1 = self.exp_fam_1.get_raster()
        raster_pre = self.pre.get_raster(trials_to_use="all")
        raster_post = self.post.get_raster(trials_to_use="all")
        raster_exp_fam_2 = self.exp_fam_2.get_raster()

        # down sample exploration familiar (much more data)
        raster_exp_fam_1 = raster_exp_fam_1[:, ::2]
        raster_exp_fam_2 = raster_exp_fam_2[:, ::2]

        mean_acc = np.zeros((4, 4, nr_subsets))
        mean_acc[:] = np.nan

        for subset_id in range(nr_subsets):

            if cells_to_use == "stable":
                cell_ids_subset = np.random.choice(a=cell_ids_stable, size=nr_cells_in_subset, replace=False)
            elif cells_to_use == "decreasing":
                cell_ids_subset = np.random.choice(a=cell_ids_decrease, size=nr_cells_in_subset, replace=False)
            elif cells_to_use == "increasing":
                cell_ids_subset = np.random.choice(a=cell_ids_increase, size=nr_cells_in_subset, replace=False)

            raster_exp_fam_1_subset = raster_exp_fam_1[cell_ids_subset, :]
            raster_exp_fam_2_subset = raster_exp_fam_2[cell_ids_subset, :]
            raster_post_subset = raster_post[cell_ids_subset, :]
            raster_pre_subset = raster_pre[cell_ids_subset, :]

            contexts = [raster_exp_fam_1_subset, raster_pre_subset, raster_post_subset, raster_exp_fam_2_subset]

            # filter population vectors that contain less than 2 spikes
            contexts_filtered = []
            for context in contexts:
                contexts_filtered.append(context[:,np.count_nonzero(context, axis=0)> min_nr_spikes_per_pv])

            contexts = contexts_filtered

            context_ids = [0, 1, 2, 3]

            for combination in itertools.combinations(context_ids, r=2):
                context_a = contexts[combination[0]]
                context_b = contexts[combination[1]]

                X = np.hstack((context_a, context_b)).T
                y = np.zeros(X.shape[0])
                y[:context_a.shape[1]] = 1

                train_index_list = []
                test_index_list = []

                sss = StratifiedShuffleSplit(n_splits=nr_splits, test_size=0.3, random_state=0)
                for split_id, (train_index, test_index) in enumerate(sss.split(X, y)):
                    train_index_list.append(train_index)
                    test_index_list.append(test_index)

                train_test_indices = list(zip(train_index_list, test_index_list))

                with mp.Pool(nr_splits) as p:
                    # compute results for different splits in parallel
                    multi_arg = partial(self.svm_with_test_train_index, X=X, y=y)
                    mean_acc_splits = p.map(multi_arg, train_test_indices)

                mean_acc[combination[0], combination[1], subset_id] = np.mean(np.array(mean_acc_splits))

        mean_mean_acc = np.mean(mean_acc, axis=2)
        if plotting:
            plt.imshow(mean_mean_acc)
            plt.xticks([0, 1, 2, 3], ["exp_fam_1", "pre", "post", "exp_fam_2"])
            plt.yticks([0, 1, 2, 3], ["exp_fam_1", "pre", "post", "exp_fam_2"])
            plt.title(cells_to_use)
            a = plt.colorbar()
            a.set_label("Mean accuracy")
            if save_fig:
                plt.savefig(os.path.join(save_path, self.params.pre_proc_dir+"/temp_data/"+"svm_dist_env_"+
                                         self.session_name+"_"+cells_to_use+".png"))
                plt.close()
            else:
                plt.show()
        else:
            return mean_mean_acc

    def svm_with_test_train_index(self, train_test_indices, X, y):

        train_index = train_test_indices[0]
        test_index = train_test_indices[1]

        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        clf = make_pipeline(StandardScaler(), svm.SVC(gamma='auto', kernel="linear"))
        clf.fit(X_train, y_train)

        return clf.score(X_test, y_test)


class SleepBeforeSleep:
    """Class for long sleep"""

    def __init__(self, sleep_before, sleep, params, session_params):
        self.params = params
        self.session_params = session_params
        self.session_name = session_params.session_name

        self.sleep_before = sleep_before
        self.sleep = sleep

    def compute_likelihoods(self, cells_to_use="all"):
        # self.sleep_before.decode_activity_using_pre_post(template_type="phmm", part_to_analyze="all_swr")

        # compute likelihoods from sleep
        pre_model = self.sleep.session_params.default_pre_phmm_model
        self.sleep.decode_phmm_one_model(template_type="phmm", template_file_name=pre_model,cells_to_use=cells_to_use,
                                                part_to_analyze="all_swr", return_results=False, plot_for_control=False)

        # compute likelihoods from sleep_before
        self.sleep_before.decode_phmm_one_model(template_type="phmm", template_file_name=pre_model,cells_to_use=cells_to_use,
                                                part_to_analyze="all_swr", return_results=False)

    def compute_likelihoods_subsets(self, plotting=True, save_fig=False, use_max=True, cells_to_use="stable", split_sleep=True):
        if cells_to_use == "stable":
            pre_model = self.sleep_before.session_params.default_pre_phmm_model_stable
        elif cells_to_use == "decreasing":
            pre_model = self.sleep_before.session_params.default_pre_phmm_model_dec
        elif cells_to_use == "increasing":
            pre_model = self.sleep_before.session_params.default_pre_phmm_model_inc
        # get likelihoods from sleep_before
        self.sleep_before.decode_phmm_one_model_cell_subset(template_file_name=pre_model,
                                                                                      part_to_analyze="all_swr",
                                                                                      return_results=False,
                                                                                   cells_to_use=cells_to_use)

        # get likelihoods from sleep_before
        self.sleep.decode_phmm_one_model_cell_subset(template_file_name=pre_model,
                                                                                      part_to_analyze="all_swr",
                                                                                      return_results=False,
                                                                     cells_to_use = cells_to_use)

    def compare_likelihoods(self, plotting=True, save_fig=False, use_max=True, cells_to_use="all", split_sleep=True):
        # get likelihoods from sleep_before
        pre_model = self.sleep_before.session_params.default_pre_phmm_model
        likelihood_sleep_before, _, _, _ = self.sleep_before.decode_phmm_one_model(template_file_name=pre_model,
                                                                                      part_to_analyze="all_swr",
                                                                                      return_results=True,
                                                                                   cells_to_use=cells_to_use)

        # get likelihoods from sleep_before
        likelihood_sleep, _, _, _ = self.sleep.decode_phmm_one_model(template_file_name=pre_model,
                                                                                      part_to_analyze="all_swr",
                                                                                      return_results=True,
                                                                     cells_to_use = cells_to_use)

        likelihood_sleep_before = np.vstack(likelihood_sleep_before)
        likelihood_sleep = np.vstack(likelihood_sleep)

        if use_max:
            likelihood_sleep_before = np.max(likelihood_sleep_before, axis=1)
            likelihood_sleep = np.max(likelihood_sleep, axis=1)
        else:
            likelihood_sleep_before = likelihood_sleep_before.flatten()
            likelihood_sleep = likelihood_sleep.flatten()

        if split_sleep:
            likelihood_sleep_1 = likelihood_sleep[:int(likelihood_sleep.shape[0]/2)]
            likelihood_sleep_2 = likelihood_sleep[int(likelihood_sleep.shape[0]/2):]

            print("Sleep before vs. sleep 1")
            print(mannwhitneyu(likelihood_sleep_before, likelihood_sleep_1))
            print("Sleep before vs. sleep 2")
            print(mannwhitneyu(likelihood_sleep_before, likelihood_sleep_2))

        else:
            print(mannwhitneyu(likelihood_sleep_before, likelihood_sleep))

        if plotting or save_fig:

            if split_sleep:
                p_sleep_before = 1. * np.arange(likelihood_sleep_before.shape[0]) / ( likelihood_sleep_before.shape[0] - 1)
                p_sleep_1 = 1. * np.arange(likelihood_sleep_1.shape[0]) / (likelihood_sleep_1.shape[0] - 1)
                p_sleep_2 = 1. * np.arange(likelihood_sleep_2.shape[0]) / (likelihood_sleep_2.shape[0] - 1)
                if save_fig:
                    plt.style.use('default')
                    plt.close()
                plt.plot(np.sort(likelihood_sleep_before), p_sleep_before, color="greenyellow", label="Sleep before")
                plt.plot(np.sort(likelihood_sleep_1), p_sleep_1, color="lightgreen", label="Sleep_1")
                plt.plot(np.sort(likelihood_sleep_2), p_sleep_2, color="limegreen", label="Sleep_2")
                plt.gca().set_xscale("log")
                if use_max:
                    plt.xlabel("Max. likelihood per PV")
                else:
                    plt.xlabel("Likelihood per PV")
                plt.ylabel("CDF")
                plt.legend()
                if save_fig:
                    plt.rcParams['svg.fonttype'] = 'none'
                    plt.savefig(os.path.join(save_path, "decoding_sleep_before_sleep_example_" + cells_to_use + ".svg"),
                                transparent="True")
                    plt.close()
                else:
                    plt.show()
            else:
                p_sleep_before = 1. * np.arange(likelihood_sleep_before.shape[0]) / (likelihood_sleep_before.shape[0] - 1)
                p_sleep = 1. * np.arange(likelihood_sleep.shape[0]) / (likelihood_sleep.shape[0] - 1)
                if save_fig:
                    plt.style.use('default')
                    plt.close()
                plt.plot(np.sort(likelihood_sleep_before), p_sleep_before, color="greenyellow", label="Sleep before")
                plt.plot(np.sort(likelihood_sleep), p_sleep, color="limegreen", label="Sleep")
                plt.gca().set_xscale("log")
                if use_max:
                    plt.xlabel("Max. likelihood per PV")
                else:
                    plt.xlabel("Likelihood per PV")
                plt.ylabel("CDF")
                plt.legend()
                if save_fig:
                    plt.rcParams['svg.fonttype'] = 'none'
                    plt.savefig(os.path.join(save_path, "decoding_sleep_before_sleep_example_"+cells_to_use+".svg"),
                                transparent="True")
                    plt.close()
                else:
                    plt.show()


        else:
            if split_sleep:
                return likelihood_sleep_before, likelihood_sleep_1, likelihood_sleep_2
            else:
                return likelihood_sleep_before, likelihood_sleep

    def compare_likelihoods_subsets(self, plotting=True, save_fig=False, use_max=True, cells_to_use="stable",
                                    split_sleep=True, z_score=False):
        if cells_to_use == "stable":
            pre_model = self.sleep_before.session_params.default_pre_phmm_model_stable
        elif cells_to_use == "decreasing":
            pre_model = self.sleep_before.session_params.default_pre_phmm_model_dec
        # get likelihoods from sleep_before
        likelihood_sleep_before, _, _, _ = self.sleep_before.decode_phmm_one_model_cell_subset(template_file_name=pre_model,
                                                                                      part_to_analyze="all_swr",
                                                                                      return_results=True,
                                                                                   cells_to_use=cells_to_use)

        # get likelihoods from sleep_before
        likelihood_sleep, _, _, _ = self.sleep.decode_phmm_one_model_cell_subset(template_file_name=pre_model,
                                                                                      part_to_analyze="all_swr",
                                                                                      return_results=True,
                                                                     cells_to_use = cells_to_use)

        likelihood_sleep_before = np.vstack(likelihood_sleep_before)
        likelihood_sleep = np.vstack(likelihood_sleep)

        if use_max:
            likelihood_sleep_before = np.max(likelihood_sleep_before, axis=1)
            likelihood_sleep = np.max(likelihood_sleep, axis=1)
        else:
            likelihood_sleep_before = likelihood_sleep_before.flatten()
            likelihood_sleep = likelihood_sleep.flatten()

        if split_sleep:
            likelihood_sleep_1 = likelihood_sleep[:int(likelihood_sleep.shape[0]/2)]
            likelihood_sleep_2 = likelihood_sleep[int(likelihood_sleep.shape[0]/2):]

            # check if data needs to be z-scored
            if z_score:
                # combine all data to z-score
                all_likeli = np.hstack((likelihood_sleep_before, likelihood_sleep_1, likelihood_sleep_2))
                all_likeli_z = zscore(all_likeli)
                likelihood_sleep_before = all_likeli_z[:likelihood_sleep_before.shape[0]]
                likelihood_sleep_1 = all_likeli_z[likelihood_sleep_before.shape[0]:(likelihood_sleep_before.shape[0]+
                                                                                    likelihood_sleep_1.shape[0])]
                likelihood_sleep_2 = all_likeli_z[(likelihood_sleep_before.shape[0]+likelihood_sleep_1.shape[0]):]

            print("Sleep before vs. sleep 1")
            print(mannwhitneyu(likelihood_sleep_before, likelihood_sleep_1))
            print("Sleep before vs. sleep 2")
            print(mannwhitneyu(likelihood_sleep_before, likelihood_sleep_2))

        else:
            if z_score:
                # combine all data to z-score
                all_likeli = np.hstack((likelihood_sleep_before, likelihood_sleep))
                all_likeli_z = zscore(all_likeli)
                likelihood_sleep_before = all_likeli_z[:likelihood_sleep_before.shape[0]]
                likelihood_sleep = all_likeli_z[likelihood_sleep_before.shape[0]:]
            print(mannwhitneyu(likelihood_sleep_before, likelihood_sleep))

        if plotting or save_fig:

            if split_sleep:
                p_sleep_before = 1. * np.arange(likelihood_sleep_before.shape[0]) / ( likelihood_sleep_before.shape[0] - 1)
                p_sleep_1 = 1. * np.arange(likelihood_sleep_1.shape[0]) / (likelihood_sleep_1.shape[0] - 1)
                p_sleep_2 = 1. * np.arange(likelihood_sleep_2.shape[0]) / (likelihood_sleep_2.shape[0] - 1)
                if save_fig:
                    plt.style.use('default')
                    plt.close()
                plt.plot(np.sort(likelihood_sleep_before), p_sleep_before, color="greenyellow", label="Sleep before")
                plt.plot(np.sort(likelihood_sleep_1), p_sleep_1, color="lightgreen", label="Sleep_1")
                plt.plot(np.sort(likelihood_sleep_2), p_sleep_2, color="limegreen", label="Sleep_2")
                plt.gca().set_xscale("log")
                if use_max:
                    plt.xlabel("Max. likelihood per PV")
                else:
                    plt.xlabel("Likelihood per PV")
                plt.ylabel("CDF")
                plt.legend()
                if save_fig:
                    plt.rcParams['svg.fonttype'] = 'none'
                    plt.savefig(os.path.join(save_path, "decoding_sleep_before_sleep_example_" + cells_to_use + ".svg"),
                                transparent="True")
                    plt.close()
                else:
                    plt.show()
            else:
                p_sleep_before = 1. * np.arange(likelihood_sleep_before.shape[0]) / (likelihood_sleep_before.shape[0] - 1)
                p_sleep = 1. * np.arange(likelihood_sleep.shape[0]) / (likelihood_sleep.shape[0] - 1)
                if save_fig:
                    plt.style.use('default')
                    plt.close()
                plt.plot(np.sort(likelihood_sleep_before), p_sleep_before, color="greenyellow", label="Sleep before")
                plt.plot(np.sort(likelihood_sleep), p_sleep, color="limegreen", label="Sleep")
                plt.gca().set_xscale("log")
                if use_max:
                    plt.xlabel("Max. likelihood per PV")
                else:
                    plt.xlabel("Likelihood per PV")
                plt.ylabel("CDF")
                plt.legend()
                if save_fig:
                    plt.rcParams['svg.fonttype'] = 'none'
                    plt.savefig(os.path.join(save_path, "decoding_sleep_before_sleep_example_"+cells_to_use+".svg"),
                                transparent="True")
                    plt.close()
                else:
                    plt.show()


        else:
            if split_sleep:
                return likelihood_sleep_before, likelihood_sleep_1, likelihood_sleep_2
            else:
                return likelihood_sleep_before, likelihood_sleep

    def compare_max_post_probabilities(self, plotting=True, save_fig=False, cells_to_use="all"):
        # get likelihoods from sleep_before
        pre_model = self.sleep_before.session_params.default_pre_phmm_model
        likelihood_sleep_before, _, _, _ = self.sleep_before.decode_phmm_one_model(template_file_name=pre_model,
                                                                                   part_to_analyze="all_swr",
                                                                                   return_results=True,
                                                                                   cells_to_use=cells_to_use)

        # get likelihoods from sleep_before
        likelihood_sleep, _, _, _ = self.sleep.decode_phmm_one_model(template_file_name=pre_model,
                                                                     part_to_analyze="all_swr",
                                                                     return_results=True,
                                                                     cells_to_use=cells_to_use)

        likelihood_sleep_before = np.vstack(likelihood_sleep_before)
        likelihood_sleep = np.vstack(likelihood_sleep)

        # compute posterior probabilities
        posterior_prob_sleep_before = likelihood_sleep_before / np.sum(likelihood_sleep_before, axis=1,
                                                                       keepdims=True)
        posterior_prob_sleep = likelihood_sleep / np.sum(likelihood_sleep, axis=1, keepdims=True)

        max_posterior_prob_sleep_before = np.max(posterior_prob_sleep_before, axis=1)
        max_posterior_prob_sleep = np.max(posterior_prob_sleep, axis=1)

        # split sleep into sleep 1 and sleep 2
        max_posterior_prob_sleep_1 = max_posterior_prob_sleep[:int(max_posterior_prob_sleep.shape[0] / 2)]
        max_posterior_prob_sleep_2 = max_posterior_prob_sleep[int(max_posterior_prob_sleep.shape[0] / 2):]

        print("Sleep before vs. sleep 1")
        print(mannwhitneyu(max_posterior_prob_sleep_before, max_posterior_prob_sleep_1))
        print("Sleep before vs. sleep 2")
        print(mannwhitneyu(max_posterior_prob_sleep_before, max_posterior_prob_sleep_2))

        if plotting or save_fig:

            p_sleep_before = 1. * np.arange(max_posterior_prob_sleep_before.shape[0]) / (
                        max_posterior_prob_sleep_before.shape[0] - 1)
            p_sleep_1 = 1. * np.arange(max_posterior_prob_sleep_1.shape[0]) / (
                        max_posterior_prob_sleep_1.shape[0] - 1)
            p_sleep_2 = 1. * np.arange(max_posterior_prob_sleep_2.shape[0]) / (
                        max_posterior_prob_sleep_2.shape[0] - 1)
            if save_fig:
                plt.style.use('default')
                plt.close()
            plt.plot(np.sort(max_posterior_prob_sleep_before), p_sleep_before, color="greenyellow",
                     label="Sleep before")
            plt.plot(np.sort(max_posterior_prob_sleep_1), p_sleep_1, color="lightgreen", label="Sleep_1")
            plt.plot(np.sort(max_posterior_prob_sleep_2), p_sleep_2, color="limegreen", label="Sleep_2")
            plt.gca().set_xscale("log")

            plt.xlabel("Max. post. probability per PV (z-scored)")
            plt.ylabel("CDF")
            plt.legend()
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "decoding_sleep_before_sleep_example_" + cells_to_use + ".svg"),
                            transparent="True")
                plt.close()
            else:
                plt.show()
        else:
            return max_posterior_prob_sleep_before, max_posterior_prob_sleep_1, max_posterior_prob_sleep_2

    def likelihood_difference(self, subset_of_sleep=None, use_decoded_modes=False, use_norm_diff=True,
                              return_mode_ids=False):

        # get models from pre for stable or decreasing cells

        pre_model = self.sleep_before.session_params.default_pre_phmm_model
        # get likelihoods from sleep_before
        likelihood_sleep_before, event_times_sleep_before, _, _ = self.sleep_before.decode_phmm_one_model(template_file_name=pre_model,
                                                                                               part_to_analyze="all_swr",
                                                                                               return_results=True)
        # get likelihoods from sleep_after
        likelihood_sleep, event_times_sleep, _, _ = self.sleep.decode_phmm_one_model(template_file_name=pre_model,
                                                                                 part_to_analyze="all_swr",
                                                                                 return_results=True)

        likelihood_sleep_before = np.vstack(likelihood_sleep_before)
        if not subset_of_sleep is None:
            event_times_sleep = event_times_sleep[:,2]
            duration_sleep = np.max(event_times_sleep)
            # separate into 4 pieces
            dur_quarter = duration_sleep*0.25
            subset_start = (subset_of_sleep-1) * dur_quarter
            subset_end = (subset_of_sleep) * dur_quarter
            events_to_use = np.logical_and(subset_start<=event_times_sleep, event_times_sleep<=subset_end).flatten()
            likelihood_sleep_subset = [x for i, x in enumerate(likelihood_sleep) if events_to_use[i]]
            likelihood_sleep = likelihood_sleep_subset

        likelihood_sleep = np.vstack(likelihood_sleep)

        # only take mean of decoded modes
        if use_decoded_modes:
            # compute mean of decoded mode
            decoded_mode_sleep = np.argmax(likelihood_sleep, axis=1)
            decoded_mode_ids_sleep = np.unique(decoded_mode_sleep)
            decoded_mode_sleep_before = np.argmax(likelihood_sleep_before, axis=1)
            decoded_mode_ids_sleep_before = np.unique(decoded_mode_sleep_before)

            decoded_mode_sleep_before_mean_likeli = np.zeros(likelihood_sleep_before.shape[1])
            for mode_id in decoded_mode_ids_sleep_before:
                decoded_mode_sleep_before_mean_likeli[mode_id] = np.mean(likelihood_sleep_before[:,mode_id]
                                                                         [decoded_mode_sleep_before==mode_id])

            decoded_mode_sleep_mean_likeli = np.zeros(likelihood_sleep.shape[1])
            for mode_id in decoded_mode_ids_sleep:
                decoded_mode_sleep_mean_likeli[mode_id] = np.mean(likelihood_sleep[:, mode_id]
                                                                  [decoded_mode_sleep==mode_id])

            # delete modes that are neither expressed in sleep before nor in sleep after
            to_delete = np.where(np.logical_and(decoded_mode_sleep_before_mean_likeli == 0, decoded_mode_sleep_before_mean_likeli==0))
            mode_ids = range(decoded_mode_sleep_before_mean_likeli.shape[0])
            # only keep mode ids of decoded modes
            mode_ids = np.delete(mode_ids, to_delete)
            decoded_mode_sleep_mean_likeli = np.delete(decoded_mode_sleep_mean_likeli, to_delete)
            decoded_mode_sleep_before_mean_likeli = np.delete(decoded_mode_sleep_before_mean_likeli, to_delete)

            # add small value to avoid division by zero
            likelihood_sleep_before = decoded_mode_sleep_before_mean_likeli + 10e-200
            likelihood_sleep = decoded_mode_sleep_mean_likeli + 10e-200
            if use_norm_diff:
                diff_sleep_sleep = (likelihood_sleep - likelihood_sleep_before) / (likelihood_sleep_before + likelihood_sleep)
            else:
                diff_sleep_sleep = likelihood_sleep_before / likelihood_sleep
        else:
            # diff_sleep_sleep = np.mean(likelihood_sleep_before, axis=0) - np.mean(likelihood_sleep, axis=0)
            if use_norm_diff:
                diff_sleep_sleep = (np.mean(likelihood_sleep_before, axis=0) - np.mean(likelihood_sleep, axis=0)) / \
                                   (np.mean(likelihood_sleep_before, axis=0) + np.mean(likelihood_sleep, axis=0))
            else:
                diff_sleep_sleep = np.mean(likelihood_sleep_before, axis=0) / np.mean(likelihood_sleep, axis=0)

        if use_decoded_modes and return_mode_ids:
            return diff_sleep_sleep, mode_ids
        else:
            return diff_sleep_sleep

    def likelihood_difference_subsets(self, cells_to_use="stable", subset_of_sleep=None,
                                      use_decoded_modes=False, use_norm_diff=True, return_mode_ids=False):

        # get models from pre for stable or decreasing cells
        if cells_to_use == "stable":
            pre_model = self.sleep_before.session_params.default_pre_phmm_model_stable
        elif cells_to_use == "decreasing":
            pre_model = self.sleep_before.session_params.default_pre_phmm_model_dec
        elif cells_to_use == "increasing":
            pre_model = self.sleep_before.session_params.default_pre_phmm_model_inc
        # get likelihoods from sleep_before
        likelihood_sleep_before, event_times_sleep_before, _, _ = self.sleep_before.decode_phmm_one_model_cell_subset(template_file_name=pre_model,
                                                                                               part_to_analyze="all_swr",
                                                                                               return_results=True,
                                                                                               cells_to_use=cells_to_use)
        # get likelihoods from sleep_after
        likelihood_sleep, event_times_sleep, _, _ = self.sleep.decode_phmm_one_model_cell_subset(template_file_name=pre_model,
                                                                                 part_to_analyze="all_swr",
                                                                                 return_results=True,
                                                                                 cells_to_use=cells_to_use)

        likelihood_sleep_before = np.vstack(likelihood_sleep_before)
        if not subset_of_sleep is None:
            event_times_sleep = event_times_sleep[:,2]
            duration_sleep = np.max(event_times_sleep)
            # separate into 4 pieces
            dur_quarter = duration_sleep*0.25
            subset_start = (subset_of_sleep-1) * dur_quarter
            subset_end = (subset_of_sleep) * dur_quarter
            events_to_use = np.logical_and(subset_start<=event_times_sleep, event_times_sleep<=subset_end).flatten()
            likelihood_sleep_subset = [x for i, x in enumerate(likelihood_sleep) if events_to_use[i]]
            likelihood_sleep = likelihood_sleep_subset

        likelihood_sleep = np.vstack(likelihood_sleep)

        # only take mean of decoded modes
        if use_decoded_modes:
            # compute mean of decoded mode
            decoded_mode_sleep = np.argmax(likelihood_sleep, axis=1)
            decoded_mode_ids_sleep = np.unique(decoded_mode_sleep)
            decoded_mode_sleep_before = np.argmax(likelihood_sleep_before, axis=1)
            decoded_mode_ids_sleep_before = np.unique(decoded_mode_sleep_before)

            decoded_mode_sleep_before_mean_likeli = np.zeros(likelihood_sleep_before.shape[1])
            for mode_id in decoded_mode_ids_sleep_before:
                decoded_mode_sleep_before_mean_likeli[mode_id] = np.mean(likelihood_sleep_before[:,mode_id]
                                                                         [decoded_mode_sleep_before==mode_id])

            decoded_mode_sleep_mean_likeli = np.zeros(likelihood_sleep.shape[1])
            for mode_id in decoded_mode_ids_sleep:
                decoded_mode_sleep_mean_likeli[mode_id] = np.mean(likelihood_sleep[:, mode_id]
                                                                  [decoded_mode_sleep==mode_id])

            # delete modes that are neither expressed in sleep before nor in sleep after
            to_delete = np.where(np.logical_and(decoded_mode_sleep_before_mean_likeli == 0, decoded_mode_sleep_before_mean_likeli==0))
            mode_ids = range(decoded_mode_sleep_before_mean_likeli.shape[0])
            # only keep mode ids of decoded modes
            mode_ids = np.delete(mode_ids, to_delete)
            decoded_mode_sleep_mean_likeli = np.delete(decoded_mode_sleep_mean_likeli, to_delete)
            decoded_mode_sleep_before_mean_likeli = np.delete(decoded_mode_sleep_before_mean_likeli, to_delete)

            # add small value to avoid division by zero
            likelihood_sleep_before = decoded_mode_sleep_before_mean_likeli + 10e-200
            likelihood_sleep = decoded_mode_sleep_mean_likeli + 10e-200
            if use_norm_diff:
                diff_sleep_sleep = (likelihood_sleep - likelihood_sleep_before) / (likelihood_sleep_before + likelihood_sleep)
            else:
                diff_sleep_sleep = likelihood_sleep_before / likelihood_sleep
        else:
            # diff_sleep_sleep = np.mean(likelihood_sleep_before, axis=0) - np.mean(likelihood_sleep, axis=0)
            if use_norm_diff:
                diff_sleep_sleep = (np.mean(likelihood_sleep_before, axis=0) - np.mean(likelihood_sleep, axis=0)) / \
                                   (np.mean(likelihood_sleep_before, axis=0) + np.mean(likelihood_sleep, axis=0))
            else:
                diff_sleep_sleep = np.mean(likelihood_sleep_before, axis=0) / np.mean(likelihood_sleep, axis=0)

        if use_decoded_modes and return_mode_ids:
            return diff_sleep_sleep, mode_ids
        else:
            return diff_sleep_sleep

    def likelihoods_phmm(self, cells_to_use="stable", subset_of_sleep=None,
                                      use_decoded_modes=False, use_norm_diff=True, return_mode_ids=False):

        # get models from pre for stable or decreasing cells
        if cells_to_use == "stable":
            pre_model = self.sleep_before.session_params.default_pre_phmm_model_stable
        elif cells_to_use == "decreasing":
            pre_model = self.sleep_before.session_params.default_pre_phmm_model_dec
        elif cells_to_use == "increasing":
            pre_model = self.sleep_before.session_params.default_pre_phmm_model_inc
        # get likelihoods from sleep_before
        likelihood_sleep_before, event_times_sleep_before, _, _ = self.sleep_before.decode_phmm_one_model_cell_subset(template_file_name=pre_model,
                                                                                               part_to_analyze="all_swr",
                                                                                               return_results=True,
                                                                                               cells_to_use=cells_to_use)
        # get likelihoods from sleep_after
        likelihood_sleep, event_times_sleep, _, _ = self.sleep.decode_phmm_one_model_cell_subset(template_file_name=pre_model,
                                                                                 part_to_analyze="all_swr",
                                                                                 return_results=True,
                                                                                 cells_to_use=cells_to_use)

        likelihood_sleep_before = np.vstack(likelihood_sleep_before)
        if not subset_of_sleep is None:
            event_times_sleep = event_times_sleep[:,2]
            duration_sleep = np.max(event_times_sleep)
            # separate into 4 pieces
            dur_quarter = duration_sleep*0.25
            subset_start = (subset_of_sleep-1) * dur_quarter
            subset_end = (subset_of_sleep) * dur_quarter
            events_to_use = np.logical_and(subset_start<=event_times_sleep, event_times_sleep<=subset_end).flatten()
            likelihood_sleep_subset = [x for i, x in enumerate(likelihood_sleep) if events_to_use[i]]
            likelihood_sleep = likelihood_sleep_subset

        likelihood_sleep = np.vstack(likelihood_sleep)

        return likelihood_sleep_before, likelihood_sleep

    def decoded_modes_before_after(self, cells_to_use="stable",
                                      use_decoded_modes=False, use_norm_diff=True, return_mode_ids=False):

        # get models from pre for stable or decreasing cells
        if cells_to_use == "stable":
            pre_model = self.sleep_before.session_params.default_pre_phmm_model_stable
        elif cells_to_use == "decreasing":
            pre_model = self.sleep_before.session_params.default_pre_phmm_model_dec
        elif cells_to_use == "increasing":
            pre_model = self.sleep_before.session_params.default_pre_phmm_model_inc
        # get likelihoods from sleep_before
        likelihood_sleep_before, _, _, _ = self.sleep_before.decode_phmm_one_model_cell_subset(template_file_name=pre_model,
                                                                                               part_to_analyze="all_swr",
                                                                                               return_results=True,
                                                                                               cells_to_use=cells_to_use)
        # get likelihoods from sleep_after
        likelihood_sleep, _, _, _ = self.sleep.decode_phmm_one_model_cell_subset(template_file_name=pre_model,
                                                                                 part_to_analyze="all_swr",
                                                                                 return_results=True,
                                                                                 cells_to_use=cells_to_use)

        likelihood_sleep_before = np.vstack(likelihood_sleep_before)
        likelihood_sleep = np.vstack(likelihood_sleep)

        # compute mean of decoded mode
        decoded_mode_sleep = np.argmax(likelihood_sleep, axis=1)
        decoded_mode_ids_sleep = np.unique(decoded_mode_sleep)
        decoded_mode_sleep_before = np.argmax(likelihood_sleep_before, axis=1)
        decoded_mode_ids_sleep_before = np.unique(decoded_mode_sleep_before)

        only_decoded_before = np.setdiff1d(decoded_mode_ids_sleep_before, decoded_mode_ids_sleep)
        only_decoded_after = np.setdiff1d(decoded_mode_ids_sleep, decoded_mode_ids_sleep_before)
        decoded_before_and_after = np.intersect1d(decoded_mode_ids_sleep_before, decoded_mode_ids_sleep)
        decoded_before_or_after = np.unique(np.hstack((decoded_mode_ids_sleep, decoded_mode_ids_sleep_before)))
        never_decoded_modes = likelihood_sleep.shape[1]-decoded_before_or_after.shape[0]

        return only_decoded_before.shape[0]/likelihood_sleep.shape[1], \
               only_decoded_after.shape[0]/likelihood_sleep.shape[1], \
               decoded_mode_ids_sleep_before.shape[0]/likelihood_sleep.shape[1], \
               never_decoded_modes/likelihood_sleep.shape[1]

    def ratio_modes_active_before_sleep_or_sleep(self, cells_to_use="stable",
                                      use_decoded_modes=False, use_norm_diff=True, return_mode_ids=False):

        # get models from pre for stable or decreasing cells
        if cells_to_use == "stable":
            pre_model = self.sleep_before.session_params.default_pre_phmm_model_stable
        elif cells_to_use == "decreasing":
            pre_model = self.sleep_before.session_params.default_pre_phmm_model_dec
        # get likelihoods from sleep_before
        likelihood_sleep_before, _, _, _ = self.sleep_before.decode_phmm_one_model_cell_subset(template_file_name=pre_model,
                                                                                               part_to_analyze="all_swr",
                                                                                               return_results=True,
                                                                                               cells_to_use=cells_to_use)
        # get likelihoods from sleep_after
        likelihood_sleep, _, _, _ = self.sleep.decode_phmm_one_model_cell_subset(template_file_name=pre_model,
                                                                                 part_to_analyze="all_swr",
                                                                                 return_results=True,
                                                                                 cells_to_use=cells_to_use)

        likelihood_sleep_before = np.vstack(likelihood_sleep_before)
        likelihood_sleep = np.vstack(likelihood_sleep)


        # compute mean of decoded mode
        decoded_mode_sleep = np.argmax(likelihood_sleep, axis=1)
        decoded_mode_ids_sleep = np.unique(decoded_mode_sleep)
        decoded_mode_sleep_before = np.argmax(likelihood_sleep_before, axis=1)
        decoded_mode_ids_sleep_before = np.unique(decoded_mode_sleep_before)

        decoded_mode_sleep_before_mean_likeli = np.zeros(likelihood_sleep_before.shape[1])
        for mode_id in decoded_mode_ids_sleep_before:
            decoded_mode_sleep_before_mean_likeli[mode_id] = np.mean(likelihood_sleep_before[:,mode_id]
                                                                     [decoded_mode_sleep_before==mode_id])

        decoded_mode_sleep_mean_likeli = np.zeros(likelihood_sleep.shape[1])
        for mode_id in decoded_mode_ids_sleep:
            decoded_mode_sleep_mean_likeli[mode_id] = np.mean(likelihood_sleep[:, mode_id]
                                                              [decoded_mode_sleep==mode_id])

        # delete modes that are neither expressed in sleep before nor in sleep after
        to_delete = np.where(np.logical_and(decoded_mode_sleep_before_mean_likeli == 0, decoded_mode_sleep_before_mean_likeli==0))
        mode_ids = range(decoded_mode_sleep_before_mean_likeli.shape[0])
        mode_ids = np.delete(mode_ids, to_delete)
        decoded_mode_sleep_mean_likeli = np.delete(decoded_mode_sleep_mean_likeli, to_delete)
        decoded_mode_sleep_before_mean_likeli = np.delete(decoded_mode_sleep_before_mean_likeli, to_delete)
        ratio_modes_before_sleep_or_sleep = mode_ids.shape[0]/likelihood_sleep.shape[1]

        return ratio_modes_before_sleep_or_sleep


class SleepBeforePreSleep:
    """Class for long sleep"""

    def __init__(self, sleep_before, pre, sleep, params, session_params):
        self.params = params
        self.session_params = session_params
        self.session_name = session_params.session_name

        self.sleep_before = sleep_before
        self.sleep = sleep
        self.pre = pre
        self.sleep_before_sleep = SleepBeforeSleep(sleep_before=sleep_before, sleep=sleep, params=params,
                                                   session_params=session_params)

    def pre_play_learning_phmm_modes(self, trials_to_use_for_decoding="all", plot_for_control=False,
                                     cells_to_use="stable", plotting=True, only_decoded_modes=False, use_norm_diff=True,
                                     n_moving=500, save_fig=False):
            
        # get likelihoods from awake decoding
        posterior_prob, seq = self.pre.learning_phmm_modes_activation(trials_to_use_for_decoding=trials_to_use_for_decoding,
                                                                 cells_to_use=cells_to_use)

        if only_decoded_modes:
            # get likelihood ratio sleep_before/sleep_after
            likeli_ratio_sleep, modes_to_delete = self.sleep_before_sleep.likelihood_difference_subsets(cells_to_use=cells_to_use,
                                                                                       use_decoded_modes=True, return_mode_ids=True)

        else:
            # get likelihood ratio sleep_before/sleep_after
            likeli_ratio_sleep = self.sleep_before_sleep.likelihood_difference_subsets(cells_to_use=cells_to_use,
                                                                                       use_decoded_modes=False,
                                                                                       use_norm_diff=use_norm_diff)

        # use only values from decoded value
        window_size = 20
        nr_windows = np.int(np.round(posterior_prob.shape[0]/window_size))
        freq_modes = np.zeros((posterior_prob.shape[1], nr_windows))
        for win_id in range(nr_windows):
            mode_id, counts = np.unique(seq[win_id*window_size:(win_id+1)*window_size], return_counts=True)
            freq_modes[mode_id, win_id] = counts

        pre_modulation = np.zeros(posterior_prob.shape[1])
        mode_id, counts = np.unique(seq, return_counts=True)
        pre_modulation[mode_id] = counts

        # compute average posterior probabilities for first 20% and last 20%
        # post_prob_first_20 = np.mean(posterior_prob[:int(posterior_prob.shape[0]*0.2),:], axis=0)
        # post_prob_last_20 = np.mean(posterior_prob[int(posterior_prob.shape[0] * 0.8):, :], axis=0)
        #
        # pre_modulation = post_prob_first_20/post_prob_last_20

        # fit regression line
        # pre_modulation = []
        # freq = freq_modes.T
        # for post_prob in freq.T:
        #     smooth_post_prob = moving_average(a=post_prob, n=int(post_prob.shape[0]/5))
        #     # normalize to one
        #     smooth_post_prob /= np.max(smooth_post_prob)
        #     # compute regression
        #     coef = np.polyfit(np.linspace(0,1, smooth_post_prob.shape[0]), smooth_post_prob, 1)
        #     pre_modulation.append(coef[0])
        #     if plot_for_control:
        #         poly1d_fn = np.poly1d(coef)
        #         plt.plot(np.linspace(0,1, smooth_post_prob.shape[0]), smooth_post_prob)
        #         plt.plot(np.linspace(0,1, smooth_post_prob.shape[0]), poly1d_fn(np.linspace(0,1, smooth_post_prob.shape[0])), '--w')
        #         plt.title(coef)
        #         plt.show()
        # pre_modulation = np.array(pre_modulation)

        # if only_decoded_modes:
            # delete modes that were not decoded during sleep
            # pre_modulation = np.delete(pre_modulation, modes_to_delete)


        if plotting or save_fig:
            # print(pearsonr(pre_modulation, likeli_ratio_sleep)[0])
            # plt.scatter(likeli_ratio_sleep, pre_modulation/pre_modulation.shape[0])
            # plt.ylabel("%times active in PRE")
            # # plt.xlabel("likeli_sleep_before/likeli_sleep_after")
            # plt.xlabel("(likelihood_sleep_before - likelihood_sleep) / (likelihood_sleep_before + likelihood_sleep)")
            # # plt.yscale("symlog")
            # # plt.xscale("symlog")
            # plt.title(cells_to_use)
            # # plt.gca().set_aspect('equal', 'box')
            # plt.show()
            #
            #
            # # select modes that are active during sleep before
            post_prob_stronger_before = posterior_prob[:, likeli_ratio_sleep > 0.5]
            post_prob_stronger_before_max = np.max(post_prob_stronger_before, axis=1)
            max_time = post_prob_stronger_before_max.shape[0]*self.params.time_bin_size/60
            smooth_before = moving_average(post_prob_stronger_before_max, n=n_moving)
            if save_fig:
                plt.style.use('default')
            plt.plot(np.linspace(0,max_time, smooth_before.shape[0]), smooth_before)
            plt.xlabel("Time during acquisition (min)")
            plt.ylabel("Max post. prob")
            plt.title(cells_to_use +" cells:\nModes that are more reactivated in sleep BEFORE")
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "learning_preplay_states_"+cells_to_use+".svg"),
                            transparent="True")
                plt.close()
            else:
                plt.show()

            post_prob_stronger_after = posterior_prob[:, likeli_ratio_sleep < 0.5]
            post_prob_stronger_after_max = np.max(post_prob_stronger_after, axis=1)
            smooth_after = moving_average(post_prob_stronger_after_max, n=n_moving)
            plt.plot(np.linspace(0,max_time, smooth_after.shape[0]), smooth_after)
            plt.xlabel("Time during acquisition (min)")
            plt.ylabel("Max post. prob")
            plt.title(cells_to_use +" cells:\nModes that are more reactivated in sleep AFTER")
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "learning_replay_states_"+cells_to_use+".svg"), transparent="True")
                plt.close()
            else:
                plt.show()

            #
            # post_prob_stronger_before_mean = np.mean(post_prob_stronger_before, axis=1)
            # plt.plot(moving_average(post_prob_stronger_before_mean, n=n_smoothing))
            # plt.xlabel("Time bin (PRE)")
            # plt.ylabel("Mean post-prob")
            # plt.title(cells_to_use +" cells:\nModes that are more reactivated in sleep BEFORE")
            # plt.show()
            #
            # post_prob_stronger_before_z = zscore(post_prob[:, likeli_ratio_sleep > 1], axis=0)
            # # plt.plot(likeli_stronger_before_z)
            # # plt.show()
            #
            # post_prob_stronger_before_z_mean = np.mean(post_prob_stronger_before_z, axis=1)
            # plt.plot(moving_average(post_prob_stronger_before_z_mean, n=n_smoothing))
            # plt.xlabel("Time bin (PRE)")
            # plt.ylabel("Mean of z-scored post-prob")
            # plt.title(cells_to_use +" cells:\nModes that are more reactivated in sleep BEFORE")
            # plt.show()
            #
            # # select modes that are active during sleep after
            # post_prob_stronger_after = post_prob[:, likeli_ratio_sleep < 1]
            #
            # # plt.plot(likeli_stronger_after)
            # # plt.show()
            #
            # post_prob_stronger_after_z = zscore(post_prob[:, likeli_ratio_sleep < 1], axis=0)
            # # plt.plot(likeli_stronger_after_z)
            # # plt.show()
            #
            # post_prob_stronger_after_z_mean = np.mean(post_prob_stronger_after_z, axis=1)
            # plt.plot(moving_average(post_prob_stronger_after_z_mean, n=n_smoothing))
            # plt.xlabel("Time bin (PRE)")
            # plt.ylabel("Mean of z-scored post. probs")
            # plt.title(cells_to_use +" cells:\nModes that are more reactivated in sleep AFTER")
            # plt.show()
            #
            # post_prob_stronger_after_mean = np.mean(post_prob_stronger_after, axis=1)
            # plt.plot(moving_average(post_prob_stronger_after_mean, n=n_smoothing))
            # plt.xlabel("Time bin (PRE)")
            # plt.ylabel("Mean post-prob")
            # plt.title(cells_to_use +" cells:\nModes that are more reactivated in sleep AFTER")
            # plt.show()
            #
            # # select modes that are active during sleep after
            # post_prob_stronger_after = post_prob[:, likeli_ratio_sleep < 1]
            # post_prob_stronger_after_max = np.max(post_prob_stronger_after, axis=1)
            # plt.plot(moving_average(post_prob_stronger_after_max, n=500))
            # plt.xlabel("Time bin (PRE)")
            # plt.ylabel("Max post. prob")
            # plt.title(cells_to_use +" cells:\nModes that are more reactivated in sleep AFTER")
            # plt.show()

        else:
            return pre_modulation, likeli_ratio_sleep

    def learning_phmm_modes_frequency_and_likelihoods(self, trials_to_use_for_decoding="all", plot_for_control=False,
                                     cells_to_use="stable", plotting=False, only_decoded_modes=False,
                                     use_norm_diff=True, save_fig=False, subset_of_sleep=None,
                                                      z_score_pre_modulation=False):

        # get likelihoods from awake decoding
        posterior_prob, seq = self.pre.learning_phmm_modes_activation(
            trials_to_use_for_decoding=trials_to_use_for_decoding,
            cells_to_use=cells_to_use)

        if only_decoded_modes:
            # get likelihood ratio sleep_before/sleep_after
            likeli_ratio_sleep, modes_ids_results = self.sleep_before_sleep.likelihood_difference_subsets(
                cells_to_use=cells_to_use,
                use_decoded_modes=True, return_mode_ids=True, subset_of_sleep=subset_of_sleep)

        else:
            # get likelihood ratio sleep_before/sleep_after
            likeli_ratio_sleep = self.sleep_before_sleep.likelihood_difference_subsets(cells_to_use=cells_to_use,
                                                                                       use_decoded_modes=False,
                                                                                       use_norm_diff=use_norm_diff)

        pre_modulation = np.zeros(posterior_prob.shape[1])
        mode_id, counts = np.unique(seq, return_counts=True)
        pre_modulation[mode_id] = counts

        if only_decoded_modes:
            # only keep modes that were decoded during sleep
            pre_modulation = pre_modulation[modes_ids_results]

        pre_modulation = pre_modulation / seq.shape[0]

        if z_score_pre_modulation:
            pre_modulation = zscore(pre_modulation)

        if plotting or save_fig:
            if save_fig:
                plt.style.use('default')
            plt.scatter(likeli_ratio_sleep, pre_modulation)
            plt.ylabel("%time active during Learning")
            # plt.xlabel("likeli_sleep_before/likeli_sleep_after")
            plt.xlabel("likelihood_sleep_after - likelihood_sleep_before /\n likelihood_sleep_before + likelihood_sleep")
            # plt.yscale("symlog")
            # plt.xscale("symlog")
            plt.title(cells_to_use)
            plt.xticks([-1, -0.5, 0, 0.5, 1], [-1, -0.5, 0, 0.5, 1])
            y_min, y_max = plt.gca().get_ylim()
            plt.text(0, y_min + (y_max - y_min) / 2, "R=" + str(np.round(pearsonr(pre_modulation, likeli_ratio_sleep)[0], 2)),
                     color="red")
            plt.text(0, y_min + (y_max - y_min) / 2 -0.01, "p=" + str(pearsonr(pre_modulation, likeli_ratio_sleep)[1]),
                     color="red")
            # plt.gca().set_aspect('equal', 'box')
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "pre_replay_states_frequency_learning_"+cells_to_use+".svg"),
                            transparent="True")
                plt.close()
            else:
                plt.show()
            #
            #
            # # select modes that are active during sleep before
            # post_prob_stronger_before = post_prob[:, likeli_ratio_sleep > 1]
            # post_prob_stronger_before_max = np.max(post_prob_stronger_before, axis=1)
            # plt.plot(moving_average(post_prob_stronger_before_max, n=500))
            # plt.xlabel("Time bin (PRE)")
            # plt.ylabel("Max post. prob")
            # plt.title(cells_to_use +" cells:\nModes that are more reactivated in sleep BEFORE")
            # plt.show()
            #
            # post_prob_stronger_before_mean = np.mean(post_prob_stronger_before, axis=1)
            # plt.plot(moving_average(post_prob_stronger_before_mean, n=n_smoothing))
            # plt.xlabel("Time bin (PRE)")
            # plt.ylabel("Mean post-prob")
            # plt.title(cells_to_use +" cells:\nModes that are more reactivated in sleep BEFORE")
            # plt.show()
            #
            # post_prob_stronger_before_z = zscore(post_prob[:, likeli_ratio_sleep > 1], axis=0)
            # # plt.plot(likeli_stronger_before_z)
            # # plt.show()
            #
            # post_prob_stronger_before_z_mean = np.mean(post_prob_stronger_before_z, axis=1)
            # plt.plot(moving_average(post_prob_stronger_before_z_mean, n=n_smoothing))
            # plt.xlabel("Time bin (PRE)")
            # plt.ylabel("Mean of z-scored post-prob")
            # plt.title(cells_to_use +" cells:\nModes that are more reactivated in sleep BEFORE")
            # plt.show()
            #
            # # select modes that are active during sleep after
            # post_prob_stronger_after = post_prob[:, likeli_ratio_sleep < 1]
            #
            # # plt.plot(likeli_stronger_after)
            # # plt.show()
            #
            # post_prob_stronger_after_z = zscore(post_prob[:, likeli_ratio_sleep < 1], axis=0)
            # # plt.plot(likeli_stronger_after_z)
            # # plt.show()
            #
            # post_prob_stronger_after_z_mean = np.mean(post_prob_stronger_after_z, axis=1)
            # plt.plot(moving_average(post_prob_stronger_after_z_mean, n=n_smoothing))
            # plt.xlabel("Time bin (PRE)")
            # plt.ylabel("Mean of z-scored post. probs")
            # plt.title(cells_to_use +" cells:\nModes that are more reactivated in sleep AFTER")
            # plt.show()
            #
            # post_prob_stronger_after_mean = np.mean(post_prob_stronger_after, axis=1)
            # plt.plot(moving_average(post_prob_stronger_after_mean, n=n_smoothing))
            # plt.xlabel("Time bin (PRE)")
            # plt.ylabel("Mean post-prob")
            # plt.title(cells_to_use +" cells:\nModes that are more reactivated in sleep AFTER")
            # plt.show()
            #
            # # select modes that are active during sleep after
            # post_prob_stronger_after = post_prob[:, likeli_ratio_sleep < 1]
            # post_prob_stronger_after_max = np.max(post_prob_stronger_after, axis=1)
            # plt.plot(moving_average(post_prob_stronger_after_max, n=500))
            # plt.xlabel("Time bin (PRE)")
            # plt.ylabel("Max post. prob")
            # plt.title(cells_to_use +" cells:\nModes that are more reactivated in sleep AFTER")
            # plt.show()

        else:
            return pre_modulation, likeli_ratio_sleep

    def learning_phmm_modes_frequency_and_likelihoods_all_cells(self, trials_to_use_for_decoding="all",
                                                                plot_for_control=False, plotting=False,
                                                                only_decoded_modes=False, use_norm_diff=True,
                                                                save_fig=False, subset_of_sleep=None):

        # get likelihoods from awake decoding
        posterior_prob, seq = self.pre.learning_phmm_modes_activation(
            trials_to_use_for_decoding=trials_to_use_for_decoding, cells_to_use="all")

        if only_decoded_modes:
            # get likelihood ratio sleep_before/sleep_after
            likeli_ratio_sleep, modes_ids_results = self.sleep_before_sleep.likelihood_difference(use_decoded_modes=True,
                                                                                                  return_mode_ids=True,
                                                                                                  subset_of_sleep=subset_of_sleep)

        else:
            # get likelihood ratio sleep_before/sleep_after
            likeli_ratio_sleep = self.sleep_before_sleep.likelihood_difference(use_decoded_modes=False,
                                                                               use_norm_diff=use_norm_diff,
                                                                               subset_of_sleep=subset_of_sleep)

        pre_modulation = np.zeros(posterior_prob.shape[1])
        mode_id, counts = np.unique(seq, return_counts=True)
        pre_modulation[mode_id] = counts

        if only_decoded_modes:
            # only keep modes that were decoded during sleep
            pre_modulation = pre_modulation[modes_ids_results]

        pre_modulation = pre_modulation / seq.shape[0]

        if plotting or save_fig:
            if save_fig:
                plt.style.use('default')
            plt.scatter(likeli_ratio_sleep, pre_modulation)
            plt.ylabel("%time active during Learning")
            # plt.xlabel("likeli_sleep_before/likeli_sleep_after")
            plt.xlabel("likelihood_sleep_after - likelihood_sleep_before /\n likelihood_sleep_before + likelihood_sleep")
            # plt.yscale("symlog")
            # plt.xscale("symlog")
            plt.xticks([-1, -0.5, 0, 0.5, 1], [-1, -0.5, 0, 0.5, 1])
            y_min, y_max = plt.gca().get_ylim()
            plt.text(0, y_min + (y_max - y_min) / 2, "R=" + str(np.round(pearsonr(pre_modulation, likeli_ratio_sleep)[0], 2)),
                     color="red")
            plt.text(0, y_min + (y_max - y_min) / 2 -0.2, "p=" + str(pearsonr(pre_modulation, likeli_ratio_sleep)[1]),
                     color="red")
            # plt.gca().set_aspect('equal', 'box')
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "pre_replay_states_frequency_learning_all_cells.svg"),
                            transparent="True")
                plt.close()
            else:
                plt.show()
            #
            #
            # # select modes that are active during sleep before
            # post_prob_stronger_before = post_prob[:, likeli_ratio_sleep > 1]
            # post_prob_stronger_before_max = np.max(post_prob_stronger_before, axis=1)
            # plt.plot(moving_average(post_prob_stronger_before_max, n=500))
            # plt.xlabel("Time bin (PRE)")
            # plt.ylabel("Max post. prob")
            # plt.title(cells_to_use +" cells:\nModes that are more reactivated in sleep BEFORE")
            # plt.show()
            #
            # post_prob_stronger_before_mean = np.mean(post_prob_stronger_before, axis=1)
            # plt.plot(moving_average(post_prob_stronger_before_mean, n=n_smoothing))
            # plt.xlabel("Time bin (PRE)")
            # plt.ylabel("Mean post-prob")
            # plt.title(cells_to_use +" cells:\nModes that are more reactivated in sleep BEFORE")
            # plt.show()
            #
            # post_prob_stronger_before_z = zscore(post_prob[:, likeli_ratio_sleep > 1], axis=0)
            # # plt.plot(likeli_stronger_before_z)
            # # plt.show()
            #
            # post_prob_stronger_before_z_mean = np.mean(post_prob_stronger_before_z, axis=1)
            # plt.plot(moving_average(post_prob_stronger_before_z_mean, n=n_smoothing))
            # plt.xlabel("Time bin (PRE)")
            # plt.ylabel("Mean of z-scored post-prob")
            # plt.title(cells_to_use +" cells:\nModes that are more reactivated in sleep BEFORE")
            # plt.show()
            #
            # # select modes that are active during sleep after
            # post_prob_stronger_after = post_prob[:, likeli_ratio_sleep < 1]
            #
            # # plt.plot(likeli_stronger_after)
            # # plt.show()
            #
            # post_prob_stronger_after_z = zscore(post_prob[:, likeli_ratio_sleep < 1], axis=0)
            # # plt.plot(likeli_stronger_after_z)
            # # plt.show()
            #
            # post_prob_stronger_after_z_mean = np.mean(post_prob_stronger_after_z, axis=1)
            # plt.plot(moving_average(post_prob_stronger_after_z_mean, n=n_smoothing))
            # plt.xlabel("Time bin (PRE)")
            # plt.ylabel("Mean of z-scored post. probs")
            # plt.title(cells_to_use +" cells:\nModes that are more reactivated in sleep AFTER")
            # plt.show()
            #
            # post_prob_stronger_after_mean = np.mean(post_prob_stronger_after, axis=1)
            # plt.plot(moving_average(post_prob_stronger_after_mean, n=n_smoothing))
            # plt.xlabel("Time bin (PRE)")
            # plt.ylabel("Mean post-prob")
            # plt.title(cells_to_use +" cells:\nModes that are more reactivated in sleep AFTER")
            # plt.show()
            #
            # # select modes that are active during sleep after
            # post_prob_stronger_after = post_prob[:, likeli_ratio_sleep < 1]
            # post_prob_stronger_after_max = np.max(post_prob_stronger_after, axis=1)
            # plt.plot(moving_average(post_prob_stronger_after_max, n=500))
            # plt.xlabel("Time bin (PRE)")
            # plt.ylabel("Max post. prob")
            # plt.title(cells_to_use +" cells:\nModes that are more reactivated in sleep AFTER")
            # plt.show()

        else:
            return pre_modulation, likeli_ratio_sleep

    def pre_play_replay_mode_expression(self, trials_to_use_for_decoding=None, cells_to_use="stable",
                                        only_decoded_modes=True):

        # get likelihoods from awake decoding
        posterior_prob, seq = self.pre.learning_phmm_modes_activation(
            trials_to_use_for_decoding=trials_to_use_for_decoding,
            cells_to_use=cells_to_use)

        if only_decoded_modes:
            # get likelihood ratio sleep_before/sleep_after
            likeli_ratio_sleep, mode_ids = self.sleep_before_sleep.likelihood_difference_subsets(cells_to_use=cells_to_use,
                                                                                                        use_decoded_modes=True,
                                                                                                        return_mode_ids=True,
                                                                                                        use_norm_diff=True)

        else:
            # get likelihood ratio sleep_before/sleep_after
            likeli_ratio_sleep = self.sleep_before_sleep.likelihood_difference_subsets(cells_to_use=cells_to_use,
                                                                                       use_decoded_modes=False,
                                                                                       use_norm_diff=True)

        # determine replay/pre-play modes (outside bounds) and others (within bounds)
        cutoff = 0.5
        within_bound = np.where(np.logical_and((-cutoff < likeli_ratio_sleep), (likeli_ratio_sleep < cutoff)))[0]
        outside_bound = np.where(np.logical_or((-cutoff > likeli_ratio_sleep), (likeli_ratio_sleep > cutoff)))[0]

        within_bound_modes = mode_ids[within_bound]
        outside_bound_modes = mode_ids[outside_bound]

        mode_ids_awake, counts = np.unique(seq, return_counts=True)

        # within_expression
        within_exp = np.zeros(within_bound_modes.shape[0])
        for id, mode in enumerate(within_bound_modes):
            within_exp[id] = (counts[mode_ids_awake==mode])/seq.shape[0]

        # outside_expression
        outside_exp = np.zeros(outside_bound_modes.shape[0])
        for id, mode in enumerate(outside_bound_modes):
            outside_exp[id] = (counts[mode_ids_awake==mode])/seq.shape[0]

        return within_exp, outside_exp

    def plot_locations_preplay_replay_modes(self, cells_to_use="stable", which_modes="replay"):
        # get likelihood ratio sleep_before/sleep_after
        likeli_ratio_sleep = self.sleep_before_sleep.likelihood_difference_subsets(cells_to_use=cells_to_use)

        if which_modes == "preplay":
            modes = np.argwhere(likeli_ratio_sleep > 1)
        elif which_modes == "replay":
            modes = np.argwhere(likeli_ratio_sleep < 1)
        for mode_id in modes:
            self.pre.plot_phmm_mode_spatial(cells_to_use=cells_to_use, mode_id=mode_id)

    def plot_locations_not_replayed_modes(self, cells_to_use="stable", which_modes="replay", n_lim=5):
        # get likelihood ratio sleep_before/sleep_after
        likeli_sleep_before, likeli_sleep = self.sleep_before_sleep.likelihoods_phmm(cells_to_use=cells_to_use)

        decoded_modes_sleep, counts = np.unique(np.argmax(likeli_sleep, axis=1), return_counts=True)

        decoded_modes_sleep = decoded_modes_sleep[counts > n_lim]

        if cells_to_use == "stable":
            pre_model = self.sleep_before.session_params.default_pre_phmm_model_stable
        elif cells_to_use == "decreasing":
            pre_model = self.sleep_before.session_params.default_pre_phmm_model_dec
        elif cells_to_use == "increasing":
            pre_model = self.sleep_before.session_params.default_pre_phmm_model_inc

        with open(self.params.pre_proc_dir + "phmm/" +cells_to_use+"_cells/"+ pre_model + '.pkl', 'rb') as f:
            model_dic = pickle.load(f)
        # get means of model (lambdas) for decoding
        all_modes = np.arange(model_dic.means_.shape[0])
        never_decoded_modes = np.array([x for x in all_modes if x not in decoded_modes_sleep])

        fig = plt.figure()
        ax = fig.add_subplot(111)
        for mode_id in never_decoded_modes:
            self.pre.plot_phmm_mode_spatial(cells_to_use=cells_to_use, mode_id=mode_id, ax=ax)
        plt.title("Never decoded modes in sleep after")
        plt.show()

        fig = plt.figure()
        ax = fig.add_subplot(111)
        for mode_id in decoded_modes_sleep:
            self.pre.plot_phmm_mode_spatial(cells_to_use=cells_to_use, mode_id=mode_id, ax=ax)
        plt.title("Decoded modes in sleep after")
        plt.show()

    def correlation_structure_similarity(self, plotting=True):
        # get subsets
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" +self.params.stable_cell_method + ".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_ids = class_dic["stable_cell_ids"].flatten()
        decreasing_ids = class_dic["decrease_cell_ids"].flatten()
        increasing_ids = class_dic["increase_cell_ids"].flatten()

        # get sleep before
        raster_sleep_before = self.sleep_before.get_raster()
        corr_sleep_before_stable = np.nan_to_num(upper_tri_without_diag(np.corrcoef(raster_sleep_before[stable_ids,:])))
        corr_sleep_before_dec = np.nan_to_num(upper_tri_without_diag(np.corrcoef(raster_sleep_before[decreasing_ids,:])))
        corr_sleep_before_inc = np.nan_to_num(upper_tri_without_diag(np.corrcoef(raster_sleep_before[increasing_ids,:])))

        # get sleep after
        raster_sleep = self.sleep.get_raster()
        corr_sleep_stable = np.nan_to_num(upper_tri_without_diag(np.corrcoef(raster_sleep[stable_ids,:])))
        corr_sleep_dec = np.nan_to_num(upper_tri_without_diag(np.corrcoef(raster_sleep[decreasing_ids,:])))
        corr_sleep_inc = np.nan_to_num(upper_tri_without_diag(np.corrcoef(raster_sleep[increasing_ids,:])))

        # get PRE and split
        nr_trials_pre = self.pre.get_nr_of_trials()
        pre_raster = self.pre.get_raster(trials_to_use=range(1,nr_trials_pre))
        pre_raster_1 = pre_raster[:, :int(0.33*pre_raster.shape[1])]
        pre_raster_2 = pre_raster[:, int(0.33*pre_raster.shape[1]):int(0.66*pre_raster.shape[1])]
        pre_raster_3 = pre_raster[:, int(0.66*pre_raster.shape[1]):]

        # get PRE correlations for different subsets of cells
        # stable
        corr_pre_1_stable = np.nan_to_num(upper_tri_without_diag(np.corrcoef(pre_raster_1[stable_ids,:])))
        corr_pre_2_stable = np.nan_to_num(upper_tri_without_diag(np.corrcoef(pre_raster_2[stable_ids,:])))
        corr_pre_3_stable = np.nan_to_num(upper_tri_without_diag(np.corrcoef(pre_raster_3[stable_ids,:])))
        # decreasing
        corr_pre_1_dec = np.nan_to_num(upper_tri_without_diag(np.corrcoef(pre_raster_1[decreasing_ids,:])))
        corr_pre_2_dec = np.nan_to_num(upper_tri_without_diag(np.corrcoef(pre_raster_2[decreasing_ids,:])))
        corr_pre_3_dec = np.nan_to_num(upper_tri_without_diag(np.corrcoef(pre_raster_3[decreasing_ids,:])))
        # increasing
        corr_pre_1_inc = np.nan_to_num(upper_tri_without_diag(np.corrcoef(pre_raster_1[increasing_ids,:])))
        corr_pre_2_inc = np.nan_to_num(upper_tri_without_diag(np.corrcoef(pre_raster_2[increasing_ids,:])))
        corr_pre_3_inc = np.nan_to_num(upper_tri_without_diag(np.corrcoef(pre_raster_3[increasing_ids,:])))

        # compute correlations for stable cells
        corr_matrix_stable = np.zeros((5,5))
        for m, temp in enumerate([corr_sleep_before_stable, corr_pre_1_stable, corr_pre_2_stable, corr_pre_3_stable, corr_sleep_stable]):
            for n, comp in enumerate([corr_sleep_before_stable, corr_pre_1_stable, corr_pre_2_stable, corr_pre_3_stable, corr_sleep_stable]):
                corr_matrix_stable[m,n] = pearsonr(temp, comp)[0]

        # compute correlations for decreasing cells
        corr_matrix_dec = np.zeros((5,5))
        for m, temp in enumerate([corr_sleep_before_dec, corr_pre_1_dec, corr_pre_2_dec, corr_pre_3_dec, corr_sleep_dec]):
            for n, comp in enumerate([corr_sleep_before_dec, corr_pre_1_dec, corr_pre_2_dec, corr_pre_3_dec, corr_sleep_dec]):
                corr_matrix_dec[m,n] = pearsonr(temp, comp)[0]

        # compute correlations for increasing cells
        corr_matrix_inc = np.zeros((5,5))
        for m, temp in enumerate([corr_sleep_before_inc, corr_pre_1_inc, corr_pre_2_inc, corr_pre_3_inc, corr_sleep_inc]):
            for n, comp in enumerate([corr_sleep_before_inc, corr_pre_1_inc, corr_pre_2_inc, corr_pre_3_inc, corr_sleep_inc]):
                corr_matrix_inc[m,n] = pearsonr(temp, comp)[0]

        if plotting:

            np.fill_diagonal(corr_matrix_inc, np.nan)
            plt.imshow(corr_matrix_inc, cmap="Greys", vmin=0.4, vmax=1)
            a = plt.colorbar()
            a.set_label("Pearson R")
            labels = np.array(["sleep_before", "PRE_1", "PRE_2", "PRE_3", "Sleep"])
            plt.yticks(np.arange(corr_matrix_inc.shape[0]), labels)
            plt.xticks(np.arange(corr_matrix_inc.shape[0]), labels, rotation='vertical')
            plt.xlim(-0.5, corr_matrix_inc.shape[0] - 0.5)
            plt.ylim(-0.5, corr_matrix_inc.shape[0] - 0.5)
            plt.title("Increasing")
            plt.show()

            np.fill_diagonal(corr_matrix_dec, np.nan)
            plt.imshow(corr_matrix_dec, cmap="Greys", vmin=0.4, vmax=1)
            a = plt.colorbar()
            a.set_label("Pearson R")
            labels = np.array(["sleep_before", "PRE_1", "PRE_2", "PRE_3", "Sleep"])
            plt.yticks(np.arange(corr_matrix_dec.shape[0]), labels)
            plt.xticks(np.arange(corr_matrix_dec.shape[0]), labels, rotation='vertical')
            plt.xlim(-0.5, corr_matrix_dec.shape[0] - 0.5)
            plt.ylim(-0.5, corr_matrix_dec.shape[0] - 0.5)
            plt.title("Decreasing")
            plt.show()

            np.fill_diagonal(corr_matrix_stable, np.nan)
            plt.imshow(corr_matrix_stable, cmap="Greys", vmin=0.4, vmax=1)
            a = plt.colorbar()
            a.set_label("Pearson R")
            labels = np.array(["sleep_before", "PRE_1", "PRE_2", "PRE_3", "Sleep"])
            plt.yticks(np.arange(corr_matrix_stable.shape[0]), labels)
            plt.xticks(np.arange(corr_matrix_stable.shape[0]), labels, rotation='vertical')
            plt.xlim(-0.5, corr_matrix_stable.shape[0] - 0.5)
            plt.ylim(-0.5, corr_matrix_stable.shape[0] - 0.5)
            plt.title("Stable")
            plt.show()

        else:
            return corr_matrix_stable, corr_matrix_dec, corr_matrix_inc

    def decoded_modes_before_after_freq_during_learning(self, cells_to_use="stable",
                                      use_decoded_modes=False, use_norm_diff=True, return_mode_ids=False):

        # get likelihoods from awake decoding
        posterior_prob, seq = self.pre.learning_phmm_modes_activation(trials_to_use_for_decoding="all",
                                                                 cells_to_use=cells_to_use)

        # get models from pre for stable or decreasing cells
        if cells_to_use == "stable":
            pre_model = self.sleep_before.session_params.default_pre_phmm_model_stable
        elif cells_to_use == "decreasing":
            pre_model = self.sleep_before.session_params.default_pre_phmm_model_dec
        elif cells_to_use == "increasing":
            pre_model = self.sleep_before.session_params.default_pre_phmm_model_inc
        # get likelihoods from sleep_before
        likelihood_sleep_before, _, _, _ = self.sleep_before.decode_phmm_one_model_cell_subset(template_file_name=pre_model,
                                                                                               part_to_analyze="all_swr",
                                                                                               return_results=True,
                                                                                               cells_to_use=cells_to_use)
        # get likelihoods from sleep_after
        likelihood_sleep, _, _, _ = self.sleep.decode_phmm_one_model_cell_subset(template_file_name=pre_model,
                                                                                 part_to_analyze="all_swr",
                                                                                 return_results=True,
                                                                                 cells_to_use=cells_to_use)

        # frequency during learning
        freq_modes = np.zeros((posterior_prob.shape[1]))
        mode_id, count = np.unique(seq, return_counts=True)
        freq_modes[mode_id] = count

        likelihood_sleep_before = np.vstack(likelihood_sleep_before)
        likelihood_sleep = np.vstack(likelihood_sleep)

        # compute mean of decoded mode
        decoded_mode_sleep = np.argmax(likelihood_sleep, axis=1)
        decoded_mode_ids_sleep = np.unique(decoded_mode_sleep)
        decoded_mode_sleep_before = np.argmax(likelihood_sleep_before, axis=1)
        decoded_mode_ids_sleep_before = np.unique(decoded_mode_sleep_before)

        only_decoded_before = np.setdiff1d(decoded_mode_ids_sleep_before, decoded_mode_ids_sleep)
        only_decoded_after = np.setdiff1d(decoded_mode_ids_sleep, decoded_mode_ids_sleep_before)
        decoded_before_and_after = np.intersect1d(decoded_mode_ids_sleep_before, decoded_mode_ids_sleep)
        decoded_before_or_after = np.unique(np.hstack((decoded_mode_ids_sleep, decoded_mode_ids_sleep_before)))
        never_decoded_modes = np.setdiff1d(np.arange(likelihood_sleep.shape[1]), decoded_before_or_after)

        per_active_only_decoded_before = np.sum(freq_modes[only_decoded_before])/seq.shape[0]
        per_active_only_decoded_after = np.sum(freq_modes[only_decoded_after]) / seq.shape[0]
        per_active_before_and_after = np.sum(freq_modes[decoded_before_or_after]) / seq.shape[0]
        per_active_never_decoded = np.sum(freq_modes[never_decoded_modes]) / seq.shape[0]

        return per_active_only_decoded_before, per_active_only_decoded_after, per_active_before_and_after,\
               per_active_never_decoded


class PreProbPrePostPostProb:
    """Class to compare PRE and POST"""

    def __init__(self, pre_probe, pre, post, post_probe, params, session_params=None):
        self.params = params
        self.session_params = session_params
        self.cell_type = self.params.cell_type
        self.session_name = session_params.session_name

        # initialize each phase
        self.pre = pre
        self.post = post
        self.pre_probe = pre_probe
        self.post_probe = post_probe

    def rate_map_stability(self, spatial_resolution=5, nr_of_splits=3, cells_to_use="all", plotting=True,
                           z_score=False, nr_shuffles=10000, use_max=False):

        # get subsets
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name +"_"+self.params.stable_cell_method + ".pickle", "rb") as f:
            class_dic = pickle.load(f)

        if cells_to_use == "stable":
            cell_ids = class_dic["stable_cell_ids"].flatten()
        elif cells_to_use == "decreasing":
            cell_ids = class_dic["decrease_cell_ids"].flatten()
        elif cells_to_use == "increasing":
            cell_ids = class_dic["increase_cell_ids"].flatten()
        elif cells_to_use == "all":
            print(" --> using all cells")

        # get rate maps and occ maps from pre-probe, pre, post, post-probe
        pre_prob_rate_maps, pre_prob_occ_maps = \
            self.pre_probe.get_rate_maps_occ_maps_temporal_splits(spatial_resolution=spatial_resolution,
                                                                  nr_of_splits=nr_of_splits)
        env_dim = self.pre_probe.get_env_dim()
        post_prob_rate_maps, post_prob_occ_maps = \
            self.post_probe.get_rate_maps_occ_maps_temporal_splits(env_dim=env_dim,
                                                                  spatial_resolution=spatial_resolution,
                                                                  nr_of_splits=nr_of_splits)
        # exclude first trial of PRE (learning)
        pre_rate_maps, pre_occ_maps = \
            self.pre.get_rate_maps_occ_maps_temporal_splits(env_dim=env_dim, spatial_resolution=spatial_resolution,
                                                            nr_of_splits=nr_of_splits)
        post_rate_maps, post_occ_maps = \
            self.post.get_rate_maps_occ_maps_temporal_splits(env_dim=env_dim, spatial_resolution=spatial_resolution,
                                                             nr_of_splits=nr_of_splits)

        all_occ_maps = pre_prob_occ_maps+ pre_occ_maps+post_occ_maps+ post_prob_occ_maps
        all_occ_maps_arr = np.array(all_occ_maps)

        all_rate_maps = pre_prob_rate_maps + pre_rate_maps + post_rate_maps + post_prob_rate_maps

        # check which bins were visited in all maps
        nr_non_zero = np.count_nonzero(all_occ_maps_arr, axis=0)

        # set all good bins to 1
        good_bins = np.zeros((all_occ_maps_arr.shape[1], all_occ_maps_arr.shape[2]))
        good_bins[nr_non_zero==all_occ_maps_arr.shape[0]] = 1
        good_bins_rep = np.repeat(good_bins[:, :, np.newaxis], pre_prob_rate_maps[0].shape[2], axis=2)

        # only select good bins for rate maps
        map_similarity = np.zeros((all_occ_maps_arr.shape[0], all_occ_maps_arr.shape[0]))

        for id_1, rate_map_comp_1 in enumerate(all_rate_maps):
            for id_2, rate_map_comp_2 in enumerate(all_rate_maps):
                # compute correlation --> cdist computes all pairs: we only need mean of diagonal
                if not cells_to_use == "all":
                    rate_map_comp_1_good = rate_map_comp_1[good_bins == 1]
                    rate_map_comp_1_subset = rate_map_comp_1_good[:,cell_ids]
                    rate_map_comp_2_good = rate_map_comp_2[good_bins == 1]
                    rate_map_comp_2_subset = rate_map_comp_2_good[:,cell_ids]
                    if z_score:
                        shuffle_list = []
                        for shuffle_id in range(nr_shuffles):
                            # np.permutate or np.random.shuffle
                            rate_map_comp_1_subset_shuffled = np.copy(rate_map_comp_1_subset).T
                            np.random.shuffle(rate_map_comp_1_subset_shuffled)
                            rate_map_comp_1_subset_shuffled = rate_map_comp_1_subset_shuffled.T

                            rate_map_comp_2_subset_shuffled = np.copy(rate_map_comp_2_subset).T
                            np.random.shuffle(rate_map_comp_2_subset_shuffled)
                            rate_map_comp_2_subset_shuffled = rate_map_comp_2_subset_shuffled.T

                            shuffle_list.append(1 - cdist(rate_map_comp_1_subset_shuffled, rate_map_comp_2_subset_shuffled,
                                      metric="correlation"))
                        # compute mean of correlations
                        shuffle_mean = np.mean(np.array(shuffle_list), axis=0)
                        shuffle_std = np.std(np.array(shuffle_list), axis=0)

                    a = 1 - cdist(rate_map_comp_1_subset, rate_map_comp_2_subset,
                                      metric="correlation")
                else:
                    if z_score:
                        raise Exception("TO BE IMPLEMENTED")
                    else:
                        a = 1-cdist(rate_map_comp_1[good_bins==1], rate_map_comp_2[good_bins==1], metric="correlation")

                if z_score:
                    a = (a-shuffle_mean)/shuffle_std

                if a.shape[0] == 0:
                    map_similarity[id_1, id_2] = np.nan
                else:
                    if use_max:
                        map_similarity[id_1, id_2] = np.mean(np.max(a, axis=1))
                    else:
                        map_similarity[id_1, id_2] = np.mean(np.diag(a))

        if plotting:
            # generate x/y label entries
            labels = []
            phases = np.array(["pre-probe", "learn-PRE", "POST", "post-probe"])
            for phase in phases:
                for subdiv in range(nr_of_splits):
                    labels.append(phase+"_"+str(subdiv))

            # plt.figure(figsize=(6,5))
            plt.imshow(map_similarity)
            plt.yticks(np.arange(map_similarity.shape[0]), labels)
            plt.xticks(np.arange(map_similarity.shape[0]), labels, rotation='vertical')
            plt.xlim(-0.5, map_similarity.shape[0]-0.5)
            plt.ylim(-0.5, map_similarity.shape[0]-0.5)
            # plt.ylim(0, map_similarity.shape[0])
            a = plt.colorbar()
            a.set_label("Mean population vector correlation")
            plt.show()

        else:
            return map_similarity

    def phmm_modes_likelihoods(self, cells_to_use="stable", decoded_mode=True, log_likeli=True):
        # get pre phmm name
        if cells_to_use == "stable":
            model_name = self.pre.session_params.default_pre_phmm_model_stable
        elif cells_to_use == "decreasing":
            model_name = self.pre.session_params.default_pre_phmm_model_dec
        likeli_pre = self.pre.decode_awake_activity_time_binning(trials_to_use="all", model_name=model_name, cells_to_use=cells_to_use)[0]
        if log_likeli:
            likeli_pre = np.log(likeli_pre)
        if decoded_mode:
            likeli_pre_decoded = np.max(likeli_pre, axis=1)
        else:
            likeli_pre_decoded = likeli_pre.flatten()
        likeli_pre_probe = self.pre_probe.decode_awake_activity_time_binning(model_name=model_name, cells_to_use=cells_to_use)[0]
        if log_likeli:
            likeli_pre_probe = np.log(likeli_pre_probe)
        if decoded_mode:
            likeli_pre_probe_decoded = np.max(likeli_pre_probe, axis=1)
        else:
            likeli_pre_probe_decoded = likeli_pre_probe.flatten()

        res = [likeli_pre_probe_decoded, likeli_pre_decoded]
        labels=["pre_probe", "pre"]
        c = "white"
        bplot = plt.boxplot(res, positions=[1, 2], patch_artist=True,
                            labels=labels,
                            boxprops=dict(color=c),
                            capprops=dict(color=c),
                            whiskerprops=dict(color=c),
                            flierprops=dict(color=c, markeredgecolor=c),
                            medianprops=dict(color=c), showfliers=False
                            )
        if log_likeli:
            plt.ylabel("log-likelihood")
        else:
            plt.ylabel("likelihood")
        plt.show()

        print(mannwhitneyu(likeli_pre_probe_decoded, likeli_pre_decoded, alternative="greater"))

    def excess_path_per_goal(self, radius_start=10, nth_trial=3, plotting=False):

        nr_trials_pre = self.pre.nr_trials
        nr_trials_post = self.post.nr_trials

        _, _, ex_path_start_learning = self.pre.excess_path_per_goal(trial_to_use=0, radius_start=radius_start)
        try:
            _, _, ex_path_end_learning = self.pre.excess_path_per_goal(trial_to_use=nr_trials_pre-1,
                                                                       radius_start=radius_start)
        except:
            _, _, ex_path_end_learning = self.pre.excess_path_per_goal(trial_to_use=nr_trials_pre - 2,
                                                                       radius_start=radius_start)

        try:
            _, _, ex_path_start_post = self.post.excess_path_per_goal(trial_to_use=0, radius_start=radius_start)
        except:
            _, _, ex_path_start_post = self.post.excess_path_per_goal(trial_to_use=1, radius_start=radius_start)

        try:
            _, _, ex_path_nth_learning = self.pre.excess_path_per_goal(trial_to_use=nth_trial,
                                                                    radius_start=radius_start)
        except:
            _, _, ex_path_nth_learning = self.pre.excess_path_per_goal(trial_to_use=nth_trial+1,
                                                                    radius_start=radius_start)
        try:
            _, _, ex_path_nth_post = self.post.excess_path_per_goal(trial_to_use=nth_trial,
                                                                    radius_start=radius_start)
        except:
            _, _, ex_path_nth_post = self.post.excess_path_per_goal(trial_to_use=nth_trial+1,
                                                                    radius_start=radius_start)

        try:
            _, _, ex_path_end_post = self.post.excess_path_per_goal(trial_to_use=nr_trials_post-1,
                                                                       radius_start=radius_start)
        except:
            _, _, ex_path_end_post = self.post.excess_path_per_goal(trial_to_use=nr_trials_post - 2,
                                                                       radius_start=radius_start)


        if plotting:
            c = "white"
            res = [ex_path_start_learning, ex_path_end_learning, ex_path_start_post, ex_path_end_post]
            bplot = plt.boxplot(res, positions=[1, 2, 3, 4], patch_artist=True,
                                labels=["Beginning learning", "End learning", "Beginning Recall", "End Recall"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            colors = ["magenta", 'magenta', "blue", "blue"]
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            plt.ylabel("Excess path (%)")
            plt.grid(color="grey", axis="y")
            plt.show()

        return ex_path_start_learning, ex_path_nth_learning, ex_path_end_learning, \
            ex_path_start_post, ex_path_nth_post, ex_path_end_post

    def excess_path_per_goal_all_trials(self, radius_start=10, plotting=False):

        ex_path_pre = self.pre.excess_path_per_goal_all_trials(radius_start=radius_start)
        ex_path_post = self.post.excess_path_per_goal_all_trials(radius_start=radius_start)

        ex_path_pre = np.vstack(ex_path_pre)
        ex_path_post = np.vstack(ex_path_post)

        if plotting:
            plt.subplot(2,1,1)
            plt.scatter(x=np.arange(ex_path_pre.shape[0]), y=np.mean(ex_path_pre, axis=1), label="PRE")
            plt.legend()
            plt.xlabel("Trials")
            plt.ylabel("Excess path")
            plt.subplot(2,1,2)
            plt.scatter(x=np.arange(ex_path_post.shape[0]), y=np.mean(ex_path_post, axis=1), label="POST")
            plt.legend()
            plt.xlabel("Trials")
            plt.ylabel("Excess path")
            plt.show()

        return ex_path_pre, ex_path_post


class PreProbPre:
    """Class to compare PRE and POST"""

    def __init__(self, pre_probe, pre, params, session_params=None):
        self.params = params
        self.session_params = session_params
        self.cell_type = self.params.cell_type
        self.session_name = session_params.session_name

        # initialize each phase
        self.pre = pre
        self.pre_probe = pre_probe

    def phmm_modes_likelihoods(self, cells_to_use="stable", decoded_mode=True, log_likeli=True, plotting=True):
        # get pre phmm name
        if cells_to_use == "stable":
            model_name = self.pre.session_params.default_pre_phmm_model_stable
        elif cells_to_use == "decreasing":
            model_name = self.pre.session_params.default_pre_phmm_model_dec
        elif cells_to_use == "increasing":
            model_name = self.pre.session_params.default_pre_phmm_model_inc
        likeli_pre = self.pre.decode_awake_activity_time_binning(trials_to_use="all", model_name=model_name, cells_to_use=cells_to_use)[0]
        if log_likeli:
            likeli_pre = np.log(likeli_pre)
        if decoded_mode:
            likeli_pre_decoded = np.max(likeli_pre, axis=1)
        else:
            likeli_pre_decoded = likeli_pre.flatten()
        likeli_pre_probe = self.pre_probe.decode_awake_activity_time_binning(model_name=model_name, cells_to_use=cells_to_use)[0]
        if log_likeli:
            likeli_pre_probe = np.log(likeli_pre_probe)
        if decoded_mode:
            likeli_pre_probe_decoded = np.max(likeli_pre_probe, axis=1)
        else:
            likeli_pre_probe_decoded = likeli_pre_probe.flatten()

        print(mannwhitneyu(likeli_pre_probe_decoded, likeli_pre_decoded, alternative="less"))

        if plotting:
            res = [likeli_pre_probe_decoded, likeli_pre_decoded]
            labels=["pre_probe", "pre"]
            c = "white"
            bplot = plt.boxplot(res, positions=[1, 2], patch_artist=True,
                                labels=labels,
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            if log_likeli:
                plt.ylabel("log-likelihood")
            else:
                plt.ylabel("likelihood")
            plt.show()

            print(mannwhitneyu(likeli_pre_probe_decoded, likeli_pre_decoded, alternative="greater"))

        else:
            return np.nanmedian(likeli_pre_probe_decoded), np.nanmedian(likeli_pre_decoded)


class AllData:
    """Class to compare PRE and POST"""
    def __init__(self, session_params, params):
        # --------------------------------------------------------------------------------------------------------------
        # creates SelectedData object
        #
        # args:     - session_name, str: session name, there needs to be a parameter file in parameter_files/ with the
        #             same name
        #           - experiment_phase, str: defines which phase of the experiment is supposed to be used
        #             (e.g. EXPLORATION_NOVEL, SLEEP_NOVEL)
        #           - cell_type, list of strings: which type of cell(s) to use (e.g. ["p1", "pe"]
        #           - pre_proc_dir, str: directory where pre-processed data is stored
        #
        # --------------------------------------------------------------------------------------------------------------

        # --------------------------------------------------------------------------------------------------------------
        # load parameter file
        # --------------------------------------------------------------------------------------------------------------
        self.session_params = session_params
        self.params = params

        # --------------------------------------------------------------------------------------------------------------
        # import parameters that describe the data
        # --------------------------------------------------------------------------------------------------------------

        self.data_dir = self.session_params.data_params_dictionary["data_dir"]
        self.cell_type = self.params.cell_type
        self.pre_proc_dir = self.params.pre_proc_dir
        self.session_name = self.session_params.session_name

    # <editor-fold desc="Helper functions">

    @staticmethod
    def get_cell_id(data_dir, session_name, cell_type):
        # --------------------------------------------------------------------------------------------------------------
        # returns cell IDs from .des file for the selected cell type
        #
        # args:         - data_dir, str: directory where data folders are stored (directory containig all mjc.. folders)
        #               - session_name, str: "e.g. mjc163_2_0104"
        #               - cell type, str: can be on of the following (updated: 16.11.2020)
        #
        #                   - u1: unidentified?
        #                   - p1: pyramidal cells of the HPC
        #                   - p2 - p3: pyramidal cells of the PFC
        #                   - b1: interneurons of HPC
        #                   - b2 - b3: interneurons of HPC
        #                   - pe: pyramidal cells MEC
        #
        # returns:      - cell_ids, list: list containing cell ids
        # --------------------------------------------------------------------------------------------------------------

        with open(data_dir + "/" + session_name + "/" + session_name[1:] + ".des") as f:
            des = f.read()
        des = des.splitlines()
        # offset by 2 entries
        cell_ids = [i + 2 for i in range(len(des)) if des[i] == cell_type]

        return cell_ids

    @staticmethod
    def assign_spikes_and_features(cell_ids, clu, res, fet, cell_spike_times_dic, cell_features_dic, cell_id_offset):
        # --------------------------------------------------------------------------------------------------------------
        # loads spike times and features for each cell and writes times (in 20kHz resolution) to dictionary
        #
        # args:     - cell_IDs, list: list containing cell ids of cells that are supposed to be used
        #           - clu, np.array: array containing entry <-> cell_id associations
        #           - res, np.array: spike times (at 20kHz resolution
        #
        # returns:  - data, dictionary:     data["cell" + cell_ID] --> for each cell one np.array with spike times in
        #                                   20kHz resolution
        #
        # --------------------------------------------------------------------------------------------------------------

        # go through all cell ids that were provided
        for cell_ID in cell_ids:

            # need to offset clu by number of cells found before

            # find all entries of the cell_ID in the clu list
            entries_cell = np.where(clu + cell_id_offset == cell_ID)

            # append entries from res file (data is shifted by -1 with respect to clu list)
            ind_res_file = entries_cell[0] - 1
            ind_fet_file = ind_res_file

            # select spikes using found indices
            cell_spikes = res[ind_res_file]

            # select features using found indices
            features = fet[ind_fet_file, :]

            # only write to dictionary if there is actually data
            if cell_spikes.shape[0] > 0:

                cell_spike_times_dic["cell" + str(cell_ID)] = cell_spikes
                cell_features_dic["cell" + str(cell_ID)] = features

            # if there is no more data for cell_ids --> return and move to next tetrode
            else:
                return

    @staticmethod
    def assign_spikes(cell_ids, clu, res, cell_spike_times_dic, cell_id_offset):
        # --------------------------------------------------------------------------------------------------------------
        # loads spike times and features for each cell and writes times (in 20kHz resolution) to dictionary
        #
        # args:     - cell_IDs, list: list containing cell ids of cells that are supposed to be used
        #           - clu, np.array: array containing entry <-> cell_id associations
        #           - res, np.array: spike times (at 20kHz resolution
        #
        # returns:  - data, dictionary:     data["cell" + cell_ID] --> for each cell one np.array with spike times in
        #                                   20kHz resolution
        #
        # --------------------------------------------------------------------------------------------------------------

        # go through all cell ids that were provided
        for cell_ID in cell_ids:

            # need to offset clu by number of cells found before

            # find all entries of the cell_ID in the clu list
            entries_cell = np.where(clu + cell_id_offset == cell_ID)

            # append entries from res file (data is shifted by -1 with respect to clu list)
            ind_res_file = entries_cell[0] - 1
            ind_fet_file = ind_res_file

            # select spikes using found indices
            cell_spikes = res[ind_res_file]

            # only write to dictionary if there is actually data
            if cell_spikes.shape[0] > 0:

                cell_spike_times_dic["cell" + str(cell_ID)] = cell_spikes

            # if there is no more data for cell_ids --> return and move to next tetrode
            else:
                return

    @staticmethod
    def assign_spikes_and_waveforms(cell_ids, clu, res, cell_spike_times_dic, cell_waveform_dic, spk, cell_id_offset):
        # --------------------------------------------------------------------------------------------------------------
        # loads spike times and features for each cell and writes times (in 20kHz resolution) to dictionary
        #
        # args:     - cell_IDs, list: list containing cell ids of cells that are supposed to be used
        #           - clu, np.array: array containing entry <-> cell_id associations
        #           - res, np.array: spike times (at 20kHz resolution
        #
        # returns:  - data, dictionary:     data["cell" + cell_ID] --> for each cell one np.array with spike times in
        #                                   20kHz resolution
        #
        # --------------------------------------------------------------------------------------------------------------

        # go through all cell ids that were provided
        for cell_ID in cell_ids:

            # need to offset clu by number of cells found before

            # find all entries of the cell_ID in the clu list
            entries_cell = np.where(clu + cell_id_offset == cell_ID)

            # append entries from res file (data is shifted by -1 with respect to clu list)
            ind_res_file = entries_cell[0] - 1

            # select spikes using found indices
            cell_spikes = res[ind_res_file]

            # wave forms
            waveforms = spk[ind_res_file, :, :]

            # only write to dictionary if there is actually data
            if cell_spikes.shape[0] > 0:

                cell_spike_times_dic["cell" + str(cell_ID)] = cell_spikes

                cell_waveform_dic["cell" + str(cell_ID)] = waveforms

            # if there is no more data for cell_ids --> return and move to next tetrode
            else:
                return

    def load_spikes(self):
        # check how many tetrodes there are
        with open(self.data_dir + "/" + self.session_name + "/" + self.session_name + ".par") as f:
            for i, line in enumerate(f):
                if i == 2:
                    nr_tetrodes = int(line.split(sep=" ")[0])
                if i > 2:
                    break

        print("Loading .res data ...")

        # get cell ids
        cell_ids_orig = self.get_cell_id(data_dir=self.data_dir, session_name=self.session_name,
                                         cell_type=self.cell_type)
        cell_ids = cell_ids_orig

        cell_spike_times_dic = {}
        cell_id_offset = 0

        # if we take not the first tetrode, we have to offset the cluster ids
        for tetrode in range(1, nr_tetrodes + 1):
            print(" - tetrode " + str(tetrode))
            # load cluster IDs (from .clu) and times of spikes (from .res)
            clu = read_integers(
                self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".clu." + str(tetrode))
            res = read_integers(
                self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".res." + str(tetrode))

            self.assign_spikes(clu=clu, res=res, cell_ids=cell_ids, cell_spike_times_dic=cell_spike_times_dic,
                                          cell_id_offset=cell_id_offset)
            # need to add all cells from tetrode
            cell_id_offset += max(clu) - 1
            # need to remove the matched cell ids from the cell_ids list
            cell_ids = cell_ids_orig[len(cell_spike_times_dic):]

        return cell_spike_times_dic

    # </editor-fold>

    # <editor-fold desc="Clustering features stability analysis">

    def load_spikes_and_features(self):

        # check if dictionaries exist already
        cell_features_dic_name = "cell_features_"+self.session_name
        cell_spike_times_dic_name = "cell_spike_times_"+self.session_name

        if os.path.isfile(self.pre_proc_dir + "features_and_spikes/"+cell_features_dic_name) & \
            os.path.isfile(self.pre_proc_dir + "features_and_spikes/"+cell_spike_times_dic_name):

            with open(self.pre_proc_dir + "features_and_spikes/"+cell_features_dic_name, "rb") as f:
                cell_features_dic = pickle.load(f)
            with open(self.pre_proc_dir + "features_and_spikes/"+cell_spike_times_dic_name, "rb") as f:
                cell_spike_times_dic = pickle.load(f)

        else:

            # check how many tetrodes there are
            with open(self.data_dir + "/" + self.session_name + "/" + self.session_name + ".par") as f:
                for i, line in enumerate(f):
                    if i == 2:
                        nr_tetrodes = np.int(line.split(sep=" ")[0])
                    if i > 2:
                        break

            print("Loading .res and .fet data ...")

            # get cell ids
            cell_ids_orig = self.get_cell_id(data_dir=self.data_dir, session_name=self.session_name, cell_type=self.cell_type)
            cell_ids = cell_ids_orig

            cell_spike_times_dic = {}
            cell_features_dic = {}
            cell_id_offset = 0

            # if we take not the first tetrode, we have to offset the cluster ids
            for tetrode in range(1, nr_tetrodes + 1):
                print(" - tetrode " + str(tetrode))
                # load cluster IDs (from .clu) and times of spikes (from .res)
                clu = read_integers(
                    self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".clu." + str(tetrode))
                res = read_integers(
                    self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".res." + str(tetrode))
                fet = read_arrays(
                    self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".fet." + str(tetrode),
                    skip_first_row=True)

                self.assign_spikes_and_features(clu=clu, res=res, fet=fet, cell_ids=cell_ids,
                                              cell_features_dic=cell_features_dic, cell_spike_times_dic=cell_spike_times_dic,
                                              cell_id_offset=cell_id_offset)
                # need to add all cells from tetrode
                cell_id_offset += max(clu) - 1
                # need to remove the matched cell ids from the cell_ids list
                cell_ids = cell_ids_orig[len(cell_spike_times_dic):]

            with open(self.pre_proc_dir + "features_and_spikes/"+cell_features_dic_name, "wb") as f:
                pickle.dump(cell_features_dic, f, protocol=pickle.HIGHEST_PROTOCOL)
            with open(self.pre_proc_dir + "features_and_spikes/"+cell_spike_times_dic_name, "wb") as f:
                pickle.dump(cell_spike_times_dic, f, protocol=pickle.HIGHEST_PROTOCOL)

        return cell_features_dic, cell_spike_times_dic

    def load_spikes_and_features_noise_cluster(self):

        # check if dictionaries exist already
        cell_features_dic_name = "cell_features_"+self.session_name
        cell_spike_times_dic_name = "cell_spike_times_"+self.session_name

        if os.path.isfile(self.pre_proc_dir + "features_and_spikes_noise/"+cell_features_dic_name) & \
                os.path.isfile(self.pre_proc_dir + "features_and_spikes_noise/"+cell_spike_times_dic_name):

            with open(self.pre_proc_dir + "features_and_spikes_noise/"+cell_features_dic_name, "rb") as f:
                cell_features_dic = pickle.load(f)
            with open(self.pre_proc_dir + "features_and_spikes_noise/"+cell_spike_times_dic_name, "rb") as f:
                cell_spike_times_dic = pickle.load(f)

        else:

            # check how many tetrodes there are
            with open(self.data_dir + "/" + self.session_name + "/" + self.session_name + ".par") as f:
                for i, line in enumerate(f):
                    if i == 2:
                        nr_tetrodes = np.int(line.split(sep=" ")[0])
                    if i > 2:
                        break

            print("Loading .res and .fet data ...")

            # get cell ids
            cell_ids_orig = [0, 1]
            cell_ids = cell_ids_orig

            cell_spike_times_dic = {}
            cell_features_dic = {}
            cell_id_offset = 0

            # if we take not the first tetrode, we have to offset the cluster ids
            for tetrode in range(1, nr_tetrodes + 1):
                print(" - tetrode " + str(tetrode))
                # load cluster IDs (from .clu) and times of spikes (from .res)
                clu = read_integers(
                    self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".clu." + str(tetrode))
                res = read_integers(
                    self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".res." + str(tetrode))
                fet = read_arrays(
                    self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".fet." + str(tetrode),
                    skip_first_row=True)

                self.assign_spikes_and_features(clu=clu, res=res, fet=fet, cell_ids=cell_ids,
                                                cell_features_dic=cell_features_dic, cell_spike_times_dic=cell_spike_times_dic,
                                                cell_id_offset=cell_id_offset)
                # need to add all cells from tetrode
                cell_id_offset += max(clu) - 1
                # need to remove the matched cell ids from the cell_ids list
                cell_ids = cell_ids_orig[len(cell_spike_times_dic):]

            with open(self.pre_proc_dir + "features_and_spikes_noise/"+cell_features_dic_name, "wb") as f:
                pickle.dump(cell_features_dic, f, protocol=pickle.HIGHEST_PROTOCOL)
            with open(self.pre_proc_dir + "features_and_spikes_noise/"+cell_spike_times_dic_name, "wb") as f:
                pickle.dump(cell_spike_times_dic, f, protocol=pickle.HIGHEST_PROTOCOL)

        return cell_features_dic, cell_spike_times_dic

    def load_spikes_and_features_per_tetrode(self, tetrode):

        # check if dictionaries exist already
        cell_feature_dic_name = "cell_features_per_tetrode"+self.session_name
        cell_spike_times_dic_name = "cell_spike_times_per_tetrode"+self.session_name

        # check if spike time dictionary exists already
        if os.path.isfile(self.pre_proc_dir + "features_and_spikes/"+cell_spike_times_dic_name) & \
                os.path.isfile(self.pre_proc_dir + "features_and_spikes/"+cell_feature_dic_name):

            with open(self.pre_proc_dir + "features_and_spikes/"+cell_spike_times_dic_name, "rb") as f:
                cell_spike_times_dic = pickle.load(f)

            with open(self.pre_proc_dir + "features_and_spikes/"+cell_feature_dic_name, "rb") as f:
                cell_feature_dic = pickle.load(f)

        else:
            # get cell ids
            cell_ids_orig = self.get_cell_id(data_dir=self.data_dir, session_name=self.session_name,
                                             cell_type=self.cell_type)
            cell_ids = cell_ids_orig

            cell_spike_times_dic = {}
            cell_feature_dic = {}
            cell_id_offset = 0

            cell_ids_all = []

            for tetrode in range(1, 16 + 1):
                per_tetrode_spike_times_dic = {}
                per_tetrode_feature_dic = {}
                print(" - Loading data from tetrode " + str(tetrode))
                # check if tetrode data exists
                if not os.path.isfile(self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".clu." +
                                      str(tetrode)):
                    print(" --> .clu file not found")
                    continue

                # load cluster IDs (from .clu) and times of spikes (from .res)
                clu = read_integers(
                    self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".clu." + str(tetrode))
                res = read_integers(
                    self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".res." + str(tetrode))
                fet = read_arrays(
                    self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".fet." + str(tetrode),
                    skip_first_row=True)

                self.assign_spikes_and_features(clu=clu, res=res, fet=fet, cell_ids=cell_ids,
                                                cell_features_dic=per_tetrode_feature_dic,
                                                cell_spike_times_dic=per_tetrode_spike_times_dic,
                                                cell_id_offset=cell_id_offset)

                cell_spike_times_dic[str(tetrode)] = per_tetrode_spike_times_dic
                cell_feature_dic[str(tetrode)] = per_tetrode_feature_dic
                curr_tet_clust_ids = np.unique(clu)
                # need to exclude the first two entries [0,1] are noise & artifact clusters
                curr_tet_clust_ids_only_cells = curr_tet_clust_ids[~np.logical_or(curr_tet_clust_ids==0, curr_tet_clust_ids==1)]
                cell_ids_all.append(curr_tet_clust_ids_only_cells+cell_id_offset)
                cell_id_offset += curr_tet_clust_ids_only_cells.shape[0]
                # trim cell_ids (for next tetrode do not need to start from the first entry)
                cell_ids = cell_ids[len(per_tetrode_feature_dic):]

            with open(self.pre_proc_dir + "features_and_spikes/"+cell_feature_dic_name, "wb") as f:
                pickle.dump(cell_feature_dic, f, protocol=pickle.HIGHEST_PROTOCOL)
            with open(self.pre_proc_dir + "features_and_spikes/"+cell_spike_times_dic_name, "wb") as f:
                pickle.dump(cell_spike_times_dic, f, protocol=pickle.HIGHEST_PROTOCOL)
        if str(tetrode) in cell_spike_times_dic and len(cell_spike_times_dic[str(tetrode)]):
            return cell_spike_times_dic[str(tetrode)], cell_feature_dic[str(tetrode)]
        else:
            return None, None

    def load_spikes_and_features_noise_cluster_per_tetrode(self, tetrode):

        # check if dictionaries exist already
        cell_feature_dic_name = "cell_features_per_tetrode"+self.session_name
        cell_spike_times_dic_name = "cell_spike_times_per_tetrode"+self.session_name

        # check if spike time dictionary exists already
        if os.path.isfile(self.pre_proc_dir + "features_and_spikes_noise/"+cell_spike_times_dic_name) & \
                os.path.isfile(self.pre_proc_dir + "features_and_spikes_noise/"+cell_feature_dic_name):

            with open(self.pre_proc_dir + "features_and_spikes_noise/"+cell_spike_times_dic_name, "rb") as f:
                cell_spike_times_dic = pickle.load(f)

            with open(self.pre_proc_dir + "features_and_spikes_noise/"+cell_feature_dic_name, "rb") as f:
                cell_feature_dic = pickle.load(f)

        else:
            # get cell ids
            cell_ids_orig = [0, 1]
            cell_ids = cell_ids_orig

            cell_spike_times_dic = {}
            cell_feature_dic = {}
            cell_id_offset = 0

            cell_ids_all = []

            for tetrode in range(1, 16 + 1):
                per_tetrode_spike_times_dic = {}
                per_tetrode_feature_dic = {}
                print(" - Loading data from tetrode " + str(tetrode))
                # check if tetrode data exists
                if not os.path.isfile(self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".clu." +
                                      str(tetrode)):
                    print(" --> .clu file not found")
                    continue

                # load cluster IDs (from .clu) and times of spikes (from .res)
                clu = read_integers(
                    self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".clu." + str(tetrode))
                res = read_integers(
                    self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".res." + str(tetrode))
                fet = read_arrays(
                    self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".fet." + str(tetrode),
                    skip_first_row=True)

                self.assign_spikes_and_features(clu=clu, res=res, fet=fet, cell_ids=cell_ids,
                                                cell_features_dic=per_tetrode_feature_dic,
                                                cell_spike_times_dic=per_tetrode_spike_times_dic,
                                                cell_id_offset=cell_id_offset)

                cell_spike_times_dic[str(tetrode)] = per_tetrode_spike_times_dic
                cell_feature_dic[str(tetrode)] = per_tetrode_feature_dic
                curr_tet_clust_ids = np.unique(clu)
                # need to exclude the first two entries [0,1] are noise & artifact clusters
                curr_tet_clust_ids_only_cells = curr_tet_clust_ids[~np.logical_or(curr_tet_clust_ids==0, curr_tet_clust_ids==1)]
                cell_ids_all.append(curr_tet_clust_ids_only_cells+cell_id_offset)
                cell_id_offset += curr_tet_clust_ids_only_cells.shape[0]
                # trim cell_ids (for next tetrode do not need to start from the first entry)
                cell_ids = cell_ids[len(per_tetrode_feature_dic):]

            with open(self.pre_proc_dir + "features_and_spikes_noise/"+cell_feature_dic_name, "wb") as f:
                pickle.dump(cell_feature_dic, f, protocol=pickle.HIGHEST_PROTOCOL)
            with open(self.pre_proc_dir + "features_and_spikes_noise/"+cell_spike_times_dic_name, "wb") as f:
                pickle.dump(cell_spike_times_dic, f, protocol=pickle.HIGHEST_PROTOCOL)
        if str(tetrode) in cell_spike_times_dic and len(cell_spike_times_dic[str(tetrode)]):
            return cell_spike_times_dic[str(tetrode)], cell_feature_dic[str(tetrode)]
        else:
            return None, None

    def per_cell_drift(self, features, spike_times, last_spike):

        # compute mean/std for first 1/10 of data
        nr_chunks = 10
        chunk_start = 0
        chunk_end = (1 / nr_chunks) * last_spike

        # compute mean and std for the chunk_start
        mean_initial = np.mean(features[np.logical_and(chunk_start < spike_times, spike_times < chunk_end)], axis=0)
        std_initial = np.std(features[np.logical_and(chunk_start < spike_times, spike_times < chunk_end)], axis=0)

        chunk_start = 0.9 * last_spike
        chunk_end = last_spike

        # compute mean and std for the chunk_start
        mean_last = np.mean(features[np.logical_and(chunk_start < spike_times,spike_times < chunk_end)], axis=0)
        std_last = np.std(features[np.logical_and(chunk_start < spike_times,spike_times < chunk_end)], axis=0)

        z_score = (mean_last - mean_initial) / std_initial

        return z_score

    def assess_feature_stability(self, plotting=True, nr_features_to_use = 12, use_mean=True,
                         plot_for_control=False, save_fig=False, hours_to_compute = [7,14,21]):
        # labels for plotting
        labels = [str(i) for i in hours_to_compute]


        # load features and spike times
        cell_feature_dic, cell_spike_times_dic = self.load_spikes_and_features()

        good_cells = np.ones(len(cell_feature_dic)).astype(bool)

        # compute mean/std of reference distribution (first hour) for each cell/feature
        per_cell_mean = []
        per_cell_std = []
        for cell_id, ((_, features_cell), (_, spike_times)) in enumerate(zip(cell_feature_dic.items(), cell_spike_times_dic.items())):
            features_in_window = features_cell[spike_times < (20e3 * 60 * 60), :nr_features_to_use]
            if features_in_window.shape[0] <= 0:
                good_cells[cell_id] = 0
                per_cell_mean.append(np.nan)
                per_cell_std.append(np.nan)
            else:
                per_cell_mean.append(np.mean(features_in_window, axis=0))
                per_cell_std.append(np.std(features_in_window, axis=0))

        cell_res = []
        cell_features = []

        # go through all cells and compute distance: (mean of nth hour - mean of 1st hour)/std_1st_hour
        for cell_id, (p_c_mean, p_c_std, (_, features_cell), (_, spike_times)) in enumerate(zip(per_cell_mean, per_cell_std,
                                                                           cell_feature_dic.items(),
                                                                           cell_spike_times_dic.items())):
            per_hour_res = []
            per_hour_features = []
            for i_subplot, hour in enumerate(hours_to_compute):
                cell_val = features_cell[np.logical_and(hour * 20e3 * 60 * 60 < spike_times,
                                                         spike_times < (hour + 1) * 20e3 * 60 * 60), :nr_features_to_use]

                if cell_val.shape[0] <= 1 or good_cells[cell_id] == 0 or int(p_c_std.shape[0]-np.count_nonzero(p_c_std)):
                    res = np.nan
                    good_cells[cell_id] = 0
                else:
                    if use_mean:
                        cell_val_mean = np.mean(cell_val, axis=0)
                        res = (cell_val_mean-p_c_mean)/p_c_std
                    else:
                        res = (cell_val-p_c_mean)/p_c_std
                per_hour_res.append(res)
                per_hour_features.append(cell_val)
            cell_res.append(per_hour_res)
            cell_features.append(per_hour_features)

        per_cell_within = []
        per_cell_across = []

        for cell_id in range(len(cell_res)):
            # only look at last hour: within vs.across --> within is already computed above, across computed now

            # check if it is a good cell
            if not good_cells[cell_id]:
                per_cell_within.append(np.nan)
                per_cell_across.append(np.nan)
            else:
                across_per_cell_mean_all_other_cells = []
                for cell_id_to_compare, cell_f in enumerate(cell_features):
                    if cell_id_to_compare == cell_id:
                        continue
                    cell_f_last_hour = cell_f[-1]
                    if use_mean:
                        # compute mean of features during last hour
                        cell_f_last_hour_mean = np.mean(cell_f_last_hour, axis=0)
                        res = (cell_f_last_hour_mean-per_cell_mean[cell_id])/per_cell_std[cell_id]
                    else:
                        res = (cell_f_last_hour-per_cell_mean[cell_id])/per_cell_std[cell_id]
                    across_per_cell_mean_all_other_cells.append(res)
                if use_mean:
                    across_mean = np.abs(np.mean(np.vstack(across_per_cell_mean_all_other_cells), axis=1))
                    within_mean = np.abs(np.mean(cell_res[cell_id][-1]))

                    per_cell_within.append(within_mean)
                    per_cell_across.append(across_mean)
                else:
                    across = np.abs(np.vstack(across_per_cell_mean_all_other_cells).flatten())
                    within = np.abs(cell_res[cell_id][-1])
                    per_cell_within.append(within)
                    per_cell_across.append(across)

                    if plot_for_control:
                        p_within = 1. * np.arange(within.shape[0]) / (within.shape[0] - 1)
                        p_across = 1. * np.arange(across.shape[0]) / (across.shape[0] - 1)
                        plt.plot(np.sort(within), p_within, label="within")
                        plt.plot(np.sort(across), p_across, label="across")

                        plt.ylabel("cdf")
                        plt.xlabel("Z-scored features last hour \n using mean/std of first hour")
                        plt.legend()
                        plt.show()

        # delete cells with issues
        per_cell_within_tmp = [per_cell_within[i] for i in np.argwhere(good_cells).flatten()]
        per_cell_within = per_cell_within_tmp
        per_cell_across_tmp = [per_cell_across[i] for i in np.argwhere(good_cells).flatten()]
        per_cell_across = per_cell_across_tmp
        cell_res_tmp = [cell_res[i] for i in np.argwhere(good_cells).flatten()]
        cell_res = cell_res_tmp

        per_cell_within = np.vstack(per_cell_within).flatten()
        per_cell_across = np.vstack(per_cell_across).flatten()

        # put all results for one time window together
        all_hours_res_all_cells = []
        for hour in range(len(cell_res[0])):
            per_hour_res_all_cells = []
            for res in cell_res:
                per_hour_res_all_cells.extend(res[hour].flatten())
            a = np.array(per_hour_res_all_cells)
            all_hours_res_all_cells.append(a[~np.isnan(a)])

        if plotting:

            p_within = 1. * np.arange(per_cell_within.shape[0]) / (per_cell_within.shape[0] - 1)
            p_across = 1. * np.arange(per_cell_across.shape[0]) / (per_cell_across.shape[0] - 1)
            plt.plot(np.sort(per_cell_within), p_within, label="within")
            plt.plot(np.sort(per_cell_across), p_across, label="across")

            plt.ylabel("cdf")
            if use_mean:
                plt.xlabel("abs. z-scored means of features last hour \n using mean/std of first hour")
            else:
                plt.xlabel("abs. z-scored features last hour \n using mean/std of first hour")
            plt.legend()
            plt.show()

            # print("1 vs. 23:")
            # print(ttest_ind(all_hours_res_all_cells[0],all_hours_res_all_cells[3]))
            # print("8 vs. 16:")
            # print(mannwhitneyu(all_hours_res_all_cells[0], all_hours_res_all_cells[3]))
            # print("1 vs. 23:")
            # print(mannwhitneyu(all_hours_res_all_cells[0], all_hours_res_all_cells[3]))
            # print("1 vs. 23:")
            # print(mannwhitneyu(all_hours_res_all_cells[0], all_hours_res_all_cells[3]))

            # check if distribution at the end of experiment is different from m=0
            print(ttest_1samp(all_hours_res_all_cells[-1], 0))

            c = "white"
            bplot = plt.boxplot(all_hours_res_all_cells, positions=[1, 2, 3], patch_artist=True,
                                labels=labels,
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            plt.ylabel("(mean_feature_nth hour-mean_feature_1h)/\nstd_feature_1h)")
            plt.show()
        else:
            return all_hours_res_all_cells, per_cell_within, per_cell_across

    def assess_feature_stability_l_ratio(self, plotting=True, nr_features_to_use = 12, use_mean=True,
                                     plot_for_control=False, save_fig=False, hours_to_compute = [7,14,21]):
        # labels for plotting
        labels = [str(i) for i in hours_to_compute]

        # load features and spike times
        cell_feature_dic, cell_spike_times_dic = self.load_spikes_and_features_noise_cluster()

        # load features and spike times
        cell_feature_dic, cell_spike_times_dic = self.load_spikes_and_features()

        print("HERE")
        # why are there actually 18 features and not 16?
        # can run the same analysis per tetrode

        cl_features_others = cell_feature_dic["cell3"]

        # compute cluster center
        cl_features = cell_feature_dic["cell2"]
        cl_center = np.expand_dims(np.mean(cl_features, axis=0), 0)
        # compute mahalanobis distance for each spike from the cluster to its center
        within_dist = distance.cdist(cl_center, cl_features, metric="mahalanobis")
        # need to square
        within_dist = within_dist.flatten()**2

        if plot_for_control:
            # fit chi-square function
            chi_square_params = stats.chi2.fit(within_dist)
            # sort the data:
            data_sorted = np.sort(within_dist)
            # calculate the proportional values of samples
            p = 1. * np.arange(len(within_dist)) / (len(within_dist) - 1)
            plt.plot(data_sorted, p,  label="from data")
            plt.plot(np.linspace(0,100,1000), stats.chi2.cdf(np.linspace(0,100,1000),
                                                           df=chi_square_params[0],
                     loc=chi_square_params[1], scale=chi_square_params[2]), label="from fit")
            plt.ylabel("CDF")
            plt.xlabel("D²")
            plt.legend()
            plt.xlim(0,100)
            plt.tight_layout()
            plt.show()

            plt.hist(within_dist, density=True, bins=5000)
            plt.xlabel("D²")
            plt.ylabel("Density")
            plt.xlim(0,100)
            plt.tight_layout()
            plt.show()

        # compute mahalanobis distance for each spike from the cluster to its center
        outside_dist = distance.cdist(cl_center, cl_features_others, metric="mahalanobis")
        # need to square
        outside_dist = outside_dist.flatten()**2

        L_ratio = np.sum(1-p_from_cdf(outside_dist, within_dist))/cl_features.shape[0]

        good_cells = np.ones(len(cell_feature_dic)).astype(bool)

        # compute mean/std of reference distribution (first hour) for each cell/feature
        per_cell_mean = []
        per_cell_std = []
        for cell_id, ((_, features_cell), (_, spike_times)) in enumerate(zip(cell_feature_dic.items(), cell_spike_times_dic.items())):
            features_in_window = features_cell[spike_times < (20e3 * 60 * 60), :nr_features_to_use]
            if features_in_window.shape[0] <= 0:
                good_cells[cell_id] = 0
                per_cell_mean.append(np.nan)
                per_cell_std.append(np.nan)
            else:
                per_cell_mean.append(np.mean(features_in_window, axis=0))
                per_cell_std.append(np.std(features_in_window, axis=0))

        cell_res = []
        cell_features = []

        # go through all cells and compute distance: (mean of nth hour - mean of 1st hour)/std_1st_hour
        for cell_id, (p_c_mean, p_c_std, (_, features_cell), (_, spike_times)) in enumerate(zip(per_cell_mean, per_cell_std,
                                                                                                cell_feature_dic.items(),
                                                                                                cell_spike_times_dic.items())):
            per_hour_res = []
            per_hour_features = []
            for i_subplot, hour in enumerate(hours_to_compute):
                cell_val = features_cell[np.logical_and(hour * 20e3 * 60 * 60 < spike_times,
                                                        spike_times < (hour + 1) * 20e3 * 60 * 60), :nr_features_to_use]

                if cell_val.shape[0] <= 1 or good_cells[cell_id] == 0 or int(p_c_std.shape[0]-np.count_nonzero(p_c_std)):
                    res = np.nan
                    good_cells[cell_id] = 0
                else:
                    if use_mean:
                        cell_val_mean = np.mean(cell_val, axis=0)
                        res = (cell_val_mean-p_c_mean)/p_c_std
                    else:
                        res = (cell_val-p_c_mean)/p_c_std
                per_hour_res.append(res)
                per_hour_features.append(cell_val)
            cell_res.append(per_hour_res)
            cell_features.append(per_hour_features)

        per_cell_within = []
        per_cell_across = []

        for cell_id in range(len(cell_res)):
            # only look at last hour: within vs.across --> within is already computed above, across computed now

            # check if it is a good cell
            if not good_cells[cell_id]:
                per_cell_within.append(np.nan)
                per_cell_across.append(np.nan)
            else:
                across_per_cell_mean_all_other_cells = []
                for cell_id_to_compare, cell_f in enumerate(cell_features):
                    if cell_id_to_compare == cell_id:
                        continue
                    cell_f_last_hour = cell_f[-1]
                    if use_mean:
                        # compute mean of features during last hour
                        cell_f_last_hour_mean = np.mean(cell_f_last_hour, axis=0)
                        res = (cell_f_last_hour_mean-per_cell_mean[cell_id])/per_cell_std[cell_id]
                    else:
                        res = (cell_f_last_hour-per_cell_mean[cell_id])/per_cell_std[cell_id]
                    across_per_cell_mean_all_other_cells.append(res)
                if use_mean:
                    across_mean = np.abs(np.mean(np.vstack(across_per_cell_mean_all_other_cells), axis=1))
                    within_mean = np.abs(np.mean(cell_res[cell_id][-1]))

                    per_cell_within.append(within_mean)
                    per_cell_across.append(across_mean)
                else:
                    across = np.abs(np.vstack(across_per_cell_mean_all_other_cells).flatten())
                    within = np.abs(cell_res[cell_id][-1])
                    per_cell_within.append(within)
                    per_cell_across.append(across)

                    if plot_for_control:
                        p_within = 1. * np.arange(within.shape[0]) / (within.shape[0] - 1)
                        p_across = 1. * np.arange(across.shape[0]) / (across.shape[0] - 1)
                        plt.plot(np.sort(within), p_within, label="within")
                        plt.plot(np.sort(across), p_across, label="across")

                        plt.ylabel("cdf")
                        plt.xlabel("Z-scored features last hour \n using mean/std of first hour")
                        plt.legend()
                        plt.show()

        # delete cells with issues
        per_cell_within_tmp = [per_cell_within[i] for i in np.argwhere(good_cells).flatten()]
        per_cell_within = per_cell_within_tmp
        per_cell_across_tmp = [per_cell_across[i] for i in np.argwhere(good_cells).flatten()]
        per_cell_across = per_cell_across_tmp
        cell_res_tmp = [cell_res[i] for i in np.argwhere(good_cells).flatten()]
        cell_res = cell_res_tmp

        per_cell_within = np.vstack(per_cell_within).flatten()
        per_cell_across = np.vstack(per_cell_across).flatten()

        # put all results for one time window together
        all_hours_res_all_cells = []
        for hour in range(len(cell_res[0])):
            per_hour_res_all_cells = []
            for res in cell_res:
                per_hour_res_all_cells.extend(res[hour].flatten())
            a = np.array(per_hour_res_all_cells)
            all_hours_res_all_cells.append(a[~np.isnan(a)])

        if plotting:

            p_within = 1. * np.arange(per_cell_within.shape[0]) / (per_cell_within.shape[0] - 1)
            p_across = 1. * np.arange(per_cell_across.shape[0]) / (per_cell_across.shape[0] - 1)
            plt.plot(np.sort(per_cell_within), p_within, label="within")
            plt.plot(np.sort(per_cell_across), p_across, label="across")

            plt.ylabel("cdf")
            if use_mean:
                plt.xlabel("abs. z-scored means of features last hour \n using mean/std of first hour")
            else:
                plt.xlabel("abs. z-scored features last hour \n using mean/std of first hour")
            plt.legend()
            plt.show()

            # print("1 vs. 23:")
            # print(ttest_ind(all_hours_res_all_cells[0],all_hours_res_all_cells[3]))
            # print("8 vs. 16:")
            # print(mannwhitneyu(all_hours_res_all_cells[0], all_hours_res_all_cells[3]))
            # print("1 vs. 23:")
            # print(mannwhitneyu(all_hours_res_all_cells[0], all_hours_res_all_cells[3]))
            # print("1 vs. 23:")
            # print(mannwhitneyu(all_hours_res_all_cells[0], all_hours_res_all_cells[3]))

            # check if distribution at the end of experiment is different from m=0
            print(ttest_1samp(all_hours_res_all_cells[-1], 0))

            c = "white"
            bplot = plt.boxplot(all_hours_res_all_cells, positions=[1, 2, 3], patch_artist=True,
                                labels=labels,
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            plt.ylabel("(mean_feature_nth hour-mean_feature_1h)/\nstd_feature_1h)")
            plt.show()
        else:
            return all_hours_res_all_cells, per_cell_within, per_cell_across

    def assess_feature_stability_per_tetrode_l_ratio(self, plot_for_control=False, plotting=True, save_fig=False):

        cell_ids = []
        l_ratio_all_tetrodes = []

        good_cells_all_tetrodes = []
        cell_id_offset=0
        # go through all tetrodes
        for tetrode in range(1, 16+1):
            # get spikes and features for current tetrode
            cell_spike_times_dic, cell_feature_dic = self.load_spikes_and_features_per_tetrode(tetrode=tetrode)

            if (cell_spike_times_dic is None) or (cell_feature_dic is None):
                print(" - no cells for current tetrode found!")
                continue
            cell_ids.append(cell_spike_times_dic.keys())
            cell_id_offset += len(cell_spike_times_dic)
            good_cells = np.ones(len(cell_spike_times_dic))
            # get keys from dictionary and get correct order
            cell_names = []
            for key in cell_spike_times_dic.keys():
                cell_names.append(key[4:])
            cell_names = np.array(cell_names).astype(int)
            cell_names.sort()

            # check if there are more than 1 cells:
            if len(cell_spike_times_dic)<= 1:
                print("Not enough cells for this tetrode .. continuing")
                continue

            l_ratio_per_tetrode = np.zeros(len(cell_spike_times_dic))

            within_dist_all_cells = []
            within_nr_spikes_all_cells = []
            cell_features_all_cells = []
            cl_center_all_cells = []
            chi_square_parameters = []
            # compute within distance for all cells
            for cell_id, ((_, cell_features), (_, spike_times)) in enumerate(zip(cell_feature_dic.items(),
                                                                                 cell_spike_times_dic.items())):

                # compute mean waveform per hour
                cl_features_window = cell_features[:, :]

                cell_features_all_cells.append(cl_features_window)
                if cl_features_window.shape[0] <= 18:
                    good_cells[cell_id] = 0
                    within_dist_all_cells.append(0)
                    within_nr_spikes_all_cells.append(0)
                    continue
                cl_center = np.expand_dims(np.mean(cl_features_window, axis=0), 0)
                within_dist = distance.cdist(cl_center, cl_features_window, metric="mahalanobis").flatten()

                # need to square
                # fit chi_square distribution --> keep shape parameder fixed
                chi_square_parameters.append(stats.chi2.fit(np.square(within_dist.flatten()), f0=16))
                if plot_for_control:
                        plt.hist(np.square(within_dist), bins=np.linspace(0, 100, 1000), histtype=u'step', density=True,
                             label="within")
                        # plt.xlim(0,100)
                        plt.xlabel("D²")
                        plt.ylabel("Density")
                        plt.legend()
                        plt.tight_layout()
                        plt.show()

                        plt.plot(np.linspace(0,50, 1000),
                                 1-p_from_cdf(np.linspace(0,50, 1000), within_dist.flatten()**2),
                                 label="from data")
                        plt.plot(np.linspace(0,50, 1000),  
                                 1-stats.chi2.cdf(np.linspace(0,50, 1000), df=chi_square_parameters[cell_id][0],
                                                loc=chi_square_parameters[cell_id][1], 
                                                scale=chi_square_parameters[cell_id][2]),label="from fit")
                        plt.legend()
                        plt.xlabel("D²")
                        plt.ylabel("P_in_cluster")
                        plt.xlim(0, 100)
                        plt.ylim(0,1.2)
                        plt.tight_layout()
                        plt.show()
                within_dist_all_cells.append(within_dist.flatten()**2)
                within_nr_spikes_all_cells.append(cl_features_window.shape[0])
                cl_center_all_cells.append(cl_center)

            # compute l_ratio using all features from other units
            for cell_id, (within_dist, nr_cells, cl_center, chi_square_par) in enumerate(zip(within_dist_all_cells,
                                                                             within_nr_spikes_all_cells,
                                                                             cl_center_all_cells, chi_square_parameters)):
                if not good_cells[cell_id]:
                    continue
                cl_features_others = np.vstack([x for i,x in enumerate(cell_features_all_cells) if i != cell_id])
                if cl_features_others.shape[0] <= 18:
                    l_ratio_per_tetrode[cell_id] = np.nan
                    continue
                # compute mahalanobis distance for each spike from the cluster to its center
                outside_dist = distance.cdist(cl_center, cl_features_others, metric="mahalanobis")
                # need to square
                outside_dist = outside_dist.flatten()**2
                if plot_for_control:

                    plt.plot(np.sort(within_dist), 1. * np.arange(within_dist.shape[0]) / (within_dist.shape[0] - 1),
                             label="within")
                    plt.plot(np.sort(outside_dist), 1. * np.arange(outside_dist.shape[0]) / (outside_dist.shape[0] - 1),
                             label="outside", zorder=-1000)
                    plt.xscale("log")
                    # plt.ylim(0, np.max(np.cumsum(range(outside_dist.shape[0]))))
                    plt.legend()
                    plt.xlabel("D²")
                    plt.ylabel("CDF")
                    plt.tight_layout()
                    plt.show()


                    p_values = stats.chi2.cdf(outside_dist, df = chi_square_parameters[cell_id][0],
                                              loc = chi_square_parameters[cell_id][1],
                                              scale = chi_square_parameters[cell_id][2])
                    p =  1. * np.arange(len(p_values)) / (len(p_values) - 1)
                    plt.plot(np.sort(p_values), p, label="cdf for outside, n="+str(outside_dist.shape[0]))
                    p_values = stats.chi2.cdf(within_dist, df = chi_square_parameters[cell_id][0],
                                              loc = chi_square_parameters[cell_id][1],
                                              scale = chi_square_parameters[cell_id][2])
                    p =  1. * np.arange(len(p_values)) / (len(p_values) - 1)
                    plt.plot(np.sort(p_values), p, label="cdf for within, n="+str(nr_cells))
                    plt.xlabel("CDF_chi2df")
                    plt.ylabel("CDF")
                    plt.legend()
                    plt.title("L_ratio = "+str(np.sum(1-p_from_cdf(outside_dist, within_dist))/nr_cells))
                    plt.tight_layout()
                    plt.show()


                    p_values = stats.chi2.cdf(outside_dist, df = chi_square_parameters[cell_id][0],
                                              loc = chi_square_parameters[cell_id][1],
                                              scale = chi_square_parameters[cell_id][2])
                    p =  1. * np.arange(len(p_values)) / (len(p_values) - 1)
                    plt.plot(np.sort(1-p_values), p)
                    plt.xlabel("L(C)")
                    plt.ylabel("CDF")
                    plt.title("L_ratio = "+str(np.sum(1-p_from_cdf(outside_dist, within_dist))/nr_cells))
                    plt.tight_layout()
                    plt.show()

                l_ratio_per_tetrode[cell_id] = np.sum(1-p_from_cdf(outside_dist, within_dist))/nr_cells

            l_ratio_all_tetrodes.append(l_ratio_per_tetrode)

        return l_ratio_all_tetrodes


    def assess_feature_stability_per_tetrode_l_ratio_noise(self, plot_for_control=False, plotting=True, save_fig=False):

        cell_ids = []
        l_ratio_all_tetrodes = []

        good_cells_all_tetrodes = []
        cell_id_offset=0
        # go through all tetrodes
        for tetrode in range(1, 16+1):
            # get spikes and features for current tetrode
            cell_spike_times_dic, cell_feature_dic = self.load_spikes_and_features_per_tetrode(tetrode=tetrode)
            cell_spike_times_dic_noise, cell_feature_dic_noise = self.load_spikes_and_features_noise_cluster_per_tetrode(tetrode=tetrode)

            outfile = open("cell_features", 'wb')
            pickle.dump(cell_feature_dic, outfile)
            outfile.close()

            outfile = open("noise_features", 'wb')
            pickle.dump(cell_feature_dic_noise, outfile)
            outfile.close()


            infile = open("noise_features", 'rb')
            features_noise= pickle.load(infile)
            infile.close()
            infile = open("cell_features", 'rb')
            features_cells= pickle.load(infile)
            infile.close()


            noise_features = cell_feature_dic_noise["cell1"]

            if (cell_spike_times_dic is None) or (cell_feature_dic is None):
                print(" - no cells for current tetrode found!")
                continue
            cell_ids.append(cell_spike_times_dic.keys())
            cell_id_offset += len(cell_spike_times_dic)
            good_cells = np.ones(len(cell_spike_times_dic))
            # get keys from dictionary and get correct order
            cell_names = []
            for key in cell_spike_times_dic.keys():
                cell_names.append(key[4:])
            cell_names = np.array(cell_names).astype(int)
            cell_names.sort()

            # check if there are more than 1 cells:
            if len(cell_spike_times_dic)<= 1:
                print("Not enough cells for this tetrode .. continuing")
                continue

            l_ratio_per_tetrode = np.zeros(len(cell_spike_times_dic))

            within_dist_all_cells = []
            within_nr_spikes_all_cells = []
            cell_features_all_cells = []
            cl_center_all_cells = []
            chi_square_parameters = []
            # compute within distance for all cells
            for cell_id, ((_, cell_features), (_, spike_times)) in enumerate(zip(cell_feature_dic.items(),
                                                                                 cell_spike_times_dic.items())):

                # compute mean waveform per hour
                cl_features_window = cell_features[:, :]

                cell_features_all_cells.append(cl_features_window)
                if cl_features_window.shape[0] <= 18:
                    good_cells[cell_id] = 0
                    within_dist_all_cells.append(0)
                    within_nr_spikes_all_cells.append(0)
                    continue
                cl_center = np.expand_dims(np.mean(cl_features_window, axis=0), 0)
                within_dist = distance.cdist(cl_center, cl_features_window, metric="mahalanobis").flatten()
                noise_dist = distance.cdist(cl_center, noise_features[:, :], metric="mahalanobis").flatten()
                # need to square
                # fit chi_square distribution --> keep shape parameder fixed
                chi_square_parameters.append(stats.chi2.fit(np.square(within_dist.flatten()), f0=16))
                if plot_for_control:
                        plt.hist(np.square(within_dist), bins=np.linspace(0, 100, 1000), histtype=u'step', density=True,
                             label="within")
                        plt.hist(np.square(noise_dist), bins=np.linspace(0, 100, 1000), histtype=u'step', density=True,
                             color="white", linestyle="--", zorder=-1000, label="noise")
                        # plt.xlim(0,100)
                        plt.xlabel("D²")
                        plt.ylabel("Density")
                        plt.legend()
                        plt.tight_layout()
                        plt.show()

                        plt.plot(np.linspace(0,50, 1000),
                                 1-p_from_cdf(np.linspace(0,50, 1000), within_dist.flatten()**2),
                                 label="from data")
                        plt.plot(np.linspace(0,50, 1000),
                                 1-stats.chi2.cdf(np.linspace(0,50, 1000), df=chi_square_parameters[cell_id][0],
                                                loc=chi_square_parameters[cell_id][1],
                                                scale=chi_square_parameters[cell_id][2]),label="from fit")
                        plt.legend()
                        plt.xlabel("D²")
                        plt.ylabel("P_in_cluster")
                        plt.xlim(0, 100)
                        plt.ylim(0,1.2)
                        plt.tight_layout()
                        plt.show()
                within_dist_all_cells.append(within_dist.flatten()**2)
                within_nr_spikes_all_cells.append(cl_features_window.shape[0])
                cl_center_all_cells.append(cl_center)

            # compute l_ratio using noise cluster
            for cell_id, (within_dist, nr_cells, cl_center, chi_square_par) in enumerate(zip(within_dist_all_cells,
                                                                             within_nr_spikes_all_cells,
                                                                             cl_center_all_cells, chi_square_parameters)):
                if not good_cells[cell_id]:
                    continue

                # compute mahalanobis distance for each spike from the cluster to its center
                outside_dist = distance.cdist(cl_center, noise_features, metric="mahalanobis")
                # need to square
                outside_dist = outside_dist.flatten()**2
                if plot_for_control:

                    plt.plot(np.sort(within_dist), 1. * np.arange(within_dist.shape[0]) / (within_dist.shape[0] - 1),
                             label="within")
                    plt.plot(np.sort(outside_dist), 1. * np.arange(outside_dist.shape[0]) / (outside_dist.shape[0] - 1),
                             label="outside", zorder=-1000)
                    plt.xscale("log")
                    # plt.ylim(0, np.max(np.cumsum(range(outside_dist.shape[0]))))
                    plt.legend()
                    plt.xlabel("D²")
                    plt.ylabel("CDF")
                    plt.tight_layout()
                    plt.show()

                    p_values = stats.chi2.cdf(outside_dist, df = chi_square_parameters[cell_id][0],
                                              loc = chi_square_parameters[cell_id][1],
                                              scale = chi_square_parameters[cell_id][2])
                    p =  1. * np.arange(len(p_values)) / (len(p_values) - 1)
                    plt.plot(np.sort(p_values), p, label="cdf for outside, n="+str(outside_dist.shape[0]))
                    p_values = stats.chi2.cdf(within_dist, df = chi_square_parameters[cell_id][0],
                                              loc = chi_square_parameters[cell_id][1],
                                              scale = chi_square_parameters[cell_id][2])
                    p =  1. * np.arange(len(p_values)) / (len(p_values) - 1)
                    plt.plot(np.sort(p_values), p, label="cdf for within, n="+str(nr_cells))
                    plt.xlabel("CDF_chi2df")
                    plt.ylabel("CDF")
                    plt.legend()
                    plt.title("L_ratio = "+str(np.sum(1-p_from_cdf(outside_dist, within_dist))/nr_cells))
                    plt.tight_layout()
                    plt.show()

                    p_values = stats.chi2.cdf(outside_dist, df = chi_square_parameters[cell_id][0],
                                              loc = chi_square_parameters[cell_id][1],
                                              scale = chi_square_parameters[cell_id][2])
                    p =  1. * np.arange(len(p_values)) / (len(p_values) - 1)
                    plt.plot(np.sort(1-p_values), p)
                    plt.xlabel("L(C)")
                    plt.ylabel("CDF")
                    plt.title("L_ratio = "+str(np.sum(1-p_from_cdf(outside_dist, within_dist))/nr_cells))
                    plt.tight_layout()
                    plt.show()

                    print("HERE")





            # compute l_ratio using all features from other units
            for cell_id, (within_dist, nr_cells, cl_center, chi_square_par) in enumerate(zip(within_dist_all_cells,
                                                                             within_nr_spikes_all_cells,
                                                                             cl_center_all_cells, chi_square_parameters)):
                if not good_cells[cell_id]:
                    continue
                cl_features_others = np.vstack([x for i,x in enumerate(cell_features_all_cells) if i != cell_id])
                if cl_features_others.shape[0] <= 18:
                    l_ratio_per_tetrode[cell_id] = np.nan
                    continue
                # compute mahalanobis distance for each spike from the cluster to its center
                outside_dist = distance.cdist(cl_center, cl_features_others, metric="mahalanobis")
                # need to square
                outside_dist = outside_dist.flatten()**2
                if plot_for_control:

                    plt.plot(np.sort(within_dist), 1. * np.arange(within_dist.shape[0]) / (within_dist.shape[0] - 1),
                             label="within")
                    plt.plot(np.sort(outside_dist), 1. * np.arange(outside_dist.shape[0]) / (outside_dist.shape[0] - 1),
                             label="outside", zorder=-1000)
                    plt.xscale("log")
                    # plt.ylim(0, np.max(np.cumsum(range(outside_dist.shape[0]))))
                    plt.legend()
                    plt.xlabel("D²")
                    plt.ylabel("CDF")
                    plt.tight_layout()
                    plt.show()


                    p_values = stats.chi2.cdf(outside_dist, df = chi_square_parameters[cell_id][0],
                                              loc = chi_square_parameters[cell_id][1],
                                              scale = chi_square_parameters[cell_id][2])
                    p =  1. * np.arange(len(p_values)) / (len(p_values) - 1)
                    plt.plot(np.sort(p_values), p, label="cdf for outside, n="+str(outside_dist.shape[0]))
                    p_values = stats.chi2.cdf(within_dist, df = chi_square_parameters[cell_id][0],
                                              loc = chi_square_parameters[cell_id][1],
                                              scale = chi_square_parameters[cell_id][2])
                    p =  1. * np.arange(len(p_values)) / (len(p_values) - 1)
                    plt.plot(np.sort(p_values), p, label="cdf for within, n="+str(nr_cells))
                    plt.xlabel("CDF_chi2df")
                    plt.ylabel("CDF")
                    plt.legend()
                    plt.title("L_ratio = "+str(np.sum(1-p_from_cdf(outside_dist, within_dist))/nr_cells))
                    plt.tight_layout()
                    plt.show()


                    p_values = stats.chi2.cdf(outside_dist, df = chi_square_parameters[cell_id][0],
                                              loc = chi_square_parameters[cell_id][1],
                                              scale = chi_square_parameters[cell_id][2])
                    p =  1. * np.arange(len(p_values)) / (len(p_values) - 1)
                    plt.plot(np.sort(1-p_values), p)
                    plt.xlabel("L(C)")
                    plt.ylabel("CDF")
                    plt.title("L_ratio = "+str(np.sum(1-p_from_cdf(outside_dist, within_dist))/nr_cells))
                    plt.tight_layout()
                    plt.show()



                l_ratio_per_tetrode[cell_id] = np.sum(1-p_from_cdf(outside_dist, within_dist))/nr_cells

            l_ratio_all_tetrodes.append(l_ratio_per_tetrode)

            # if len(first_hour_mean) == 0:
            #     print(" - no cell activity during reference hour")
            #     continue
        print("HERE")

        if plotting:

            plt.figure(figsize=(3, 5))
            if save_fig:
                plt.style.use('default')
                c = "black"
            else:
                c = "white"
            bplot = plt.boxplot([stable_last_hour, dec_last_hour, inc_last_hour], positions=[1, 2, 3], patch_artist=True,
                                labels=["stable", "dec", "inc"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            plt.ylabel("similarity within using mean / similarity across using mean")
            plt.title("Hour "+str(hours_to_compute[-1]))
            plt.ylim(0.3, 1.7)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "feature_stab_subsets_example.svg"), transparent="True")
                plt.close()
            else:
                plt.show()

        else:
            return stable_last_hour, dec_last_hour, inc_last_hour


    def assess_feature_stability_per_tetrode_isolation_distance(self, plot_for_control=False, plotting=True, save_fig=False):

        isolation_distance_all_tetrodes = []

        # go through all tetrodes
        for tetrode in range(1, 16+1):
            # get spikes and features for current tetrode
            cell_spike_times_dic, cell_feature_dic = self.load_spikes_and_features_per_tetrode(tetrode=tetrode)
            cell_spike_times_dic_noise, cell_feature_dic_noise = self.load_spikes_and_features_noise_cluster_per_tetrode(tetrode=tetrode)
            if (cell_spike_times_dic is None) or (cell_feature_dic is None):
                print(" - no cells for current tetrode found!")
                continue

            # get keys from dictionary and get correct order
            cell_names = []
            for key in cell_spike_times_dic.keys():
                cell_names.append(key[4:])
            cell_names = np.array(cell_names).astype(int)
            cell_names.sort()

            # check if there are more than 1 cells:
            if len(cell_spike_times_dic)<= 1:
                print("Not enough cells for this tetrode .. continuing")
                continue

            isolation_distance_per_tetrode = np.zeros(len(cell_spike_times_dic))
            cl_features_noise =cell_feature_dic_noise["cell1"]
            # compute within distance for all cells
            for cell_id_o, ((_, cell_features), (_, spike_times)) in enumerate(zip(cell_feature_dic.items(),
                                                                                 cell_spike_times_dic.items())):

                fig = plt.figure(figsize=(12, 6))
                # fig = plt.figure(figsize=(8, 12))
                gs = fig.add_gridspec(12, 15)
                ax1 = fig.add_subplot(gs[:, :6])
                ax2 = fig.add_subplot(gs[:, 8:])
                ax1.scatter(cl_features_noise[:, 0], cl_features_noise[:, 1], color="grey", label="noise")

                # compute mean waveform per hour
                cl_features_window = cell_features[:, :]
                n_c = cl_features_window.shape[0]

                cl_center = np.expand_dims(np.mean(cl_features_window, axis=0), 0)

                for cell_id, ((_, cell_features), (_, spike_times)) in enumerate(zip(cell_feature_dic.items(),
                                                                                     cell_spike_times_dic.items())):

                    # compute distance of noise cluster from center
                    dist = distance.cdist(cl_center, cell_features, metric="mahalanobis").flatten()**2
                    # noise_dist_sorted = np.sort(noise_dist)
                    if cell_id == cell_id_o:
                        ax1.scatter(cell_features[:, 0], cell_features[:, 1], zorder=10000)
                    else:
                        ax1.scatter(cell_features[:, 0], cell_features[:, 1])

                    if cell_id == cell_id_o:
                        n, x, _ = ax2.hist(dist, bins=np.linspace(0, 100000, 100000),histtype=u'step',
                                           density=True, label="within")
                    else:
                        n, x, _ = ax2.hist(dist, bins=np.linspace(0, 100000, 100000),histtype=u'step',
                                           density=True, label=str(cell_id), alpha=0.5)
                noisedist = distance.cdist(cl_center, cl_features_noise, metric="mahalanobis").flatten()**2
                ax2.hist(noisedist, bins=np.linspace(0, 100000, 100000),histtype=u'step', density=True,
                         color="white", linestyle="--", zorder=-1000, label="noise")
                ax2.legend()
                ax1.legend()
                ax1.set_xlabel("Feature 0")
                ax1.set_ylabel("Feature 1")
                ax2.set_xlabel("Mahal. distance")
                ax2.set_xscale("log")
                ax2.set_ylabel("Density")
                plt.show()
            print("HERE")

            within_dist = distance.cdist(cl_center, cl_features_window, metric="mahalanobis").flatten()**2
            within_dist_sorted = np.sort(within_dist)

            plt.scatter(cl_features_window[:, 0], cl_features_window[:, 1])
            plt.scatter(cl_center[:,0], cl_center[:,1])
            plt.show()

            plt.plot(noise_dist_sorted, np.cumsum(np.arange(noise_dist_sorted.shape[0])), label="noise")
            plt.plot(within_dist_sorted, np.cumsum(np.arange(within_dist_sorted.shape[0])), label="cluster")
            plt.show()
            # sort and determine the distance of the n_c (number spikes within cluster) noise value
            isolation_distance_per_tetrode[cell_id] = noise_dist_sorted[n_c]

            isolation_distance_all_tetrodes.append(isolation_distance_per_tetrode)

        print("HERE")


    def assess_feature_stability_per_tetrode_l_ratio_per_hour(self, plot_for_control=False, plotting=False, save_fig=False,
                                                    hours_to_compute = [0, 20]):

        cell_ids = []
        l_ratio_all_tetrodes = []

        cell_id_offset=0
        # go through all tetrodes
        for tetrode in range(1, 16+1):
            # get spikes and features for current tetrode
            cell_spike_times_dic, cell_feature_dic = self.load_spikes_and_features_per_tetrode(tetrode=tetrode)
            if (cell_spike_times_dic is None) or (cell_feature_dic is None):
                print(" - no cells for current tetrode found!")
                continue
            cell_ids.append(cell_spike_times_dic.keys())
            cell_id_offset += len(cell_spike_times_dic)
            good_cells = np.ones(len(cell_spike_times_dic))
            # get keys from dictionary and get correct order
            cell_names = []
            for key in cell_spike_times_dic.keys():
                cell_names.append(key[4:])
            cell_names = np.array(cell_names).astype(int)
            cell_names.sort()

            # check if there are more than 1 cells:
            if len(cell_spike_times_dic)<= 1:
                print("Not enough cells for this tetrode .. continuing")
                continue

            l_ratio_per_tetrode = np.zeros((len(cell_spike_times_dic), len(hours_to_compute)))

            for i_hour, hour in enumerate(hours_to_compute):

                within_dist_all_cells = []
                within_nr_spikes_all_cells = []
                cell_features_all_cells = []
                cl_center_all_cells = []
                # compute within distance for all cells
                for cell_id, ((_, cell_features), (_, spike_times)) in enumerate(zip(cell_feature_dic.items(),
                                                                                     cell_spike_times_dic.items())):
                    if cell_features[np.logical_and(hour * 20e3 * 60 * 60 < spike_times,
                                                    spike_times < (hour + 1) * 20e3 * 60 * 60),:].shape[0] > 0:
                        # compute mean waveform per hour
                        cl_features_window = cell_features[np.logical_and(hour * 20e3 * 60 * 60 < spike_times,
                                                                            spike_times < (hour + 1) * 20e3 * 60 * 60),:]

                        cell_features_all_cells.append(cl_features_window)
                        if cl_features_window.shape[0] <= 18:
                            good_cells[cell_id] = 0
                            within_dist_all_cells.append(0)
                            within_nr_spikes_all_cells.append(0)
                            continue
                        cl_center = np.expand_dims(np.mean(cl_features_window, axis=0), 0)
                        within_dist = distance.cdist(cl_center, cl_features_window, metric="mahalanobis")
                        # need to square
                        within_dist_all_cells.append(within_dist.flatten()**2)
                        within_nr_spikes_all_cells.append(cl_features_window.shape[0])
                        cl_center_all_cells.append(cl_center)                    
                    else:
                        continue

                # compute l_ratio using all features from other units
                for cell_id, (within_dist, nr_spikes_cells, cl_center) in enumerate(zip(within_dist_all_cells,
                                                                                 within_nr_spikes_all_cells, 
                                                                                 cl_center_all_cells)):
                    if not good_cells[cell_id]:
                        continue
                    cl_features_others = np.vstack([x for i,x in enumerate(cell_features_all_cells) if i != cell_id])
                    if cl_features_others.shape[0] <= 18:
                        l_ratio_per_tetrode[cell_id, i_hour] = np.nan
                        continue
                    # compute mahalanobis distance for each spike from the cluster to its center
                    outside_dist = distance.cdist(cl_center, cl_features_others, metric="mahalanobis")
                    # need to square
                    outside_dist = outside_dist.flatten()**2

                    l_ratio_per_tetrode[cell_id, i_hour] = \
                        np.sum(1-p_from_cdf(outside_dist, within_dist))/nr_spikes_cells

            l_ratio_all_tetrodes.append(l_ratio_per_tetrode)

            # if len(first_hour_mean) == 0:
            #     print(" - no cell activity during reference hour")
            #     continue

        l_ratio_all_tetrodes = np.vstack(l_ratio_all_tetrodes)
        good_cells = np.ones(l_ratio_all_tetrodes.shape[0])
        good_cells[np.sum(np.isnan(l_ratio_all_tetrodes), axis=1)>0] = 0

        l_ratio_all_tetrodes = l_ratio_all_tetrodes[good_cells.astype(bool), :]

        if plotting:

            plt.figure(figsize=(3, 5))
            if save_fig:
                plt.style.use('default')
                c = "black"
            else:
                c = "white"
            bplot = plt.boxplot([l_ratio_all_tetrodes[:, 0], l_ratio_all_tetrodes[:, 1]], positions=[1, 2],
                                patch_artist=True,
                                labels=["1st hour", "20th hour"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            plt.ylabel("L_ratio")
            plt.xticks(rotation=45)
            # plt.ylim(0.3, 1.7)
            plt.tight_layout()
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "feature_stab_subsets_example.svg"), transparent="True")
                plt.close()
            else:
                plt.show()

        return l_ratio_all_tetrodes

    def assess_feature_stability_subsets(self, plotting=True):

        nr_features_to_use = 10

        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle",
                  "rb") as f:
            class_dic = pickle.load(f)

        stable = class_dic["stable_cell_ids"]
        dec = class_dic["decrease_cell_ids"]
        inc = class_dic["increase_cell_ids"]

        # load features and spike times
        cell_feature_dic, cell_spike_times_dic = self.load_spikes_and_features()

        # find last spike
        last_spike = 0
        for _, cell_fir in cell_spike_times_dic.items():
            if max(cell_fir) > 0:
                last_spike = max(cell_fir)

        # get keys from dictionary and get correct order
        cell_names = []
        for key in cell_spike_times_dic.keys():
            cell_names.append(key[4:])
        cell_names = np.array(cell_names).astype(int)
        cell_names.sort()

        stable_cell_z = np.zeros((stable.shape[0], nr_features_to_use))

        for i, cell_id in enumerate(stable):

            features = cell_feature_dic["cell"+str(cell_names[cell_id])]
            spike_times = cell_spike_times_dic["cell" + str(cell_names[cell_id])]

            z = self.per_cell_drift(features=features, spike_times=spike_times, last_spike=last_spike)

            stable_cell_z[i,:] = z[:nr_features_to_use]

        dec_cell_z = np.zeros((dec.shape[0], nr_features_to_use))

        for i, cell_id in enumerate(dec):

            features = cell_feature_dic["cell"+str(cell_names[cell_id])]
            spike_times = cell_spike_times_dic["cell" + str(cell_names[cell_id])]

            z = self.per_cell_drift(features=features, spike_times=spike_times, last_spike=last_spike)

            dec_cell_z[i,:] = z[:nr_features_to_use]

        inc_cell_z = np.zeros((inc.shape[0], nr_features_to_use))

        for i, cell_id in enumerate(inc):

            features = cell_feature_dic["cell"+str(cell_names[cell_id])]
            spike_times = cell_spike_times_dic["cell" + str(cell_names[cell_id])]

            z = self.per_cell_drift(features=features, spike_times=spike_times, last_spike=last_spike)

            inc_cell_z[i,:] = z[:nr_features_to_use]

        all_inc_z = inc_cell_z.flatten()
        all_dec_z = dec_cell_z.flatten()
        all_stable_z = stable_cell_z.flatten()

        if plotting:

            p_stable = 1. * np.arange(all_stable_z.shape[0]) / (all_stable_z.shape[0] - 1)
            p_inc = 1. * np.arange(all_inc_z.shape[0]) / (all_inc_z.shape[0] - 1)
            p_dec = 1. * np.arange(all_dec_z.shape[0]) / (all_dec_z.shape[0] - 1)

            plt.plot(np.sort(all_stable_z), p_stable, color="violet", label="stable")
            plt.plot(np.sort(all_dec_z), p_dec, color="turquoise", label="dec")
            plt.plot(np.sort(all_inc_z), p_inc, color="orange", label="inc")
            plt.ylabel("cdf")
            plt.xlabel("(mean_last_10%-mean_first_10%)/std_first_10% \n all features")
            plt.legend()
            plt.show()

            plt.plot(np.sort(np.abs(all_stable_z)), p_stable, color="violet", label="stable")
            plt.plot(np.sort(np.abs(all_dec_z)), p_dec, color="turquoise", label="dec")
            plt.plot(np.sort(np.abs(all_inc_z)), p_inc, color="orange", label="inc")
            plt.ylabel("cdf")
            plt.xlabel("abs((mean_last_10%-mean_first_10%)/std_first_10%) \n all features")
            plt.legend()
            plt.show()
        else:
            return all_stable_z, all_dec_z, all_inc_z

    def cluster_quality_over_time(self, plotting=True, save_fig=False, hours_to_compute=None, nr_pca_comp=10,
                                  distance_measure="correlation"):
        """
        Computes separation of clusters over time:
        - distance of waveforms from one cell to its own mean vs. distance of waveforms from one cell to
          means from other cells)
        - distance amongst waveforms from one cell and distance with waveforms from other cells

        Parameters
        ----------
        plot_for_control
        plotting
        save_fig
        hours_to_compute
        nr_pca_comp
        distance_measure

        Returns
        -------

        """
        if hours_to_compute is None:
            hours_to_compute = [0, 5, 10, 15]

        labels = [str(i) for i in hours_to_compute[1:]]

        all_tetrodes_distance_own_spikes_hour_n_to_own_mean_first_hour_result_last_hour = []
        all_tetrodes_distance_other_cell_spikes_hour_n_to_own_mean_first_hour_result_first_hour = []
        all_tetrodes_ratio_own_spikes_to_own_mean_first_hour_vs_own_spikes_to_mean_of_other_cells_first_hour = []
        all_tetrodes_ratio_own_spikes_to_own_mean_first_hour_vs_other_spikes_to_own_mean_first_hour = []
        all_tetrodes_ratio_own_mean_to_own_mean_first_hour_vs_other_cells_means_to_own_first_hour = []
        all_tetrodes_ratio_own_mean_to_own_mean_first_hour_vs_own_mean_to_means_of_others_first_hour = []

        # go through all tetrodes
        for tetrode in range(1, 16+1):
            # get spikes and waveforms for current tetrode
            cell_spike_times_dic, cell_feature_dic = self.load_spikes_and_features_per_tetrode(tetrode=tetrode)

            if (cell_spike_times_dic is None) or (cell_feature_dic is None):
                print(" - no cells for current tetrode found!")
                continue
            good_cells = np.ones(len(cell_spike_times_dic))
            # get keys from dictionary and get correct order
            cell_names = []
            for key in cell_spike_times_dic.keys():
                cell_names.append(key[4:])
            cell_names = np.array(cell_names).astype(int)
            cell_names.sort()

            # compute mean features in first hour per cell
            # ----------------------------------------------------------------------------------------------------------
            first_hour_mean = []
            ref_hour = 0
            for cell_id, ((_, cell_features), (_, spike_times)) in enumerate(zip(cell_feature_dic.items(),
                                                                                 cell_spike_times_dic.items())):
                if cell_features[np.logical_and(ref_hour * 20e3 * 60 * 60 < spike_times,
                                                         spike_times < (ref_hour + 1) * 20e3 * 60 * 60),:].shape[0] > 0:
                    # compute mean waveform per hour
                    first_hour_mean.append(np.mean(cell_features[np.logical_and(ref_hour * 20e3 * 60 * 60 < spike_times,
                                                             spike_times < (ref_hour + 1) * 20e3 * 60 * 60),:], axis=0))
                else:
                    first_hour_mean.append(np.zeros(cell_features.shape[1]))

            if len(first_hour_mean) == 0:
                print(" - no cell activity during reference hour")
                continue
            first_hour_mean = np.vstack(first_hour_mean)

            cell_names = []
            for key in cell_feature_dic.keys():
                cell_names.append(key)

            # get all waveforms and mean waveforms for requested hours
            # ----------------------------------------------------------------------------------------------------------
            hour_data = []
            hour_mean_data = []
            nr_waveforms_per_hour_per_cell = []
            for hour in hours_to_compute:
                per_hour_all_cells = []
                per_hour_mean_all_cells = []
                this_hour_nr_waveforms_per_cell = np.zeros(len(cell_names))
                for cell_id, ((_, cell_features), (_, spike_times)) in enumerate(zip(cell_feature_dic.items(),
                                                                                      cell_spike_times_dic.items())):

                    # need to check if there are any waveforms of the current cell in the current window
                    cell_features_in_window = cell_features[np.logical_and(hour * 20e3 * 60 * 60 < spike_times,
                                                                           spike_times < (hour + 1) * 20e3 * 60 * 60),:]

                    # check if cell has a mean to compare to
                    if np.sum(first_hour_mean[cell_id]) and cell_features_in_window.shape[0]:
                        # check how many waveforms from the current cell in the current time window
                        this_hour_nr_waveforms_per_cell[cell_id] = cell_features_in_window.shape[0]
                        per_hour_all_cells.append(cell_features_in_window)
                        per_hour_mean_all_cells.append(np.mean(cell_features_in_window, axis=0))
                    else:
                        this_hour_nr_waveforms_per_cell[cell_id] = np.nan
                        per_hour_mean_all_cells.append(np.nan)
                        per_hour_all_cells.append(np.nan)
                        good_cells[cell_id] = 0
                hour_data.append(per_hour_all_cells)
                hour_mean_data.append(per_hour_mean_all_cells)
                nr_waveforms_per_hour_per_cell.append(this_hour_nr_waveforms_per_cell)

            # delete cells that don't have a mean waveform during first hour (because no spikes)
            # or don't have spikes in one window
            # ----------------------------------------------------------------------------------------------------------
            first_hour_mean = first_hour_mean[good_cells.astype(bool), :]
            # entire data from the specific hour
            hour_data_tmp = []
            # mean data from the specific hour
            hour_mean_data_tmp = []
            nr_waveforms_per_hour_per_cell_tmp = []
            for all_hour_dat, all_hour_mean_dat, nr_waveforms in zip(hour_data, hour_mean_data,
                                                                     nr_waveforms_per_hour_per_cell):
                hour_data_tmp.append([all_hour_dat[i] for i in np.argwhere(good_cells).flatten()])
                hour_mean_data_tmp.append([all_hour_mean_dat[i] for i in np.argwhere(good_cells).flatten()])
                nr_waveforms_per_hour_per_cell_tmp.append(nr_waveforms[good_cells.astype(bool)])

            # assign good data
            # hour_data: per cell and hour all spikes (e.g. features of all spikes)
            hour_data = hour_data_tmp
            # hour_mean_data: per cell and hour the mean of all spikes (e.g. the mean features of all spikes)
            hour_mean_data = hour_mean_data_tmp
            nr_waveforms_per_hour_per_cell = nr_waveforms_per_hour_per_cell_tmp

            # check if there are more than 1 cell:
            if len(hour_mean_data[0]) <= 1:
                print("Not enough cells for this tetrode .. continuing")
                continue

            # go through per hour data and compute distance of mean of first hour with mean waveforms
            # ----------------------------------------------------------------------------------------------------------
            # per hour: matrix (n_cells x n_cells) --> (m x n)
            #               - diagonal: distance between mean of first hour and mean of hour_x for each cell (within)
            #               - out of diagonal: distance between mean of first hour cell_m and mean of hour_x with cell_n
            per_hour_all_cells_distance_to_first_hour_mean = []
            for hour_id, per_hour_mean_waveforms_cells in enumerate(hour_mean_data):
                per_hour_mean_waveforms_cells_arr = np.vstack(per_hour_mean_waveforms_cells)[:,:nr_pca_comp]
                distance_hour = distance.cdist(per_hour_mean_waveforms_cells_arr, first_hour_mean[:,:nr_pca_comp],
                                               metric=distance_measure)
                per_hour_all_cells_distance_to_first_hour_mean.append(distance_hour)

            # compute within and across cell distance using mean waveform per hour and first hour
            # ----------------------------------------------------------------------------------------------------------
            ratio_own_mean_to_own_first_hour_vs_other_cells_means_to_own_first_hour = []
            ratio_own_mean_to_own_first_hour_vs_own_mean_to_means_of_others_first_hour = []
            for hour_feature_data in per_hour_all_cells_distance_to_first_hour_mean:
                # select distance within for all cells (each diagonal element corresponds to distance of mean of cell_n
                # of first hour with mean of cell_n of hour x)
                within_mean_feature = np.diagonal(hour_feature_data)
                # select distance for different cells (distance between mean of cell_n during first hour with means
                # of all other cells at hour x)
                across_mean_feature = np.mean(hour_feature_data[~np.eye(hour_feature_data.shape[0],
                                                                        dtype=bool)].reshape(hour_feature_data.shape[0],
                                                                                             -1).T, axis=0)
                # ratio: is mean of cell_n from hour x more similar to its own mean from the first hour than the means
                # of all other cells to the cell_n's mean from the first hour
                ratio_own_mean_to_own_first_hour_vs_other_cells_means_to_own_first_hour.append(within_mean_feature/
                                                                                               across_mean_feature)
                # go through columns and drop diagonal elements
                tmp_list = []
                for col_id, col in enumerate(hour_feature_data.T):
                    tmp_list.append(np.delete(col, col_id))
                tmp = np.vstack(tmp_list).T
                sim_with_other_means = np.mean(tmp, axis=0)
                # compare the mean of cell_n from hour x with the means of all other cells from the first hour
                # ratio: is mean of cell_n at hour x more similar to the same cell's mean from first hour or more
                # similar to the means of all the other cells from the first hour
                ratio_own_mean_to_own_first_hour_vs_own_mean_to_means_of_others_first_hour.append(within_mean_feature/
                                                                                                  sim_with_other_means)

            # go through per hour data and compute similarity of mean of first hour with all other waveforms
            # ----------------------------------------------------------------------------------------------------------
            # in this case: NOT THE MEAN FROM ALL THE SPIKES OF THE CORRESPONDING HOUR, BUT ALL SPIKES
            # result format:
            #               - for each hour: n_
            per_hour_all_cells_results = []
            for hour_id, per_hour_all_cells in enumerate(hour_data):
                # concatenate all spikes from all cells from current hour and compute distance
                distance_to_first_hour= distance.cdist(np.vstack(per_hour_all_cells)[:,:nr_pca_comp],
                                       first_hour_mean[:,:nr_pca_comp], metric=distance_measure)
                per_hour_all_cells_results.append(distance_to_first_hour)

            # all within distances and then mean is computed
            distance_own_spikes_hour_n_to_own_mean_first_hour_mean = []
            # all across distances and then the mean is computed
            distance_other_cell_spikes_hour_n_to_own_mean_first_hour_mean = []
            # all within distances
            distance_own_spikes_hour_n_to_own_mean_first_hour = []
            # all across distances
            distance_other_cell_spikes_hour_n_to_own_mean_first_hour = []
            # distances of spikes from hour n to mean from first our for ALL CELLS
            distance_all_spikes_hour_n_to_own_mean_first_hour = []

            # go through all requested hours to extract similarity for all waveforms
            # ----------------------------------------------------------------------------------------------------------
            # per_hour_all_cells_results: n_cells x n_spikes_per_hour (from all cells)
            #                               - first row: distance of all spikes to mean of FIRST cell
            #                               - second row: distance of all spikes to mean of SECOND cell
            #                               - ...
            for current_hour_all_cell_results, \
                current_hour_all_cells_nr_waveforms in zip(per_hour_all_cells_results, nr_waveforms_per_hour_per_cell):
                per_cell_within_mean = np.zeros(np.count_nonzero(np.sum(first_hour_mean,axis=1)))
                per_cell_across_mean = np.zeros(np.count_nonzero(np.sum(first_hour_mean,axis=1)))
                per_cell_within = []
                per_cell_across = []
                per_cell_within_z_scored = []
                per_cell_across_using_all_means = [[] for i in range(len(hour_data[0]))]
                # extract within vs. across correlations per cell
                for cell_id, cur_hour_cell_res, in enumerate(current_hour_all_cell_results.T):
                    # count all spikes (e.g. waveforms) until the first waveform of the current cell is reached
                    start_id_within_corr = int(np.sum(current_hour_all_cells_nr_waveforms[:cell_id]))
                    # add #spikes that occur for the current cell during the current hour to find last spike of cell
                    end_id_within_corr = start_id_within_corr+int(current_hour_all_cells_nr_waveforms[cell_id])
                    # check if cell has a mean to compare to
                    if np.sum(first_hour_mean[cell_id]):
                        # within correlations: correlations of features from all own spikes (not mean) with the own mean
                        # of the first hour
                        # ----------------------------------------------------------------------------------------------
                        per_cell_within_mean[cell_id] = np.mean(cur_hour_cell_res[
                                                                start_id_within_corr:end_id_within_corr])
                        corr_within = cur_hour_cell_res[start_id_within_corr:end_id_within_corr]
                        per_cell_within.append(corr_within)
                        # across correlations: correltions of features from all other cell spikes (not means) with own
                        # mean from first hour
                        # ----------------------------------------------------------------------------------------------
                        # delete within results after making copy
                        cur_hour_cell_res_copy = cur_hour_cell_res.copy()
                        cur_hour_cell_res_copy= np.delete(cur_hour_cell_res_copy, np.arange(start_id_within_corr,
                                                                                            end_id_within_corr))
                        mean_across = np.mean(cur_hour_cell_res_copy)
                        std_across = np.std(cur_hour_cell_res_copy)
                        per_cell_across_mean[cell_id] = mean_across
                        per_cell_across.append(cur_hour_cell_res_copy)
                        per_cell_within_z_scored.append((corr_within-mean_across)/std_across)
                        # extract correlations of all waveforms (own and from other cells) with the mean of the current
                        # cell from the first hour
                        ind_to_split = np.cumsum(current_hour_all_cells_nr_waveforms)[:-1].astype(int)
                        split_data = np.split(cur_hour_cell_res, ind_to_split)
                        for id, data in enumerate(split_data):
                            per_cell_across_using_all_means[id].append(data)

                distance_own_spikes_hour_n_to_own_mean_first_hour_mean.append(np.nan_to_num(per_cell_within_mean))
                distance_other_cell_spikes_hour_n_to_own_mean_first_hour_mean.append(np.nan_to_num(per_cell_across_mean))
                distance_own_spikes_hour_n_to_own_mean_first_hour.append(per_cell_within)
                distance_other_cell_spikes_hour_n_to_own_mean_first_hour.append(per_cell_across)
                distance_all_spikes_hour_n_to_own_mean_first_hour.append(per_cell_across_using_all_means)

            # similarity of cell waveforms with its own mean from first hour vs. similarity of own cell waveforms
            # with means from other cells from the first hour
            ratio_own_spikes_to_own_mean_first_hour_vs_own_spikes_to_mean_of_other_cells_first_hour = []
            for per_hour_data in distance_all_spikes_hour_n_to_own_mean_first_hour:
                # go through all the cells
                per_cell_res = np.zeros(len(hour_data[0]))
                for cell_id, per_hour_per_cell_data in enumerate(per_hour_data):
                    within = per_hour_per_cell_data[cell_id]
                    tmp_cp = per_hour_per_cell_data.copy()
                    del tmp_cp[cell_id]
                    # if there is only one cell on this tetrode
                    if len(tmp_cp) == 0:
                        continue
                    across = np.hstack(tmp_cp)
                    per_cell_res[cell_id] = np.mean(within)/np.mean(across)
                ratio_own_spikes_to_own_mean_first_hour_vs_own_spikes_to_mean_of_other_cells_first_hour.append(per_cell_res)

            ratio_own_spikes_to_own_mean_first_hour_vs_other_spikes_to_own_mean_first_hour = []
            for within, across in zip(distance_own_spikes_hour_n_to_own_mean_first_hour_mean,
                                      distance_other_cell_spikes_hour_n_to_own_mean_first_hour_mean):
                ratio_own_spikes_to_own_mean_first_hour_vs_other_spikes_to_own_mean_first_hour.append(within/across)

            # append data to collect across tetrodes
            # ----------------------------------------------------------------------------------------------------------
            # ratio: mean within similarity per hour / mean across similarity per hour
            all_tetrodes_ratio_own_spikes_to_own_mean_first_hour_vs_other_spikes_to_own_mean_first_hour.append(
                ratio_own_spikes_to_own_mean_first_hour_vs_other_spikes_to_own_mean_first_hour)
            # ratio: mean within similarity per hour using mean waveform / mean across similarity
            # per hour usin mean waveform
            all_tetrodes_ratio_own_mean_to_own_mean_first_hour_vs_other_cells_means_to_own_first_hour.append(
                ratio_own_mean_to_own_first_hour_vs_other_cells_means_to_own_first_hour)
            # ratio: distance of mean with own mean from first hour / similarity of mean with other
            # means from first hour
            all_tetrodes_ratio_own_mean_to_own_mean_first_hour_vs_own_mean_to_means_of_others_first_hour.append(
                ratio_own_mean_to_own_first_hour_vs_own_mean_to_means_of_others_first_hour)
            # ----------------------------------------------------------------------------------------------------------
            ratio_own_spikes_to_own_mean_first_hour_vs_own_spikes_to_mean_of_other_cells_first_hour = \
                np.vstack(ratio_own_spikes_to_own_mean_first_hour_vs_own_spikes_to_mean_of_other_cells_first_hour)
            all_tetrodes_ratio_own_spikes_to_own_mean_first_hour_vs_own_spikes_to_mean_of_other_cells_first_hour.append(
                ratio_own_spikes_to_own_mean_first_hour_vs_own_spikes_to_mean_of_other_cells_first_hour)
            # ----------------------------------------------------------------------------------------------------------
            distance_own_spikes_hour_n_to_own_mean_first_hour_result_last_hour = \
                distance_own_spikes_hour_n_to_own_mean_first_hour[-1]
            distance_other_cell_spikes_hour_n_to_own_mean_first_hour_result_first_hour = \
                distance_other_cell_spikes_hour_n_to_own_mean_first_hour[1]
            per_tetrode_distance_own_spikes_hour_n_to_own_mean_first_hour_result_last_hour = \
                np.hstack(distance_own_spikes_hour_n_to_own_mean_first_hour_result_last_hour)
            per_tetrode_distance_other_cell_spikes_hour_n_to_own_mean_first_hour_result_first_hour = \
                np.hstack(distance_other_cell_spikes_hour_n_to_own_mean_first_hour_result_first_hour)

            all_tetrodes_distance_other_cell_spikes_hour_n_to_own_mean_first_hour_result_first_hour.append(
                per_tetrode_distance_other_cell_spikes_hour_n_to_own_mean_first_hour_result_first_hour)

            all_tetrodes_distance_own_spikes_hour_n_to_own_mean_first_hour_result_last_hour.append(
                per_tetrode_distance_own_spikes_hour_n_to_own_mean_first_hour_result_last_hour)

        # make arrays
        all_tetrodes_ratio_own_spikes_to_own_mean_first_hour_vs_own_spikes_to_mean_of_other_cells_first_hour = \
         np.hstack(all_tetrodes_ratio_own_spikes_to_own_mean_first_hour_vs_own_spikes_to_mean_of_other_cells_first_hour)
        all_tetrodes_ratio_own_spikes_to_own_mean_first_hour_vs_own_spikes_to_mean_of_other_cells_first_hour = \
            all_tetrodes_ratio_own_spikes_to_own_mean_first_hour_vs_own_spikes_to_mean_of_other_cells_first_hour[:,
            ~np.isnan(np.sum(
                all_tetrodes_ratio_own_spikes_to_own_mean_first_hour_vs_own_spikes_to_mean_of_other_cells_first_hour,
                axis=0))]

        all_tetrodes_ratio_own_spikes_to_own_mean_first_hour_vs_other_spikes_to_own_mean_first_hour = \
            np.hstack(all_tetrodes_ratio_own_spikes_to_own_mean_first_hour_vs_other_spikes_to_own_mean_first_hour)

        all_tetrodes_ratio_own_mean_to_own_mean_first_hour_vs_other_cells_means_to_own_first_hour = \
            np.hstack(all_tetrodes_ratio_own_mean_to_own_mean_first_hour_vs_other_cells_means_to_own_first_hour)

        all_tetrodes_ratio_own_mean_to_own_mean_first_hour_vs_own_mean_to_means_of_others_first_hour = \
            np.hstack(all_tetrodes_ratio_own_mean_to_own_mean_first_hour_vs_own_mean_to_means_of_others_first_hour)

        all_tetrodes_distance_other_cell_spikes_hour_n_to_own_mean_first_hour_result_first_hour = \
            np.hstack(all_tetrodes_distance_other_cell_spikes_hour_n_to_own_mean_first_hour_result_first_hour)

        all_tetrodes_distance_own_spikes_hour_n_to_own_mean_first_hour_result_last_hour = \
            np.hstack(all_tetrodes_distance_own_spikes_hour_n_to_own_mean_first_hour_result_last_hour)

        # compute stats --> first using all spikes
        # --------------------------------------------------------------------------------------------------------------
        # ratio own_spikes_to_own_mean_first_hour / own_spikes_to_mean_other_cells_first_hour
        stats_data = \
            all_tetrodes_ratio_own_spikes_to_own_mean_first_hour_vs_own_spikes_to_mean_of_other_cells_first_hour

        p_own_spikes_own_mean_vs_own_spikes_other_means = np.array([mannwhitneyu(stats_data[1],stats_data[2])[1],
                                           mannwhitneyu(stats_data[2], stats_data[3])[1],
                                           mannwhitneyu(stats_data[2], stats_data[3])[1]])

        print("p_own_spikes_own_mean_vs_own_spikes_other_means:")
        print(p_own_spikes_own_mean_vs_own_spikes_other_means)

        stats_data = all_tetrodes_ratio_own_spikes_to_own_mean_first_hour_vs_other_spikes_to_own_mean_first_hour

        p_own_spikes_own_mean_vs_other_spikes_own_mean = np.array([mannwhitneyu(stats_data[1],stats_data[2])[1],
                                           mannwhitneyu(stats_data[2], stats_data[3])[1],
                                           mannwhitneyu(stats_data[2], stats_data[3])[1]])

        print("p_own_spikes_own_mean_vs_other_spikes_own_mean")
        print(p_own_spikes_own_mean_vs_other_spikes_own_mean)

        # compute stats --> using mean per hour
        # --------------------------------------------------------------------------------------------------------------

        stats_data = all_tetrodes_ratio_own_mean_to_own_mean_first_hour_vs_other_cells_means_to_own_first_hour

        p_own_mean_own_mean_vs_other_means_own_mean = np.array([mannwhitneyu(stats_data[1],stats_data[2])[1],
                                           mannwhitneyu(stats_data[2], stats_data[3])[1],
                                           mannwhitneyu(stats_data[2], stats_data[3])[1]])

        print("p_own_mean_own_mean_vs_other_means_own_mean")
        print(p_own_mean_own_mean_vs_other_means_own_mean)

        stats_data = all_tetrodes_ratio_own_mean_to_own_mean_first_hour_vs_own_mean_to_means_of_others_first_hour

        p_own_mean_own_mean_vs_own_mean_other_means = np.array([mannwhitneyu(stats_data[1],stats_data[2])[1],
                                           mannwhitneyu(stats_data[2], stats_data[3])[1],
                                           mannwhitneyu(stats_data[2], stats_data[3])[1]])

        print("p_own_mean_own_mean_vs_own_mean_other_means")
        print(p_own_mean_own_mean_vs_own_mean_other_means)

        p_other_cell_spikes_own_mean_first_hour_vs_own_spikes_hour_own_mean_last_hour = \
            mannwhitneyu(all_tetrodes_distance_other_cell_spikes_hour_n_to_own_mean_first_hour_result_first_hour,
                         all_tetrodes_distance_own_spikes_hour_n_to_own_mean_first_hour_result_last_hour)[1]
        print("p_other_cell_spikes_own_mean_first_hour_vs_own_spikes_hour_own_mean_last_hour")
        print(p_other_cell_spikes_own_mean_first_hour_vs_own_spikes_hour_own_mean_last_hour)

        if plotting:
            # plots using all spikes
            # ----------------------------------------------------------------------------------------------------------
            plot_data = \
                all_tetrodes_ratio_own_spikes_to_own_mean_first_hour_vs_own_spikes_to_mean_of_other_cells_first_hour
            plt.figure(figsize=(3, 4))
            cmap = matplotlib.cm.get_cmap('viridis')
            colors_to_plot = cmap(np.linspace(0, 1, 4))
            if save_fig:
                plt.style.use('default')
                c = "black"
            else:
                c = "white"
            bplot = plt.boxplot(plot_data[1:, :].T, positions=[1, 2, 3],
                                patch_artist=True,
                                labels=labels,
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            plt.ylabel("All spikes: distance to own mean from first hour / \n "
                       "distance to means of other cells from first hour")
            plt.ylim(-0.01, 1.5)
            plt.tight_layout()
            for patch, color in zip(bplot['boxes'], colors_to_plot[1:]):
                patch.set_facecolor(color)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "spikes_own_other_mean_vs_other_means_example.svg"),
                            transparent="True")
                plt.close()
            else:
                plt.show()

            # ----------------------------------------------------------------------------------------------------------
            plot_data = all_tetrodes_ratio_own_spikes_to_own_mean_first_hour_vs_other_spikes_to_own_mean_first_hour

            plt.figure(figsize=(3, 4))

            bplot = plt.boxplot(plot_data[1:, :].T, positions=[1, 2, 3], patch_artist=True,
                                labels=labels,
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            plt.ylabel("All spikes: distance own spikes with own mean /\n distance other cells spike with own mean")
            plt.tight_layout()
            plt.ylim(-0.02, 1.5)
            for patch, color in zip(bplot['boxes'], colors_to_plot[1:]):
                patch.set_facecolor(color)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "spikes_own_spikes_own_mean_vs_other_cell_spikes_own_mean.svg"),
                            transparent="True")
                plt.close()
            else:
                plt.show()

            # plots using mean spike per hour
            # ----------------------------------------------------------------------------------------------------------

            plot_data = all_tetrodes_ratio_own_mean_to_own_mean_first_hour_vs_other_cells_means_to_own_first_hour

            plt.figure(figsize=(3, 4))

            bplot = plt.boxplot(plot_data[1:, :].T, positions=[1, 2, 3], patch_artist=True,
                                labels=labels,
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            plt.ylabel("Mean per hour: distance to own mean from first hour / \n "
                       "distance to means of other cells from first hour")
            for patch, color in zip(bplot['boxes'], colors_to_plot[1:]):
                patch.set_facecolor(color)
            plt.tight_layout()
            plt.ylim(-0.02, 1.0)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "mean_per_hour_own_mean_vs_mean_per_hour_other_means_example.svg"),
                            transparent="True")
                plt.close()
            else:
                plt.show()

            # ----------------------------------------------------------------------------------------------------------

            plot_data = all_tetrodes_ratio_own_mean_to_own_mean_first_hour_vs_own_mean_to_means_of_others_first_hour

            plt.figure(figsize=(3, 4))
            bplot = plt.boxplot(plot_data[1:, :].T, positions=[1, 2, 3], patch_artist=True,
                                labels=labels,
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            plt.ylabel("Mean per hour: distance own spikes with own mean /\n distance other cells spike with own mean")
            for patch, color in zip(bplot['boxes'], colors_to_plot[1:]):
                patch.set_facecolor(color)
            plt.tight_layout()
            plt.ylim(-0.02, 1.0)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path,
                                         "mean_per_hour_own_mean_vs_mean_per_hour_other_cells_own_mean_example.svg"),
                            transparent="True")
                plt.close()
            else:
                plt.show()

            # plot first hour across vs last hour within
            # ----------------------------------------------------------------------------------------------------------

            plot_data = [all_tetrodes_distance_other_cell_spikes_hour_n_to_own_mean_first_hour_result_first_hour,
                         all_tetrodes_distance_own_spikes_hour_n_to_own_mean_first_hour_result_last_hour]

            plt.figure(figsize=(3, 4))
            bplot = plt.boxplot(plot_data, positions=[1, 2], patch_artist=True,
                                labels=["across cells \n first hour", "within cells \n last hour"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            plt.ylabel("Distance")
            for patch, color in zip(bplot['boxes'], colors_to_plot[1:]):
                patch.set_facecolor(color)
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.ylim(-0.02, 1.0)

            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path,  "first_hour_across_last_our_within.svg"),
                            transparent="True")
                plt.close()
            else:
                plt.show()

        return p_own_spikes_own_mean_vs_own_spikes_other_means, p_own_spikes_own_mean_vs_other_spikes_own_mean, \
            p_own_mean_own_mean_vs_other_means_own_mean, p_own_mean_own_mean_vs_own_mean_other_means, \
            p_other_cell_spikes_own_mean_first_hour_vs_own_spikes_hour_own_mean_last_hour

    def cluster_quality_over_time_subsets(self, plotting=True, save_fig=False, hours_to_compute=[15], nr_pca_comp=10,
                                  distance_measure="correlation"):
        """
        Computes separation of clusters over time:
        - distance of waveforms from one cell to its own mean vs. distance of waveforms from one cell to
          means from other cells)
        - distance amongst waveforms from one cell and distance with waveforms from other cells

        Parameters
        ----------
        plot_for_control
        plotting
        save_fig
        hours_to_compute
        nr_pca_comp
        distance_measure

        Returns
        -------

        """
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle",
                  "rb") as f:
            class_dic = pickle.load(f)

        stable = class_dic["stable_cell_ids"]
        dec = class_dic["decrease_cell_ids"]
        inc = class_dic["increase_cell_ids"]

        all_tetrodes_distance_own_spikes_hour_n_to_own_mean_first_hour_result_last_hour = []
        all_tetrodes_distance_other_cell_spikes_hour_n_to_own_mean_first_hour_result_first_hour = []
        all_tetrodes_ratio_own_spikes_to_own_mean_first_hour_vs_own_spikes_to_mean_of_other_cells_first_hour = []
        all_tetrodes_ratio_own_spikes_to_own_mean_first_hour_vs_other_spikes_to_own_mean_first_hour = []
        all_tetrodes_ratio_own_mean_to_own_mean_first_hour_vs_other_cells_means_to_own_first_hour = []
        all_tetrodes_ratio_own_mean_to_own_mean_first_hour_vs_own_mean_to_means_of_others_first_hour = []

        cell_ids = []
        good_cells_all_tetrodes = []
        cell_id_offset=0

        # go through all tetrodes
        for tetrode in range(1, 16+1):
            # get spikes and waveforms for current tetrode
            cell_spike_times_dic, cell_feature_dic = self.load_spikes_and_features_per_tetrode(tetrode=tetrode)

            if (cell_spike_times_dic is None) or (cell_feature_dic is None):
                print(" - no cells for current tetrode found!")
                continue
            cell_ids.append(cell_spike_times_dic.keys())
            cell_id_offset += len(cell_spike_times_dic)
            good_cells = np.ones(len(cell_spike_times_dic))
            # get keys from dictionary and get correct order
            cell_names = []
            for key in cell_spike_times_dic.keys():
                cell_names.append(key[4:])
            cell_names = np.array(cell_names).astype(int)
            cell_names.sort()

            # compute mean features in first hour per cell
            # ----------------------------------------------------------------------------------------------------------
            first_hour_mean = []
            ref_hour = 0
            for cell_id, ((_, cell_features), (_, spike_times)) in enumerate(zip(cell_feature_dic.items(),
                                                                                 cell_spike_times_dic.items())):
                if cell_features[np.logical_and(ref_hour * 20e3 * 60 * 60 < spike_times,
                                                         spike_times < (ref_hour + 1) * 20e3 * 60 * 60),:].shape[0] > 0:
                    # compute mean waveform per hour
                    first_hour_mean.append(np.mean(cell_features[np.logical_and(ref_hour * 20e3 * 60 * 60 < spike_times,
                                                             spike_times < (ref_hour + 1) * 20e3 * 60 * 60),:], axis=0))
                else:
                    first_hour_mean.append(np.zeros(cell_features.shape[1]))

            if len(first_hour_mean) == 0:
                print(" - no cell activity during reference hour")
                continue
            first_hour_mean = np.vstack(first_hour_mean)

            cell_names = []
            for key in cell_feature_dic.keys():
                cell_names.append(key)

            # get all waveforms and mean waveforms for requested hours
            # ----------------------------------------------------------------------------------------------------------
            hour_data = []
            hour_mean_data = []
            nr_waveforms_per_hour_per_cell = []
            for hour in hours_to_compute:
                per_hour_all_cells = []
                per_hour_mean_all_cells = []
                this_hour_nr_waveforms_per_cell = np.zeros(len(cell_names))
                for cell_id, ((_, cell_features), (_, spike_times)) in enumerate(zip(cell_feature_dic.items(),
                                                                                      cell_spike_times_dic.items())):

                    # need to check if there are any waveforms of the current cell in the current window
                    cell_features_in_window = cell_features[np.logical_and(hour * 20e3 * 60 * 60 < spike_times,
                                                                           spike_times < (hour + 1) * 20e3 * 60 * 60),:]

                    # check if cell has a mean to compare to
                    if np.sum(first_hour_mean[cell_id]) and cell_features_in_window.shape[0]:
                        # check how many waveforms from the current cell in the current time window
                        this_hour_nr_waveforms_per_cell[cell_id] = cell_features_in_window.shape[0]
                        per_hour_all_cells.append(cell_features_in_window)
                        per_hour_mean_all_cells.append(np.mean(cell_features_in_window, axis=0))
                    else:
                        this_hour_nr_waveforms_per_cell[cell_id] = np.nan
                        per_hour_mean_all_cells.append(np.nan)
                        per_hour_all_cells.append(np.nan)
                        good_cells[cell_id] = 0
                hour_data.append(per_hour_all_cells)
                hour_mean_data.append(per_hour_mean_all_cells)
                nr_waveforms_per_hour_per_cell.append(this_hour_nr_waveforms_per_cell)

            # delete cells that don't have a mean waveform during first hour (because no spikes)
            # or don't have spikes in one window
            # ----------------------------------------------------------------------------------------------------------
            first_hour_mean = first_hour_mean[good_cells.astype(bool), :]
            # entire data from the specific hour
            hour_data_tmp = []
            # mean data from the specific hour
            hour_mean_data_tmp = []
            nr_waveforms_per_hour_per_cell_tmp = []
            for all_hour_dat, all_hour_mean_dat, nr_waveforms in zip(hour_data, hour_mean_data,
                                                                     nr_waveforms_per_hour_per_cell):
                hour_data_tmp.append([all_hour_dat[i] for i in np.argwhere(good_cells).flatten()])
                hour_mean_data_tmp.append([all_hour_mean_dat[i] for i in np.argwhere(good_cells).flatten()])
                nr_waveforms_per_hour_per_cell_tmp.append(nr_waveforms[good_cells.astype(bool)])

            # assign good data
            # hour_data: per cell and hour all spikes (e.g. features of all spikes)
            hour_data = hour_data_tmp
            # hour_mean_data: per cell and hour the mean of all spikes (e.g. the mean features of all spikes)
            hour_mean_data = hour_mean_data_tmp
            nr_waveforms_per_hour_per_cell = nr_waveforms_per_hour_per_cell_tmp

            # check if there are more than 1 cell:
            if len(hour_mean_data[0]) <= 1:
                print("Not enough cells for this tetrode .. continuing")
                good_cells[:] = 0
                good_cells_all_tetrodes.append(good_cells)
                continue

            # go through per hour data and compute distance of mean of first hour with mean waveforms
            # ----------------------------------------------------------------------------------------------------------
            # per hour: matrix (n_cells x n_cells) --> (m x n)
            #               - diagonal: distance between mean of first hour and mean of hour_x for each cell (within)
            #               - out of diagonal: distance between mean of first hour cell_m and mean of hour_x with cell_n
            per_hour_all_cells_distance_to_first_hour_mean = []
            for hour_id, per_hour_mean_waveforms_cells in enumerate(hour_mean_data):
                per_hour_mean_waveforms_cells_arr = np.vstack(per_hour_mean_waveforms_cells)[:,:nr_pca_comp]
                distance_hour = distance.cdist(per_hour_mean_waveforms_cells_arr, first_hour_mean[:,:nr_pca_comp],
                                               metric=distance_measure)
                per_hour_all_cells_distance_to_first_hour_mean.append(distance_hour)

            # compute within and across cell distance using mean waveform per hour and first hour
            # ----------------------------------------------------------------------------------------------------------
            ratio_own_mean_to_own_first_hour_vs_other_cells_means_to_own_first_hour = []
            ratio_own_mean_to_own_first_hour_vs_own_mean_to_means_of_others_first_hour = []
            for hour_feature_data in per_hour_all_cells_distance_to_first_hour_mean:
                # select distance within for all cells (each diagonal element corresponds to distance of mean of cell_n
                # of first hour with mean of cell_n of hour x)
                within_mean_feature = np.diagonal(hour_feature_data)
                # select distance for different cells (distance between mean of cell_n during first hour with means
                # of all other cells at hour x)
                across_mean_feature = np.mean(hour_feature_data[~np.eye(hour_feature_data.shape[0],
                                                                        dtype=bool)].reshape(hour_feature_data.shape[0],
                                                                                             -1).T, axis=0)
                # ratio: is mean of cell_n from hour x more similar to its own mean from the first hour than the means
                # of all other cells to the cell_n's mean from the first hour
                ratio_own_mean_to_own_first_hour_vs_other_cells_means_to_own_first_hour.append(within_mean_feature/
                                                                                               across_mean_feature)
                # go through columns and drop diagonal elements
                tmp_list = []
                for col_id, col in enumerate(hour_feature_data.T):
                    tmp_list.append(np.delete(col, col_id))
                tmp = np.vstack(tmp_list).T
                sim_with_other_means = np.mean(tmp, axis=0)
                # compare the mean of cell_n from hour x with the means of all other cells from the first hour
                # ratio: is mean of cell_n at hour x more similar to the same cell's mean from first hour or more
                # similar to the means of all the other cells from the first hour
                ratio_own_mean_to_own_first_hour_vs_own_mean_to_means_of_others_first_hour.append(within_mean_feature/
                                                                                                  sim_with_other_means)

            # go through per hour data and compute similarity of mean of first hour with all other waveforms
            # ----------------------------------------------------------------------------------------------------------
            # in this case: NOT THE MEAN FROM ALL THE SPIKES OF THE CORRESPONDING HOUR, BUT ALL SPIKES
            # result format:
            #               - for each hour: n_
            per_hour_all_cells_results = []
            for hour_id, per_hour_all_cells in enumerate(hour_data):
                # concatenate all spikes from all cells from current hour and compute distance
                distance_to_first_hour= distance.cdist(np.vstack(per_hour_all_cells)[:,:nr_pca_comp],
                                       first_hour_mean[:,:nr_pca_comp], metric=distance_measure)
                per_hour_all_cells_results.append(distance_to_first_hour)

            # all within distances and then mean is computed
            distance_own_spikes_hour_n_to_own_mean_first_hour_mean = []
            # all across distances and then the mean is computed
            distance_other_cell_spikes_hour_n_to_own_mean_first_hour_mean = []
            # all within distances
            distance_own_spikes_hour_n_to_own_mean_first_hour = []
            # all across distances
            distance_other_cell_spikes_hour_n_to_own_mean_first_hour = []
            # distances of spikes from hour n to mean from first our for ALL CELLS
            distance_all_spikes_hour_n_to_own_mean_first_hour = []

            # go through all requested hours to extract similarity for all waveforms
            # ----------------------------------------------------------------------------------------------------------
            # per_hour_all_cells_results: n_cells x n_spikes_per_hour (from all cells)
            #                               - first row: distance of all spikes to mean of FIRST cell
            #                               - second row: distance of all spikes to mean of SECOND cell
            #                               - ...
            for current_hour_all_cell_results, \
                current_hour_all_cells_nr_waveforms in zip(per_hour_all_cells_results, nr_waveforms_per_hour_per_cell):
                per_cell_within_mean = np.zeros(np.count_nonzero(np.sum(first_hour_mean,axis=1)))
                per_cell_across_mean = np.zeros(np.count_nonzero(np.sum(first_hour_mean,axis=1)))
                per_cell_within = []
                per_cell_across = []
                per_cell_within_z_scored = []
                per_cell_across_using_all_means = [[] for i in range(len(hour_data[0]))]
                # extract within vs. across correlations per cell
                for cell_id, cur_hour_cell_res, in enumerate(current_hour_all_cell_results.T):
                    # count all spikes (e.g. waveforms) until the first waveform of the current cell is reached
                    start_id_within_corr = int(np.sum(current_hour_all_cells_nr_waveforms[:cell_id]))
                    # add #spikes that occur for the current cell during the current hour to find last spike of cell
                    end_id_within_corr = start_id_within_corr+int(current_hour_all_cells_nr_waveforms[cell_id])
                    # check if cell has a mean to compare to
                    if np.sum(first_hour_mean[cell_id]):
                        # within correlations: correlations of features from all own spikes (not mean) with the own mean
                        # of the first hour
                        # ----------------------------------------------------------------------------------------------
                        per_cell_within_mean[cell_id] = np.mean(cur_hour_cell_res[
                                                                start_id_within_corr:end_id_within_corr])
                        corr_within = cur_hour_cell_res[start_id_within_corr:end_id_within_corr]
                        per_cell_within.append(corr_within)
                        # across correlations: correltions of features from all other cell spikes (not means) with own
                        # mean from first hour
                        # ----------------------------------------------------------------------------------------------
                        # delete within results after making copy
                        cur_hour_cell_res_copy = cur_hour_cell_res.copy()
                        cur_hour_cell_res_copy= np.delete(cur_hour_cell_res_copy, np.arange(start_id_within_corr,
                                                                                            end_id_within_corr))
                        mean_across = np.mean(cur_hour_cell_res_copy)
                        std_across = np.std(cur_hour_cell_res_copy)
                        per_cell_across_mean[cell_id] = mean_across
                        per_cell_across.append(cur_hour_cell_res_copy)
                        per_cell_within_z_scored.append((corr_within-mean_across)/std_across)
                        # extract correlations of all waveforms (own and from other cells) with the mean of the current
                        # cell from the first hour
                        ind_to_split = np.cumsum(current_hour_all_cells_nr_waveforms)[:-1].astype(int)
                        split_data = np.split(cur_hour_cell_res, ind_to_split)
                        for id, data in enumerate(split_data):
                            per_cell_across_using_all_means[id].append(data)

                distance_own_spikes_hour_n_to_own_mean_first_hour_mean.append(np.nan_to_num(per_cell_within_mean))
                distance_other_cell_spikes_hour_n_to_own_mean_first_hour_mean.append(np.nan_to_num(per_cell_across_mean))
                distance_own_spikes_hour_n_to_own_mean_first_hour.append(per_cell_within)
                distance_other_cell_spikes_hour_n_to_own_mean_first_hour.append(per_cell_across)
                distance_all_spikes_hour_n_to_own_mean_first_hour.append(per_cell_across_using_all_means)

            # similarity of cell waveforms with its own mean from first hour vs. similarity of own cell waveforms
            # with means from other cells from the first hour
            ratio_own_spikes_to_own_mean_first_hour_vs_own_spikes_to_mean_of_other_cells_first_hour = []
            for per_hour_data in distance_all_spikes_hour_n_to_own_mean_first_hour:
                # go through all the cells
                per_cell_res = np.zeros(len(hour_data[0]))
                for cell_id, per_hour_per_cell_data in enumerate(per_hour_data):
                    within = per_hour_per_cell_data[cell_id]
                    tmp_cp = per_hour_per_cell_data.copy()
                    del tmp_cp[cell_id]
                    # if there is only one cell on this tetrode
                    if len(tmp_cp) == 0:
                        continue
                    across = np.hstack(tmp_cp)
                    per_cell_res[cell_id] = np.mean(within)/np.mean(across)
                ratio_own_spikes_to_own_mean_first_hour_vs_own_spikes_to_mean_of_other_cells_first_hour.append(per_cell_res)

            ratio_own_spikes_to_own_mean_first_hour_vs_other_spikes_to_own_mean_first_hour = []
            for within, across in zip(distance_own_spikes_hour_n_to_own_mean_first_hour_mean,
                                      distance_other_cell_spikes_hour_n_to_own_mean_first_hour_mean):
                ratio_own_spikes_to_own_mean_first_hour_vs_other_spikes_to_own_mean_first_hour.append(within/across)

            # append data to collect across tetrodes
            # ----------------------------------------------------------------------------------------------------------
            # ratio: mean within similarity per hour / mean across similarity per hour
            all_tetrodes_ratio_own_spikes_to_own_mean_first_hour_vs_other_spikes_to_own_mean_first_hour.append(
                ratio_own_spikes_to_own_mean_first_hour_vs_other_spikes_to_own_mean_first_hour)
            # ratio: mean within similarity per hour using mean waveform / mean across similarity
            # per hour usin mean waveform
            all_tetrodes_ratio_own_mean_to_own_mean_first_hour_vs_other_cells_means_to_own_first_hour.append(
                ratio_own_mean_to_own_first_hour_vs_other_cells_means_to_own_first_hour)
            # ratio: distance of mean with own mean from first hour / similarity of mean with other
            # means from first hour
            all_tetrodes_ratio_own_mean_to_own_mean_first_hour_vs_own_mean_to_means_of_others_first_hour.append(
                ratio_own_mean_to_own_first_hour_vs_own_mean_to_means_of_others_first_hour)
            # ----------------------------------------------------------------------------------------------------------
            ratio_own_spikes_to_own_mean_first_hour_vs_own_spikes_to_mean_of_other_cells_first_hour = \
                np.vstack(ratio_own_spikes_to_own_mean_first_hour_vs_own_spikes_to_mean_of_other_cells_first_hour)
            all_tetrodes_ratio_own_spikes_to_own_mean_first_hour_vs_own_spikes_to_mean_of_other_cells_first_hour.append(
                ratio_own_spikes_to_own_mean_first_hour_vs_own_spikes_to_mean_of_other_cells_first_hour)
            # ----------------------------------------------------------------------------------------------------------
            distance_own_spikes_hour_n_to_own_mean_first_hour_result_last_hour = \
                distance_own_spikes_hour_n_to_own_mean_first_hour[-1]
            distance_other_cell_spikes_hour_n_to_own_mean_first_hour_result_first_hour = \
                distance_other_cell_spikes_hour_n_to_own_mean_first_hour[1]
            per_tetrode_distance_own_spikes_hour_n_to_own_mean_first_hour_result_last_hour = \
                np.hstack(distance_own_spikes_hour_n_to_own_mean_first_hour_result_last_hour)
            per_tetrode_distance_other_cell_spikes_hour_n_to_own_mean_first_hour_result_first_hour = \
                np.hstack(distance_other_cell_spikes_hour_n_to_own_mean_first_hour_result_first_hour)

            all_tetrodes_distance_other_cell_spikes_hour_n_to_own_mean_first_hour_result_first_hour.append(
                per_tetrode_distance_other_cell_spikes_hour_n_to_own_mean_first_hour_result_first_hour)

            all_tetrodes_distance_own_spikes_hour_n_to_own_mean_first_hour_result_last_hour.append(
                per_tetrode_distance_own_spikes_hour_n_to_own_mean_first_hour_result_last_hour)
            good_cells_all_tetrodes.append(good_cells)
        # stack good cells
        good_cells_all_tetrodes = np.hstack(good_cells_all_tetrodes)
        res_cell_ids = np.argwhere(good_cells_all_tetrodes==1).flatten()

        # make arrays
        all_tetrodes_ratio_own_spikes_to_own_mean_first_hour_vs_own_spikes_to_mean_of_other_cells_first_hour = \
         np.hstack(all_tetrodes_ratio_own_spikes_to_own_mean_first_hour_vs_own_spikes_to_mean_of_other_cells_first_hour)

        all_tetrodes_ratio_own_spikes_to_own_mean_first_hour_vs_own_spikes_to_mean_of_other_cells_first_hour = \
            all_tetrodes_ratio_own_spikes_to_own_mean_first_hour_vs_own_spikes_to_mean_of_other_cells_first_hour[:,
            ~np.isnan(np.sum(
                all_tetrodes_ratio_own_spikes_to_own_mean_first_hour_vs_own_spikes_to_mean_of_other_cells_first_hour,
                axis=0))]

        all_tetrodes_ratio_own_spikes_to_own_mean_first_hour_vs_other_spikes_to_own_mean_first_hour = \
            np.hstack(all_tetrodes_ratio_own_spikes_to_own_mean_first_hour_vs_other_spikes_to_own_mean_first_hour)

        all_tetrodes_ratio_own_mean_to_own_mean_first_hour_vs_other_cells_means_to_own_first_hour = \
            np.hstack(all_tetrodes_ratio_own_mean_to_own_mean_first_hour_vs_other_cells_means_to_own_first_hour)

        all_tetrodes_ratio_own_mean_to_own_mean_first_hour_vs_own_mean_to_means_of_others_first_hour = \
            np.hstack(all_tetrodes_ratio_own_mean_to_own_mean_first_hour_vs_own_mean_to_means_of_others_first_hour)

        all_tetrodes_distance_other_cell_spikes_hour_n_to_own_mean_first_hour_result_first_hour = \
            np.hstack(all_tetrodes_distance_other_cell_spikes_hour_n_to_own_mean_first_hour_result_first_hour)

        all_tetrodes_distance_own_spikes_hour_n_to_own_mean_first_hour_result_last_hour = \
            np.hstack(all_tetrodes_distance_own_spikes_hour_n_to_own_mean_first_hour_result_last_hour)

        res_to_use = all_tetrodes_ratio_own_spikes_to_own_mean_first_hour_vs_own_spikes_to_mean_of_other_cells_first_hour

        # need to modify cell_ids, because there might have been "bad cells" that were removed
        stable_ids_mod = []
        for stable_id in stable:
            if stable_id in res_cell_ids:
                stable_ids_mod.append(np.argwhere(res_cell_ids==stable_id)[0][0])
        stable_ids_mod = np.array(stable_ids_mod)

        dec_ids_mod = []
        for dec_id in dec:
            if dec_id in res_cell_ids:
                dec_ids_mod.append(np.argwhere(res_cell_ids==dec_id)[0][0])
        dec_ids_mod = np.array(dec_ids_mod)

        inc_ids_mod = []
        for inc_id in inc:
            if inc_id in res_cell_ids:
                inc_ids_mod.append(np.argwhere(res_cell_ids==inc_id)[0][0])
        inc_ids_mod = np.array(inc_ids_mod)

        to_own_mean_vs_to_other_means_stable = res_to_use[:, stable_ids_mod]

        to_own_mean_vs_to_other_means_dec = res_to_use[:, dec_ids_mod]

        to_own_mean_vs_to_other_means_inc = res_to_use[:, inc_ids_mod]

        if plotting:
            # plots using all spikes
            # ----------------------------------------------------------------------------------------------------------
            plot_data = [to_own_mean_vs_to_other_means_stable[0,:], to_own_mean_vs_to_other_means_dec[0,:],
                         to_own_mean_vs_to_other_means_inc[0,:]]
            plt.figure(figsize=(5, 8))
            cmap = matplotlib.cm.get_cmap('viridis')
            colors_to_plot = cmap(np.linspace(0, 1, 4))
            if save_fig:
                plt.style.use('default')
                c = "black"
            else:
                c = "white"
            bplot = plt.boxplot(plot_data, positions=[1, 2, 3],
                                patch_artist=True,
                                labels=["persistent", "decreasing", "increasing"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            plt.ylabel("All spikes: distance to own mean from first hour / \n "
                       "distance to means of other cells from first hour")
            plt.ylim(-0.01, 1.5)
            plt.title(str(hours_to_compute[0])+"th hour")
            plt.xticks(rotation=45)
            plt.tight_layout()
            colors = ["violet", 'turquoise', "orange"]
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "spikes_own_other_mean_vs_other_means_subsets_example.svg"),
                            transparent="True")
                plt.close()
            else:
                plt.show()

        return to_own_mean_vs_to_other_means_stable, to_own_mean_vs_to_other_means_dec,\
            to_own_mean_vs_to_other_means_inc

    def feature_stability_per_tetrode_subsets(self, plot_for_control=False, plotting=True, save_fig=False,
                                      hours_to_compute = [0, 7, 14, 21], nr_pca_comp = 10):

        # get cell labels

        with open(self.params.pre_proc_dir + "cell_classification/" + self.session_name + "_k_means.pickle",
                "rb") as f:
            class_dic = pickle.load(f)

        stable_ids = class_dic["stable_cell_ids"]
        dec_ids = class_dic["decrease_cell_ids"]
        inc_ids = class_dic["increase_cell_ids"]

        labels = [str(i) for i in hours_to_compute[1:]]

        all_tetrodes_within = []
        all_tetrodes_across = []
        all_tetrodes_using_all_means = []
        all_tetrodes_ratios = []
        all_tetrodes_ratios_mean_features = []
        all_tetrodes_ratios_mean_features_using_all_means = []

        cell_ids = []
        good_cells_all_tetrodes = []
        cell_id_offset=0
        # go through all tetrodes
        for tetrode in range(1, 16+1):
            # get spikes and features for current tetrode
            cell_spike_times_dic, cell_feature_dic = self.load_spikes_and_features_per_tetrode(tetrode=tetrode)

            if (cell_spike_times_dic is None) or (cell_feature_dic is None):
                print(" - no cells for current tetrode found!")
                continue
            cell_ids.append(cell_spike_times_dic.keys())
            cell_id_offset += len(cell_spike_times_dic)
            good_cells = np.ones(len(cell_spike_times_dic))
            # get keys from dictionary and get correct order
            cell_names = []
            for key in cell_spike_times_dic.keys():
                cell_names.append(key[4:])
            cell_names = np.array(cell_names).astype(int)
            cell_names.sort()

            # compute mean features in first hour per cell
            first_hour_mean = []
            ref_hour = 0
            for cell_id, ((_, cell_features), (_, spike_times)) in enumerate(zip(cell_feature_dic.items(),
                                                                                  cell_spike_times_dic.items())):
                if cell_features[np.logical_and(ref_hour * 20e3 * 60 * 60 < spike_times,
                                                         spike_times < (ref_hour + 1) * 20e3 * 60 * 60),:].shape[0] > 0:
                    # compute mean waveform per hour
                    first_hour_mean.append(np.mean(cell_features[np.logical_and(ref_hour * 20e3 * 60 * 60 < spike_times,
                                                             spike_times < (ref_hour + 1) * 20e3 * 60 * 60),:], axis=0))
                else:
                    first_hour_mean.append(np.zeros(cell_features.shape[1]))

            # if len(first_hour_mean) == 0:
            #     print(" - no cell activity during reference hour")
            #     continue
            first_hour_mean = np.vstack(first_hour_mean)

            cell_names = []
            for key in cell_feature_dic.keys():
                cell_names.append(key)

            # get all feature values and mean feature values for requested hours
            hour_data = []
            hour_mean_data = []
            nr_waveforms_per_hour_per_cell = []
            for hour in hours_to_compute:
                per_hour_all_cells = []
                per_hour_mean_all_cells = []
                this_hour_nr_waveforms_per_cell = np.zeros(len(cell_names))
                for cell_id, ((_, cell_features), (_, spike_times)) in enumerate(zip(cell_feature_dic.items(),
                                                                                      cell_spike_times_dic.items())):

                    # need to check if there are any waveforms of the current cell in the current window
                    cell_features_in_window = cell_features[np.logical_and(hour * 20e3 * 60 * 60 < spike_times,
                                                                           spike_times < (hour + 1) * 20e3 * 60 * 60),:]

                    # check if cell has a mean to compare to
                    if np.sum(first_hour_mean[cell_id]) and cell_features_in_window.shape[0]:
                        # check how many waveforms from the current cell in the current time window
                        this_hour_nr_waveforms_per_cell[cell_id] = cell_features_in_window.shape[0]
                        per_hour_all_cells.append(cell_features_in_window)
                        per_hour_mean_all_cells.append(np.mean(cell_features_in_window, axis=0))
                    else:
                        this_hour_nr_waveforms_per_cell[cell_id] = np.nan
                        per_hour_mean_all_cells.append(np.nan)
                        per_hour_all_cells.append(np.nan)
                        good_cells[cell_id] = 0
                hour_data.append(per_hour_all_cells)
                hour_mean_data.append(per_hour_mean_all_cells)
                nr_waveforms_per_hour_per_cell.append(this_hour_nr_waveforms_per_cell)

            # delete cells that don't have a mean feature values during first hour (because no spikes)
            # or don't have spikes in one window
            first_hour_mean = first_hour_mean[good_cells.astype(bool), :]
            hour_data_tmp = []
            hour_mean_data_tmp = []
            nr_waveforms_per_hour_per_cell_tmp = []
            for all_hour_dat, all_hour_mean_dat, nr_waveforms in zip(hour_data, hour_mean_data,
                                                                     nr_waveforms_per_hour_per_cell):
                hour_data_tmp.append([all_hour_dat[i] for i in np.argwhere(good_cells).flatten()])
                hour_mean_data_tmp.append([all_hour_mean_dat[i] for i in np.argwhere(good_cells).flatten()])
                nr_waveforms_per_hour_per_cell_tmp.append(nr_waveforms[good_cells.astype(bool)])
            hour_data = hour_data_tmp
            hour_mean_data = hour_mean_data_tmp
            nr_waveforms_per_hour_per_cell = nr_waveforms_per_hour_per_cell_tmp

            # check if there are more than 1 cells:
            if len(hour_mean_data[0])<= 1:
                print("Not enough cells for this tetrode .. continuing")
                good_cells[:] = 0
                good_cells_all_tetrodes.append(good_cells)
                continue

            # go trough per hour data and compute similarity of mean of first hour with mean features
            per_hour_all_cells_mean_feature_results = []
            for hour_id, per_hour_mean_waveforms_cells in enumerate(hour_mean_data):
                sim = 1-distance.cdist(np.vstack(per_hour_mean_waveforms_cells)[:,:nr_pca_comp],
                                       first_hour_mean[:,:nr_pca_comp], metric="cosine")
                per_hour_all_cells_mean_feature_results.append(sim)
            # compute within across using mean waveform per hour
            all_hours_ratio_mean_feature = []
            all_hours_ratio_mean_feature_using_all_means = []
            for hour_feature_data in per_hour_all_cells_mean_feature_results:
                within_mean_feature = np.diagonal(hour_feature_data)
                across_mean_feature = np.mean(hour_feature_data[~np.eye(hour_feature_data.shape[0],dtype=bool)].reshape(hour_feature_data.shape[0],-1).T, axis=0)
                all_hours_ratio_mean_feature.append(within_mean_feature/across_mean_feature)
                # go through columns and drop diagonal elements
                tmp_list = []
                for col_id, col in enumerate(hour_feature_data.T):
                    tmp_list.append(np.delete(col, col_id))
                tmp = np.vstack(tmp_list).T
                sim_with_other_means = np.mean(tmp, axis=0)
                all_hours_ratio_mean_feature_using_all_means.append(within_mean_feature/sim_with_other_means)

            # go trough per hour data and compute similarity of mean of first hour with all other waveforms
            per_hour_all_cells_results = []
            for hour_id, per_hour_all_cells in enumerate(hour_data):
                sim = 1-distance.cdist(np.vstack(per_hour_all_cells)[:,:nr_pca_comp],
                                       first_hour_mean[:,:nr_pca_comp], metric="cosine")
                per_hour_all_cells_results.append(sim)

            all_cells_within_mean = []
            all_cells_across_mean = []
            all_cells_within = []
            all_cells_across = []
            all_cells_across_using_all_means = []

            # go through all requested hours to compute similarity for all waveforms
            for current_hour_all_cell_results, current_hour_all_cells_nr_waveforms in zip(per_hour_all_cells_results,
                                                                                          nr_waveforms_per_hour_per_cell):
                per_cell_within_mean = np.zeros(np.count_nonzero(np.sum(first_hour_mean,axis=1)))
                per_cell_across_mean = np.zeros(np.count_nonzero(np.sum(first_hour_mean,axis=1)))
                per_cell_within = []
                per_cell_across = []
                per_cell_within_z_scored = []
                per_cell_across_using_all_means = [[] for i in range(len(hour_data[0]))]
                # extract within vs. across correlations per cell
                for cell_id, cur_hour_cell_res, in enumerate(current_hour_all_cell_results.T):
                    start_id_within_corr = int(np.sum(current_hour_all_cells_nr_waveforms[:cell_id]))
                    end_id_within_corr = start_id_within_corr+int(current_hour_all_cells_nr_waveforms[cell_id])
                    # check if cell has a mean to compare to
                    if np.sum(first_hour_mean[cell_id]):
                        # within correlations
                        per_cell_within_mean[cell_id] = np.mean(cur_hour_cell_res[start_id_within_corr:end_id_within_corr])
                        corr_within = cur_hour_cell_res[start_id_within_corr:end_id_within_corr]
                        per_cell_within.append(corr_within)
                        # across correlations
                        # delete within results after making copy
                        cur_hour_cell_res_copy = cur_hour_cell_res.copy()
                        cur_hour_cell_res_copy= np.delete(cur_hour_cell_res_copy, np.arange(start_id_within_corr,end_id_within_corr))
                        mean_across = np.mean(cur_hour_cell_res_copy)
                        std_across = np.std(cur_hour_cell_res_copy)
                        per_cell_across_mean[cell_id] = mean_across
                        per_cell_across.append(cur_hour_cell_res_copy)
                        per_cell_within_z_scored.append((corr_within-mean_across)/std_across)
                        # extract correlations of the waveforms of the other cells with the mean of the current cell
                        # from the first hour
                        ind_to_split = np.cumsum(current_hour_all_cells_nr_waveforms)[:-1].astype(int)
                        split_data = np.split(cur_hour_cell_res, ind_to_split)
                        for id, data in enumerate(split_data):
                            per_cell_across_using_all_means[id].append(data)

                all_cells_within_mean.append(np.nan_to_num(per_cell_within_mean))
                all_cells_across_mean.append(np.nan_to_num(per_cell_across_mean))
                all_cells_within.append(per_cell_within)
                all_cells_across.append(per_cell_across)
                all_cells_across_using_all_means.append(per_cell_across_using_all_means)

            # similarity of cell waveforms with its own mean from first hour vs. similarity of cell waveforms
            # with mean from other cells from the first hour
            all_hours_using_all_means = []
            for per_hour_data in all_cells_across_using_all_means:
                # go through all the cells
                per_cell_res = np.zeros(len(hour_data[0]))
                for cell_id, per_hour_per_cell_data in enumerate(per_hour_data):
                    within = per_hour_per_cell_data[cell_id]
                    tmp_cp = per_hour_per_cell_data.copy()
                    del tmp_cp[cell_id]
                    # if there is only one cell on this tetrode
                    if len(tmp_cp) == 0:
                        continue
                    across = np.hstack(tmp_cp)
                    per_cell_res[cell_id] = np.mean(within)/np.mean(across)
                all_hours_using_all_means.append(per_cell_res)

            all_hours_ratio = []
            for within, across in zip(all_cells_within_mean, all_cells_across_mean):
                all_hours_ratio.append(within/across)

            # append data to collect across tetrodes
            # ----------------------------------------------------------------------------------------------------------
            # ratio: mean within similarity per hour / mean across similarity per hour
            all_tetrodes_ratios.append(all_hours_ratio)
            # ratio: mean within similarity per hour using mean feature / mean across similarity per hour usin mean feature
            all_tetrodes_ratios_mean_features.append(all_hours_ratio_mean_feature)
            # ratio: similarity of mean with own mean from first hour / similarity of mean with other means from first hour
            all_tetrodes_ratios_mean_features_using_all_means.append(all_hours_ratio_mean_feature_using_all_means)
            all_hours_using_all_means = np.vstack(all_hours_using_all_means)
            all_tetrodes_using_all_means.append(all_hours_using_all_means)
            all_cells_within_last_hour = all_cells_within[-1]
            all_cells_across_first_hour = all_cells_across[1]
            per_tetrode_cells_within_last_hour = np.hstack(all_cells_within_last_hour)
            per_tetrode_cells_across_first_hour = np.hstack(all_cells_across_first_hour)
            all_tetrodes_across.append(per_tetrode_cells_within_last_hour)
            all_tetrodes_within.append(per_tetrode_cells_across_first_hour)
            good_cells_all_tetrodes.append(good_cells)
            if plot_for_control:
                plt.figure(figsize=(4, 5))
                c = "white"
                bplot = plt.boxplot(all_hours_using_all_means[1:,:].T, positions=[1, 2, 3], patch_artist=True,
                                    labels=["Hour 5", "Hour 10", "Hour 15"],
                                    boxprops=dict(color=c),
                                    capprops=dict(color=c),
                                    whiskerprops=dict(color=c),
                                    flierprops=dict(color=c, markeredgecolor=c),
                                    medianprops=dict(color=c), showfliers=False
                                    )
                plt.ylabel("(similarity with own mean)/(similarity with other means))")
                plt.xlabel("Time")
                plt.title("Tetrode "+str(tetrode))
                plt.show()

                p_within = 1. * np.arange(chunked_cells_within_last_hour.shape[0]) / (
                            chunked_cells_within_last_hour.shape[0] - 1)
                p_across = 1. * np.arange(chunked_cells_across_first_hour.shape[0]) / (
                            chunked_cells_across_first_hour.shape[0] - 1)
                plt.plot(np.sort(chunked_cells_within_last_hour), p_within, label="Within", color="magenta")
                plt.plot(np.sort(chunked_cells_across_first_hour), p_across, label="Across", color="darkmagenta")
                plt.legend()
                plt.ylabel("CDF")
                plt.xlabel("PEARSON R")
                plt.title("Tetrode " + str(tetrode) +", #cells="+str(len(all_cells_across_first_hour)))
                plt.show()

        good_cells_all_tetrodes = np.hstack(good_cells_all_tetrodes)
        res_cell_ids = np.argwhere(good_cells_all_tetrodes==1).flatten()

        all_tetrodes_across = np.hstack(all_tetrodes_across)
        all_tetrodes_within = np.hstack(all_tetrodes_within)
        all_tetrodes_using_all_means = np.hstack(all_tetrodes_using_all_means)
        all_tetrodes_using_all_means = all_tetrodes_using_all_means[:,~np.isnan(np.sum(all_tetrodes_using_all_means, axis=0))]

        # separate into cells to apply subset analysis
        all_tetrodes_ratios_mean_features_per_hour = [[] for x in range(len(hours_to_compute))]
        for tetrode_data in all_tetrodes_ratios_mean_features:
            for hour in range(len(hours_to_compute)):
                all_tetrodes_ratios_mean_features_per_hour[hour].extend(tetrode_data[hour])

        all_tetrodes_ratios_per_hour = [[] for x in range(len(hours_to_compute))]
        for tetrode_data in all_tetrodes_ratios:
            for hour in range(len(hours_to_compute)):
                all_tetrodes_ratios_per_hour[hour].extend(tetrode_data[hour])

        all_tetrodes_ratios_mean_features_using_all_means_per_hour = [[] for x in range(len(hours_to_compute))]
        for tetrode_data in all_tetrodes_ratios_mean_features_using_all_means:
            for hour in range(len(hours_to_compute)):
                all_tetrodes_ratios_mean_features_using_all_means_per_hour[hour].extend(tetrode_data[hour])

        # need to modify cell_ids, because there might have been "bad cells" that were removed
        stable_ids_mod = []
        for stable_id in stable_ids:
            if stable_id in res_cell_ids:
                stable_ids_mod.append(np.argwhere(res_cell_ids==stable_id)[0][0])
        stable_ids_mod = np.array(stable_ids_mod)

        dec_ids_mod = []
        for dec_id in dec_ids:
            if dec_id in res_cell_ids:
                dec_ids_mod.append(np.argwhere(res_cell_ids==dec_id)[0][0])
        dec_ids_mod = np.array(dec_ids_mod)

        inc_ids_mod = []
        for inc_id in inc_ids:
            if inc_id in res_cell_ids:
                inc_ids_mod.append(np.argwhere(res_cell_ids==inc_id)[0][0])
        inc_ids_mod = np.array(inc_ids_mod)

        # combine results for subsets for plotting
        stable_last_hour = np.array(all_tetrodes_ratios_mean_features_per_hour[-1])[stable_ids_mod]
        dec_last_hour = np.array(all_tetrodes_ratios_mean_features_per_hour[-1])[dec_ids_mod]
        inc_last_hour = np.array(all_tetrodes_ratios_mean_features_per_hour[-1])[inc_ids_mod]

        print(ttest_ind(stable_last_hour, dec_last_hour))
        print(ttest_ind(stable_last_hour, inc_last_hour))
        print(ttest_ind(dec_last_hour,inc_last_hour))

        if plotting:

            plt.figure(figsize=(3, 5))
            if save_fig:
                plt.style.use('default')
                c = "black"
            else:
                c = "white"
            bplot = plt.boxplot([stable_last_hour, dec_last_hour, inc_last_hour], positions=[1, 2, 3], patch_artist=True,
                                labels=["stable", "dec", "inc"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            plt.ylabel("similarity within using mean / similarity across using mean")
            plt.title("Hour "+str(hours_to_compute[-1]))
            plt.ylim(0.3, 1.7)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "feature_stab_subsets_example.svg"), transparent="True")
                plt.close()
            else:
                plt.show()

        else:
            return stable_last_hour, dec_last_hour, inc_last_hour

    def assess_stability_per_tetrode(self, plotting=True, nr_features_to_use=10):

        all_tetrodes_within = []
        all_tetrodes_across = []

        for tetrode in range(1, 16+1):

            cell_feature_dic, cell_spike_times_dic = self.load_spikes_and_features_per_tetrode(tetrode=tetrode)

            if (cell_spike_times_dic is None) or (cell_feature_dic is None):
                continue

            # compute mean/std of reference distribution (first hour) for each cell/feature
            per_cell_mean = []
            per_cell_std = []
            for (_, features_cell), (_, spike_times) in zip(cell_feature_dic.items(), cell_spike_times_dic.items()):
                per_cell_mean.append(np.mean(features_cell[spike_times < (20e3 * 60 * 120), :nr_features_to_use], axis=0))
                per_cell_std.append(np.std(features_cell[spike_times < (20e3 * 60 * 120), :nr_features_to_use], axis=0))

            cell_res = []
            # go through all cells and compute distance
            for p_c_mean, p_c_std, (_, features_cell), (_, spike_times) in zip(per_cell_mean, per_cell_std,
                                                                               cell_feature_dic.items(),
                                                                               cell_spike_times_dic.items()):
                per_hour_res = []
                for i_subplot, hour in enumerate([8, 16, 23]):
                    cell_val = features_cell[np.logical_and(hour * 20e3 * 60 * 30 < spike_times,
                                                            spike_times < (hour + 1) * 20e3 * 60 * 30), :nr_features_to_use]
                    cell_val_mean = np.mean(cell_val, axis=0)
                    per_hour_res.append((cell_val_mean - p_c_mean) / p_c_std)
                cell_res.append(per_hour_res)

            cell_res_array = np.vstack(cell_res)
            # put all results for one time window together
            all_hours_res_all_cells = []
            for hour in range(len(cell_res[0])):
                per_hour_res_all_cells = []
                for res in cell_res:
                    per_hour_res_all_cells.extend(res[hour].flatten())
                a = np.array(per_hour_res_all_cells)
                all_hours_res_all_cells.append(a[~np.isnan(a)])

            if plotting:

                c = "white"
                bplot = plt.boxplot(all_hours_res_all_cells, positions=[1, 2, 3], patch_artist=True,
                                    labels=["8h", "16h", "23h"],
                                    boxprops=dict(color=c),
                                    capprops=dict(color=c),
                                    whiskerprops=dict(color=c),
                                    flierprops=dict(color=c, markeredgecolor=c),
                                    medianprops=dict(color=c), showfliers=False
                                    )
                plt.ylabel("(feature-mean_feature_1h)/std_feature_1h)")
                plt.title("Tetrode "+str(tetrode))
                plt.show()
                print(ttest_ind(all_hours_res_all_cells[0], all_hours_res_all_cells[1], alternative="less"))
                print(ttest_ind(all_hours_res_all_cells[1], all_hours_res_all_cells[2], alternative="less"))
                print(ttest_ind(all_hours_res_all_cells[0], all_hours_res_all_cells[2], alternative="less"))
                print("HERE")

    def subsets_per_tetrode(self, plot_for_control=False, plotting=True, save_fig=False,
                                      hours_to_compute = [0, 7, 14, 21], nr_pca_comp = 10):

        # get cell labels
        with open(self.params.pre_proc_dir + "cell_classification/" + self.session_name + "_k_means.pickle",
                "rb") as f:
            class_dic = pickle.load(f)

        stable_ids = class_dic["stable_cell_ids"]
        dec_ids = class_dic["decrease_cell_ids"]
        inc_ids = class_dic["increase_cell_ids"]

        cell_on_tetrode = []

        # go through all tetrodes
        for tetrode in range(1, 16+1):
            # get spikes and features for current tetrode
            cell_spike_times_dic, cell_feature_dic = self.load_spikes_and_features_per_tetrode(tetrode=tetrode)
            if cell_spike_times_dic is None:
                continue
            cell_on_tetrode.append(np.ones(len(cell_spike_times_dic))*tetrode)

        cell_on_tetrode = np.hstack(cell_on_tetrode)

        # combine results for subsets for plotting
        stable_on_tetrode = cell_on_tetrode[stable_ids]
        dec_on_tetrode = cell_on_tetrode[dec_ids]
        inc_on_tetrode = cell_on_tetrode[inc_ids]

        if plotting:

            plt.figure(figsize=(5, 3))
            if save_fig:
                plt.style.use('default')

            plt.subplot(1,3,1)
            plt.hist(stable_on_tetrode, bins=16)
            plt.xlim(0.5,16)
            plt.xticks([1.5,15.5 ],[1, 16])
            plt.ylabel("# cells")
            plt.xlabel("Tetrode")
            plt.title("Stable")
            plt.subplot(1,3,2)
            plt.hist(dec_on_tetrode, bins=16)
            plt.xlim(0.5,16)
            plt.xticks([1.5,15.5 ],[1, 16])
            plt.xlabel("Tetrode")
            plt.title("Dec")
            plt.subplot(1,3,3)
            plt.hist(inc_on_tetrode, bins=16)
            plt.xlim(0.5,16)
            plt.xticks([1.5,15.5 ],[1, 16])
            plt.title("Inc")
            plt.xlabel("Tetrode")
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "subsets_per_tetrode_example.svg"), transparent="True")
                plt.close()
            else:
                plt.show()

        else:
            return stable_on_tetrode, dec_on_tetrode, inc_on_tetrode

    # </editor-fold>

    # <editor-fold desc="Waveform stability analysis">

    def load_spikes_and_waveforms(self, nr_tetrodes=16):

        # check if dictionaries exist already
        cell_waveform_dic_name = "cell_waveforms_"+self.session_name
        cell_spike_times_dic_name = "cell_spike_times_"+self.session_name

        if os.path.isfile(self.pre_proc_dir + "features_and_spikes/"+cell_waveform_dic_name) & \
            os.path.isfile(self.pre_proc_dir + "features_and_spikes/"+cell_spike_times_dic_name):

            with open(self.pre_proc_dir + "features_and_spikes/"+cell_waveform_dic_name, "rb") as f:
                cell_waveform_dic = pickle.load(f)
            with open(self.pre_proc_dir + "features_and_spikes/"+cell_spike_times_dic_name, "rb") as f:
                cell_spike_times_dic = pickle.load(f)

        else:
            # get cell ids
            cell_ids_orig = self.get_cell_id(data_dir=self.data_dir, session_name=self.session_name,
                                             cell_type=self.cell_type)
            cell_ids = cell_ids_orig

            cell_spike_times_dic = {}
            cell_waveform_dic = {}
            cell_id_offset = 0

            for tetrode in range(1, nr_tetrodes + 1):
                print(" - tetrode " + str(tetrode))
                # check if tetrode data exists
                if not os.path.isfile(self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".clu." +
                                      str(tetrode)):
                    print(" --> .clu file not found. Continuing ...")
                    break

                clu = read_integers(
                    self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".clu." + str(tetrode))
                res = read_integers(
                    self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".res." + str(tetrode))
                # load cluster IDs (from .clu) and times of spikes (from .res)
                spk = np.fromfile(
                    self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".spk." + str(tetrode),
                    dtype=np.int16)
                with open(self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".par." + str(
                        tetrode)) as f:
                    for i, line in enumerate(f):
                        if i == 0:
                            nr_channels = np.int(line.split(sep=" ")[1])
                        if i == 4:
                            nr_tet = np.int(line.split(sep=" ")[0])
                        if i > 4:
                            break
                spk = spk.reshape([-1, nr_tet, nr_channels])

                self.assign_spikes_and_waveforms(cell_ids=cell_ids, res=res, clu=clu, spk=spk,
                                                 cell_spike_times_dic=cell_spike_times_dic,
                                                 cell_waveform_dic=cell_waveform_dic, cell_id_offset=cell_id_offset)
                # need to add all cells from tetrode
                cell_id_offset += max(clu) - 1
                # need to remove the matched cell ids from the cell_ids list
                cell_ids = cell_ids_orig[len(cell_spike_times_dic):]

            with open(self.pre_proc_dir + "features_and_spikes/"+cell_waveform_dic_name, "wb") as f:
                pickle.dump(cell_waveform_dic, f, protocol=pickle.HIGHEST_PROTOCOL)
            with open(self.pre_proc_dir + "features_and_spikes/"+cell_spike_times_dic_name, "wb") as f:
                pickle.dump(cell_spike_times_dic, f, protocol=pickle.HIGHEST_PROTOCOL)

        return cell_spike_times_dic, cell_waveform_dic

    def load_spikes_and_waveforms_per_tetrode(self, tetrode=1):
        # check if dictionaries exist already
        cell_waveform_dic_name = "cell_waveforms_per_tetrode"+self.session_name
        cell_spike_times_dic_name = "cell_spike_times_per_tetrode"+self.session_name

        # check if spike time dictionary exists already
        if os.path.isfile(self.pre_proc_dir + "features_and_spikes/"+cell_spike_times_dic_name) & \
                os.path.isfile(self.pre_proc_dir + "features_and_spikes/"+cell_waveform_dic_name):

            with open(self.pre_proc_dir + "features_and_spikes/"+cell_spike_times_dic_name, "rb") as f:
                cell_spike_times_dic = pickle.load(f)

            with open(self.pre_proc_dir + "features_and_spikes/"+cell_waveform_dic_name, "rb") as f:
                cell_waveform_dic = pickle.load(f)

        else:
            # get cell ids
            cell_ids_orig = self.get_cell_id(data_dir=self.data_dir, session_name=self.session_name,
                                             cell_type=self.cell_type)
            cell_ids = cell_ids_orig
            cell_id_offset = 0
            cell_spike_times_dic = {}
            cell_waveform_dic = {}

            for tetrode in range(1, 16 + 1):
                per_tetrode_spike_times_dic = {}
                per_tetrode_waveform_dic = {}
                print(" - Loading data from tetrode " + str(tetrode))
                # check if tetrode data exists
                if not os.path.isfile(self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".clu." +
                                      str(tetrode)):
                    print(" --> .clu file not found")
                    continue

                clu = read_integers(
                    self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".clu." + str(tetrode))
                res = read_integers(
                    self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".res." + str(tetrode))
                # load cluster IDs (from .clu) and times of spikes (from .res)
                spk = np.fromfile(
                    self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".spk." + str(tetrode),
                    dtype=np.int16)
                with open(self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".par." + str(
                        tetrode)) as f:
                    for i, line in enumerate(f):
                        if i == 0:
                            nr_channels = np.int(line.split(sep=" ")[1])
                        if i == 4:
                            nr_tet = np.int(line.split(sep=" ")[0])
                        if i > 4:
                            break
                spk = spk.reshape([-1, nr_tet, nr_channels])

                self.assign_spikes_and_waveforms(cell_ids=cell_ids, res=res, clu=clu, spk=spk,
                                                 cell_spike_times_dic=per_tetrode_spike_times_dic,
                                                 cell_waveform_dic=per_tetrode_waveform_dic, cell_id_offset=cell_id_offset)

                cell_spike_times_dic[str(tetrode)] = per_tetrode_spike_times_dic
                cell_waveform_dic[str(tetrode)] = per_tetrode_waveform_dic
                curr_tet_clust_ids = np.unique(clu)
                # need to exclude the first two entries [0,1] are noise & artifact clusters
                curr_tet_clust_ids_only_cells = curr_tet_clust_ids[~np.logical_or(curr_tet_clust_ids==0, curr_tet_clust_ids==1)]
                cell_id_offset += curr_tet_clust_ids_only_cells.shape[0]
                # trim cell_ids (for next tetrode do not need to start from the first entry)
                cell_ids = cell_ids[len(per_tetrode_waveform_dic):]
            with open(self.pre_proc_dir + "features_and_spikes/"+cell_waveform_dic_name, "wb") as f:
                pickle.dump(cell_waveform_dic, f, protocol=pickle.HIGHEST_PROTOCOL)
            with open(self.pre_proc_dir + "features_and_spikes/"+cell_spike_times_dic_name, "wb") as f:
                pickle.dump(cell_spike_times_dic, f, protocol=pickle.HIGHEST_PROTOCOL)
        if str(tetrode) in cell_spike_times_dic and len(cell_spike_times_dic[str(tetrode)]):
            return cell_spike_times_dic[str(tetrode)], cell_waveform_dic[str(tetrode)]
        else:
            return None, None

    def plot_waveforms_single_cells(self, cell_id, electrode, save_fig=False, y_min=None, y_max=None,
                                    hours_to_plot=[0, 7, 14, 21]):

        scale_bar_y = 200
        scale_bar_x = 5

        cell_spike_times_dic, cell_waveform_dic = self.load_spikes_and_waveforms()

        # find last spike
        # last_spike = 0
        # for _, cell_fir in cell_spike_times_dic.items():
        #     if max(cell_fir) > 0:
        #         last_spike = max(cell_fir)
        #
        # dur_exp_h = last_spike * (1/20e3)/60/60mjc169R4R_0114

        # get keys from dictionary and get correct order
        cell_names = []
        for key in cell_spike_times_dic.keys():
            cell_names.append(key[4:])
        cell_names = np.array(cell_names).astype(int)
        cell_names.sort()

        waveforms = cell_waveform_dic["cell" + str(cell_names[cell_id])]
        spike_times = cell_spike_times_dic["cell" + str(cell_names[cell_id])]
        all_mean_waveforms = []
        for i_subplot, hour in enumerate(hours_to_plot):
            wf_within_hour = waveforms[
                np.logical_and(hour * 20e3 * 60 * 60 < spike_times, spike_times < (hour + 1) * 20e3 * 60 * 60)]
            mean = np.mean(wf_within_hour, axis=0)
            # check if one or several (up to 4 electrodes) of the tetrode
            if isinstance(electrode, list):
                mean_smoothed = []
                for el_id in electrode:
                    mean_smoothed.append(moving_average(a=mean[:, el_id], n=1))
            else:
                mean_smoothed = moving_average(a=mean[:, electrode], n=1)
            # example = wf_within_hour[::10, :, 0]
            # for i in range(example.shape[0]):
            #     plt.plot(example[i, :], color="lightgray")
            all_mean_waveforms.append(mean_smoothed)

        # determine min and max y values for plotting
        if y_min is None and y_max is None:
            y_min = np.min(np.hstack(all_mean_waveforms).flatten())
            y_max = np.max(np.hstack(all_mean_waveforms).flatten())
            y_min -= np.max([-1 * y_min, y_max]) * 0.05
            y_max += np.max([-1 * y_min, y_max]) * 0.05

        # plotting
        # --------------------------------------------------------------------------------------------------------------

        if save_fig:
            plt.style.use('default')
        plt.figure(figsize=(13, 8))
        cmap = matplotlib.cm.get_cmap('viridis')
        colors_to_plot = cmap(np.linspace(0, 1, 4))
        for i_subplot, (hour, mean_smoothed) in enumerate(zip(hours_to_plot, all_mean_waveforms)):
            plt.subplot(1, 5, i_subplot + 1)
            if isinstance(mean_smoothed, list):
                for tet_id, el_res in enumerate(mean_smoothed):
                    plt.plot(el_res - tet_id * 1000, color=colors_to_plot[i_subplot])
            else:
                plt.plot(mean_smoothed, color=colors_to_plot[i_subplot])
            if i_subplot == 0:
                # plt.ylabel("Amplitude")
                plt.title("1st hour")
                plt.hlines(0.9 * y_min + scale_bar_y, 0, scale_bar_x, colors="k", linewidth=2)
                plt.vlines(scale_bar_x, 0.9 * y_min, 0.9 * y_min + scale_bar_y, colors="k", linewidth=2)
                plt.gca().get_xaxis().set_ticks([])
                plt.gca().get_yaxis().set_ticks([])
            else:
                plt.title(str(hour) + "th hour")
                plt.gca().get_yaxis().set_ticks([])
                plt.gca().get_xaxis().set_ticks([])
            # plt.ylim(y_min, y_max)
        plt.subplot(1, 5, 5)
        for i, wf in enumerate(all_mean_waveforms):
            if isinstance(wf, list):
                for tet_id, el_res in enumerate(wf):
                    plt.plot(el_res - tet_id * 1000, color=colors_to_plot[i])
            else:
                plt.plot(wf, color=colors_to_plot[i])
        plt.title("Superimposed")
        # plt.ylim(y_min, y_max)
        plt.gca().get_yaxis().set_ticks([])
        plt.gca().get_xaxis().set_ticks([])
        if save_fig:
            plt.tight_layout()
            plt.rcParams['svg.fonttype'] = 'none'
            plt.savefig(os.path.join(save_path, "waveform_example_cell_" + str(cell_id) + str(electrode) + ".svg"),
                        transparent="True")
            plt.close()
        else:
            plt.show()

    def plot_clusters_across_time(self, save_fig=False, debug=False,
                                        hours_to_plot=[0, 5, 15], tetrode=2):

        cell_spike_times_dic, cell_feature_dic = self.load_spikes_and_features_per_tetrode(tetrode=tetrode)

        feat_1 = 0
        feat_2 = 6

        # plot clusters across time
        x_max_ = -np.inf
        x_min_ = np.inf
        y_max_ = -np.inf
        y_min_ = np.inf
        if save_fig:
            plt.style.use("default")
        fig = plt.figure(figsize=(13, 5))
        gs = fig.add_gridspec(10, 12)
        ax1 = fig.add_subplot(gs[:,:4])
        ax2 = fig.add_subplot(gs[:,4:8])
        ax3 = fig.add_subplot(gs[:,8:])
        ax_list = [ax1, ax2, ax3]
        for i_hour, hour in enumerate(hours_to_plot):
            for cell_id, ((_, cell_features), (_, spike_times)) in enumerate(zip(cell_feature_dic.items(),
                                                                                 cell_spike_times_dic.items())):
                # compute mean waveform per hour
                cl_features_window = cell_features[np.logical_and(hour * 20e3 * 60 * 60 < spike_times,
                                                                  spike_times < (hour + 1) * 20e3 * 60 * 60), :]

                ax_list[i_hour].scatter(cl_features_window[:, feat_1], cl_features_window[:, feat_2])
            x_min_ = np.min([x_min_, np.min(ax_list[i_hour].get_xlim()[0])])
            x_max_ = np.max([x_max_, np.max(ax_list[i_hour].get_xlim()[1])])
            y_min_ = np.min([y_min_, np.min(ax_list[i_hour].get_ylim()[0])])
            y_max_ = np.max([y_max_, np.max(ax_list[i_hour].get_ylim()[1])])

            if i_hour > 0:
                ax_list[i_hour].set_yticks([])
            else:
                ax_list[i_hour].set_ylabel("Feature "+str(feat_1)+" (a.u.)")
            ax_list[i_hour].set_title(str(hour)+"hour")
            ax_list[i_hour].set_xlabel("Feature "+str(feat_2)+" (a.u.)")

        # adjust x and y lim
        for i_hour, _ in enumerate(hours_to_plot):
            ax_list[i_hour].set_xlim(x_min_, x_max_)
            ax_list[i_hour].set_ylim(y_min_, y_max_)
        plt.tight_layout()
        if save_fig:
            plt.tight_layout()
            plt.rcParams['svg.fonttype'] = 'none'
            plt.savefig("clusters_across_time_"+self.session_name+"_tet_"+str(tetrode) +"_feat_"+str(feat_1)+"_"+
                        str(feat_2)+".svg", transparent="True")
            plt.close()
        else:
            plt.show()

        if debug:
            features = np.arange(16)

            n_combis = factorial(features.shape[0])/factorial(features.shape[0]-2)

            for combo in combinations(features, 2):

                for cell_id, ((_, cell_features), (_, spike_times)) in enumerate(zip(cell_feature_dic.items(),
                                                                                     cell_spike_times_dic.items())):
                    # compute mean waveform per hour
                    cl_features_window = cell_features[:, :16]
                    plt.scatter(cell_features[::100, combo[0]], cell_features[::100, combo[1]])
                plt.xlabel("Feature "+str(combo[0]))
                plt.ylabel("Feature "+str(combo[1]))
                plt.tight_layout()
                plt.show()
                print("HERE")
                plt.close()

    def plot_waveforms(self, nr_tetrodes=16, cells_to_plot="stable"):

        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle",
                  "rb") as f:
            class_dic = pickle.load(f)

        if cells_to_plot == "stable":
            subset = class_dic["stable_cell_ids"]
        elif cells_to_plot == "dec":
            subset = class_dic["decrease_cell_ids"]
        elif cells_to_plot == "inc":
            subset = class_dic["increase_cell_ids"]

        cell_spike_times_dic, cell_waveform_dic = self.load_spikes_and_waveforms()

        # get keys from dictionary and get correct order
        cell_names = []
        for key in cell_spike_times_dic.keys():
            cell_names.append(key[4:])
        cell_names = np.array(cell_names).astype(int)
        cell_names.sort()

        for e, cell_id in enumerate(subset):

            waveforms = cell_waveform_dic["cell" + str(cell_names[cell_id])]

            example = waveforms[::100, :, 0]
            for i in range(example.shape[0]):
                plt.plot(example[i, :], color="lightgray")
            plt.ylabel("Amplitude")
            plt.xlabel("Time")
            plt.title("Waveform - cell "+str(cell_id))
            plt.show()

    def waveform_stability_per_tetrode(self, plot_for_control=False, plotting=True, save_fig=False, hours_to_compute = [0, 7, 14, 21]):

        labels = [str(i) for i in hours_to_compute[1:]]

        all_tetrodes_within = []
        all_tetrodes_across = []
        all_tetrodes_using_all_means = []
        all_tetrodes_ratios = []
        all_tetrodes_ratios_mean_waveforms = []
        all_tetrodes_ratios_mean_waveforms_using_all_means = []

        # go through all tetrodes
        for tetrode in range(1, 16+1):
            # get spikes and waveforms for current tetrode
            cell_spike_times_dic, cell_waveform_dic = self.load_spikes_and_waveforms_per_tetrode(tetrode=tetrode)

            if (cell_spike_times_dic is None) or (cell_waveform_dic is None):
                print(" - no cells for current tetrode found!")
                continue
            good_cells = np.ones(len(cell_spike_times_dic))
            # get keys from dictionary and get correct order
            cell_names = []
            for key in cell_spike_times_dic.keys():
                cell_names.append(key[4:])
            cell_names = np.array(cell_names).astype(int)
            cell_names.sort()

            # compute mean waveform in first hour per cell
            first_hour_mean = []
            first_hour_std = []
            ref_hour = 0
            for cell_id, ((_, cell_waveforms), (_, spike_times)) in enumerate(zip(cell_waveform_dic.items(),
                                                                                  cell_spike_times_dic.items())):
                if cell_waveforms[np.logical_and(ref_hour * 20e3 * 60 * 60 < spike_times,
                                                         spike_times < (ref_hour + 1) * 20e3 * 60 * 60),:,:].shape[0] > 0:
                    # compute mean waveform per hour
                    first_hour_mean.append(np.mean(np.mean(cell_waveforms[np.logical_and(ref_hour * 20e3 * 60 * 60 < spike_times,
                                                             spike_times < (ref_hour + 1) * 20e3 * 60 * 60),:,:], axis=2), axis=0))
                    first_hour_std.append(np.std(np.mean(cell_waveforms[np.logical_and(ref_hour * 20e3 * 60 * 60 < spike_times,
                                                             spike_times < (ref_hour + 1) * 20e3 * 60 * 60),:,:], axis=2), axis=0))
                else:
                    first_hour_mean.append(np.zeros(cell_waveforms.shape[1]))
                    first_hour_std.append(np.zeros(cell_waveforms.shape[1]))

            if len(first_hour_mean) == 0:
                print(" - no cell activity during reference hour")
                continue
            first_hour_mean = np.vstack(first_hour_mean)

            cell_names = []
            for key in cell_waveform_dic.keys():
                cell_names.append(key)

            # get all waveforms and mean waveforms for requested hours
            hour_data = []
            hour_mean_data = []
            nr_waveforms_per_hour_per_cell = []
            for hour in hours_to_compute:
                per_hour_all_cells = []
                per_hour_mean_all_cells = []
                this_hour_nr_waveforms_per_cell = np.zeros(len(cell_names))
                for cell_id, ((_, cell_waveforms), (_, spike_times)) in enumerate(zip(cell_waveform_dic.items(),
                                                                                      cell_spike_times_dic.items())):

                    # need to check if there are any waveforms of the current cell in the current window
                    cell_waveforms_in_window = cell_waveforms[np.logical_and(hour * 20e3 * 60 * 60 < spike_times,
                                                                             spike_times < (hour + 1) * 20e3 * 60 * 60),
                                               :, :]

                    # check if cell has a mean to compare to
                    if np.sum(first_hour_mean[cell_id]) and cell_waveforms_in_window.shape[0]:
                        selected_waveforms = np.mean(cell_waveforms_in_window, axis=2)
                        # check how many waveforms from the current cell in the current time window
                        this_hour_nr_waveforms_per_cell[cell_id] = selected_waveforms.shape[0]
                        per_hour_all_cells.append(selected_waveforms)
                        per_hour_mean_all_cells.append(np.mean(selected_waveforms, axis=0))
                    else:
                        this_hour_nr_waveforms_per_cell[cell_id] = np.nan
                        per_hour_mean_all_cells.append(np.nan)
                        per_hour_all_cells.append(np.nan)
                        good_cells[cell_id] = 0
                hour_data.append(per_hour_all_cells)
                hour_mean_data.append(per_hour_mean_all_cells)
                nr_waveforms_per_hour_per_cell.append(this_hour_nr_waveforms_per_cell)

            # delete cells that don't have a mean waveform during first hour (because no spikes) or don't have spikes in one window
            first_hour_mean = first_hour_mean[good_cells.astype(bool), :]
            hour_data_tmp = []
            hour_mean_data_tmp = []
            nr_waveforms_per_hour_per_cell_tmp = []
            for all_hour_dat, all_hour_mean_dat, nr_waveforms in zip(hour_data, hour_mean_data,
                                                                     nr_waveforms_per_hour_per_cell):
                hour_data_tmp.append([all_hour_dat[i] for i in np.argwhere(good_cells).flatten()])
                hour_mean_data_tmp.append([all_hour_mean_dat[i] for i in np.argwhere(good_cells).flatten()])
                nr_waveforms_per_hour_per_cell_tmp.append(nr_waveforms[good_cells.astype(bool)])
            hour_data = hour_data_tmp
            hour_mean_data = hour_mean_data_tmp
            nr_waveforms_per_hour_per_cell = nr_waveforms_per_hour_per_cell_tmp

            # check if there are more than 1 cells:
            if len(hour_mean_data[0])<= 1:
                print("Not enough cells for this tetrode .. continuing")
                continue

            # go trough per hour data and compute similarity of mean of first hour with mean waveforms
            per_hour_all_cells_mean_waveform_results = []
            for hour_id, per_hour_mean_waveforms_cells in enumerate(hour_mean_data):
                per_hour_all_cells_mean_waveform_results.append(compute_correlations_col_fast(x=first_hour_mean.T,
                                                                                y=np.vstack(per_hour_mean_waveforms_cells).T))

            # compute within across using mean waveform per hour
            all_hours_ratio_mean_waveform = []
            all_hours_ratio_mean_waveform_using_all_means = []
            for hour_waveform_data in per_hour_all_cells_mean_waveform_results:
                within_mean_waveform = np.diagonal(hour_waveform_data)
                across_mean_waveform = np.mean(hour_waveform_data[~np.eye(hour_waveform_data.shape[0],dtype=bool)].reshape(hour_waveform_data.shape[0],-1).T, axis=0)
                all_hours_ratio_mean_waveform.append(within_mean_waveform/across_mean_waveform)
                # go through columns and drop diagonal elements
                tmp_list = []
                for col_id, col in enumerate(hour_waveform_data.T):
                    tmp_list.append(np.delete(col, col_id))
                tmp = np.vstack(tmp_list).T
                sim_with_other_means = np.mean(tmp, axis=0)
                all_hours_ratio_mean_waveform_using_all_means.append(within_mean_waveform/sim_with_other_means)

            # go trough per hour data and compute similarity of mean of first hour with all other waveforms
            per_hour_all_cells_results = []
            for hour_id, per_hour_all_cells in enumerate(hour_data):
                per_hour_all_cells_results.append(compute_correlations_col_fast(x=first_hour_mean.T,
                                                                                y=np.vstack(per_hour_all_cells).T))

            all_cells_within_mean = []
            all_cells_across_mean = []
            all_cells_within = []
            all_cells_across = []
            all_cells_across_using_all_means = []

            # go through all requested hours to compute similarity for all waveforms
            for current_hour_all_cell_results, current_hour_all_cells_nr_waveforms in zip(per_hour_all_cells_results,
                                                                                          nr_waveforms_per_hour_per_cell):
                per_cell_within_mean = np.zeros(np.count_nonzero(np.sum(first_hour_mean,axis=1)))
                per_cell_across_mean = np.zeros(np.count_nonzero(np.sum(first_hour_mean,axis=1)))
                per_cell_within = []
                per_cell_across = []
                per_cell_within_z_scored = []
                per_cell_across_using_all_means = [[] for i in range(len(hour_data[0]))]
                # extract within vs. across correlations per cell
                for cell_id, cur_hour_cell_res, in enumerate(current_hour_all_cell_results):
                    start_id_within_corr = int(np.sum(current_hour_all_cells_nr_waveforms[:cell_id]))
                    end_id_within_corr = start_id_within_corr+int(current_hour_all_cells_nr_waveforms[cell_id])
                    # check if cell has a mean to compare to
                    if np.sum(first_hour_mean[cell_id]):
                        # within correlations
                        per_cell_within_mean[cell_id] = np.mean(cur_hour_cell_res[start_id_within_corr:end_id_within_corr])
                        corr_within = cur_hour_cell_res[start_id_within_corr:end_id_within_corr]
                        per_cell_within.append(corr_within)
                        # across correlations
                        # delete within results after making copy
                        cur_hour_cell_res_copy = cur_hour_cell_res.copy()
                        cur_hour_cell_res_copy= np.delete(cur_hour_cell_res_copy, np.arange(start_id_within_corr,end_id_within_corr))
                        mean_across = np.mean(cur_hour_cell_res_copy)
                        std_across = np.std(cur_hour_cell_res_copy)
                        per_cell_across_mean[cell_id] = mean_across
                        per_cell_across.append(cur_hour_cell_res_copy)
                        per_cell_within_z_scored.append((corr_within-mean_across)/std_across)
                        # extract correlations of the waveforms of the other cells with the mean of the current cell
                        # from the first hour
                        ind_to_split = np.cumsum(current_hour_all_cells_nr_waveforms)[:-1].astype(int)
                        split_data = np.split(cur_hour_cell_res, ind_to_split)
                        for id, data in enumerate(split_data):
                            per_cell_across_using_all_means[id].append(data)

                all_cells_within_mean.append(np.nan_to_num(per_cell_within_mean))
                all_cells_across_mean.append(np.nan_to_num(per_cell_across_mean))
                all_cells_within.append(per_cell_within)
                all_cells_across.append(per_cell_across)
                all_cells_across_using_all_means.append(per_cell_across_using_all_means)

            # similarity of cell waveforms with its own mean from first hour vs. similarity of cell waveforms
            # with mean from other cells from the first hour
            all_hours_using_all_means = []
            for per_hour_data in all_cells_across_using_all_means:
                # go through all the cells
                per_cell_res = np.zeros(len(hour_data[0]))
                for cell_id, per_hour_per_cell_data in enumerate(per_hour_data):
                    within = per_hour_per_cell_data[cell_id]
                    tmp_cp = per_hour_per_cell_data.copy()
                    del tmp_cp[cell_id]
                    # if there is only one cell on this tetrode
                    if len(tmp_cp) == 0:
                        continue
                    across = np.hstack(tmp_cp)
                    per_cell_res[cell_id] = np.mean(within)/np.mean(across)
                all_hours_using_all_means.append(per_cell_res)

            all_hours_ratio = []
            for within, across in zip(all_cells_within_mean, all_cells_across_mean):
                all_hours_ratio.append(within/across)

            # append data to collect across tetrodes
            # ----------------------------------------------------------------------------------------------------------
            # ratio: mean within similarity per hour / mean across similarity per hour
            all_tetrodes_ratios.append(all_hours_ratio)
            # ratio: mean within similarity per hour using mean waveform / mean across similarity per hour usin mean waveform
            all_tetrodes_ratios_mean_waveforms.append(all_hours_ratio_mean_waveform )
            # ratio: similarity of mean with own mean from first hour / similarity of mean with other means from first hour
            all_tetrodes_ratios_mean_waveforms_using_all_means.append(all_hours_ratio_mean_waveform_using_all_means)
            all_hours_using_all_means = np.vstack(all_hours_using_all_means)
            all_tetrodes_using_all_means.append(all_hours_using_all_means)
            all_cells_within_last_hour = all_cells_within[-1]
            all_cells_across_first_hour = all_cells_across[1]
            per_tetrode_cells_within_last_hour = np.hstack(all_cells_within_last_hour)
            per_tetrode_cells_across_first_hour = np.hstack(all_cells_across_first_hour)
            all_tetrodes_across.append(per_tetrode_cells_within_last_hour)
            all_tetrodes_within.append(per_tetrode_cells_across_first_hour)

            if plot_for_control:
                plt.figure(figsize=(4, 5))
                c = "white"
                bplot = plt.boxplot(all_hours_using_all_means[1:,:].T, positions=[1, 2, 3], patch_artist=True,
                                    labels=["Hour 5", "Hour 10", "Hour 15"],
                                    boxprops=dict(color=c),
                                    capprops=dict(color=c),
                                    whiskerprops=dict(color=c),
                                    flierprops=dict(color=c, markeredgecolor=c),
                                    medianprops=dict(color=c), showfliers=False
                                    )
                plt.ylabel("(similarity with own mean)/(similarity with other means))")
                plt.xlabel("Time")
                plt.title("Tetrode "+str(tetrode))
                plt.show()

                p_within = 1. * np.arange(chunked_cells_within_last_hour.shape[0]) / (
                            chunked_cells_within_last_hour.shape[0] - 1)
                p_across = 1. * np.arange(chunked_cells_across_first_hour.shape[0]) / (
                            chunked_cells_across_first_hour.shape[0] - 1)
                plt.plot(np.sort(chunked_cells_within_last_hour), p_within, label="Within", color="magenta")
                plt.plot(np.sort(chunked_cells_across_first_hour), p_across, label="Across", color="darkmagenta")
                plt.legend()
                plt.ylabel("CDF")
                plt.xlabel("PEARSON R")
                plt.title("Tetrode " + str(tetrode) +", #cells="+str(len(all_cells_across_first_hour)))
                plt.show()

        all_tetrodes_across = np.hstack(all_tetrodes_across)
        all_tetrodes_within = np.hstack(all_tetrodes_within)
        all_tetrodes_using_all_means = np.hstack(all_tetrodes_using_all_means)
        all_tetrodes_using_all_means = all_tetrodes_using_all_means[:,~np.isnan(np.sum(all_tetrodes_using_all_means, axis=0))]

        all_tetrodes_ratios = np.hstack(all_tetrodes_ratios)
        all_tetrodes_ratios_mean_waveforms = np.hstack(all_tetrodes_ratios_mean_waveforms)
        all_tetrodes_ratios_mean_waveforms_using_all_means = np.hstack(all_tetrodes_ratios_mean_waveforms_using_all_means)

        if plotting or save_fig:

            plt.figure(figsize=(3, 5))
            if save_fig:
                plt.style.use('default')
                c = "black"
            else:
                c = "white"
            bplot = plt.boxplot(all_tetrodes_ratios_mean_waveforms_using_all_means[1:, :].T, positions=[1, 2, 3], patch_artist=True,
                                labels=labels,
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            plt.ylabel("similarity with own mean / similarity with other mean")
            plt.ylim(0.5, 1.5)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "waveform_stab_sim_own_other_means_using_means_example.svg"),
                            transparent="True")
                plt.close()
            else:
                plt.show()

            print("STATS OWN MEAN / OTHER MEANS USING MEAN WAVEFORM:")

            print(ttest_ind(all_tetrodes_ratios_mean_waveforms_using_all_means[1], all_tetrodes_ratios_mean_waveforms_using_all_means[2], alternative="greater"))
            print(ttest_ind(all_tetrodes_ratios_mean_waveforms_using_all_means[2], all_tetrodes_ratios_mean_waveforms_using_all_means[3], alternative="greater"))
            print(ttest_ind(all_tetrodes_ratios_mean_waveforms_using_all_means[1], all_tetrodes_ratios_mean_waveforms_using_all_means[3], alternative="greater"))
            print(mannwhitneyu(all_tetrodes_ratios_mean_waveforms_using_all_means[1], all_tetrodes_ratios_mean_waveforms_using_all_means[2], alternative="greater"))
            print(mannwhitneyu(all_tetrodes_ratios_mean_waveforms_using_all_means[2], all_tetrodes_ratios_mean_waveforms_using_all_means[3], alternative="greater"))
            print(mannwhitneyu(all_tetrodes_ratios_mean_waveforms_using_all_means[1], all_tetrodes_ratios_mean_waveforms_using_all_means[3], alternative="greater"))

            plt.figure(figsize=(3, 5))
            if save_fig:
                plt.style.use('default')
                c = "black"
            else:
                c = "white"
            bplot = plt.boxplot(all_tetrodes_ratios_mean_waveforms[1:, :].T, positions=[1, 2, 3], patch_artist=True,
                                labels=labels,
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            plt.ylabel("mean within similarity / mean across similarity")
            plt.ylim(0.7, 1.3)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "waveform_stab_within_across_using_mean_example.svg"),
                            transparent="True")
                plt.close()
            else:
                plt.show()

            print("STATS WITHIN / ACROSS USING MEAN WAVEFORM:")

            print(ttest_ind(all_tetrodes_ratios_mean_waveforms[1], all_tetrodes_ratios_mean_waveforms[2], alternative="greater"))
            print(ttest_ind(all_tetrodes_ratios_mean_waveforms[2], all_tetrodes_ratios_mean_waveforms[3], alternative="greater"))
            print(ttest_ind(all_tetrodes_ratios_mean_waveforms[1], all_tetrodes_ratios_mean_waveforms[3], alternative="greater"))
            print(mannwhitneyu(all_tetrodes_ratios_mean_waveforms[1], all_tetrodes_ratios_mean_waveforms[2], alternative="greater"))
            print(mannwhitneyu(all_tetrodes_ratios_mean_waveforms[2], all_tetrodes_ratios_mean_waveforms[3], alternative="greater"))
            print(mannwhitneyu(all_tetrodes_ratios_mean_waveforms[1], all_tetrodes_ratios_mean_waveforms[3], alternative="greater"))

            plt.figure(figsize=(3, 5))
            if save_fig:
                plt.style.use('default')
                c = "black"
            else:
                c = "white"
            bplot = plt.boxplot(all_tetrodes_ratios[1:, :].T, positions=[1, 2, 3], patch_artist=True,
                                labels=labels,
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            plt.ylabel("mean within similarity / mean across similarity")
            plt.ylim(0.0, 2)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "waveform_stab_within_across_example.svg"), transparent="True")
                plt.close()
            else:
                plt.show()

            print("STATS WITHIN / ACROSS:")

            print(ttest_ind(all_tetrodes_ratios[1], all_tetrodes_ratios[2], alternative="greater"))
            print(ttest_ind(all_tetrodes_ratios[2], all_tetrodes_ratios[3], alternative="greater"))
            print(ttest_ind(all_tetrodes_ratios[1], all_tetrodes_ratios[3], alternative="greater"))
            print(mannwhitneyu(all_tetrodes_ratios[1], all_tetrodes_ratios[2], alternative="greater"))
            print(mannwhitneyu(all_tetrodes_ratios[2], all_tetrodes_ratios[3], alternative="greater"))
            print(mannwhitneyu(all_tetrodes_ratios[1], all_tetrodes_ratios[3], alternative="greater"))

            p_within = 1. * np.arange(all_tetrodes_within.shape[0]) / (all_tetrodes_within.shape[0] - 1)
            p_across = 1. * np.arange(all_tetrodes_across.shape[0]) / (all_tetrodes_across.shape[0] - 1)

            plt.plot(np.sort(all_tetrodes_within), p_within, label="Within", color="magenta")
            plt.plot(np.sort(all_tetrodes_across), p_across, label="Across", color="darkmagenta")
            plt.legend()
            plt.ylabel("CDF")
            plt.xlabel("PEARSON R")
            plt.title("Within similarity: waveforms of last hour with mean \n Across similarity: waveforms of first hour from other cells")
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "waveform_stab_within_time_across_cell_cdf.svg"),
                            transparent="True")
                plt.close()
            else:
                plt.show()

            print("WITHIN TIME / ACROSS CELLS")
            print(mannwhitneyu(all_tetrodes_within, all_tetrodes_across))

            plt.figure(figsize=(3, 5))
            if save_fig:
                plt.style.use('default')
                c = "black"
            else:
                c = "white"
            bplot = plt.boxplot(all_tetrodes_using_all_means[1:, :].T, positions=[1, 2, 3], patch_artist=True,
                                labels=labels,
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            plt.ylabel("similarity with own mean/\n similarity with means from same tetrode")
            plt.ylim(0.0, 1.50)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "waveform_stab_sim_own_other_means_example.svg"),
                            transparent="True")
                plt.close()
            else:
                plt.show()

            print("STATS WITH OWN/OTHER MEAN:")

            print(ttest_ind(all_tetrodes_using_all_means[1], all_tetrodes_using_all_means[2], alternative="greater"))
            print(ttest_ind(all_tetrodes_using_all_means[2], all_tetrodes_using_all_means[3], alternative="greater"))
            print(ttest_ind(all_tetrodes_using_all_means[1], all_tetrodes_using_all_means[3], alternative="greater"))
            print(mannwhitneyu(all_tetrodes_using_all_means[1], all_tetrodes_using_all_means[2], alternative="greater"))
            print(mannwhitneyu(all_tetrodes_using_all_means[2], all_tetrodes_using_all_means[3], alternative="greater"))
            print(mannwhitneyu(all_tetrodes_using_all_means[1], all_tetrodes_using_all_means[3], alternative="greater"))

        else:

            return all_tetrodes_ratios_mean_waveforms[1:, :]

    def waveform_stability(self, plotting=True, save_fig=False, hours_to_compute=[0, 7, 14, 21]):

        labels = [str(i) for i in hours_to_compute[1:]]

        cell_spike_times_dic, cell_waveform_dic = self.load_spikes_and_waveforms()
        good_cells = np.ones(len(cell_spike_times_dic))

        # get keys from dictionary and get correct order
        cell_names = []
        for key in cell_spike_times_dic.keys():
            cell_names.append(key[4:])
        cell_names = np.array(cell_names).astype(int)
        cell_names.sort()

        # compute mean waveform in first hour per cell
        first_hour_mean = []
        first_hour_std = []
        ref_hour = 0
        for cell_id, ((_, cell_waveforms), (_, spike_times)) in enumerate(zip(cell_waveform_dic.items(), cell_spike_times_dic.items())):
            if cell_waveforms[np.logical_and(ref_hour * 20e3 * 60 * 60 < spike_times,
                                                     spike_times < (ref_hour + 1) * 20e3 * 60 * 60), :, :].shape[0] > 0:
                # compute mean waveform per hour
                first_hour_mean.append(np.mean(np.mean(cell_waveforms[np.logical_and(ref_hour * 20e3 * 60 * 60 < spike_times,
                                                         spike_times < (ref_hour + 1) * 20e3 * 60 * 60),:,:], axis=2), axis=0))
                first_hour_std.append(np.std(np.mean(cell_waveforms[np.logical_and(ref_hour * 20e3 * 60 * 60 < spike_times,
                                                         spike_times < (ref_hour + 1) * 20e3 * 60 * 60),:,:], axis=2), axis=0))
            else:
                first_hour_mean.append(np.zeros(cell_waveforms.shape[1]))
                first_hour_std.append(np.zeros(cell_waveforms.shape[1]))

        first_hour_mean = np.vstack(first_hour_mean)

        cell_names = []
        for key in cell_waveform_dic.keys():
            cell_names.append(key)

        hour_data = []
        hour_mean_data = []
        nr_waveforms_per_hour_per_cell = []
        # get all waveforms & mean waveform per cell for requested hours
        for hour in hours_to_compute:
            per_hour_all_cells = []
            per_hour_mean_all_cells = []
            this_hour_nr_waveforms_per_cell = np.zeros(len(cell_names))
            for cell_id, ((_, cell_waveforms), (_, spike_times)) in enumerate(zip(cell_waveform_dic.items(),
                                                                                  cell_spike_times_dic.items())):
                # need to check if there are any waveforms of the current cell in the current window
                cell_waveforms_in_window = cell_waveforms[np.logical_and(hour * 20e3 * 60 * 60 < spike_times,
                                                                       spike_times < (hour + 1) * 20e3 * 60 * 60), :,:]
                # check if cell has a mean to compare to
                if np.sum(first_hour_mean[cell_id]) and cell_waveforms_in_window.shape[0]:
                    selected_waveforms = np.mean(cell_waveforms_in_window, axis=2)
                    # check how many waveforms from the current cell in the current time window
                    this_hour_nr_waveforms_per_cell[cell_id] = selected_waveforms.shape[0]
                    per_hour_all_cells.append(selected_waveforms)
                    per_hour_mean_all_cells.append(np.mean(selected_waveforms, axis=0))
                else:
                    this_hour_nr_waveforms_per_cell[cell_id] = np.nan
                    per_hour_mean_all_cells.append(np.nan)
                    per_hour_all_cells.append(np.nan)
                    good_cells[cell_id] = 0
            hour_data.append(per_hour_all_cells)
            hour_mean_data.append(per_hour_mean_all_cells)
            nr_waveforms_per_hour_per_cell.append(this_hour_nr_waveforms_per_cell)

        # delete cells that don't have a mean waveform during first hour (because no spikes) or don't have spikes in one window
        first_hour_mean = first_hour_mean[good_cells.astype(bool), :]
        hour_data_tmp = []
        hour_mean_data_tmp = []
        nr_waveforms_per_hour_per_cell_tmp = []
        for all_hour_dat, all_hour_mean_dat, nr_waveforms in zip(hour_data, hour_mean_data, nr_waveforms_per_hour_per_cell):
            hour_data_tmp.append([all_hour_dat[i] for i in np.argwhere(good_cells).flatten()])
            hour_mean_data_tmp.append([all_hour_mean_dat[i] for i in np.argwhere(good_cells).flatten()])
            nr_waveforms_per_hour_per_cell_tmp.append(nr_waveforms[good_cells.astype(bool)])
        hour_data = hour_data_tmp
        hour_mean_data = hour_mean_data_tmp
        nr_waveforms_per_hour_per_cell = nr_waveforms_per_hour_per_cell_tmp
        # go trough per hour data and compute similarity of mean of first hour with all other waveforms
        per_hour_all_cells_results = []
        for hour_id, per_hour_all_cells in enumerate(hour_data):
            print("Computing waveform similarity for " + str(hours_to_compute[hour_id]) + "th hour")
            per_hour_all_cells_results.append(compute_correlations_col_fast(x=first_hour_mean.T,
                                                                            y=np.vstack(per_hour_all_cells).T))

        # go trough per hour data and compute similarity of mean of first hour with mean waveforms
        per_hour_all_cells_mean_waveform_results = []
        for hour_id, per_hour_mean_waveforms_cells in enumerate(hour_mean_data):
            per_hour_all_cells_mean_waveform_results.append(compute_correlations_col_fast(x=first_hour_mean.T,
                                                                                          y=np.vstack(per_hour_mean_waveforms_cells).T))
        # compute within across using mean waveform per hour
        all_hours_ratio_mean_waveform = []
        all_hours_ratio_mean_waveform_using_all_means = []
        for hour_waveform_data in per_hour_all_cells_mean_waveform_results:
            within_mean_waveform = np.diagonal(hour_waveform_data)
            across_mean_waveform = np.mean(hour_waveform_data[~np.eye(hour_waveform_data.shape[0], dtype=bool)].reshape(
                hour_waveform_data.shape[0], -1).T, axis=0)
            all_hours_ratio_mean_waveform.append(within_mean_waveform / across_mean_waveform)
            # go through columns and drop diagonal elements
            tmp_list = []
            for col_id, col in enumerate(hour_waveform_data.T):
                tmp_list.append(np.delete(col, col_id))
            tmp = np.vstack(tmp_list).T
            sim_with_other_means = np.mean(tmp, axis=0)
            ratio = within_mean_waveform / sim_with_other_means
            all_hours_ratio_mean_waveform_using_all_means.append(ratio)


        all_cells_within_mean = []
        all_cells_across_mean = []
        all_cells_within = []
        all_cells_across = []
        all_cells_within_z_scored = []
        all_cells_across_using_all_means = []

        #mjc169R4R_0114 go through all requested hours
        for current_hour_all_cell_results, current_hour_all_cells_nr_waveforms in zip(per_hour_all_cells_results,
                                                                                      nr_waveforms_per_hour_per_cell):
            per_cell_within_mean = np.zeros(np.count_nonzero(np.sum(first_hour_mean,axis=1)))
            per_cell_across_mean = np.zeros(np.count_nonzero(np.sum(first_hour_mean,axis=1)))
            per_cell_within = []
            per_cell_across = []
            per_cell_within_z_scored = []
            per_cell_across_using_all_means = [[] for i in range(len(hour_data[0]))]
            # compute within vs. across correlations per cell
            for cell_id, cur_hour_cell_res, in enumerate(current_hour_all_cell_results):
                start_id_within_corr = int(np.sum(current_hour_all_cells_nr_waveforms[:cell_id]))
                end_id_within_corr = start_id_within_corr+int(current_hour_all_cells_nr_waveforms[cell_id])
                # check if cell has a mean to compare to
                if np.sum(first_hour_mean[cell_id]):
                    # within correlations
                    per_cell_within_mean[cell_id] = np.mean(cur_hour_cell_res[start_id_within_corr:end_id_within_corr])
                    corr_within = cur_hour_cell_res[start_id_within_corr:end_id_within_corr]
                    per_cell_within.append(corr_within)
                    # across correlations
                    # delete within results after making a copy
                    cur_hour_cell_res_copy = cur_hour_cell_res.copy()
                    cur_hour_cell_res_copy= np.delete(cur_hour_cell_res_copy, np.arange(start_id_within_corr,end_id_within_corr))
                    mean_across = np.mean(cur_hour_cell_res_copy)
                    std_across = np.std(cur_hour_cell_res_copy)
                    per_cell_across_mean[cell_id] = mean_across
                    per_cell_across.append(cur_hour_cell_res_copy)
                    per_cell_within_z_scored.append((corr_within-mean_across)/std_across)
                    # extract correlations of the waveforms of the other cells with the mean of the current cell
                    # from the first hour
                    ind_to_split = np.cumsum(current_hour_all_cells_nr_waveforms)[:-1].astype(int)
                    split_data = np.split(cur_hour_cell_res, ind_to_split)
                    for id, data in enumerate(split_data):
                        per_cell_across_using_all_means[id].append(data)

            all_cells_within_mean.append(np.nan_to_num(per_cell_within_mean))
            all_cells_across_mean.append(np.nan_to_num(per_cell_across_mean))
            all_cells_within.append(per_cell_within)
            all_cells_across.append(per_cell_across)
            all_cells_within_z_scored.append(per_cell_within_z_scored)
            all_cells_across_using_all_means.append(per_cell_across_using_all_means)

        # compute ratio mean within similarity / mean across similarity
        all_cells_ratio_mean = []
        for within, across in zip(all_cells_within_mean, all_cells_across_mean):
            all_cells_ratio_mean.append(within/across)

        # similarity of cell waveforms with its own mean from first hour vs. similarity of cell waveforms
        # with mean from other cells from the first hour
        all_hours_using_all_means = []
        for per_hour_data in all_cells_across_using_all_means:
            # go through all the cells
            per_cell_res = np.zeros(len(hour_data[0]))
            for cell_id, per_hour_per_cell_data in enumerate(per_hour_data):
                within = per_hour_per_cell_data[cell_id]
                tmp_cp = per_hour_per_cell_data.copy()
                del tmp_cp[cell_id]
                # if there is only one cell on this tetrode
                if len(tmp_cp) == 0:
                    continue
                across = np.hstack(tmp_cp)
                per_cell_res[cell_id] = np.mean(within) / np.mean(across)
            all_hours_using_all_means.append(per_cell_res)

        # comparing waveforms with means from first hour from all cells
        all_hours_using_all_means = np.vstack(all_hours_using_all_means)

        all_cells_within_last_hour = all_cells_within[-1]
        all_cells_across_first_hour = all_cells_across[1]
        all_cells_within_last_hour = np.hstack(all_cells_within_last_hour)
        all_cells_across_first_hour = np.hstack(all_cells_across_first_hour)
        all_cells_ratio_mean = np.vstack(all_cells_ratio_mean)
        all_tetrodes_ratios_mean_waveforms = np.vstack(all_hours_ratio_mean_waveform)
        all_hours_ratio_mean_waveform_using_all_means = np.vstack(all_hours_ratio_mean_waveform_using_all_means)

        if plotting:

            plt.figure(figsize=(3, 5))
            if save_fig:
                plt.style.use('default')
                c = "black"
            else:
                c = "white"
            bplot = plt.boxplot(all_hours_ratio_mean_waveform_using_all_means[1:, :].T, positions=[1, 2, 3], patch_artist=True,
                                labels=labels,
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            plt.ylabel("similarity with own mean / similarity with other mean")
            plt.title("Using mean waveforms")
            plt.ylim(0.0, 1.5)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "waveform_stab_sim_own_other_means_using_means_example.svg"),
                            transparent="True")
                plt.close()
            else:
                plt.show()

            print("STATS OWN MEAN / OTHER MEANS USING MEAN WAVEFORM:")

            print(ttest_ind(all_hours_ratio_mean_waveform_using_all_means[1], all_hours_ratio_mean_waveform_using_all_means[2], alternative="greater"))
            print(ttest_ind(all_hours_ratio_mean_waveform_using_all_means[2], all_hours_ratio_mean_waveform_using_all_means[3], alternative="greater"))
            print(ttest_ind(all_hours_ratio_mean_waveform_using_all_means[1], all_hours_ratio_mean_waveform_using_all_means[3], alternative="greater"))
            print(mannwhitneyu(all_hours_ratio_mean_waveform_using_all_means[1], all_hours_ratio_mean_waveform_using_all_means[2], alternative="greater"))
            print(mannwhitneyu(all_hours_ratio_mean_waveform_using_all_means[2], all_hours_ratio_mean_waveform_using_all_means[3], alternative="greater"))
            print(mannwhitneyu(all_hours_ratio_mean_waveform_using_all_means[1], all_hours_ratio_mean_waveform_using_all_means[3], alternative="greater"))


            plt.figure(figsize=(3, 5))
            if save_fig:
                plt.style.use('default')
                c = "black"
            else:
                c = "white"
            bplot = plt.boxplot(all_tetrodes_ratios_mean_waveforms[1:, :].T, positions=[1, 2, 3], patch_artist=True,
                                labels=labels,
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            plt.ylabel("mean within similarity / mean across similarity")
            plt.title("Using mean waveforms")
            plt.ylim(0.0, 1.5)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "waveform_stab_within_across_example.svg"), transparent="True")
                plt.close()
            else:
                plt.show()

            print("STATS WITHIN / ACROSS USING MEAN WAVEFORM:")

            print(ttest_ind(all_tetrodes_ratios_mean_waveforms[1], all_tetrodes_ratios_mean_waveforms[2], alternative="greater"))
            print(ttest_ind(all_tetrodes_ratios_mean_waveforms[2], all_tetrodes_ratios_mean_waveforms[3], alternative="greater"))
            print(ttest_ind(all_tetrodes_ratios_mean_waveforms[1], all_tetrodes_ratios_mean_waveforms[3], alternative="greater"))
            print(mannwhitneyu(all_tetrodes_ratios_mean_waveforms[1], all_tetrodes_ratios_mean_waveforms[2], alternative="greater"))
            print(mannwhitneyu(all_tetrodes_ratios_mean_waveforms[2], all_tetrodes_ratios_mean_waveforms[3], alternative="greater"))
            print(mannwhitneyu(all_tetrodes_ratios_mean_waveforms[1], all_tetrodes_ratios_mean_waveforms[3], alternative="greater"))


            plt.figure(figsize=(3, 5))
            if save_fig:
                plt.style.use('default')
                c = "black"
            else:
                c = "white"
            bplot = plt.boxplot(all_cells_ratio_mean[1:, :].T, positions=[1, 2, 3], patch_artist=True,
                                labels=labels,
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            plt.ylabel("mean within similarity / mean across similarity")
            plt.ylim(0.0, 1.5)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "waveform_stab_within_across_example.svg"), transparent="True")
                plt.close()
            else:
                plt.show()

            print("STATS WITHIN / ACROSS")

            print(ttest_ind(all_cells_ratio_mean[1], all_cells_ratio_mean[2], alternative="greater"))
            print(ttest_ind(all_cells_ratio_mean[2], all_cells_ratio_mean[3], alternative="greater"))
            print(ttest_ind(all_cells_ratio_mean[1], all_cells_ratio_mean[3], alternative="greater"))
            print(mannwhitneyu(all_cells_ratio_mean[1], all_cells_ratio_mean[2], alternative="greater"))
            print(mannwhitneyu(all_cells_ratio_mean[2], all_cells_ratio_mean[3], alternative="greater"))
            print(mannwhitneyu(all_cells_ratio_mean[1], all_cells_ratio_mean[3], alternative="greater"))

            plt.figure(figsize=(3, 5))
            if save_fig:
                plt.style.use('default')
                c = "black"
            else:
                c = "white"
            bplot = plt.boxplot(all_hours_using_all_means[1:, :].T, positions=[1, 2, 3], patch_artist=True,
                                labels=labels,
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            plt.ylabel("similarity with own mean/\n similarity with means from same tetrode")
            plt.ylim(0.0, 1.50)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig(os.path.join(save_path, "waveform_stab_example.svg"), transparent="True")
                plt.close()
            else:
                plt.show()

            print("STATS WITH OWN/OTHER MEAN:")

            print(ttest_ind(all_hours_using_all_means[1], all_hours_using_all_means[2], alternative="greater"))
            print(ttest_ind(all_hours_using_all_means[2], all_hours_using_all_means[3], alternative="greater"))
            print(ttest_ind(all_hours_using_all_means[1], all_hours_using_all_means[3], alternative="greater"))
            print(mannwhitneyu(all_hours_using_all_means[1], all_hours_using_all_means[2], alternative="greater"))
            print(mannwhitneyu(all_hours_using_all_means[2], all_hours_using_all_means[3], alternative="greater"))
            print(mannwhitneyu(all_hours_using_all_means[1], all_hours_using_all_means[3], alternative="greater"))

            # p_within = 1. * np.arange(all_cells_within_last_hour.shape[0]) / (all_cells_within_last_hour.shape[0] - 1)
            # p_across = 1. * np.arange(all_cells_across_first_hour.shape[0]) / (all_cells_across_first_hour.shape[0] - 1)
            #
            # plt.plot(np.sort(all_cells_within_last_hour), p_within, label="Within", color="magenta")
            # plt.plot(np.sort(all_cells_across_first_hour), p_across, label="Across", color="darkmagenta")
            # plt.legend()
            # plt.ylabel("CDF")
            # plt.xlabel("PEARSON R")
            # plt.title("Within similarity: waveforms of last hour with mean \n Across similarity: waveforms of first hour from other cells")
            # plt.show()

    # </editor-fold>

    # <editor-fold desc="LFP analysis">

    def gamma_phase_preference_analysis(self):

        cell_spike_dic = self.load_spikes()
        print("HERE")

    # </editor-fold>